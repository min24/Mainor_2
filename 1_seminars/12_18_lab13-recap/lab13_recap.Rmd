---
title: "Lab 13. Recap"
output: html_document
editor_options: 
  chunk_output_type: console
---

* задачи обучения с учителем (supervised learning): классификация и регрессия
* независимые переменные (предикторы, X), зависимая переменная (outcome, Y)
* оценка моделей классификации: accuracy (точность), таблица ошибок, false positives, false negatives, true positives, true negatives, gini impurity
* оценка моделей регрессии: residual sum of squares (RSS), total sum of squares (TSS), $R^2$
* деревья: стратегия (supervised segmentation, рекурсивное разбиение), предсказание (y среднее для регрессии, наиболее частый класс для классификации), выбор разбиения (максимальное падение impurity, максимальное падение rss), сложность модели (количество узлов, complexity parameter)
* переобучение (overfitting)
* оценка на тестовой выборке, оценка на обучающей выборке


Сегодня мы вспомним то, что рассматривали в течение семестра.
Потренируемся на датасете, содержащем данные о клиентах банка. Задача -- исследовать факторы, позволяющие оценить, сделает ли клиент срочный вклад.
Информация об используемых переменных находится в файле bank-names.txt.

Загрузим датасет.

```{r}
library(dplyr)
library(readr)
library(rpart)
library(rpart.plot)

bank = read_csv2("~/shared/minor2_2018/1-intro/lab13-recap/bank.csv")

str(bank, max.level=1)
```

Посмотрим на наши данные
```{r}
table(bank$y)
```

Попытаемся предсказать решение клиента о срочном вкладе -- нужно же минимизировать свои затраты и не звонить тем, кто заведомо не согласится

```{r}
# y - has the client subscribed a term deposit? (binary: "yes","no")

bankTree = rpart(y~ ., data = bank)
prp(bankTree)
```

* Какие правила у нас получились?

Разделим на тренировочную и тестовую выборки

```{r}
set.seed(57)
# зададим идентификаторы наблюдений
bank$num <- 1:nrow(bank)
# "зерно" для генератора случайных чисел
set.seed(1234) 

bank.train = bank %>% dplyr::sample_frac(.8)
bank.test = dplyr::anti_join(bank, bank.train, by = 'num') %>% select(-num)

bank.train = bank.train %>% select(-num)

```

Построим дерево! 
```{r}
tree.bank <- rpart(y ~ ., data = bank.train, method = "class")

#нарисуем
prp(tree.bank)
```

Помните как выбираются сплиты/разделения в дереве? В rpart реализуется алгоритм расчета коэфициента Джинни на каждом шаге (Gini impurity -- not Gini coefficient). 

Этот показатель оценивает насколько перемешанны классы, которые мы предсказываем, в тех сегментах, на которые мы их разделили. При идеальном разделении Gini impurity будет равен 0. В случае, если в сегментах предсказываемые классы оказываются представленны поровну, тогда Gini impurity = 0.5.

Как понять в целом, что наша модель хороша? 
```{r}
#посмотрим предсказания для новых данных -- не забываем указывать тип предсказывания
bank.pred <- predict(tree.bank, bank.test, type = "class")
bank.pred

#матрица смежности
table(bank.pred, bank.test$y) 
conf = table(bank.pred, bank.test$y) 
sum(diag(conf))/sum(conf)
```

Что такое матрица ошибок (сconfusion matrix)? 

![](https://ncss-wpengine.netdna-ssl.com/wp-content/uploads/2016/06/ROC-Curves-Classification-Table-1.png)


На основе получившейся матрицы смежности по банковским данным ответьте на следующие вопросы:

1. Какова точность модели?
2. Сколько всего людей сделало срочные вклады?
3. Какая доля сделавших вклады наша модель определила правильно?
4. Какая доля клиентов, классифицированных как сделавшие вклады, на самом деле, относится к этой группе?

Сейчас мы предсказывали категориальную переменную (перезвонит нам клиент или нет). 
А что, если мы хотим предсказывать количественную переменную? 

Вспомним немного регрессионные деревья

```{r}
tree.bank <- rpart(age ~ balance + job + marital + education + default + housing + loan,
                   data = bank.train)
prp(tree.bank)
```

Помните, как мы выбираем разделение для регрессионного дерева? Hint: not confusion table

Для них, при разделении на сегменты мы пытаемся минимизировать сумму квадратов отклонений (residual sum of squared error (RSS)) для каждого получившегося сегмента на обучающей выборке.

RSS -- это сумма квадратов разницы между наблюдаемым значением (observed) и предсказываемым (predicted) 
`RSS = sum((Observeds - Predicteds)^2)`

Перед самым первым разделением, когда у нас еще нет никаких сегментов -- мы сравниваем наблюдаемые значения со средним по всей выборке. Назовем это RSS_root.

После разбиения на две группы, мы стремимся максимизировать формулу:
RSS_root − (RSS(L) + RSS(R))

В лучшем случае у нас произошло разбиение на две группы, в которых мы идеально предсказываем значение Y, тогда RSS_root - (0 + 0) = RSS_root

В плохом случае, у нас прогноз в одном из дочерних узлов не улучшился, а другая, например, оказалось "пустой", в таком случае RSS - (0 + RSS(R)) = 0



Сплиты выбраны. Как будем теперь считать точность нашей итоговой модели?  
Посчитаем, насколько наше предсказание отличается от реального значения

![](https://www.machinelearningplus.com/wp-content/uploads/2017/03/R_Squared_Computation.png)

```{r}
age.pred = predict(tree.bank, bank.test)
# RSS - Residual Sum of Squares
rss = sum((age.pred - bank.test$age)^2)
# TSS - Total Sum of Squares
tss = sum((mean(bank.test$age) - bank.test$age)^2) 
# R^2 - Доля объясненной дисперсии
1 - (rss / tss)
```

---
Можем ли мы еще вырастить дерево (сделать его более точным и сложным)? Или дерево оказалось переобученным?

```{r}
plotcp(tree.bank)
printcp(tree.bank)
```

Если по получившейся таблице (колонка xerror) или графику (Ось У) ошибка продолжает падать, тогда модель можно усложнить.

Если по таблице (колонка xerror) или графику (ось У) ошибка начинает расти, тогда ищем значение *cp*, при котором значение ошибки минимально и обрезаем дерево.

"Вырастим" большое дерево! Уменьшаем параметр *cp*. Он по-умолчанию равен 0.01.

```{r}
tree.bank2 = rpart(age ~ balance + job + marital + education + default + housing + loan,
                   data = bank.train, control = rpart.control(cp = 10^-4))

# Смотрим на ошибку 
# prp(tree.bank2)
plotcp(tree.bank2)

# делаем предсказание
age.pred2 = predict(tree.bank2, bank.test)

# считаем R^2
rss = sum((age.pred2 - bank.test$age)^2)
tss = sum((mean(bank.test$age) - bank.test$age)^2)
1 - (rss / tss)
```

А теперь найдем место, в котором необходимо обрезать это дерево. И посмотрим будет ли оно предсказывать лучше.

Сохраняем таблицу с ошибками в переменную, затем ищем строчку с минимальным значением xerror.

```{r}
cp_tbl = printcp(tree.bank2)

cp_tbl %>%
  as.data.frame() %>% 
  arrange(xerror) %>% 
  head(n = 1)
```

Обрезаем дерево через prune()

```{r}
tree.bank3 = prune(tree.bank2, cp = 0.00198139)
```

