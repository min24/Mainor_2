Id,OwnerUserId,CreationDate,ParentId,Score,IsAcceptedAnswer,Body
79741,3259,2008-09-17T03:43:22Z,79709,-1,FALSE,"<p>It's tough to say definitively without knowing the language/compiler used.  However, if you can simply pass a pointer/reference to the object that you're creating, then the size of the object itself has nothing to do with the speed of the function calls.  Manipulating this data down the road could be a different story.</p>
"
79768,6043,2008-09-17T03:48:29Z,79709,9,FALSE,"<p>use variables in the outer function instead of global variables. This gets you the best of both approaches: you're not mutating global state, and you're not copying a big wad of data. If you have to exit early, just return the partial results.</p>

<p>(See the ""Scope"" section in the R manual: <a href=""http://cran.r-project.org/doc/manuals/R-intro.html#Scope"" rel=""noreferrer"">http://cran.r-project.org/doc/manuals/R-intro.html#Scope</a>)</p>
"
79779,8002,2008-09-17T03:49:36Z,79709,0,FALSE,"<p>Third approach: inner function returns a reference to the large array, which the next statement inside the loop then dereferences and stores wherever it's needed (ideally with a single pointer store and not by having to memcopy the entire array).</p>

<p>This gets rid of both the side effect and the passing of large datastructures.</p>
"
79788,NA,2008-09-17T03:51:30Z,79709,4,FALSE,"<p>It's not going to make much difference to memory use, so you might as well make the code clean.</p>

<p>Since R has copy-on-modify for variables, modifying the global object will have the same memory implications as passing something up in return values.</p>

<p>If you store the outputs in a database (or even in a file) you won't have the memory use issues, and the data will be incrementally available as it is created, rather than just at the end.  Whether it's faster with the database depends primarily on how much memory you are using: is the reduction is garbage collection going to pay for the cost of writing to disk.</p>

<p>There are both time and memory profilers in R, so you can see empirically what the impacts are.</p>
"
79827,14257,2008-09-17T03:58:26Z,79709,1,FALSE,"<p>I'm not sure I understand the question, but I have a couple of solutions.</p>

<ol>
<li><p>Inside the function, create a list of the vectors and return that.</p></li>
<li><p>Inside the function, create an environment and store all the vectors inside of that.  Just make sure that you return the environment in case of errors.</p></li>
</ol>

<p>in R:</p>

<pre><code>help(environment)

# You might do something like this:

outer &lt;- function(datasets) {
  # create the return environment
  ret.env &lt;- new.env()
  for(set in dataset) {
    tmp &lt;- inner(set)
    # check for errors however you like here.  You might have inner return a list, and
    # have the list contain an error component
    assign(set, tmp, envir=ret.env)
  }
  return(ret.env)
}

#The inner function might be defined like this

inner &lt;- function(dataset) {
  # I don't know what you are doing here, but lets pretend you are reading a data file
  # that is named by dataset
  filedata &lt;- read.table(dataset, header=T)
  return(filedata)
}
</code></pre>

<p>leif</p>
"
79893,14928,2008-09-17T04:11:08Z,79709,6,FALSE,"<p>Remember your Knuth.  ""Premature optimization is the root of all programming evil.""</p>

<p>Try the side effect free version.  See if it meets your performance goals.  If it does, great, you don't have a problem in the first place; if it doesn't, then use the side effects, and make a note for the next programmer that your hand was forced.</p>
"
83162,15842,2008-09-17T13:27:17Z,77434,70,FALSE,"<p>If you're looking for something as nice as Python's x[-1] notation, I think you're out of luck.  The standard idiom is</p>

<pre><code>x[length(x)]  
</code></pre>

<p>but it's easy enough to write a function to do this:</p>

<pre><code>last &lt;- function(x) { return( x[length(x)] ) }
</code></pre>

<p>This missing feature in R annoys me too!</p>
"
83222,1428,2008-09-17T13:32:45Z,77434,236,FALSE,"<p>I use the <code>tail()</code> function:</p>

<pre><code>tail(vector, n=1)
</code></pre>

<p>The nice thing with <code>tail()</code> is that it works on dataframes too, unlike the <code>x[length(x)]</code> idiom.</p>
"
86804,NA,2008-09-17T19:39:37Z,79709,1,FALSE,"<p>FYI, here's a full sample toy solution that avoids side effects:</p>

<pre><code>outerfunc &lt;- function(names) {
  templist &lt;- list()
  for (aname in names) {
    templist[[aname]] &lt;- innerfunc(aname)
  }
  templist
}

innerfunc &lt;- function(aname) {
  retval &lt;- NULL
  if (""one"" %in% aname) retval &lt;- c(1)
  if (""two"" %in% aname) retval &lt;- c(1,2)
  if (""three"" %in% aname) retval &lt;- c(1,2,3)
  retval
}

names &lt;- c(""one"",""two"",""three"")

name_vals &lt;- outerfunc(names)

for (name in names) assign(name, name_vals[[name]])
</code></pre>
"
95598,1179,2008-09-18T18:49:09Z,95007,5,FALSE,"<p>There are various ways of computing quantiles when you give it a vector, and don't have a known CDF. </p>

<p>Consider the question of what to do when your observations don't fall on quantiles exactly. </p>

<p>The ""types"" are just determining how to do that. So, the methods say, ""use a linear interpolation between the k-th order statistic and p(k)"".</p>

<p>So, what's p(k)? One guy says, ""well, I like to use k/n"". Another guy says, ""I like to use (k-1)/(n-1)"" etc.  Each of these methods have different properties that are better suited for one problem or another. </p>

<p>The \alpha's and \beta's are just ways to parameterize the functions p. In one case, they're 1 and 1. In another case, they're 3/8 and -1/4. I don't think the p's are ever a constant in the documentation. They just don't always show the dependency explicitly. </p>

<p>See what happens with the different types when you put in vectors like 1:5 and 1:6. </p>

<p>(also note that even if your observations fall exactly on the quantiles, certain types will still use linear interpolation). </p>
"
103396,4482,2008-09-19T16:20:30Z,103312,6,FALSE,"<p>The <a href=""http://wiki.r-project.org/rwiki/doku.php?id=rdoc:base:readlines"" rel=""noreferrer"">readLines</a> function will return a zero-length value when it reaches the EOF.</p>
"
153852,23813,2008-09-30T16:28:14Z,77434,42,FALSE,"<p>Combining lindelof's and Gregg Lind's ideas:</p>

<pre><code>last &lt;- function(x) { tail(x, n = 1) }
</code></pre>

<p>Working at the prompt, I usually omit the ""<code>n=</code>"", i.e. <code>tail(x, 1)</code>.</p>

<p>Unlike <code>last</code> from  the <code>pastecs</code> package, <code>head</code> and <code>tail</code> (from <code>utils</code>) work not only on vectors but also on data frames etc., and also can return data ""without first/last n elements"", e.g. </p>

<pre><code>but.last &lt;- function(x) { head(x, n = -1) }
</code></pre>

<p>(Note that you have to use <code>head</code> for this, instead of <code>tail</code>.)</p>
"
255992,23263,2008-11-01T19:29:54Z,255697,2,TRUE,"<p>I've only come across both R and the Dirichlet distribution in passing, so I hope I'm not too much off the mark.</p>

<p><a href=""https://stat.ethz.ch/pipermail/r-help/2006-September/113258.html"" rel=""nofollow noreferrer"">This mailing list message</a> seems to answer your question:</p>

<blockquote>
  <p>Scrolling through the results of
  RSiteSearch(""dirichlet"") suggests some useful tools
  in the VGAM package.  The gtools package and
  MCMC packages also have ddirichlet() functions
  that you could use to construct a (negative log) likelihood
  function and optimize with optim/nlmin/etc.</p>
</blockquote>

<p>The deal, DPpackage and mix packages also may or may not provide what you need.</p>

<p>Then again, these are all still CRAN packages, so I'm not sure if you already found these and found them unsuitable.</p>

<p>As for searching for R, <a href=""http://www.r-project.org/"" rel=""nofollow noreferrer"">the R project site</a> itself already provides a few links on <a href=""http://www.r-project.org/search.html"" rel=""nofollow noreferrer"">its search page</a>.</p>
"
359458,3201,2008-12-11T14:06:56Z,359438,2,FALSE,"<p>I have used <a href=""http://cran.r-project.org/web/packages/linprog/index.html"" rel=""nofollow noreferrer"">linprog</a> for linear problems in the past.</p>
"
435942,37751,2009-01-12T16:20:54Z,359438,5,FALSE,"<p>Linprog, mentioned by Galwegian, focuses on linear programming via the simplex algorithm. In addition you may be interested in <a href=""http://cran.r-project.org/web/packages/fPortfolio/"" rel=""noreferrer"">fPortfolio</a> if you are doing portfolio optimization. </p>
"
440066,37751,2009-01-13T18:00:54Z,439526,9,TRUE,"<p>Clearly I should have worked on this for another hour before I posted my question. It's so obvious in retrospect. :)</p>

<p>To use R's vector logic I took out the loop and replaced it with this:</p>

<pre><code>st &lt;-   sample(c(12,17,24),10000,prob=c(20,30,50),replace=TRUE)
p1 &lt;-   sample(c(12,17,24),10000,prob=c(20,30,50),replace=TRUE)
p2 &lt;-   sample(c(12,17,24),10000,prob=c(20,30,50),replace=TRUE)
year &lt;- rep(1991:2000,1000)
</code></pre>

<p>I can now do 100,000 samples almost instantaneous. I knew that vectors were faster, but dang. I presume 100,000 loops would have taken over an hour using a loop and the vector approach takes &lt;1 second. Just for kicks I made the vectors a million. It took ~2 seconds to complete. Since I must test to failure, I tried 10mm but ran out of memory on my 2GB laptop. I switched over to my Vista 64 desktop with 6GB ram and created vectors of length 10mm in 17 seconds. 100mm made things fall apart as one of the vectors was over 763mb which resulted in an allocation issue with R. </p>

<p>Vectors in R are amazingly fast to me. I guess that's why I am an economist and not a computer scientist. </p>
"
441029,1447,2009-01-13T22:09:25Z,439526,6,FALSE,"<p>To answer your question about why the loop of 10000 took much longer than your loop of 1000:</p>

<p>I think the primary suspect is the concatenations that are happening every loop.  As the data gets longer R is probably copying every element of the vector into a new vector that is one longer.  Copying a small (500 elements on average) data set 1000 times is fast.  Copying a larger (5000 elements on average) data set 10000 times is slower.</p>
"
451800,54904,2009-01-16T20:12:55Z,445059,0,FALSE,"<p>This is no answer, but I wonder if any insight lies in this direction:</p>

<pre><code>&gt; tapply((my.data$item[my.data$fixed==0])[-1], my.data$year[my.data$fixed==0][-1], sum)
</code></pre>

<p>tapply produces a table of statistics (sums, in this case; the third argument) grouped by the parameter given as the second argument. For example</p>

<pre><code>2001 2003 2005 2007
1    3    5    7
</code></pre>

<p>The [-1] notation drops observation (row) one from the selected rows. So, you could loop and use [-i] on each loop</p>

<pre><code>for (i in 1:length(my.data$item)) {
  tapply((my.data$item[my.data$fixed==0])[-i], my.data$year[my.data$fixed==0][-i], sum)
}
</code></pre>

<p>keeping in mind that if you have any years with only 1 observation, then the tables returned by the successive tapply calls won't have the same number of columns. (i.e., if you drop out the only observation for 2001, then 2003, 2005, and 2007 would be te only columns returned).</p>
"
455286,54904,2009-01-18T15:12:24Z,445059,7,TRUE,"<p>Here's what seems like another very R-type way to generate the sums. Generate a vector that is as long as your input vector, containing nothing but the repeated sum of n elements. Then, subtract your original vector from the sums vector. The result: a vector (isums) where each entry is your original vector less the ith element.</p>

<pre><code>&gt; (my.data$item[my.data$fixed==0])
[1] 1 1 3 5 7
&gt; sums &lt;- rep(sum(my.data$item[my.data$fixed==0]),length(my.data$item[my.data$fixed==0]))
&gt; sums
[1] 17 17 17 17 17
&gt; isums &lt;- sums - (my.data$item[my.data$fixed==0])
&gt; isums
[1] 16 16 14 12 10
</code></pre>
"
467131,57626,2009-01-21T21:38:10Z,467110,12,TRUE,"<p>In most cases R is an interpreted language that runs in a read-evaluate-print loop.  There are numerous extensions to R that are written in other languages like C and Fortran where speed or interfacing with native libraries is helpful. </p>
"
467561,25188,2009-01-21T23:54:03Z,467110,6,FALSE,"<p>I've often rewritten R code in C++ and made it run 100x faster.  Looping is especially inefficient in R.  </p>
"
476739,58681,2009-01-24T22:07:32Z,476726,7,TRUE,"<p>If <code>x</code> is your <code>data.frame</code> (or <code>matrix</code>) then</p>

<pre><code>x[ ,apply(x, 2, function(z) !any(is.na(z)))]
</code></pre>

<p>Since your example uses <code>NULL</code>, <code>is.na(·)</code> will be replaced by <code>is.null(·)</code></p>

<p>Alternatively you can look at <code>subset(·)</code>.</p>
"
476928,NA,2009-01-25T00:05:19Z,476726,19,FALSE,"<p>You can drop any row containing a missing using na.omit(), however that's not what you want.  Moreover, the currently accepted answer is wrong.  It gives you complete columns, but does not drop the rows that have one or more missing values, which is what was asked for.  The correct answer can be obtained as:</p>

<pre><code>&gt; a &lt;- data.frame(a=c(1,2),b=c(NA,1), c=c(3,4))
&gt; a
  a  b c
1 1 NA 3
2 2  1 4
&gt; na.omit(a)[,colSums(is.na(a))==0]
  a c
2 2 4
</code></pre>

<p>To see that the above answer is wrong:</p>

<pre><code>&gt; a[ ,apply(a, 2, function(z) !any(is.na(z)))]
  a c
1 1 3
2 2 4
</code></pre>

<p>Row 1 should be dropped because of the NA in column 2.</p>
"
495862,28006,2009-01-30T15:16:50Z,495744,3,FALSE,"<p>CRAN shows a package that is actively updated called ""chron"" that handles dates. You might want to check that and some of the other modules found here: <a href=""http://cran.r-project.org/web/views/TimeSeries.html"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/views/TimeSeries.html</a></p>

<p>xts and zoo handle irregular time series data on top of that. I'm not familiar with these packages, but a quick look over indicates you should be able to use them fairly easily by splitting on the hyphen and loading into the structures they provide.</p>
"
498951,20417,2009-01-31T15:03:43Z,498932,1,FALSE,"<p>Have a look at the project file. I believe is text based. You might be able to insert the file paths directly there with some copy-paste-replace.</p>
"
511763,445,2009-02-04T15:12:59Z,498932,1,TRUE,"<p>I couldn't work out the project file so what I did in the end was to zip up all the files I wanted to deploy, add the zip file to the application and create a custom Installer class to unzip them (using CSharp ziplib)</p>
"
552764,41338,2009-02-16T09:43:18Z,551113,2,FALSE,"<p>According to <a href=""http://astor.som.jhmi.edu/BayesMendel/brcapro.html"" rel=""nofollow noreferrer"">this link</a>:</p>

<blockquote>
  <p>The BRCAPRO model is now included in the R package  BayesMendel for carrier probabiity [sic] prediction. </p>
</blockquote>

<p>So it looks like you use it from R.</p>

<p>And the <a href=""http://astor.som.jhmi.edu/BayesMendel/BayesMendel.pdf"" rel=""nofollow noreferrer"">documentation for the BayesMendel</a> package at least might get you started. Though it looks like you're going to have to learn R. :)</p>
"
571930,37751,2009-02-21T01:54:56Z,560329,5,TRUE,"<p>I really want to help answer your question, but I gotta tell you, I can't make heads or tails of your data. I see a lot of opening parenthesis but no closing ones. The data looks sorted descending by whatever the values are on the bottom of each row. I have no idea what to make out of a value like ""(8.048,18.05]""</p>

<p>Am I missing something obvious? Can you make a more simple example where your data structure is not a factor?</p>

<p>I would generally expect a data frame or a matrix with two columns, one for the X and one for the Y. </p>

<p>See if this example of sorting helps (I'm sort of shooting in the dark here)</p>

<pre><code>tN &lt;- table(Ni &lt;- rpois(100, lambda=5))
r &lt;- barplot(tN)

#stop here and examine the plot
#the next bit converts the matrix to a data frame,
#  sorts it, and plots it again

df&lt;-data.frame(tN)
df2&lt;-df[order(df$Freq),]
barplot(df2$Freq)
</code></pre>
"
584545,60020,2009-02-25T02:51:45Z,582653,1,FALSE,"<p>one option is to use regular expressions. if you are not familiar with them, they are used to parse strings using patterns. i would research regular expressions and then <a href=""http://www.regular-expressions.info/rlanguage.html"" rel=""nofollow noreferrer"">here are the functions in r</a></p>

<p>hope it helps</p>
"
585795,19410,2009-02-25T12:31:12Z,551113,1,FALSE,"<p>I don't think anyone will be able to fit code for a full application into a tiny window, but I'll give you some thoughts, based on how I might approach this:</p>

<ol>
<li><a href=""http://www.r-project.org/"" rel=""nofollow noreferrer"">Install <code>R</code></a></li>
<li><a href=""http://astor.som.jhmi.edu/BayesMendel/Rpackage.html"" rel=""nofollow noreferrer"">Install <code>BayesMendel</code> package</a> -- this includes <code>BRCAPRO</code> routines</li>
<li><a href=""http://rpy.sourceforge.net/"" rel=""nofollow noreferrer"">Install <code>RPy</code></a> -- a Python-to-R bridge</li>
<li>Write <a href=""http://rpy.sourceforge.net/rpy_demo.html"" rel=""nofollow noreferrer""><code>RPy</code>-based <code>Python</code> code</a> for bringing your data into <code>R</code>, turning it into <a href=""http://fs6.depauw.edu:50080/~harvey/Chem%20351/PDF%20Files/Handouts/RDocs/Using%20Data%20Frames%20and%20Indexing%20in%20R.pdf"" rel=""nofollow noreferrer"">a data frame</a>, and <a href=""http://astor.som.jhmi.edu/BayesMendel/BayesMendel-manual.pdf"" rel=""nofollow noreferrer"">analyzing the data frame with the <code>BRCAPRO</code> component of <code>BayesMendel</code></a></li>
<li>Bridging the analytical output of <code>BayesMendel</code> to a <a href=""http://pytut.infogami.com/node11-baseline.html"" rel=""nofollow noreferrer""><code>brcaResults</code> class that you write</a></li>
<li><a href=""http://www.python.org/doc/faq/gui/"" rel=""nofollow noreferrer"">Wrap accessors to your <code>brcaResults</code> class in a GUI</a>, using any one of many Python GUI frameworks</li>
</ol>

<p>That's an overview of one way to do this. </p>

<p>The nice thing about this approach is that this should be easy to glue together and keep up-to-date with new <code>BRCAPRO</code> features.</p>
"
587944,4892,2009-02-25T21:28:11Z,582653,15,TRUE,"<p>You need to use <code>strptime()</code> to convert the string to a date. For example:</p>

<pre><code>strptime(""9:24 am"",format=""%I:%M %p"")
</code></pre>

<p>Then you can take differences just by taking one away from the other:</p>

<pre><code>strptime(""9:24 am"",format=""%I:%M %p"")-strptime(""12:14 am"",format=""%I:%M %p"")
Time difference of 9.166667 hours
</code></pre>

<p>You can store this and then do an <code>as.numeric()</code> if you just want the number out, otherwise you can pass around the time objects.</p>

<p>Hope this helps!</p>
"
597001,63225,2009-02-27T22:56:38Z,596976,3,TRUE,"<p>Most common application: it enables an anonymous function to call itself recursively.</p>

<p>An excellent explanation can be found on <a href=""http://mvanier.livejournal.com/2897.html"" rel=""nofollow noreferrer"">Mike Vanier's livejournal page</a></p>
"
612518,37751,2009-03-04T21:28:26Z,596819,10,TRUE,"<p>You are, indeed, passing the object around and using some memory. But I don't think you can do an operation on an object in R without passing the object around. Even if you didn't create a function and did your operations outside of the function, R would behave basically the same. </p>

<p>The best way to see this is to set up an example. If you are in Windows open Windows Task Manager. If you are in Linux open a terminal window and run the top command. I'm going to assume Windows in this example. In R run the following:</p>

<pre><code>col1&lt;-rnorm(1000000,0,1)
col2&lt;-rnorm(1000000,1,2)
myframe&lt;-data.frame(col1,col2)

rm(col1)
rm(col2)
gc()
</code></pre>

<p>this creates a couple of vectors called col1 and col2 then combines them into a data frame called myframe. It then drops the vectors and forces garbage collection to run. Watch in your windows task manager at the mem usage for the Rgui.exe task. When I start R it uses about 19 meg of mem. After I run the above commands my machine is using just under 35 meg for R. </p>

<p>Now try this:</p>

<pre><code>myframe&lt;-myframe+1
</code></pre>

<p>your memory usage for R should jump to over 144 meg. If you force garbage collection using gc() you will see it drop back to around 35 meg. To try this using a function, you can do the following:</p>

<pre><code>doSomething &lt;- function(df) {
    df&lt;-df+1-1
return(df)
}
myframe&lt;-doSomething(myframe)
</code></pre>

<p>when you run the code above, memory usage will jump up to 160 meg or so. Running gc() will drop it back to 35 meg. </p>

<p>So what to make of all this? Well, doing an operation outside of a function is not that much more efficient (in terms of memory) than doing it in a function. Garbage collection cleans things up real nice. Should you force gc() to run? Probably not as it will run automatically as needed, I just ran it above to show how it impacts memory usage. </p>

<p>I hope that helps!</p>
"
612573,NA,2009-03-04T21:41:41Z,596819,8,FALSE,"<p>I'm no R expert, but most languages use a reference counting scheme for big objects. A copy of the object data will not be made until you modify the copy of the object. If your functions only read the data (i.e. for analysis) then no copy should be made.</p>
"
652180,41665,2009-03-16T21:16:57Z,652136,145,TRUE,"<p>I don't know R at all, but a bit of creative googling led me here: <a href=""http://tolstoy.newcastle.edu.au/R/help/05/04/1919.html"" rel=""noreferrer""><a href=""http://tolstoy.newcastle.edu.au/R/help/05/04/1919.html"" rel=""noreferrer"">http://tolstoy.newcastle.edu.au/R/help/05/04/1919.html</a></a></p>

<p>The key quote from there:</p>

<blockquote>
  <p>I do not find explicit documentation for R on how to remove elements from lists, but trial and error tells me</p>
  
  <p>myList[[5]] &lt;- NULL</p>
  
  <p>will remove the 5th element and then ""close up"" the hole caused by deletion of that element. That suffles the index values, So I have to be careful in dropping elements. I must work from the back of the list to the front.</p>
</blockquote>

<p>A <a href=""http://tolstoy.newcastle.edu.au/R/help/05/04/1917.html"" rel=""noreferrer"">response to that post later in the thread</a> states:</p>

<blockquote>
  <p>For deleting an element of a list, see R FAQ 7.1</p>
</blockquote>

<p>And the <a href=""http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-set-components-of-a-list-to-NULL_003f"" rel=""noreferrer"">relevant section of the R FAQ</a> says:</p>

<blockquote>
  <p>... Do not set x[i] or x[[i]] to NULL, because this will remove the corresponding component from the list. </p>
</blockquote>

<p>Which seems to tell you (in a somewhat backwards way) how to remove an element.</p>

<p>Hope that helps, or at least leads you in the right direction.</p>
"
658574,37751,2009-03-18T14:44:54Z,657440,7,TRUE,"<p>I think the function you are looking for is <code>cumsum()</code> which will do a cumulative sum on a vector. </p>

<pre><code>#put your data into 3 vectors
x&lt;-c(-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10)
dat1&lt;-c(0.0140149,0.00890835,0.00672276,0.00876399,0.00806879,0.0088366,0.00856872,0.0195384,0.0160239,0.0254455,0.0397413,0.0743316,0.0247501,0.0214285,0.0241462,0.0150943,0.0141398,0.0101515,0.0308843,0.0095504,0.00729676)
dat2&lt;-c(0.014015,0.008918,0.006725,0.008794,0.008081,0.008851,0.008578,0.019609,0.016183,0.025785,0.040091,0.075545,0.025332,0.021778,0.024497,0.015241,0.014237,0.010295,0.031294,0.009606,0.007371)

#create a new vector called cdat1 to hold the cumulative sum
cdat1&lt;-cumsum(dat1)
plot(x,cdat1)
points(x,dat2,col=""red"")
</code></pre>

<p>I use the function points above in order to add dat2 to the existing plot. Run this in R and see if it gives you what you need. </p>
"
659982,63369,2009-03-18T20:33:17Z,659725,0,FALSE,"<p>It looks like an implementation change (2-D array in column-major order, instead of row-major order), rather than an interface change.</p>

<p>Think ""strategy"" pattern, rather than being an entire paradigm shift.  Of course, I've never used these products, so they may in fact force a paradigm shift down your throat.  I don't know why, though.</p>
"
660238,49720,2009-03-18T21:43:51Z,659725,0,FALSE,"<p>We might be better able to help you reach an informed decision if you described [1] your specific goal and [2] the issues you're running into with SQL Server.</p>
"
679244,23813,2009-03-24T21:08:08Z,652136,132,FALSE,"<p>If you don't want to modify the list in-place (e.g. for passing the list with an element removed to a function), you can use indexing: negative indices mean ""don't include this element"".</p>

<pre><code>x &lt;- list(""a"", ""b"", ""c"", ""d"", ""e""); # example list

x[-2];       # without 2nd element

x[-c(2, 3)]; # without 2nd and 3rd
</code></pre>

<p>Also, logical index vectors are useful:</p>

<pre><code>x[x != ""b""]; # without elements that are ""b""
</code></pre>

<p>This works with dataframes, too:</p>

<pre><code>df &lt;- data.frame(number = 1:5, name = letters[1:5])

df[df$name != ""b"", ];     # rows without ""b""

df[df$number %% 2 == 1, ] # rows with odd numbers only
</code></pre>
"
688781,26575,2009-03-27T06:57:58Z,509595,3,TRUE,"<p>What I personally would do is to make a script in some scripting language to separate the different data sets before the file is read into R, and possibly do some of the necessary data conversions, too.</p>

<p>If you want to do the splitting in R, look up <code>readLines</code> and <code>scan</code> &ndash; <code>read.csv2</code> is too high-level and is meant for reading a single data frame. You could write the different data sets into different files, or if you are ambitious, cook up file-like R objects that are usable with <code>read.csv2</code> and read from the correct parts of the underlying big file.</p>

<p>Once you have dealt with separating the data sets into different files, use <code>read.csv2</code> on those (or whichever <code>read.table</code> variant is best &ndash; if those are not tabs but fixed-width fields, see <code>read.fwf</code>). If <code>&lt;NA&gt;</code> indicates ""not available"" in your file, be sure to specify it as part of <code>na.strings</code>. If you don't do that, R thinks you have non-numeric data in that field, but with the right <code>na.strings</code>, you automatically get the field converted into numbers. It seems that one of your fields can include time stamps like <code>00:00</code>, so you need to use <code>colClasses</code> and specify a class to which your time stamp format can be converted. If the built-in <code>Date</code> class doesn't work, just define your own <code>timestamp</code> class and an <code>as.timestamp</code> function that does the conversion.</p>
"
696135,84458,2009-03-30T06:05:50Z,657440,3,FALSE,"<p>If you have that data in a text file (e.g. data.txt) you can also do the following:</p>

<pre><code>A &lt;- read.table(""data.txt"",header=TRUE)
attach(A)
plot(x.axis, cumsum(dat1))
points(x.axis, cumsum(dat2), col='red')
</code></pre>

<p>As JD Long said, the cumsum function is what you were looking for.</p>
"
713900,12960,2009-04-03T13:26:52Z,713878,5,FALSE,"<p>I would take a look at <a href=""http://en.wikipedia.org/wiki/Eigenvalue_algorithm"" rel=""noreferrer"">Eigenvalue algorithms</a>, which link to a number of different methods. They'll all have different characteristics, and hopefully one will be suitable for your purposes.</p>
"
714164,60711,2009-04-03T14:29:51Z,713878,11,FALSE,"<blockquote>
  <p>I assume it helps if the matrix is
  sparse?</p>
</blockquote>

<p>Yes, there are algorithms, that perform well on sparse matrices. </p>

<p>See for example: <a href=""http://www.cise.ufl.edu/research/sparse/"" rel=""noreferrer"">http://www.cise.ufl.edu/research/sparse/</a></p>
"
714340,83479,2009-04-03T15:05:37Z,713878,18,FALSE,"<p>Most of the algorithms for eigen value computations scale to big-Oh(n^3), where n is the row/col dimension of the (symmetric and square) matrix.  </p>

<p>For knowing the time complexity of the best algorithm till date you would have to refer to the latest research papers in Scientific Computing/Numerical Methods.  </p>

<p>But even if you assume the worse case, you would still need at least 1000^3 operations for a 1000x1000 matrix. </p>

<p>R uses the LAPACK routine's (DSYEVR, DGEEV, ZHEEV and ZGEEV) implementation by default.   However you could specify the EISPACK=TRUE as a parameter to use a EISPACK's RS, RG, CH and CG routines.  </p>

<p>The most popular and good open source packages for eigenvalue computation are LAPACK and EISPACK.</p>
"
722586,53143,2009-04-06T18:19:57Z,713878,7,FALSE,"<blockquote>
  <p>How long might it take in practice if
  I have a 1000x1000 matrix?</p>
</blockquote>

<p>MATLAB (based on LAPACK) computes on a dual-core 1.83 GHz machine all eigenvalues of a 1000x1000 random in roughly 5 seconds. When the matrix is <em>symmetric</em>, the computation can be done significantly faster and requires only about 1 second.</p>
"
732550,62970,2009-04-09T02:14:22Z,717747,2,TRUE,"<p>So you've actually asked about five questions (5 +/- 3).  As far as writing your own rect.hclust like function, the source is in <code>library/stats/R/identify.hclust.R</code> if you want to look at it.  </p>

<p>I took a quick glance at it myself and am not sure it does what I thought it did from reading your description--it seems to be drawing <em>multiple</em> rectangles,  Also, the <code>x</code> selector appears to be hard coded to segregate the tags horizontally (which isn't what you want and there's no <code>y</code>).</p>

<p>I'll be back, but in the meantime you might (in addition to looking at the source) try doing multiple rect.hclust with different <code>border=</code> colors and different <code>h=</code> values to see if a failure pattern emerges.</p>

<p><strong>Update</strong></p>

<p>I haven't had much luck poking at this either.  </p>

<p>One possible kludge for the clipping would be to pad the labels with trailing spaces and then bring the edge of your rectangle in slightly (the idea being that just bringing the rectangle in would get it out of the clipping zone but overwrite the ends of the labels).</p>

<p>Another idea would be to fill the rectangle with a translucent (low alpha) color, making a shaded area rather than a bounding box.</p>
"
736618,66519,2009-04-10T03:15:17Z,736541,16,TRUE,"<p>See <a href=""http://blog.revolution-computing.com/2009/01/10-tips-for-making-your-r-graphics-look-their-best.html"" rel=""noreferrer"">tip 7</a> about adjusting the margins.</p>

<p>Excerpt:</p>

<p>To remove the space reserved for labels, use par(mar=...).  For example</p>

<pre><code>png(file=""notitle.png"",width=400, height=350)
par(mar=c(5,3,2,2)+0.1)
hist(rnorm(100),ylab=NULL,main=NULL)
dev.off()
</code></pre>
"
736633,6372,2009-04-10T03:29:02Z,736541,9,FALSE,"<p>If you're willing to entertain an alternate plotting package, <a href=""http://had.co.nz/ggplot2/"" rel=""noreferrer"">ggplot2</a> does this automatically when you set <code>xlab</code>/<code>ylab</code> to <code>NULL</code> (and there is no plot title/<code>main</code> by default).  For simple plots, just <code>require(ggplot2)</code> and replace <code>plot</code> by <code>qplot</code>.</p>

<p>Really, ggplot2 is the most fun I've had with plotting in years and I can't resist the opportunity to evangelize it to everyone I meet. :-)</p>
"
736657,84458,2009-04-10T03:52:48Z,736541,1,FALSE,"<p>I usually use</p>

<pre><code>par(mar=c(1,1,1,1))
</code></pre>

<p>when I keep the border to a minimum.</p>
"
743646,25571,2009-04-13T11:33:40Z,743622,26,FALSE,"<p>See <code>?order</code>.  You just need the last index (or first, in decreasing order), so this should do the trick:</p>

<pre><code>order(matrix[,2],decreasing=T)[1]
</code></pre>
"
743664,19241,2009-04-13T11:44:58Z,743622,151,FALSE,"<p>See <code>?which.max</code></p>

<pre><code>&gt; which.max( matrix[,2] )
[1] 2
</code></pre>
"
743846,60617,2009-04-13T13:06:55Z,743812,97,TRUE,"<ul>
<li>Rolling Means/Maximums/Medians in the <a href=""http://cran.r-project.org/package=zoo"" rel=""nofollow noreferrer"">zoo</a> package (rollmean)</li>
<li>MovingAverages in <a href=""http://cran.r-project.org/package=TTR"" rel=""nofollow noreferrer"">TTR</a></li>
<li>ma in <a href=""http://cran.r-project.org/package=forecast"" rel=""nofollow noreferrer"">forecast</a></li>
</ul>
"
750710,85950,2009-04-15T07:59:03Z,750703,33,FALSE,"<p>For no good reason I'm aware of, <code>dev.off()</code>, unlike device related functions like <code>png()</code> returns a value: ""the number and name of the new active device.""  That value is what's being echoed to stdout.</p>

<p>Suppressing it can thus be achieved by just putting it somewhere, i.e.,</p>

<pre><code>garbage &lt;- dev.off()
</code></pre>
"
750834,4892,2009-04-15T08:51:01Z,750703,2,FALSE,"<p>Another option would be to use <code>sink()</code> and output everything to a log file, so you can check up on whether the plots worked if you need to.</p>
"
750852,NA,2009-04-15T08:58:03Z,750786,-1,FALSE,"<p>The following works for me using MSYS bash on Windows - I don't have R on my Linux box so can't try it there. You need two files - the first one called <strong>runr</strong> executes R with a file parameter</p>

<pre><code># this is runr
# following is path to R on my Windows machine
# plus any R params you need
c:/r/bin/r --file=$1
</code></pre>

<p>You need to make this executable with <strong>chmod +x runr</strong>.</p>

<p>Then in your script file:</p>

<pre><code>#!runr
# some R commands
x = 1
x
</code></pre>

<p>Note the #! runr line may need to include the full path to runr, depending on how you are using the command, how your PATH variable is set etc.</p>

<p>Not pretty, but it does seem to work!</p>
"
751308,26575,2009-04-15T11:37:36Z,750786,16,FALSE,"<p>Try <a href=""http://dirk.eddelbuettel.com/code/littler.html"" rel=""noreferrer"">littler</a>. <code>littler</code> provides hash-bang (i.e. script starting with #!/some/path) capability for GNU R, as well as simple command-line and piping use.</p>
"
751342,26575,2009-04-15T11:45:36Z,750703,18,TRUE,"<p>One of the nice things about R is that you can view the source of many functions:</p>

<pre><code>&gt; dev.off
function (which = dev.cur()) 
{
    if (which == 1) 
        stop(""cannot shut down device 1 (the null device)"")
    .Internal(dev.off(as.integer(which)))
    dev.cur()
}
&lt;environment: namespace:grDevices&gt;
</code></pre>

<p>So it calls <code>.Internal(dev.off(...))</code> and then returns dev.cur(), which I suppose would be useful if you have several devices open so you know which one became active. You could use <code>.Internal(dev.off(as.integer(dev.cur())))</code> in your script, or even patch <code>dev.off</code> to only return the value of <code>dev.cur()</code> if it is something else than the null device, and send the patch to the maintainers of R.</p>

<p>Also, <code>graphics.off()</code> calls <code>dev.off()</code> for all devices and doesn't return anything.</p>
"
753931,24762,2009-04-15T22:07:38Z,750786,2,FALSE,"<p>If the program you're using to execute your script needs parameters, you can put them at the end of the #! line:</p>

<pre><code>#!/usr/bin/R --random --switches --f
</code></pre>

<p>Not knowing R, I can't test properly, but this seems to work:</p>

<pre><code>axa@artemis:~$ cat r.test
#!/usr/bin/R -q -f
error
axa@artemis:~$ ./r.test
&gt; #!/usr/bin/R -q -f
&gt; error
Error: object ""error"" not found
Execution halted
axa@artemis:~$
</code></pre>
"
762107,72994,2009-04-17T20:35:26Z,750786,0,FALSE,"<p>You might want to use python's rpy2 module. However, the ""right"" way to do this is with R CMD BATCH. You can modify this to write to STDOUT, but the default is to write to a .Rout file. See example below: </p>

<pre><code>[ramanujan:~]$cat foo.R
print(rnorm(10))
[ramanujan:~]$R CMD BATCH foo.R
[ramanujan:~]$cat foo.Rout

R version 2.7.2 (2008-08-25)
Copyright (C) 2008 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]


 ~/.Rprofile loaded.
Welcome at  Fri Apr 17 13:33:17 2009
&gt; print(rnorm(10))
 [1]  1.5891276  1.1219071 -0.6110963  0.1579430 -0.3104579  1.0072677 -0.1303165  0.6998849  1.9918643 -1.2390156
&gt;

Goodbye at  Fri Apr 17 13:33:17 2009
&gt; proc.time()
   user  system elapsed
  0.614   0.050   0.721
</code></pre>

<p>Note: you'll want to try out the --vanilla and other options to remove all the startup cruft. </p>
"
762124,72994,2009-04-17T20:41:02Z,713878,18,FALSE,"<p>With big matrices you usually don't want all the eigenvalues. You just want the top few to do (say) a dimension reduction. </p>

<p><strong>The canonical algorithm is the Arnoldi-Lanczos iterative algorithm implemented in ARPACK:</strong> </p>

<p>www.caam.rice.edu/software/ARPACK/</p>

<p>There is a matlab interface in eigs: </p>

<p><a href=""http://www.mathworks.com/access/helpdesk/help/techdoc/index.html?/access/helpdesk/help/techdoc/ref/eigs.html"" rel=""noreferrer"">http://www.mathworks.com/access/helpdesk/help/techdoc/index.html?/access/helpdesk/help/techdoc/ref/eigs.html</a></p>

<pre><code>eigs(A,k) and eigs(A,B,k) return the k largest magnitude eigenvalues.
</code></pre>

<p>And there is now an R interface as well: </p>

<p><a href=""http://igraph.sourceforge.net/doc-0.5/R/arpack.html"" rel=""noreferrer"">http://igraph.sourceforge.net/doc-0.5/R/arpack.html</a></p>
"
776377,26575,2009-04-22T09:32:47Z,775116,9,TRUE,"<p>It's pretty tricky:</p>

<pre><code>m &lt;- match.call(expand.dots = FALSE)
# ...
m[[1L]] &lt;- as.name(""model.frame"")
m &lt;- eval(m, parent.frame())
</code></pre>

<p>The function uses <code>match.call</code> to find out how it is being called, modifies the call to replace the called function by <code>model.frame</code>, and calls it via <code>eval</code> with the parameters it received (although the part I replaced by <code># ...</code> removes several of the parameters), and <code>model.frame</code> uses the <code>formula</code> parameter. See the documentation of <code>match.call</code>, <code>eval</code>, and <code>model.frame</code>, and experiment a little, e.g. try to understand what is happening here:</p>

<pre><code>f &lt;- function(formula, data) { 
  m &lt;- match.call()
  m[[1L]] &lt;- as.name('model.frame')
  eval(m, parent.frame())
}
f(x ~ y)
Error in eval(expr, envir, enclos) : object 'x' not found
x &lt;- c(1,2,3)
f(x ~ y)
Error in eval(expr, envir, enclos) : object 'y' not found
y &lt;- c(3,4,5)
f(x ~ y)
  x y
1 1 3
2 2 4
3 3 5
d &lt;- as.data.frame(matrix(c(1,2,3,4),nrow=2))
names(d) &lt;- c('foo', 'bar')
f(foo ~ bar, d)
  foo bar
1   1   3
2   2   4
</code></pre>
"
780929,26575,2009-04-23T09:20:56Z,780796,28,FALSE,"<p>Either</p>

<pre><code>(setq ess-fancy-comments nil)
</code></pre>

<p>if you never want to indent single-<code>#</code> comments, or</p>

<pre><code>(add-hook 'ess-mode-hook 
          (lambda () 
            (local-set-key (kbd ""RET"") 'newline)))
</code></pre>

<p>if you want to change the behavior of Enter so it doesn't indent.</p>
"
781200,16240,2009-04-23T10:51:09Z,780796,47,TRUE,"<p>Use '###' if you don't want the comments indented.  According to the <a href=""http://ess.r-project.org/Manual/ess.html#Indenting"" rel=""noreferrer"">manual</a>,</p>

<blockquote>
  <p>By default, comments beginning with
  ‘###’ are aligned to the beginning of
  the line. Comments beginning with ‘##’
  are aligned to the current level of
  indentation for the block containing
  the comment. Finally, comments
  beginning with ‘#’ are aligned to a
  column on the right (the 40th column
  by default, but this value is
  controlled by the variable
  comment-column,) or just after the
  expression on the line containing the
  comment if it extends beyond the
  indentation column.</p>
</blockquote>
"
789618,57428,2009-04-25T19:59:05Z,789602,5,TRUE,"<p>It's most likely that %% means integer division by modulo - the result is within 0..360 range. It's used for cases when some value can't get out of some reasonable range like longitute fo example that can be only within 0..360 degrees.</p>
"
789674,90889,2009-04-25T20:30:57Z,789602,5,FALSE,"<p>Taking an educated guess that the language here is either R or S/Splus.  As others have said: %% is the mod operator.</p>
"
805043,74060,2009-04-30T01:40:37Z,805027,3,FALSE,"<p>While this doesn't actually answer your question, it's usually best to exclude automatically generated files from source control, exactly for this reason.</p>
"
823251,NA,2009-05-05T03:50:25Z,750786,8,FALSE,"<p><code>#!/path/to/R</code> won't work because R is itself a script, so <code>execve</code> is unhappy.</p>

<p>I use <code>R --slave -f script</code></p>
"
833756,90567,2009-05-07T09:54:05Z,445059,8,FALSE,"<p>Strangely enough, learning to vectorize in R is what helped me get used to basic functional programming.  A basic technique would be to define your operations inside the loop as a function:</p>

<pre><code>data = ...;
items = ...;

leave_one_out = function(i) {
   data1 = data[items != i];
   delta = ...;  # some operation on data1
   return delta;
}


for (j in items) {
   delta.list = cbind(delta.list, leave_one_out(j));
}
</code></pre>

<p>To vectorize, all you do is replace the <code>for</code> loop with the <code>sapply</code> mapping function:</p>

<pre><code>delta.list = sapply(items, leave_one_out);
</code></pre>
"
839718,90567,2009-05-08T13:09:40Z,736514,23,TRUE,"<blockquote>
  <p>An explanation that uses the words 'error', 'summation', or 'permutated'
  would be less helpful then a simpler explanation that didn't involve any
  discussion of how random forests works.</p>
  
  <p>Like if I wanted someone to explain to me how to use a radio, I wouldn't
  expect the explanation to involve how a radio converts radio waves into sound.</p>
</blockquote>

<p>How would you explain what the numbers in WKRP 100.5 FM ""mean"" without going into the pesky technical details of wave frequencies?  Frankly parameters and related performance issues with Random Forests are difficult to get your head around even if you understand some technical terms.</p>

<p>Here's my shot at some answers:</p>

<blockquote>
  <p>-mean raw importance score of variable x for class 0</p>
  
  <p>-mean raw importance score of variable x for class 1</p>
</blockquote>

<p>Simplifying from the Random Forest <a href=""http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp"" rel=""noreferrer"">web page</a>, raw importance score measures how much more helpful than random a particular predictor variable is in successfully classifying data.</p>

<blockquote>
  <p>-MeanDecreaseAccuracy</p>
</blockquote>

<p>I think this is only in the <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"" rel=""noreferrer"">R module</a>, and I believe it measures how much inclusion of this predictor in the model reduces classification error.</p>

<blockquote>
  <p>-MeanDecreaseGini</p>
</blockquote>

<p><a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""noreferrer"">Gini</a> is defined as ""inequity"" when used in describing a society's distribution of income, or a measure of ""node impurity"" in tree-based classification.  A low Gini (i.e. higher descrease in Gini) means that a particular predictor variable plays a greater role in partitioning the data into the defined classes.  It's a hard one to describe without talking about the fact that data in classification trees are split at individual nodes based on values of predictors.  I'm not so clear on how this translates into better performance.</p>
"
855992,26575,2009-05-13T03:58:26Z,855798,1,FALSE,"<p>Do you mean like this?</p>

<pre><code>myList &lt;- c(""Bob"", ""Mary"", ""Bob"", ""Bob"", ""Joe"")
r &lt;- rle(sort(myList))
result &lt;- as.data.frame(cbind(r$values, r$lengths))
names(result) &lt;- c(""Name"", ""Occurrences"")
result
  Name Occurrences
1  Bob           3
2  Joe           1
3 Mary           1
</code></pre>
"
862039,90567,2009-05-14T07:39:15Z,855798,2,TRUE,"<p>Using <code>table</code>, no need to sort:</p>

<pre><code>ctable &lt;- table(myList);
counts &lt;- data.frame(Name = names(ctable),Count = as.vector(ctable));
</code></pre>
"
866766,14257,2009-05-15T02:36:48Z,805027,3,TRUE,"<p>I agree with Tal, generated files should not be in version control.</p>

<p>In regards to your original question, I think the answer is no you can't stop R from doing that.  I found no mention of such options in the R pdf help file, <a href=""http://www.r-cookbook.com/rhelp/pdf.html"" rel=""nofollow noreferrer"">http://www.r-cookbook.com/rhelp/pdf.html</a></p>

<p>If you take a look at the pdf function inside of R (just execute 'pdf' without any parenthesis for a printout of the code) it actually creates the pdf file with a call to:</p>

<pre><code>.External(PDF, file, old$paper, old$family, old$encoding, old$bg, old$fg, 
      old$width, old$height, old$pointsize, onefile, old$pagecentre, old$title, 
      old$fonts, version[1L], version[2L], old$colormodel, old$useDingbats,
      old$useKerning)
</code></pre>

<p>No mention of the options you are looking for, so you are probably out of luck.  Unless you want to track down the code R uses to generate a PDF and change it.</p>
"
876778,NA,2009-05-18T08:56:02Z,876711,2,FALSE,"<p>I am far from being an R expert, but I think you need a data.frame:</p>

<pre><code>plot(data.frame(data[1],data[2]))
</code></pre>

<p>It does at least plot something on my R setup!</p>

<p>Following advice in luapyad's answer, I came up with this. I renamed the header ""scale"":</p>

<pre><code>scaling, serial, spawn, for, worker
5, 0.000178, 0.000288, 0.000292, 0.000300
10, 0.156986, 0.297926, 0.064509, 0.066297
12, 2.658998, 6.059502, 0.912733, 0.923606
15, 188.023411, 719.463264, 164.111459, 161.687982
</code></pre>

<p>then:</p>

<pre><code>foo &lt;- read.table(""foo.csv"", header=T,sep="","")
attach(foo)
plot( scaling, serial );
</code></pre>
"
876856,39578,2009-05-18T09:25:08Z,876711,11,TRUE,"<p>You don't need the two lines:</p>

<pre><code>scale &lt;- data[1]
serial &lt;- data[2]
</code></pre>

<p>as scale and serial are already set from the headers in the <code>read.table</code>.</p>

<p>Also <code>scale &lt;- data[1]</code> creates an element from a <code>data.frame</code></p>

<pre><code>  data[1]
1     5
2    10
3    12
4    15
</code></pre>

<p>whereas <code>scale</code> from the <code>read.table</code> is a vector</p>

<pre><code>5 10 12 15
</code></pre>

<p>and the <code>plot(scale, serial)</code> function expects vector rather than a data.frame, so you just need to do </p>

<pre><code>plot(scale, serial)
</code></pre>

<p>One approach to plotting the other columns of data on the y-axis:</p>

<pre><code>plot(scale,serial, ylab="""")
par(new=TRUE) 
plot(scale,spawn,axes=F, ylab="""", type=""b"")
par(new=TRUE) 
plot(scale,for., axes=F, ylab="""", type=""b"")
par(new=TRUE) 
plot(scale,worker,axes=F, ylab="""", type=""b"")
</code></pre>

<p>There are probably better ways of doing this, but that is beyond my current R knowledge....</p>
"
876967,26575,2009-05-18T09:55:31Z,876711,2,FALSE,"<p>Try this:</p>

<pre><code>data &lt;- read.csv('foo.csv')
plot(serial ~ scale, data)
dev.new()
plot(spawn ~ scale, data)
dev.new()
plot(for. ~ scale, data)
dev.new()
plot(worker ~ scale, data)
</code></pre>
"
936780,90272,2009-06-01T21:04:47Z,936748,-1,FALSE,"<p>I took the answer below from <a href=""http://www.stat.auckland.ac.nz/~paul/ItDT/HTML/node63.html"" rel=""nofollow noreferrer"">this website</a></p>

<p>The simplest sort of R expression is just a constant value, typically a numeric value (a number) or a character value (a piece of text). For example, if we need to specify a number of seconds corresponding to 10 minutes, we specify a number. </p>

<pre><code>&gt; 600
[1] 600
</code></pre>

<p>If we need to specify the name of a file that we want to read data from, we specify the name as a character value. Character values must be surrounded by either double-quotes or single-quotes. </p>

<pre><code>&gt; ""http://www.census.gov/ipc/www/popclockworld.html""
[1] ""http://www.census.gov/ipc/www/popclockworld.html""
</code></pre>
"
937593,113939,2009-06-02T01:37:14Z,936748,3,FALSE,"<p>(Edited for new idea:) The <a href=""http://stat.ethz.ch/R-manual/R-devel/library/base/html/bindenv.html"" rel=""nofollow noreferrer""><code>bindenv</code></a> functions provide an</p>

<blockquote>
  <p>experimental interface for adjustments to environments and bindings within environments. They allow for locking environments as well as individual bindings, and for linking a variable to a function.</p>
</blockquote>

<p>This seems like the sort of thing that could give a false sense of security (like a <code>const</code> pointer to a non-<code>const</code> variable) but it might help.</p>

<p>(Edited for focus:) <code>const</code> is a <a href=""http://en.wikipedia.org/wiki/Const_correctness"" rel=""nofollow noreferrer"">compile-time guarantee</a>, not a lock-down on bits in memory. Since R doesn't have a compile phase where it looks at all the code at once (it is built for interactive use), there's no way to check that future instructions won't violate any guarantee. If there's a right way to do this, the folks at the <a href=""https://stat.ethz.ch/mailman/listinfo/r-help"" rel=""nofollow noreferrer"">R-help</a> list will know. My suggested workaround: fake your own compilation. Write a script to preprocess your R code that will manually substitute the corresponding literal for each appearance of your ""constant"" variables.</p>

<p>(Original:) What benefit are you hoping to get from having a variable that acts like a C ""const""?</p>

<p>Since R has exclusively <a href=""http://cran.r-project.org/doc/manuals/R-lang.html#Argument-evaluation"" rel=""nofollow noreferrer"">call-by-value semantics</a> (unless you do some munging with environments), there isn't any reason to worry about clobbering your variables by calling functions on them. Adopting some sort of naming conventions or using some OOP structure is probably the right solution if you're worried about you and your collaborators accidentally using variables with the same names.</p>

<p>The feature you're looking for may exist, but I doubt it given the origin of R as a interactive environment where you'd want to be able to undo your actions.</p>
"
938632,4892,2009-06-02T09:25:43Z,937346,2,TRUE,"<p>I've not tried doing this with C/C++ but rather with Fortran. I had a similar problem in that some standard IO libraries weren't being included in the library I was created. In the end I just included them all and compiled using the Fortran compiler. I didn't use any of the R compiler utilities, just compiled as if I were compiling a static Fortran library normally for use with anything else. This worked fine.</p>

<p>A debug path might be to compile as a static library using gcc (or whatever you're using) then try to include and call that static library from another C program, then if that works try with R.</p>

<p>Hope this is helpful, writing these R packages is pretty hard unless you're using vanilla C or Fortran as far as I can tell.</p>
"
938717,4892,2009-06-02T09:45:33Z,936748,5,FALSE,"<p>I'm pretty sure that this isn't possible in R. If you're worried about accidentally re-writing the value then the easiest thing to do would be to put all of your constants into a list structure then you know when you're using those values. Something like:</p>

<pre><code>my.consts&lt;-list(pi=3.14159,e=2.718,c=3e8)
</code></pre>

<p>Then when you need to access them you have an aide memoir to know what not to do and also it pushes them out of your normal namespace.</p>

<p>Another place to ask would be R development mailing list. Hope this helps.</p>
"
953496,4892,2009-06-04T22:44:29Z,952275,33,FALSE,"<p>gsub does this, from your example:</p>

<pre><code>gsub(""\\((.*?) :: (0\\.[0-9]+)\\)"",""\\1 \\2"", ""(sometext :: 0.1231313213)"")
[1] ""sometext 0.1231313213""
</code></pre>

<p>you need to double escape the \s in the quotes then they work for the regex.</p>

<p>Hope this helps.</p>
"
956725,69749,2009-06-05T16:06:42Z,952275,2,FALSE,"<p>This is how I ended up working around this problem.  I used two separate regexes to match the first and second capture groups and run two <code>gregexpr</code> calls, then pull out the matched substrings:</p>

<pre><code>regex.string &lt;- ""(?&lt;=\\().*?(?= :: )""
regex.number &lt;- ""(?&lt;= :: )\\d\\.\\d+""

match.string &lt;- gregexpr(regex.string, str, perl=T)[[1]]
match.number &lt;- gregexpr(regex.number, str, perl=T)[[1]]

strings &lt;- mapply(function (start, len) substr(str, start, start+len-1),
                  match.string,
                  attr(match.string, ""match.length""))
numbers &lt;- mapply(function (start, len) as.numeric(substr(str, start, start+len-1)),
                  match.number,
                  attr(match.number, ""match.length""))
</code></pre>
"
969680,NA,2009-06-09T12:03:40Z,750786,111,TRUE,"<p>Content of <code>script.r</code>:</p>

<pre><code>#!/usr/bin/Rscript

cat(""Hello"")
</code></pre>

<p>Invocation from command line:</p>

<pre><code>./script.r
</code></pre>
"
993277,119759,2009-06-14T17:36:56Z,876711,5,FALSE,"<p>I'm new in R, but if you want to draw scale vs. all other columns in one plot, easy and with some elegance :) for printing or presentation, you may use Prof. Hadley Wickham's packages ggplot2 &amp; reshape.</p>

<p>Installation:</p>

<pre><code>install.packages(“ggplot2”,dep=T)
install.packages(“reshape”,dep=T)
</code></pre>

<p>Drawing your example:</p>

<pre><code>library(ggplot2)
library(reshape)

#read data
data = read.table(""foo.csv"", header=T,sep="","")

#melt data “scale vs. all”
data2=melt(data,id=c(""scale""))
data2

   scale variable      value
1      5   serial   0.000178
2     10   serial   0.156986
3     12   serial   2.658998
4     15   serial 188.023411
5      5    spawn   0.000288
6     10    spawn   0.297926
7     12    spawn   6.059502
8     15    spawn 719.463264
9      5     for.   0.000292
10    10     for.   0.064509
11    12     for.   0.912733
12    15     for. 164.111459
13     5   worker   0.000300
14    10   worker   0.066297
15    12   worker   0.923606
16    15   worker 161.687982

#draw all variables at once as line with different linetypes
qplot(scale,value,data=data2,geom=""line"",linetype=variable)
</code></pre>

<p>You could also use points (<code>geom=”points”</code>), choose different colours or shapes for different variables dots (<code>colours=variable or shape=variable</code>), adjust axis, set individual options for every line etc.</p>

<p>Link to <a href=""http://had.co.nz/ggplot2/"" rel=""noreferrer"" title=""ggplot2 online documentation"">online documentation</a>.</p>
"
994036,119759,2009-06-14T23:30:32Z,659725,2,FALSE,"<p>I have some experience with Infobright Community edition --- column-or. db, based on  mysql.</p>

<p>Pro:</p>

<ul>
<li>you can use mysql interfaces/odbc mysql drivers, from R too</li>
<li>fast enough queries on big chunks of data selection (because of KnowledgeGrid &amp; data packs)</li>
<li>very fast native data loader and connectors for ETL (talend, kettle)</li>
<li>optimized exactly that operations what I (and I think most of us) use (selection by factor levels, joining etc)</li>
<li>special ""lookup"" option for optimized storing R factor variables ;) (ok, char/varchar variables with relatively small levels number/rows number)</li>
<li>FOSS</li>
<li>paid support option</li>
<li>?</li>
</ul>

<p>Cons:</p>

<ul>
<li>no insert/update operations in Community edition (yet?), data loading only via native data loader/ETL connectors</li>
<li>no utf-8 official support (collation/sort etc), planned for q3 2009</li>
<li>no functions in aggregate queries f.e. select month (date) from ...) yet, planned for July(?) 2009, but because of column storage, I prefer simply create date columns for every aggregation levels (week number, month, ...) I need</li>
<li>cannot installed on existing mysql server as storage engine (because of own optimizer, if I understood correctly), but you may install Infobright &amp; mysql on different ports if you need</li>
<li>?</li>
</ul>

<p>Resume:
Good FOSS solution for daily analytical tasks, and, I think, your tasks as well.</p>
"
1007622,95810,2009-06-17T15:04:11Z,1007495,4,FALSE,"<p>While a bit costly, you can mimic your sample's distribution exactly (without needing any hypothesis on underlying population distribution) as follows.</p>

<p>You need a file structure that's rapidly searchable for ""highest entry with key &lt;= X"" -- Sleepycat's Berkeley database has a btree structure for that, for example; SQLite is even easier though maybe not quite as fast (but with an index on the key it should be OK).</p>

<p>Put your data in the form of pairs where the key is the cumulative count up to that point (sorted by increasing value).  Call K the highest key.</p>

<p>To generate a random pair that follows exactly the same distribution as the sample, generate a random integer X between 0 and K and look it up in that file structure with the mentioned ""highest that's &lt;="" and use the corresponding value.</p>

<p>Not sure how to do all this in R -- in your shoes I'd try a Python/R bridge, do the logic and control in Python and only the statistics in R itself, but, that's a personal choice!</p>
"
1008175,25188,2009-06-17T16:26:29Z,1007495,4,FALSE,"<p>To see whether you have a real power law distribution, make a log-log plot of frequencies and see whether they line up roughly on a straight line.  If you do have a straight line, you might want to read this article on the <a href=""http://en.wikipedia.org/wiki/Pareto_distribution"" rel=""nofollow noreferrer"">Pareto distribution</a> for more on how to describe your data.</p>
"
1052429,2002705,2009-06-27T08:54:55Z,1007495,1,FALSE,"<p>I'm assuming that you're interested in understanding the distribution over your categorical values.</p>

<p>The best way to generate ""new"" data is to sample from your existing data using R's sample() function.  This will give you values which follow the probability distribution indicated by your existing counts.</p>

<p>To give a trivial example, let's assume you had a file of voter data for a small town, where the values are voters' political affiliations, and counts are number of voters:</p>

<pre><code>affils &lt;- as.factor(c('democrat','republican','independent'))
counts &lt;- c(552,431,27)
## Simulate 20 new voters, sampling from affiliation distribution
new.voters &lt;- sample(affils,20, replace=TRUE,prob=counts)
new.counts &lt;- table(new.voters)
</code></pre>

<p>In practice, you will probably bring in your 100m rows of values and counts using R's read.csv() function.  Assuming you've got a header line labeled ""values\t counts"", that code might look something like this:</p>

<pre><code>dat &lt;- read.csv('values-counts.txt',sep=""\t"",colClasses=c('factor','numeric'))
new.dat &lt;- sample(dat$values,100,replace=TRUE,prob=dat$counts)
</code></pre>

<p>One caveat:  as you may know, R keeps all of its objects in memory, so be sure you've got enough freed up for 100m rows of data (storing character strings as factors will help reduce the footprint).</p>
"
1054561,2611,2009-06-28T08:57:29Z,1022649,9,FALSE,"<p>I used the same reference (Bryan O'Sullivan's python code) to write an R version of Slope One a while back.  I'm pasting the code below in case it helps.</p>

<pre><code>predict &lt;- function(userprefs, data.freqs, data.diffs) {
    seen &lt;- names(userprefs)

    preds &lt;- sweep(data.diffs[ , seen, drop=FALSE], 2, userprefs, '+') 
    preds &lt;- preds * data.freqs[ , seen]
    preds &lt;- apply(preds, 1, sum)

    freqs &lt;- apply(data.freqs[ , seen, drop=FALSE], 1, sum)

    unseen &lt;- setdiff(names(preds), seen)
    result &lt;- preds[unseen] / freqs[unseen]
    return(result[is.finite(result)])
}

update &lt;- function(userdata, freqs, diffs) {
    for (ratings in userdata) {
        items &lt;- names(ratings)
        n &lt;- length(ratings)

        ratdiff &lt;- rep(ratings, n) - rep(ratings, rep(n, n))
        diffs[items, items] &lt;- diffs[items, items] + ratdiff

        freqs[items, items] &lt;- freqs[items, items] + 1
    }
    diffs &lt;- diffs / freqs
    return(list(freqs=freqs, diffs=diffs))
}


userdata &lt;- list(alice=c(squid=1.0, cuttlefish=0.5, octopus=0.2),
                 bob=c(squid=1.0, octopus=0.5, nautilus=0.2),
                 carole=c(squid=0.2, octopus=1.0, cuttlefish=0.4, nautilus=0.4),
                 dave=c(cuttlefish=0.9, octopus=0.4, nautilus=0.5))

items &lt;- c('squid', 'cuttlefish', 'nautilus', 'octopus')
n.items &lt;- length(items)
freqs &lt;- diffs &lt;- matrix(0, nrow=n.items, ncol=n.items, dimnames=list(items, items))

result &lt;- update(userdata, freqs, diffs)
print(result$freqs)
print(result$diffs)

userprefs &lt;- c(squid=.4)
predresult &lt;- predict(userprefs, result$freqs, result$diffs)
print(predresult)
</code></pre>
"
1088800,2611,2009-07-06T19:25:31Z,1088639,25,TRUE,"<p>Here's one way by using a closure (in the programming language sense), i.e. store the count variable in an enclosing environment accessible only by your function:</p>

<pre><code>make.f &lt;- function() {
    count &lt;- 0
    f &lt;- function(x) {
        count &lt;&lt;- count + 1
        return( list(mean=mean(x), count=count) )
    }
    return( f )
}

f1 &lt;- make.f()
result &lt;- f1(1:10)
print(result$count, result$mean)
result &lt;- f1(1:10)
print(result$count, result$mean)

f2 &lt;- make.f()
result &lt;- f2(1:10)
print(result$count, result$mean)
result &lt;- f2(1:10)
print(result$count, result$mean)
</code></pre>
"
1092713,95810,2009-07-07T14:35:37Z,1092506,3,FALSE,"<p>First decide the bounds, say 0 to 100, and make an empty plot including those points:</p>

<pre><code>plot(c(0,100), c(0,100))
</code></pre>

<p>possibly of course with optional parameters such as <code>axes=</code>, <code>xlab=</code>, <code>ylab=</code>, and so on, to control various details of the axes and titling/labeling; then, add each line with <code>abline(a, b)</code> where b is the slope and a is the intercept, so, in your examples:</p>

<pre><code>abline(1, 3)
abline(2, 4)
abline(1, 1)
</code></pre>

<p>Of course there are many more details you can control such as color (<code>col=</code> optional parameter), line type (<code>lty=</code>) and width (<code>lwd=</code>), etc, but this is the gist of it.</p>
"
1095173,2611,2009-07-07T22:17:38Z,1092506,0,FALSE,"<p>Here's another way using matplot:</p>

<pre><code>&gt; x &lt;- 0:10
&gt; matplot(cbind(x, x, x), cbind(3*x+1, 4*x+2, x+1), 
          type='l', xlab='x', ylab='y')
</code></pre>

<p>matplot(X, Y, ...) takes two matrix arguments. Each column of X is plotted against each column of Y.  </p>

<p>In our case, X is a 11 x 3 matrix with each column a sequence of 0 to 10 (our x-values for each line).  Y is a 11 x 3 matrix with each column computed off the x vector (per your line equations).  </p>

<p><code>xlab</code> and <code>ylab</code> just label the x and y axes.  The <code>type='l'</code> specifies that lines are to be drawn (see other options by typing <code>?matplot</code> or <code>?plot</code> at the R prompt).  </p>

<p>One nice thing about matplot is that the defaults can be nice for plotting multiple lines -- it chooses different colors and styles per line.  These can also be modified: see <code>?matplot</code> (and <code>lty</code> for more detail).</p>
"
1106010,135870,2009-07-09T19:33:34Z,1105659,52,TRUE,"<p>R lists can be thought of as hashes- vectors of objects that can be accessed by name. Using this approach you can add a new entry to the list like so:</p>

<pre><code>key &lt;- ""width""
value &lt;- 32

mylist &lt;- list()
mylist[[ key ]] &lt;- value
</code></pre>

<p>Here we use the string stored in the variable key to access a position in the list much like using the value stored in a loop variable i to access a vector through:</p>

<pre><code>vector[ i ]
</code></pre>

<p>The result is:</p>

<pre><code>myList
$width
[1] 32
</code></pre>
"
1107343,2611,2009-07-10T02:00:19Z,1098210,2,FALSE,"<p>Try adding something like the following to your ~/.emacs file:</p>

<pre><code>(setq inferior-R-program-name ""c:/path/to/Rterm.exe"")
</code></pre>

<p>and restart emacs.</p>
"
1109033,3201,2009-07-10T11:30:50Z,1109017,7,FALSE,"<blockquote>
  <p>Is it possible to configure the print
  function to print to stderr?</p>
</blockquote>

<p><a href=""https://stat.ethz.ch/pipermail/r-help/2004-November/060575.html"" rel=""noreferrer"">From Ripley himself</a>:</p>

<blockquote>
  <p>No, but where standard output goes is
  controlled by sink(), so you can 
  achieve the same effect.  R internally
  has no idea what output comes from 
  print() (which is not just one
  function but hundreds of methods).</p>
</blockquote>
"
1109211,60628,2009-07-10T12:21:28Z,1109017,43,TRUE,"<p>Actually the following works for me:</p>

<pre><code>write(""prints to stderr"", stderr())

write(""prints to stdout"", stdout())
</code></pre>
"
1109867,41153,2009-07-10T14:26:52Z,1107605,-1,FALSE,"<p>Try <a href=""http://www.emacswiki.org/emacs/ShMode"" rel=""nofollow noreferrer"">shell-script-mode</a> - with some <a href=""http://keramida.wordpress.com/2008/08/08/tweaking-shell-script-indentation-in-gnu-emacs/"" rel=""nofollow noreferrer"">notes on customized indentation</a></p>

<p>It indented your code just fine, for me.</p>
"
1110408,7536,2009-07-10T15:53:07Z,1110363,8,TRUE,"<p><a href=""http://cran.r-project.org/web/packages/getopt/index.html"" rel=""nofollow noreferrer""><code>getopt</code> for R</a></p>
"
1114736,86684,2009-07-11T21:55:28Z,1114699,0,FALSE,"<p>how would you even represent an adjacency list in R?  it needs variable-sized lists for the set of adjacent nodes; so then you have to use a list(); but then what good is it having it in R?</p>

<p>i can think of lame tricks with sapply-like functions but they do a linear scan for every node.  but playing around for 1 minute, here is: a list of pairlists, where the second item of each pair is the adjacency list.  output is crazier than the datstructure really is.</p>

<pre><code>&gt; edgelist=data.frame(A=c(1,1,2,2,2),B=c(1,2,2,3,4))
&gt; library(plyr)
&gt; llply(1:max(edgelist), function(a) list(node=a, adjacents=as.list(edgelist$B[edgelist$A==a])))
[[1]]
[[1]]$node
[1] 1

[[1]]$adjacents
[[1]]$adjacents[[1]]
[1] 1

[[1]]$adjacents[[2]]
[1] 2



[[2]]
[[2]]$node
[1] 2

[[2]]$adjacents
[[2]]$adjacents[[1]]
[1] 2

[[2]]$adjacents[[2]]
[1] 3

[[2]]$adjacents[[3]]
[1] 4



[[3]]
[[3]]$node
[1] 3

[[3]]$adjacents
list()


[[4]]
[[4]]$node
[1] 4

[[4]]$adjacents
list()
</code></pre>
"
1114828,2611,2009-07-11T22:34:52Z,1114699,6,TRUE,"<p>Quick and dirty ...</p>

<pre><code>&gt; edges &lt;- data.frame(nodea=c(1,2,4,2,1), nodeb=c(1,2,3,4,5))

&gt; adjlist &lt;- by(edges, edges$nodea, function(x) x$nodeb)

&gt; for (i in as.character(unique(edges$nodea))) {
+   cat(i, ' -&gt; ', adjlist[[i]], '\n')
+ }

1  -&gt;  1 5
2  -&gt;  2 4
4  -&gt;  3

&gt; adjlist
edges$nodea: 1
[1] 1 5
------------------------------------------------------------
edges$nodea: 2
[1] 2 4
------------------------------------------------------------
edges$nodea: 4
[1] 3
</code></pre>
"
1116218,NA,2009-07-12T15:18:02Z,1114699,4,FALSE,"<pre><code>&gt; edges &lt;- data.frame(nodea=c(1,2,4,2,1), nodeb=c(1,2,3,4,5))

&gt; attach(edges)

&gt; tapply(nodeb,nodea,unique)

$`1`
[1] 1 5

$`2`
[1] 2 4

$`4`
[1] 3
</code></pre>
"
1128618,2611,2009-07-14T23:20:29Z,1125907,4,TRUE,"<p>Try tweaking <code>mar</code>:</p>

<pre><code>mar.old &lt;- par('mar')
print(mar.old)

par(mar=rep(10, 4)) # some ridiculous values
plot(density(rnorm(1000)), ylab='foo\nbar\nbaz\nquux')

par(mar=mar.old) # restore original
</code></pre>

<p>See <code>?par</code> for more info on <code>mar</code>:</p>

<blockquote>
  <p><em>mar</em>
  A numerical vector of the form c(bottom, left, top, right) which gives the number of lines of margin to be specified on the four sides of the plot. The default is c(5, 4, 4, 2) + 0.1. </p>
</blockquote>
"
1133209,9122,2009-07-15T18:46:44Z,1133172,2,FALSE,"<p>You can use the R command-line tools if you install R for Mac OS X.  The R website has <a href=""http://cran.r-project.org/bin/macosx/"" rel=""nofollow noreferrer"">disk images</a> with installers, or you can install <a href=""http://www.macports.org/"" rel=""nofollow noreferrer"">via MacPorts</a>, like this:</p>

<pre><code>$ sudo port install R
</code></pre>

<p>The R website has a slightly later version (2.9.1) than MacPorts (which is at 2.8.1).</p>
"
1136802,12960,2009-07-16T10:48:58Z,1136709,8,TRUE,"<p>You can use <a href=""http://rosuda.org/JRI/"" rel=""noreferrer"">JRI</a>. From that website:</p>

<blockquote>
  <p>JRI is a Java/R Interface, which
  allows to run R inside Java
  applications as a single thread.
  Basically it loads R dynamic library
  into Java and provides a Java API to R
  functionality. It supports both simple
  calls to R functions and a full
  running REPL.</p>
</blockquote>

<p>This is part of the <a href=""http://www.rforge.net/rJava/"" rel=""noreferrer"">rJava</a> project (which allows calling of Java from R)</p>
"
1143173,5856,2009-07-17T13:10:54Z,1142294,17,TRUE,"<p>First of all, the <code>plot.svm</code> function assumes that the data varies across two dimensions. The data you have used in your example is only one-dimensional and so the decision boundary would have to be plotted on a line, which isn't supported. Secondly, the function seems to need a data frame as input and you are working with vectors.</p>

<p>This should work...</p>

<pre><code>library(e1071)

day = c(0,1,2,3,4,5,6)
weather = c(1,0,0,0,0,0,0)
happy = factor(c(T,F,F,F,F,F,F))

d = data.frame(day=day, weather=weather, happy=happy)
model = svm(happy ~ day + weather, data = d)
plot(model, d)
</code></pre>
"
1154326,134830,2009-07-20T15:43:54Z,1154242,51,FALSE,"<p>Using base graphics, the standard way to do this is to use axes=FALSE, then create your own axes using Axis (or axis).  For example,</p>

<pre><code>x &lt;- 1:20
y &lt;- runif(20)
plot(x, y, axes=FALSE, frame.plot=TRUE)
Axis(side=1, labels=FALSE)
Axis(side=2, labels=FALSE)
</code></pre>

<p>The lattice equivalent is </p>

<pre><code>library(lattice)
xyplot(y ~ x, scales=list(alternating=0))
</code></pre>
"
1154397,2611,2009-07-20T15:59:29Z,1154242,134,FALSE,"<p>Remove numbering on x-axis or y-axis:</p>

<pre><code>plot(1:10, xaxt='n')
plot(1:10, yaxt='n')
</code></pre>

<p>If you want to remove the labels as well:</p>

<pre><code>plot(1:10, xaxt='n', ann=FALSE)
plot(1:10, yaxt='n', ann=FALSE)
</code></pre>
"
1162817,142477,2009-07-22T02:44:39Z,876711,7,FALSE,"<p>In your example, </p>

<pre><code>plot(scale, serial) 
</code></pre>

<p>won't work because <code>scale</code> and <code>serial</code> are both data frames, e.g.</p>

<pre><code>class(scale)
[1] ""data.frame""
</code></pre>

<p>You could try the following and use <code>points()</code>, once the plot has been generated, to plot the remaining columns.  Note, I used the <code>ylim</code> parameter in <code>plot</code> to accommodate the range in the third column.</p>

<pre><code>data &lt;- read.csv('foo.csv', header=T)
plot(data$scale, data$serial, ylim=c(0,750))
points(data$scale, data$spawn, col='red')
points(data$scale, data$for., col='green')
points(data$scale, data$worker, col='blue')
</code></pre>
"
1163415,86684,2009-07-22T06:39:40Z,936748,2,FALSE,"<p>R doesn't have a language constant feature.  The list idea above is good; I personally use a naming convention like ALL_CAPS.</p>
"
1163483,86684,2009-07-22T06:54:54Z,736514,20,FALSE,"<p>For your immediate concern: higher values mean the variables are more important.  This should be true for all the measures you mention.</p>

<p>Random forests give you pretty complex models, so it can be tricky to interpret the importance measures.  If you want to easily understand what your variables are doing, don't use RFs.  Use linear models or a (non-ensemble) decision tree instead.</p>

<p>You said:</p>

<blockquote>
  <p>An explanation that uses the words
  'error', 'summation', or 'permutated'
  would be less helpful then a simpler
  explanation that didn't involve any
  discussion of how random forests
  works.</p>
</blockquote>

<p>It's going to be awfully tough to explain much more than the above unless you dig in and learn what about random forests.  I assume you're complaining about either the manual, or the section from Breiman's manual:</p>

<p><a href=""http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp"" rel=""noreferrer"">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp</a></p>

<p>To figure out how important a variable is, they fill it with random junk (""permute"" it), then see how much predictive accuracy decreases.  MeanDecreaseAccuracy and MeanDecreaseGini work this way.  I'm not sure what the raw importance scores are.</p>
"
1163668,73070,2009-07-22T07:40:48Z,1163640,1,FALSE,"<p>Well, EPS just contains instructions to draw the plot, so its size would greatly depend on how many data points you have. It's likely smaller in a PDF where compression is used but your best bet might probably be to use a raster format, which can get smaller than that.</p>

<p>I would suspect the EPS R generates is already as small as it can get (I'm sure they have an own function in Postscript to handle plotting the data with a one-char name, etc. as that is fairly common practice). I doubt there are many ways in which to optimize that. I might be mistaken, though, but chances are that R is the only program which has enough high-level information to reasonably compress the output.</p>
"
1164047,142651,2009-07-22T09:14:57Z,1163640,3,FALSE,"<p>You have three options.</p>

<ol>
<li>Accept the large file size</li>
<li>Save the file in a non-vector format like png</li>
<li><p>Create the QQplot on a random sample of the data. A random sample of a few hundred points should give a similar QQplot.</p>

<p>postscript(filename)
Samp &lt;- sample(n, size = 200)
qqnorm(Samp, main=title))
qqline(Samp)
dev.off()</p></li>
</ol>
"
1164111,134830,2009-07-22T09:30:23Z,1163640,3,TRUE,"<p>I've just tried several things that didn't work - I'm including them here to save others wasting their time.  For reference, I set <code>n &lt;- rnorm(1e5)</code> in your code above.</p>

<p>Things that don't work:</p>

<ol>
<li><p>Setting <code>colormodel &lt;- ""gray""</code>.</p></li>
<li><p>Using a different value of pch.  (Some other values <em>increase</em> the file size, but I found none that decrease it.)</p></li>
<li><p>Setting <code>useKerning = FALSE</code>.</p></li>
<li><p>Changing the width and height settings.</p></li>
<li><p>Using pdf instead of postscript.</p></li>
<li><p>Using CarioPS from the Cairo package.</p></li>
</ol>

<p>In the light of this, I think that you are unlikely to be able to decrease the file size using a vector format.  This means that you will have to use a raster format (most likely PNG).</p>
"
1165055,133354,2009-07-22T12:54:23Z,1136709,2,FALSE,"<p>I've had a good experience integrating <a href=""http://jgr.markushelbig.org/JGR.html"" rel=""nofollow noreferrer"">JGR, the Java Gui for R</a> into my java application.</p>

<p>Note that the REngine is not multithread-safe. Thus, you need serialize access to the REngine (by, for example, letting it run in it's own thread). Your application and JGR would both update R variables and data frames using JRI. Now the great part about using JGR is that an R console is available so that the user can access the data being updated via your app, play with it, even change it on the fly, plot it, etc;! This combination of compiled (Java) and interpreted (R) modes is quite satisfying in terms of user experience.</p>

<p>Also, it looks like the JGR project is quite alive; I was using JGR 1.4, now it's at 1.7 (updated June 2009), so by all means, download it and give it a try.</p>

<p>If that works well and you're getting ambitious, look at biocep.</p>
"
1165749,142651,2009-07-22T14:36:30Z,936748,6,FALSE,"<p>Since you are planning to distribute your code to others, you could (should?) consider to create a package. Create within that package a NAMESPACE. There you can define variables that will have a constant value. At least to the functions that your package uses. Have a look at <a href=""http://www.r-project.org/doc/Rnews/Rnews_2003-1.pdf"" rel=""noreferrer"">Tierney (2003) Name Space Management for R</a></p>
"
1166098,NA,2009-07-22T15:26:31Z,736541,2,FALSE,"<p>With lattice, it's just a matter of setting the xlab, ylab, and main arguments to NULL:</p>

<pre><code>library(lattice)
bwplot(rnorm(100),xlab=NULL,ylab=NULL,main=NULL)
</code></pre>
"
1166442,134830,2009-07-22T16:16:31Z,1166157,4,FALSE,"<p>Karen Chine gave a presentation on using BIOCEP at UseR 2009.  It's not quite step-by-step, but it may get you started.</p>

<p><a href=""http://www2.agrocampus-ouest.fr/math/useR-2009/slides/Chine.pdf"" rel=""nofollow noreferrer"">http://www2.agrocampus-ouest.fr/math/useR-2009/slides/Chine.pdf</a></p>
"
1167524,86684,2009-07-22T19:12:25Z,1133172,11,FALSE,"<p>I use the ""R"" command with the standard R.app GUI download, and would recommend using that instead of macports.  After running the installer, I see:</p>

<pre><code>$ which R
/usr/local/bin/R

$ ls -l /usr/local/bin/R
lrwxr-xr-x  1 root  wheel  47 Nov 12  2008 /usr/local/bin/R -&gt; /Library/Frameworks/R.framework/Resources/bin/R

$ R
R version 2.8.0 (2008-10-20)
...
&gt; 
</code></pre>

<p>I actually prefer to use this rather than the GUI, because it uses the current working directory for the workspace and history files (.Rhistory and .RData).  It makes it easier to organize projects in this way by filesystem directory, and is very natural if you're using the commandline for other tasks as well (like running data preprocessing scripts).</p>

<p>Also, the terminal version lets you more easily cancel an expensive computation by pressing Ctrl-C.  The GUI sometimes locks up during these.</p>

<p>By default, I think Mac terminal R uses the X11 display system, which isn't as good as the Quartz one used by the GUI.  You can change this though: get the <a href=""http://www.rforge.net/CarbonEL/"" rel=""noreferrer"">CarbonEL</a> package, then put the following into your <code>~/.Rprofile</code>:</p>

<pre><code>goquartz = function() {
  library(""CarbonEL"")
  options(device='quartz')
  Sys.unsetenv(""DISPLAY"")
}

if (.Platform$GUI == ""X11"") {
  # this means we're running in the terminal (not GUI) version.
  # if you actually want the X11 display, comment out the following line
  goquartz()
}
</code></pre>
"
1167658,143141,2009-07-22T19:31:26Z,1167448,7,FALSE,"<p>In my experience, Matrix is the best supported and most mature of the packages you mention. Its C architecture should also be fairly well-exposed and relatively straightforward to work with.</p>
"
1168147,4892,2009-07-22T20:48:01Z,1167448,21,TRUE,"<p>Matrix is the most common and has also just been accepted R standard installation (as of 2.9.0), so should be broadly available.</p>

<p>Matrix in base:
<a href=""https://stat.ethz.ch/pipermail/r-announce/2009/000499.html"" rel=""noreferrer"">https://stat.ethz.ch/pipermail/r-announce/2009/000499.html</a></p>
"
1169262,2002705,2009-07-23T02:25:24Z,1169248,347,TRUE,"<p>Both the <code>match()</code> (returns the first appearance) and <code>%in%</code> (returns a Boolean) functions are designed for this.</p>

<pre><code>v &lt;- c('a','b','c','e')

'b' %in% v
## returns TRUE

match('b',v)
## returns the first location of 'b', in this case: 2
</code></pre>
"
1169265,2611,2009-07-23T02:25:52Z,1169248,31,FALSE,"<p>You can use the <code>%in%</code> operator:</p>

<pre><code>vec &lt;- c(1, 2, 3, 4, 5)
1 %in% vec # true
10 %in% vec # false
</code></pre>
"
1169308,NA,2009-07-23T02:40:45Z,1092506,2,FALSE,"<p>You can also use the <code>curve</code> function. For example:</p>

<pre><code>curve(3*x+1, from=-5, to=5)
curve(4*x+2, add=T)
curve(x+1, add=T)
</code></pre>

<p>Here the add parameter causes the plots to be put on the same graph</p>
"
1169399,84458,2009-07-23T03:13:38Z,1169388,27,TRUE,"<p>This is one way to do it.  First I get the indices at which x is either 8 or 9.  Then we can verify that at those indices, x is indeed 8 and 9.</p>

<pre><code>&gt; inds &lt;- which(x %in% c(8,9))
&gt; inds
[1]  1  3  4 12 15 19
&gt; x[inds]
[1] 8 9 9 8 9 8
</code></pre>
"
1169423,37751,2009-07-23T03:23:00Z,1169376,1,FALSE,"<p>I scratched up a good answer from Achim Zeileis over on the r list. Here's what he said: </p>

<pre><code>library(zoo)
## create data

x &lt;- rnorm(365)
## transform to regular zoo series with ""Date"" index

x &lt;- zooreg(x, start = as.Date(""2004-01-01"")) plot(x)

## add rolling/running/moving average with window size 7 

lines(rollmean(x, 7), col = 2, lwd = 2)

## if you don't want the rolling mean but rather a weekly ## time series of means you can do
nextfri &lt;- function(x) 7 * ceiling(as.numeric(x - 1)/7) + as.Date(1) xw &lt;- aggregate(x, nextfri, mean)

## nextfri is a function which computes for a certain ""Date"" ## the next friday. xw is then the weekly series. 

lines(xw, col = 4)
</code></pre>

<p>Achim went on to say:</p>

<blockquote>
<p>
 Note, that the difference between is
rolling mean and the aggregated series
is due to different alignments. This
can be changed by changing the 'align'
argument in <code>rollmean()</code> or the
<code>nextfri()</code> function in the aggregate
call.
</p>
</blockquote>

<p>All this came from Achim, not from me:
<a href=""http://tolstoy.newcastle.edu.au/R/help/05/06/6785.html"" rel=""nofollow noreferrer"">http://tolstoy.newcastle.edu.au/R/help/05/06/6785.html</a></p>
"
1169451,84458,2009-07-23T03:31:37Z,1169426,14,TRUE,"<p>One option is the <a href=""http://www.jstatsoft.org/v24/i06/"" rel=""noreferrer"">network</a> package, part of the <a href=""http://www.jstatsoft.org/v24/i09/"" rel=""noreferrer"">statnet</a> family of R packages for statistical social network analysis.  It handles network data in a sparse way, which is nice for larger data sets.</p>

<p>Below, I do the following:</p>

<ul>
<li>load the edgelist (the first two columns) into a network object</li>
<li>assign the counts to be an edge attribute called weight.  </li>
<li>plot the network with gplot.  (See the help page for changing the thickness of the edges.)</li>
<li>plot a sociomatrix (just a 5x5 set of blocks representing the adjacency matrix, where the (i,j) cell is shaded by the relative count)</li>
</ul>

<pre>
A = read.table(file=""so.txt"",header=T)
A
      p1 p2 counts
    1  a  b    100
    2  a  c    200
    3  a  d    100
    4  b  c     80
    5  b  d     90
    6  b  e    100
    7  c  d    100
    8  c  e     40
    9  d  e     60

library(network)
net = network(A[,1:2])
# Get summary information about your network
net
     Network attributes:
      vertices = 5 
      directed = TRUE 
      hyper = FALSE 
      loops = FALSE 
      multiple = FALSE 
      bipartite = FALSE 
      total edges= 9 
        missing edges= 0 
        non-missing edges= 9 
        Vertex attribute names: 
        vertex.names 
     adjacency matrix:
      a b c d e
    a 0 1 1 1 0
    b 0 0 1 1 1
    c 0 0 0 1 1
    d 0 0 0 0 1
    e 0 0 0 0 0

set.edge.attribute(net,""weight"",A[,3])
gplot(net)

## Another cool feature
s = as.sociomatrix(net,attrname=""weight"")
plot.sociomatrix(s)
</pre>
"
1169495,2611,2009-07-23T03:46:55Z,1169456,217,TRUE,"<p>The R Language Definition is handy for answering these types of questions:</p>

<ul>
<li><a href=""http://cran.r-project.org/doc/manuals/R-lang.html#Indexing"" rel=""noreferrer"">http://cran.r-project.org/doc/manuals/R-lang.html#Indexing</a></li>
</ul>

<blockquote>
<p>
R has three basic indexing operators, with syntax displayed by the following examples
<p>
<code><pre>
    x[i]
    x[i, j]
    x[[i]]
    x[[i, j]]
    x$a
    x$""a""
</pre></code>
<p>
For vectors and matrices the <code>[[</code> forms are rarely used, although they have some slight semantic differences from the [ form (e.g. it drops any names or dimnames attribute, and that partial matching is used for character indices). When indexing multi-dimensional structures with a single index, <code>x[[i]]</code> or <code>x[i]</code> will return the <code>i</code>th sequential element of <code>x</code>. 
<p>
For lists, one generally uses <code>[[</code> to select any single element, whereas <code>[</code> returns a list of the selected elements. 
<p>
The <code>[[</code> form allows only a single element to be selected using integer or character indices, whereas <code>[</code> allows indexing by vectors. Note though that for a list, the index can be a vector and each element of the vector is applied in turn to the list, the selected component, the selected component of that component, and so on. The result is still a single element.

</blockquote>
"
1169501,2002705,2009-07-23T03:48:28Z,1169456,71,FALSE,"<p>Double brackets accesses a list <strong>element</strong>, while a single bracket gives you back a list with a single element.</p>

<pre><code>lst &lt;- list('one','two','three')

a &lt;- lst[1]
class(a)
## returns ""list""

a &lt;- lst[[1]]
class(a)
## returns ""character""
</code></pre>
"
1169508,NA,2009-07-23T03:50:46Z,1169456,32,FALSE,"<p><code>[]</code> extracts a list, <code>[[]]</code> extracts elements within the list</p>

<pre><code>alist &lt;- list(c(""a"", ""b"", ""c""), c(1,2,3,4), c(8e6, 5.2e9, -9.3e7))

str(alist[[1]])
 chr [1:3] ""a"" ""b"" ""c""

str(alist[1])
List of 1
 $ : chr [1:3] ""a"" ""b"" ""c""

str(alist[[1]][1])
 chr ""a""
</code></pre>
"
1169530,135870,2009-07-23T03:57:21Z,1169456,119,FALSE,"<p>The significant differences between the two methods are the class of the objects they return when used for extraction and whether they may accept a range of values, or just a single value during assignment.</p>

<p>Consider the case of data extraction on the following list:</p>

<pre><code>foo &lt;- list( str='R', vec=c(1,2,3), bool=TRUE )
</code></pre>

<p>Say we would like to extract the value stored by bool from foo and use it inside an <code>if()</code> statement. This will illustrate the differences between the return values of <code>[]</code> and <code>[[]]</code> when they are used for data extraction. The <code>[]</code> method returns objects of class list (or data.frame if foo was a data.frame) while the <code>[[]]</code> method returns objects whose class is determined by the type of their values. </p>

<p>So, using the <code>[]</code> method results in the following:</p>

<pre><code>if( foo[ 'bool' ] ){ print(""Hi!"") }
Error in if (foo[""bool""]) { : argument is not interpretable as logical

class( foo[ 'bool' ] )
[1] ""list""
</code></pre>

<p>This is because the <code>[]</code> method returned a list and a list is not valid object to pass directly into an <code>if()</code> statement. In this case we need to use <code>[[]]</code> because it will return the ""bare"" object stored in 'bool' which will have the appropriate class:</p>

<pre><code>if( foo[[ 'bool' ]] ){ print(""Hi!"") }
[1] ""Hi!""

class( foo[[ 'bool' ]] )
[1] ""logical""
</code></pre>

<p>The second difference is that the <code>[]</code> operator may be used to access a <strong>range</strong> of slots in a list or columns in a data frame while the <code>[[]]</code> operator is limited to accessing a <strong>single</strong> slot or column. Consider the case of value assignment using a second list, <code>bar()</code>:</p>

<pre><code>bar &lt;- list( mat=matrix(0,nrow=2,ncol=2), rand=rnorm(1) )
</code></pre>

<p>Say we want to overwrite the last two slots of foo with the data contained in bar. If we try to use the <code>[[]]</code> operator, this is what happens:</p>

<pre><code>foo[[ 2:3 ]] &lt;- bar
Error in foo[[2:3]] &lt;- bar : 
more elements supplied than there are to replace
</code></pre>

<p>This is because <code>[[]]</code> is limited to accessing a single element. We need to use <code>[]</code>:</p>

<pre><code>foo[ 2:3 ] &lt;- bar
print( foo )

$str
[1] ""R""

$vec
     [,1] [,2]
[1,]    0    0
[2,]    0    0

$bool
[1] -0.6291121
</code></pre>

<p>Note that while the assignment was successful, the slots in foo kept their original names.</p>
"
1169552,18484,2009-07-23T04:04:24Z,1169373,3,FALSE,"<p>My impression is that multiple forms of <code>gc()</code> are tried before R reports failed memory allocation.  I'm not aware of a solution for this at present, other than restarting R as you suggest.  It appears that R does not defragment memory.</p>
"
1169585,84458,2009-07-23T04:19:53Z,1169573,2,TRUE,"<p>For loops in R are notoriously slow, but here there's another issue.  It's much faster to preallocate the results vector, res, rather append to res at each iteration.</p>

<p>Below we can compare the speed of the above version with a version that simply starts with a vector, res, of length N and changes the ith element during the loop.</p>

<pre><code>fn1 &lt;- function(N) {
  res &lt;- c()
  for (i in 1:N) {
     x &lt;- rnorm(2)
     res &lt;- c(res,x[2]-x[1])
  }
  res
}
fn2 &lt;- function(N) {
  res &lt;- rep(0,N)
  for (i in 1:N) {
     x &lt;- rnorm(2)
     res[i] &lt;- x[2]-x[1]
  }
  res
}
&gt; N &lt;- 50000
&gt; system.time(res1 &lt;- fn1(N))
   user  system elapsed 
  6.568   0.256   6.826 
&gt; system.time(res2 &lt;- fn2(N))
   user  system elapsed 
  0.452   0.004   0.496 
</code></pre>

<p>Also, as <a href=""https://stackoverflow.com/questions/1169573/large-loops-hang-in-r/1169607#1169607"">Sharpie points out</a>, we can make this slightly faster by using R functions like <code>apply</code> (or its relatives, <code>sapply</code> and <code>lapply</code>).</p>

<pre><code>fn3 &lt;- function(N) {
  sapply( 1:N, function( i ){ x &lt;- rnorm(2); return( x[2] - x[1] ) } )
}
&gt; system.time(res3 &lt;- fn3(N))
   user  system elapsed 
  0.397   0.004   0.397 
</code></pre>
"
1169594,143377,2009-07-23T04:22:58Z,1169539,6,FALSE,"<pre><code>## make fake data
&gt; ngroups &lt;- 2
&gt; group &lt;- 1:ngroups
&gt; nobs &lt;- 100
&gt; dta &lt;- data.frame(group=rep(group,each=nobs),y=rnorm(nobs*ngroups),x=runif(nobs*ngroups))
&gt; head(dta)
  group          y         x
1     1  0.6482007 0.5429575
2     1 -0.4637118 0.7052843
3     1 -0.5129840 0.7312955
4     1 -0.6612649 0.9028034
5     1 -0.5197448 0.1661308
6     1  0.4240346 0.8944253
&gt; 
&gt; ## function to extract the results of one model
&gt; foo &lt;- function(z) {
+   ## coef and se in a data frame
+   mr &lt;- data.frame(coef(summary(lm(y~x,data=z))))
+   ## put row names (predictors/indep variables)
+   mr$predictor &lt;- rownames(mr)
+   mr
+ }
&gt; ## see that it works
&gt; foo(subset(dta,group==1))
              Estimate Std..Error   t.value  Pr...t..   predictor
(Intercept)  0.2176477  0.1919140  1.134090 0.2595235 (Intercept)
x           -0.3669890  0.3321875 -1.104765 0.2719666           x
&gt; ## one option: use command by
&gt; res &lt;- by(dta,dta$group,foo)
&gt; res
dta$group: 1
              Estimate Std..Error   t.value  Pr...t..   predictor
(Intercept)  0.2176477  0.1919140  1.134090 0.2595235 (Intercept)
x           -0.3669890  0.3321875 -1.104765 0.2719666           x
------------------------------------------------------------ 
dta$group: 2
               Estimate Std..Error    t.value  Pr...t..   predictor
(Intercept) -0.04039422  0.1682335 -0.2401081 0.8107480 (Intercept)
x            0.06286456  0.3020321  0.2081387 0.8355526           x
&gt; ## using package plyr is better
&gt; library(plyr)
&gt; res &lt;- ddply(dta,""group"",foo)
&gt; res
  group    Estimate Std..Error    t.value  Pr...t..   predictor
1     1  0.21764767  0.1919140  1.1340897 0.2595235 (Intercept)
2     1 -0.36698898  0.3321875 -1.1047647 0.2719666           x
3     2 -0.04039422  0.1682335 -0.2401081 0.8107480 (Intercept)
4     2  0.06286456  0.3020321  0.2081387 0.8355526           x
&gt; 
</code></pre>
"
1169601,143377,2009-07-23T04:26:33Z,1169551,11,TRUE,"<p>look at the package sqldf. <a href=""http://code.google.com/p/sqldf/"" rel=""noreferrer"">http://code.google.com/p/sqldf/</a> It seems perfect for your needs.</p>
"
1169607,135870,2009-07-23T04:27:32Z,1169573,9,FALSE,"<p>The efficiency of loops can be increased tremendously in R through the use of the apply functions which essentially process whole vectors of data at once rather than looping through them. For the loop shown above, there are two basic operations happening during each iteration:</p>

<pre><code># A vector of two random numbers is generated
x &lt;- rnorm( 2 )

# The difference between those numbers is calculated
x[2] - x[1]
</code></pre>

<p>In this case the appropriate function would be <code>sapply()</code>. <code>sapply()</code> operates on a list of objects, such as the vector generated by the loop statement <code>1:N</code> and returns a vector of results:</p>

<pre><code>sapply( 1:N, function( i ){ x &lt;- rnorm(2); return( x[2] - x[1] ) } )
</code></pre>

<p>Note that the index value <code>i</code> is available during the function call and successively takes on the values between <code>1</code> and <code>N</code>, however it is not needed in this case.</p>

<p>Getting into the habit of recognizing where <code>apply</code> can be used over <code>for</code> is a very valuable skill- many R libraries for parallel computation provide plug-and-play parallelization through <code>apply</code> functions. Using <code>apply</code> can often allow access to significant performance increases on multicore systems with <strong>zero</strong> refactoring of code.</p>
"
1169615,2611,2009-07-23T04:30:39Z,1169426,4,FALSE,"<p>Here's how to make a network plot of the data in <a href=""http://igraph.sourceforge.net/"" rel=""nofollow noreferrer"">igraph</a>:</p>

<pre><code>d &lt;- data.frame(p1=c('a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd'),
                p2=c('b', 'c', 'd', 'c', 'd', 'e', 'd', 'e', 'e'),
                counts=c(100, 200, 100,80, 90,100, 100,40,60))

library(igraph)
g &lt;- graph.data.frame(d, directed=TRUE)
print(g, e=TRUE, v=TRUE)
tkplot(g, vertex.label=V(g)$name)
</code></pre>
"
1169672,2611,2009-07-23T04:55:41Z,1169539,30,TRUE,"<p>Here's one way using the <code>lme4</code> package.</p>

<pre><code>&gt; library(lme4)
&gt; d &lt;- data.frame(state=rep(c('NY', 'CA'), c(10, 10)),
+                 year=rep(1:10, 2),
+                 response=c(rnorm(10), rnorm(10)))

&gt; xyplot(response ~ year, groups=state, data=d, type='l')

&gt; fits &lt;- lmList(response ~ year | state, data=d)
&gt; fits
Call: lmList(formula = response ~ year | state, data = d)
Coefficients:
   (Intercept)        year
CA -1.34420990  0.17139963
NY  0.00196176 -0.01852429

Degrees of freedom: 20 total; 16 residual
Residual standard error: 0.8201316
</code></pre>
"
1169724,80741,2009-07-23T05:15:11Z,1169534,2,FALSE,"<p>Does the problem come about when you're just using a global variable in a function or when you try to assign the variable? If it's the latter I suspect it's because you're not using <code>&lt;&lt;-</code> as an assignment within the function. And while using <code>&lt;&lt;-</code> appears to be the dark side <a href=""http://tolstoy.newcastle.edu.au/R/help/06/01/19773.html"" rel=""nofollow noreferrer"">1</a> it may very well work for your purposes. If it is the former, the function is probably masking the global variable.</p>

<p>Naming global variables in a manner that it would be difficult to mask them locally might help. e.g.: <code>global.pimultiples &lt;- 1:4*pi</code></p>
"
1170217,142892,2009-07-23T07:34:45Z,1169388,0,FALSE,"<p>Alternatively, if you do not need to use the indices but just the elements you can do</p>

<pre><code>&gt; x &lt;- sample(1:10,20,replace=TRUE)
&gt; x
 [1]  6  4  7  2  9  3  3  5  4  7  2  1  4  9  1  6 10  4  3 10
&gt; x[8&lt;=x &amp; x&lt;=9]
[1] 9 9
</code></pre>
"
1170484,134830,2009-07-23T08:49:28Z,1169534,8,FALSE,"<p>There's a reason that some languages don't allow global variables: they can easily lead to broken code.</p>

<p>The scoping rules in R allow you to write code in a lazy fashion - letting functions use variables in other environments can save you some typing, and it's great for playing around in simple cases.</p>

<p>If you are doing anything remotely complicated however, then I recommend that you pass a function all the variables that it needs (or at the very least, have some thorough sanity checking in place to have a fallback in case the variables don't exist).</p>

<p>In the example above:</p>

<p>The best practise is to use fn1.</p>

<p>Alternatively, try something like</p>

<pre><code> fn3 &lt;- function(x)
   {
      if(!exists(""a"", envir=.GlobalEnv))
      {
         warning(""Variable 'a' does not exist in the global environment"")
         a &lt;- 1
      }

      if(!exists(""b"", envir=.GlobalEnv))
      {
         warning(""Variable 'b' does not exist in the global environment"")
         b &lt;- 2
      }

      x + a + b
   }
</code></pre>
"
1171222,134830,2009-07-23T11:46:06Z,1169373,7,FALSE,"<p>On Windows, the technique you describe works for me.  Try the following example.</p>

<p>Open the Windows Task Manager (CTRL+SHIFT+ESC).</p>

<p>Start RGui. RGui.exe mem usage is 27 460K.</p>

<p>Type</p>

<pre><code>gcinfo(TRUE)
x &lt;- rnorm(1e8)
</code></pre>

<p>RGui.exe mem usage is now 811 100K.</p>

<p>Type <code>rm(""x"")</code>. RGui.exe mem usage is still 811 100K.</p>

<p>Type <code>gc()</code>.  RGui.exe mem usage is now 28 332K.</p>

<p>Note that gc shoud be called automatically if you have removed objects from your workspace, and then you try to allocate more memory to new variables.</p>
"
1171599,NA,2009-07-23T13:10:53Z,1169388,5,FALSE,"<p>You could try the <code>|</code> operator for short conditions</p>

<pre><code>which(x == 8 | x == 9)
</code></pre>
"
1171653,143305,2009-07-23T13:18:21Z,750703,2,FALSE,"<p>You can use <a href=""http://dirk.eddelbuettel.com/code/littler.html"" rel=""nofollow noreferrer"" title=""littler"">littler</a> instead which is a) an easier way to write R 'scripts' and b) suppresses output so you get the side effect of dev.off being silent:</p>

<pre><code>$ foo.r /tmp/foo.txt /tmp/foo.png
Plotting /tmp/foo.txt to /tmp/foo.png
$ cat /tmp/foo.r
#!/usr/bin/r
cat(""Plotting"", argv[1], ""to"", argv[2], ""\n"")
input &lt;- read.table(argv[1])
png(argv[2])
plot(as.numeric(input[1,]))
dev.off()
$
</code></pre>

<p>Rscript will probably work too; I tend to prefer littler.</p>
"
1171697,143305,2009-07-23T13:24:46Z,1169330,1,FALSE,"<ul>
<li>It's an empirical question, so why don't measure it for the combination you are interested in?</li>
<li>Public code is not hidden, so why don't you count what other DB interfaces CRAN has?  For DBI alone, we have SQLite,  MySQL, Postgresql, Oracle; for custom db backends there are things like Vhayu. </li>
<li>Specialised forums exist, so why don't you ask on r-sig-db?</li>
<li>Lastly, as soon as there is an API and a need people tend to combine the two.  I have written two different (at-work and hence unreleased) packages to two highly specialised and fast backends.  </li>
</ul>
"
1172305,136862,2009-07-23T14:49:18Z,1169551,10,FALSE,"<p>I'm also more comfortable with SQL, but when working with large data sets in R, my favourite manipulation tool is the <code>data.table</code> package. Unlike <code>sqldf</code>, which lets you write SQL in R, <code>data.table</code> lets you write R in R - but gives you the ability to add indexes on data frames (well, <code>data.table</code>s, to be precise). The ability to index data frames makes 'joins' much much much faster. And being an R implementation, your code still looks like R.</p>
"
1172367,136862,2009-07-23T14:57:11Z,1169376,9,TRUE,"<p>While zoo is great, sometimes there are simpler ways. If you data behaves nicely, and is evenly spaced, the embed() function effectively lets you create multiple lagged version of a time series. If you look inside the VARS package for vector auto-regression, you will see that the package author chooses this route.</p>

<p>For example, to calculate the 3 period rolling average of x, where x = (1 -> 20)^2:</p>

<pre><code>&gt; x &lt;- (1:20)^2
&gt; embed (x, 3)
      [,1] [,2] [,3]
 [1,]    9    4    1
 [2,]   16    9    4
 [3,]   25   16    9
 [4,]   36   25   16
 [5,]   49   36   25
 [6,]   64   49   36
 [7,]   81   64   49
 [8,]  100   81   64
 [9,]  121  100   81
[10,]  144  121  100
[11,]  169  144  121
[12,]  196  169  144
[13,]  225  196  169
[14,]  256  225  196
[15,]  289  256  225
[16,]  324  289  256
[17,]  361  324  289
[18,]  400  361  324
&gt; apply (embed (x, 3), 1, mean)
 [1]   4.666667   9.666667  16.666667  25.666667  36.666667  49.666667
 [7]  64.666667  81.666667 100.666667 121.666667 144.666667 169.666667
[13] 196.666667 225.666667 256.666667 289.666667 324.666667 361.666667
</code></pre>
"
1172507,106979,2009-07-23T15:17:44Z,1172485,0,FALSE,"<p>You can use the TK gui, I think the option was <code>--ui=TK</code> or something like this.</p>

<p>Or is it a hard requirement to use it in the terminal?</p>
"
1172818,143305,2009-07-23T16:05:32Z,1172485,33,FALSE,"<p>Set it with something like</p>

<pre><code>options(""width""=200)
</code></pre>

<p>which is in fact what I have in ~/.Rprofile.  See help(options) for details.</p>
"
1172884,136862,2009-07-23T16:17:41Z,1172485,14,FALSE,"<p>Stealing an idea from Brendan O'Connor's util.R (<a href=""http://github.com/brendano/dlanalysis/blob/master/util.R"" rel=""noreferrer"">http://github.com/brendano/dlanalysis/blob/master/util.R</a>), you can get your R terminal to set the default width using the stty command. Remunging his script to work on linux, you get the following sexy 1 liner:</p>

<pre><code>options(width=as.integer(system(""stty -a | head -n 1 | awk '{print $7}' | sed 's/;//'"", intern=T)))
</code></pre>
"
1173161,83761,2009-07-23T17:04:13Z,1172485,43,TRUE,"<p>Here is a function I have in my <code>~/.Rprofile</code> file:</p>

<pre><code>wideScreen &lt;- function(howWide=Sys.getenv(""COLUMNS"")) {
  options(width=as.integer(howWide))
}
</code></pre>

<p>Calling the function without the <code>howWide</code> argument sets the column to be the width of your terminal. You can optionally pass in the argument to set the width to an arbitrary number of your choosing.</p>

<p>Almost like Josh's suggestion, but less magic :-)</p>
"
1174826,143305,2009-07-23T22:24:39Z,1174799,100,TRUE,"<p>See <code>help(Sys.sleep)</code>.</p>

<p>For example, from <code>?Sys.sleep</code></p>

<pre><code>testit &lt;- function(x)
{
    p1 &lt;- proc.time()
    Sys.sleep(x)
    proc.time() - p1 # The cpu usage should be negligible
}
testit(3.7)
</code></pre>

<p>Yielding</p>

<pre><code>&gt; testit(3.7)
   user  system elapsed 
  0.000   0.000   3.704 
</code></pre>
"
1175345,143813,2009-07-24T01:13:09Z,1169534,0,FALSE,"<p>Usage of global variables is general discouraged in most languages, and R is no exception. Very often short function use short and generic variable names, which could be populated in the global environment. It is safest to a) include all the variables in the function definition b) <em>not</em> to assign default values. E.g., write f=function(a,b), rather f=function(a=0,b=NA).</p>
"
1176240,144297,2009-07-24T07:31:41Z,1169573,2,FALSE,"<p>Sometimes loop is not needed. Since rnorm gives iid sample (theoretically), you will achieve the same result (sampling 
<code>X-Y</code>  where X and Y are N(0,1)) by doing:</p>

<pre><code>res &lt;- rnorm(N)-rnorm(N)
</code></pre>
"
1176337,144297,2009-07-24T08:06:03Z,1105659,13,FALSE,"<p>List elements in R can be named.  So in your case just do</p>

<pre><code> &gt; mylist = list()
 &gt; mylist$width = value
</code></pre>

<p>When R encounters this code </p>

<pre><code>&gt; l$somename=something
</code></pre>

<p>where l is a list. It appends to a list an element something, and names it with name somename. It is then can be accessed by using</p>

<pre><code>&gt; l[[""somename""]]
</code></pre>

<p>or </p>

<pre><code>&gt; l$somename
</code></pre>

<p>The name can be changed with command names:</p>

<pre><code>&gt; names(l)[names(l)==""somename""] &lt;- ""othername""
</code></pre>

<p>Or if you now the position of the element in the list by:</p>

<pre><code>&gt; names(l)[1] &lt;- ""someothername""
</code></pre>
"
1176813,4892,2009-07-24T10:19:41Z,1176455,0,TRUE,"<p>You need to look at the manual page for <code>library.dynam()</code>. It should allow you to do what you want, eg.</p>

<pre><code>function(mydata)
{
library.dynam(""mysharedobject"",package=c(""mypkg"")) 
try(
        output &lt;- .C(""myfunc_cversion"",
                     in_data    = as.double(mydata),
                     res_data   = as.double(res),
                     PACKAGE    = ""mypkg"")
        )
        result &lt;- as.matrix(output$res_data)
        return(result)
}
</code></pre>

<p>where <code>mysharedobject</code> is the name of the shared object file without .dll/.so etc on the end.</p>

<p>The man page also recommends that you only use it in the <code>.onLoad()</code> or <code>.First.lib()</code> functions.</p>

<p>HTH</p>

<hr>

<p><a href=""http://sekhon.berkeley.edu/base/html/library.dynam.html"" rel=""nofollow noreferrer"">http://sekhon.berkeley.edu/base/html/library.dynam.html</a></p>
"
1177086,142651,2009-07-24T11:32:35Z,1169539,19,FALSE,"<p>In my opinion is a mixed linear model a better approach for this kind of data. The code below given in the fixed effect the overall trend. The random effects indicate how the trend for each individual state differ from the global trend. The correlation structure takes the temporal autocorrelation into account. Have a look at Pinheiro &amp; Bates (Mixed Effects Models in S and S-Plus).</p>

<pre><code>library(nlme)
lme(response ~ year, random = ~year|state, correlation = corAR1(~year))
</code></pre>
"
1177101,143305,2009-07-24T11:35:47Z,1176455,2,FALSE,"<p>I think you are making it too complicated.  You could always consult some of the existing 1800+ packages on <a href=""http://cran.r-project.org"" rel=""nofollow noreferrer"">CRAN</a>.  As a general rule, most packages load the object code on startup via .onLoad (and that can even be automated via the NAMESPACE file) --- see the R Extensions manual.</p>

<p>As a simple example, you could look at my <a href=""http://cran.r-project.org/web/packages/digest/index.html"" rel=""nofollow noreferrer"">digest</a> package, it use the following from a file R/zzz.R (which is the standard approach suggested in the manual)</p>

<pre><code>.onLoad &lt;- function(lib, pkg) {
   library.dynam(""digest"", pkg, lib )
}
</code></pre>

<p>So after package load all functions from the dynamic library are available to all R functions for calling.  That is more general than adding a library.dynam() to each function (and on top you only need library.dynam() once per R session anyway).</p>

<p>R itself deals with extensions (.dll, .so, .dylib, ...) and all other per-platform nitty gritty.  I see no reason to divert from that approach.  So see the manual, and the (literally) hundreds of published packages that do this.</p>

<hr>
"
1177117,4907,2009-07-24T11:38:34Z,1176455,2,FALSE,"<p>I discovered that if I use a name space in my package then I can also solve this problem by using the <code>useDynLib</code> directive inside the NAMESPACE file for the package (as described in section 1.6.3 of the manual ""Writing R Extensions"" version 2.9.1 at <a href=""http://www.r-project.org"" rel=""nofollow noreferrer"">www.r-project.org</a>).</p>

<p>My NAMESPACE file now looks like:</p>

<pre><code>useDynLib(mypkg, myfunc_cversion)
export(myfunc)
</code></pre>

<p>I then modify the R function definition to:</p>

<pre><code>myfunc &lt;- function(mydata)
{ 
try(
    output &lt;- .C(myfunc_cversion,
                 in_data  = as.double(mydata),
                 res_data = as.double(res) )
    )
    result &lt;- as.matrix(output$res_data)
    return(result)
}
</code></pre>

<p>That is, with no quotes around the C function name and without the <code>PACKAGE</code> argument in the <code>.C</code> call.</p>
"
1178066,143305,2009-07-24T14:28:22Z,1177919,17,TRUE,"<p>1) Testing for existence:  Use %in% on the colnames, e.g. </p>

<pre><code>&gt; example(data.frame)    # to get 'd'
&gt; ""fac"" %in% colnames(d)
[1] TRUE
&gt; ""bar"" %in% colnames(d)
[1] FALSE
</code></pre>

<p>2) You essentially have to create a new data.frame from the first half of the old, your new column, and the second half:</p>

<pre><code>&gt; bar &lt;- data.frame(d[1:3,1:2], LastName=c(""Flim"", ""Flom"", ""Flam""), fac=d[1:3,3])
&gt; bar
  x y LastName fac
1 1 1     Flim   C
2 1 2     Flom   A
3 1 3     Flam   A
&gt; 
</code></pre>
"
1178084,143305,2009-07-24T14:31:39Z,1177926,10,FALSE,"<p>If I get 'someObject', say via</p>

<pre><code>someObject &lt;- myMagicFunction(...)
</code></pre>

<p>then I usually proceed by</p>

<pre><code>class(someObject)
str(someObject)
</code></pre>

<p>which can be followed by head(), summary(), print(), ... depending on the class you have.</p>
"
1178110,136862,2009-07-24T14:36:31Z,1177926,5,FALSE,"<pre><code>attributes(someObject) 
</code></pre>

<p>Can also be useful</p>
"
1178261,2611,2009-07-24T15:00:44Z,1177926,96,TRUE,"<p>I usually start out with some combination of:</p>

<pre><code>typeof(obj)
class(obj)
sapply(obj, class)
sapply(obj, attributes)
attributes(obj)
names(obj)
</code></pre>

<p>as appropriate based on what's revealed.  For example, try with:</p>

<pre><code>obj &lt;- data.frame(a=1:26, b=letters)
obj &lt;- list(a=1:26, b=letters, c=list(d=1:26, e=letters))
data(cars)
obj &lt;- lm(dist ~ speed, data=cars)
</code></pre>

<p>..etc.</p>

<p>If <code>obj</code> is an S3 or S4 object, you can also try <code>methods</code> or <code>showMethods</code>, <code>showClass</code>, etc.  Patrick Burns' <a href=""http://www.burns-stat.com/pages/Tutor/R_inferno.pdf"" rel=""noreferrer"">R Inferno</a> has a pretty good section on this (sec #7).</p>

<p><strong>EDIT</strong>: Dirk and Hadley mention <code>str(obj)</code> in their answers.  It really is much better than any of the above for a quick and even detailed peek into an object.</p>
"
1179452,142477,2009-07-24T18:47:00Z,855798,2,FALSE,"<p>This is a combination of the above answers (as suggested by Thierry)</p>

<pre><code>data.frame(table(myList[,1]))
</code></pre>

<p>which gives you</p>

<pre><code>  Var1 Freq
1  Bob    3
2  Joe    1
3 Mary    1
</code></pre>
"
1181038,60067,2009-07-25T02:43:09Z,1181025,7,FALSE,"<p>The <code>nls()</code> function (<a href=""http://sekhon.berkeley.edu/stats/html/nls.html"" rel=""noreferrer"">http://sekhon.berkeley.edu/stats/html/nls.html</a>) is pretty standard for nonlinear least-squares curve fitting. Chi squared (the sum of the squared residuals) is the metric that is optimized in that case, but it is not normalized so you can't readily use it to determine how good the fit is. The main thing you should ensure is that your residuals are normally distributed. Unfortunately I'm not sure of an automated way to do that.</p>
"
1181051,143813,2009-07-25T02:47:38Z,1181021,4,FALSE,"<p>You can implement the full algorithm to check if the matrix reduces to a Jordan form or a diagonal one (see e.g., <a href=""http://www.sosmath.com/matrix/diagonal/diagonal.html"" rel=""nofollow noreferrer"">this document</a>). Or you can take the quick and dirty way: for an n-dimensional square matrix, use eigen(M)$values and check that they are n distinct values. For random matrices, this always suffices: degeneracy has prob.0.</p>

<p>P.S.: based on a simple observation by JD Long below, I recalled that a necessary and sufficient condition for diagonalizability is that the eigenvectors span the original space. To check this, just see that eigenvector matrix has full rank (no zero eigenvalue). So here is the code:</p>

<pre><code>diagflag = function(m,tol=1e-10){
    x = eigen(m)$vectors
    y = min(abs(eigen(x)$values))
    return(y&gt;tol)
}
# nondiagonalizable matrix 
m1 = matrix(c(1,1,0,1),nrow=2) 
# diagonalizable matrix
m2 = matrix(c(-1,1,0,1),nrow=2) 

&gt; m1
     [,1] [,2]
[1,]    1    0
[2,]    1    1

&gt; diagflag(m1)
[1] FALSE

&gt; m2
     [,1] [,2]
[1,]   -1    0
[2,]    1    1

&gt; diagflag(m2)
[1] TRUE
</code></pre>
"
1181098,143305,2009-07-25T03:18:41Z,1181025,24,TRUE,"<p>Just the first part of that question can fill entire books. Just some quick choices:</p>

<ul>
<li><code>lm()</code> for standard linear models</li>
<li><code>glm()</code> for generalised linear models (eg for logistic regression)</li>
<li><code>rlm()</code> from package MASS for robust linear models</li>
<li><code>lmrob()</code> from package robustbase for robust linear models</li>
<li><code>loess()</code> for non-linear / non-parametric models</li>
</ul>

<p>Then there are domain-specific models as e.g. time series, micro-econometrics, mixed-effects and much more.  Several of the Task Views as e.g.  <a href=""http://cran.r-project.org/web/views/Econometrics.html"" rel=""noreferrer"">Econometrics</a> discuss this in more detail.  As for goodness of fit, that is also something one can spend easily an entire book discussing.</p>
"
1181180,2611,2009-07-25T04:15:55Z,1181021,2,FALSE,"<p>You might want to check out <a href=""http://zoonek2.free.fr/UNIX/48_R/02.html"" rel=""nofollow noreferrer"">this page</a> for some basic discussion and code.  You'll need to search for ""diagonalized"" which is where the relevant portion begins.  </p>
"
1181292,2611,2009-07-25T05:33:29Z,1181025,6,FALSE,"<p>The Quick R site has a reasonable good summary of basic functions used for fitting models and testing the fits, along with sample R code:</p>

<ul>
<li><a href=""http://www.statmethods.net/stats/regression.html"" rel=""noreferrer"">http://www.statmethods.net/stats/regression.html</a></li>
</ul>
"
1182530,143813,2009-07-25T17:12:58Z,1181025,10,FALSE,"<p>The workhorses of canonical curve fitting in R are <code>lm()</code>, <code>glm()</code> and <code>nls()</code>. To me, goodness-of-fit is a subproblem in the larger problem of model selection. Infact, using goodness-of-fit incorrectly (e.g., via stepwise regression) can give rise to seriously misspecified model (see Harrell's book on ""Regression Modeling Strategies""). Rather than discussing the issue from scratch, I recommend Harrell's book for <code>lm</code> and <code>glm</code>. Venables and Ripley's bible is terse, but still worth a reading. ""Extending the Linear Model with R"" by Faraway is comprehensive and readable. nls is not covered in these sources, but ""Nonlinear Regression with R"" by Ritz &amp; Streibig fills the gap and is very hands-on.</p>
"
1183170,143305,2009-07-25T21:56:08Z,1182932,0,FALSE,"<p>I am not sure if your example makes sense.  For the basic grep(), pattern is often a simple or a regular expression, and x is a vector whose element get matched to pattern.  Having pattern as longer string that x strikes me as odd.</p>

<p>Consider this where we just use grep instead of substr:</p>

<pre><code>R&gt; grep(""vo"", c(""foo"",""bar"",""baz""))   # vo is not in the vector
integer(0)
R&gt; agrep(""vo"", c(""foo"",""bar"",""baz""), value=TRUE) # but is close enough to foo
[1] ""foo""
R&gt; agrep(""vo"", c(""foo"",""bar"",""baz""), value=TRUE, max.dist=0.25) # still foo
[1] ""foo""
R&gt; agrep(""vo"", c(""foo"",""bar"",""baz""), value=TRUE, max.dist=0.75) # now all match
[1] ""foo"" ""bar"" ""baz""
R&gt;  
</code></pre>
"
1183240,143377,2009-07-25T22:32:32Z,1182932,2,TRUE,"<p>I posted this on the R list a while back and reported as a bug in R-bugs-list. I had no useful responses, so I twitted to see if the bug was reproducible or I was just missing something. JD Long was able to reproduce it and kindly posted the question here.</p>

<p>Note that, at least in R, then, agrep is a misnomer since it does <em>not</em> matches regular expressions, while grep stands for ""Globally search for the Regular Expression and Print"". It shouldn't have a problem with patterns longer than the target vector. (i think!)</p>

<p>In my linux server, all is well  but not so in my Mac and Windows machines.</p>

<p>Mac:
sessionInfo()
R version 2.9.1 (2009-06-26) 
i386-apple-darwin8.11.1 
locale:
en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8</p>

<p>agrep(pattern,x,max.distance=30) 
[1] 1</p>

<blockquote>
  <p>agrep(pattern,x,max.distance=31)
  integer(0)
  agrep(pattern,x,max.distance=32) 
  integer(0)
  agrep(pattern,x,max.distance=33)
  [1] 1</p>
</blockquote>

<p>Linux:
R version 2.9.1 (2009-06-26) 
x86_64-unknown-linux-gnu </p>

<p>locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=C;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C</p>

<blockquote>
  <p>agrep(pattern,x,max.distance=30) 
  [1] 1
  agrep(pattern,x,max.distance=31)
  [1] 1
  agrep(pattern,x,max.distance=32) 
  [1] 1
  agrep(pattern,x,max.distance=33)
  [1] 1</p>
</blockquote>
"
1183426,144278,2009-07-26T00:28:12Z,1181025,3,FALSE,"<blockquote>
  <p>The main thing you should ensure is
  that your residuals are normally
  distributed. Unfortunately I'm not
  sure of an automated way to do that.</p>
</blockquote>

<p><code>qqnorm()</code> could probably be modified to find the correlation between the sample quantiles and the theoretical quantiles. Essentially, this would just be a numerical interpretation of the normal quantile plot. Perhaps providing several values of the correlation coefficient for different ranges of quantiles could be useful. For example, if the correlation coefficient is close to 1 for the middle 97% of the data and much lower at the tails, this tells us the distribution of residuals is approximately normal, with some funniness going on in the tails.</p>
"
1183488,37751,2009-07-26T01:14:56Z,1181021,7,TRUE,"<p>If you have a given matrix, m, then one way is the take the eigen vectors times the diagonal of the eigen values times the inverse of the original matrix. That should give us back the original matrix. In R that looks like:</p>

<pre><code>m &lt;- matrix( c(1:16), nrow = 4)
p &lt;- eigen(m)$vectors
d &lt;- diag(eigen(m)$values)
p %*% d %*% solve(p)
m
</code></pre>

<p>so in that example [p %<em>% d %</em>% solve(p)] should be the same as m</p>

<p>and yes, I totally stole that from the <a href=""http://wiki.r-project.org/rwiki/doku.php?id=guides:stats-with-r:02programming_in_r:03datastructures"" rel=""noreferrer"">R Wiki</a>. </p>
"
1183715,2611,2009-07-26T04:15:48Z,1169534,34,TRUE,"<p>If I know that I'm going to need a function parametrized by some values and called repeatedly, I avoid globals by using a closure:</p>

<pre><code>make.fn2 &lt;- function(a, b) {
    fn2 &lt;- function(x) {
        return( x + a + b )
    }
    return( fn2 )
}

a &lt;- 2; b &lt;- 3
fn2.1 &lt;- make.fn2(a, b)
fn2.1(3)    # 8
fn2.1(4)    # 9

a &lt;- 4
fn2.2 &lt;- make.fn2(a, b)
fn2.2(3)    # 10
fn2.1(3)    # 8
</code></pre>

<p>This neatly avoids referencing global variables, instead using the enclosing environment of the function for a and b.  Modification of globals a and b doesn't lead to unintended side effects when fn2 instances are called.</p>
"
1183739,2611,2009-07-26T04:34:14Z,1169573,4,FALSE,"<p>Expanding on my comment to chris_dubois's answer, here's some timing information:</p>

<pre><code>&gt; system.time(res &lt;- rnorm(50000) - rnorm(50000))
user  system elapsed
0.06    0.00    0.06
</code></pre>

<p>Contrast this with fn3 from that same answer:</p>

<pre><code>&gt; system.time(res3 &lt;- fn3(50000))
user  system elapsed
1.33    0.01    1.36
</code></pre>

<p>The first thing to notice is that my laptop is slower than chris_dubois's machine. :)</p>

<p>The second, and more important, point is that the vector approach, quite applicable here, is an order of magnitude faster.  (Also pointed out by Richie Cotton in a comment to the same answer).</p>

<p>This brings me to the final point: it is a <strong>myth</strong> that <code>apply</code> and its friends are much faster than <code>for</code> loops in R.  They're on the same order in most measurements I've seen.  Because they're just <code>for</code> loops behind the scenes.  See also this post:</p>

<blockquote>
  <p><a href=""http://yusung.blogspot.com/2008/04/speed-issue-in-r-computing-apply-vs.html"" rel=""nofollow noreferrer"">http://yusung.blogspot.com/2008/04/speed-issue-in-r-computing-apply-vs.html</a></p>
  
  <p>According to Professor Brian Ripley, ""apply() is just a wrapper for a loop."" The only advantage for using apply() is that it makes your code neater!</p>
</blockquote>

<p>Exactly.  You should use <code>apply</code> if it's more <em>expressive</em>, especially if you're programming in a functional style.  Not because it's faster.</p>
"
1184010,143591,2009-07-26T08:01:10Z,1177919,1,FALSE,"<p>or using cbind:</p>

<pre><code>&gt; example(data.frame)    # to get 'd'
&gt; bar &lt;- cbind(d[1:3,1:2],LastName=c(""Flim"", ""Flom"", ""Flam""),fac=d[1:3,3])

&gt; bar
  x y LastName fac
1 1 1     Flim   A
2 1 2     Flom   B
3 1 3     Flam   B
</code></pre>
"
1184311,143591,2009-07-26T11:18:07Z,1142294,12,FALSE,"<p>Alternatively, you can use the <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"" rel=""noreferrer"">kernlab</a> package:</p>

<pre><code>library(kernlab)

model.ksvm = ksvm(happy ~ day + weather, data = d, type=""C-svc"")
plot(model.ksvm, data=d)
</code></pre>
"
1184679,16632,2009-07-26T14:08:09Z,1177919,23,FALSE,"<p>One approach is to just add the column to the end of the data frame, and then use subsetting to move it into the desired position:</p>

<pre><code>d$LastName &lt;- c(""Flim"", ""Flom"", ""Flam"")
bar &lt;- d[c(""x"", ""y"", ""Lastname"", ""fac"")]
</code></pre>
"
1184691,16632,2009-07-26T14:13:18Z,1177926,44,FALSE,"<pre><code>str(x)
</code></pre>

<p>It's all you need to remember for 99% of cases.</p>
"
1185200,142477,2009-07-26T18:26:30Z,1169388,1,FALSE,"<p><code>grepl</code> maybe a useful function.  Note that <code>grepl</code> appears in versions of R 2.9.0 and later.  What's handy about <code>grepl</code> is that it returns a logical vector of the same length as <code>x</code>. </p>

<pre><code>grepl(8, x)
[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[13] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE

grepl(9, x)
[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE
[13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE
</code></pre>

<p>To arrive at your answer, you could do the following</p>

<pre><code>grepl(8,x) | grepl(9,x)
</code></pre>
"
1186511,76235,2009-07-27T05:04:34Z,1169426,0,FALSE,"<p>I've also been working in igraph. One way to create a graph is to write out a list of all ""from"" ""to"" nodes to a text file a read it back in as a graph object. The graph object can be subjected to many graph theoretic processes and can handle quite large networks.</p>
"
1188629,143305,2009-07-27T14:51:12Z,1188544,4,FALSE,"<p>I would do two things differently:</p>

<ol>
<li><p>Use R itself to dispatch nine different jobs; the <a href=""http://cran.r-project.org/web/packages/snow/"" rel=""nofollow noreferrer"">snow</a> package is very good at this even when do not use MPI / PVM / NWS for distributed work. Some examples for snow use are for example in my 'introduction to high performance computing with R' tutorials linked from <a href=""http://dirk.eddelbuettel.com/presentations.html"" rel=""nofollow noreferrer"">this page</a>.  With snow, you get 'parallel' versions of the apply functions that you can run over multiple instances of R running on the local computer (or of course a network of computers if have one).  The r-sig-hpc list is helpful for more detailed questions.</p></li>
<li><p>Switch to using Rscript.exe instead of using 'R CMD BATCH'.  On Linux / OS X you also get a choice of using littler</p></li>
</ol>

<p>That said, I run almost all my jobs on Linux so there may be a Windows-specific answer here too that I just do not know.  But the above is generic and stays in the platform-agnostic spirit of R.</p>
"
1188689,143813,2009-07-27T15:00:44Z,1169573,0,FALSE,"<p>Maybe the most efficient replacement for your function would simply be:</p>

<pre><code>fn &lt;- function(n) rnorm(N,0,sqrt(2))
</code></pre>

<p>which is twice as fast as taking difference of iid normal variates. More generally though, if your goal is to run simple simulations, vector/array preallocation and calls to native functions speed up the process greatly. </p>

<p>If you want to perform monte-carlo simulations for statistical estimations (e.g., MCMC), R has a number of native packages. For general stochastic simulation, I am not aware of R packages but you may want to try Simpy (<a href=""http://simpy.sourceforge.net/"" rel=""nofollow noreferrer"">http://simpy.sourceforge.net/</a>), which is excellent.</p>
"
1190707,143813,2009-07-27T21:23:39Z,359438,13,FALSE,"<p>R has many, many packages for optimization; check the CRAN Task view on Optimization: <a href=""http://cran.r-project.org/web/views/Optimization.html"" rel=""noreferrer"">http://cran.r-project.org/web/views/Optimization.html</a>. Of course, for nonlinear programs, there is <code>optim()</code>, which is standard and includes Broyden-Fletcher-Goldfarb-Shanno's algorithm, and Nelder-Mead. It's a good first start.</p>
"
1191708,2611,2009-07-28T02:55:08Z,1191689,13,FALSE,"<p>There's OpenBUGS and R helper packages.  Check out Gelman's site for his book, which has most of the relevant links:</p>

<ul>
<li><a href=""http://www.stat.columbia.edu/~gelman/software/"" rel=""noreferrer"">http://www.stat.columbia.edu/~gelman/software/</a></li>
<li><a href=""http://www.stat.columbia.edu/~gelman/bugsR/software.pdf"" rel=""noreferrer"">Example of computation in R and Bugs</a></li>
</ul>

<p>On the Python side, I only know of PyMC:</p>

<ul>
<li><a href=""http://code.google.com/p/pymc/"" rel=""noreferrer"">http://code.google.com/p/pymc/</a></li>
<li><a href=""http://pymc.googlecode.com/svn/doc/tutorial.html"" rel=""noreferrer"">An example statistical model</a></li>
</ul>

<p>EDIT:  Added a link to the appropriate appendix from Gelman's book, available online, for an example using R and BUGS.  </p>
"
1192044,76235,2009-07-28T05:17:52Z,1191689,9,TRUE,"<p>Here are four books on hierarchical modeling and bayesian analysis written with R code throughout the books.</p>

<p>Hierarchical Modeling and Analysis for Spatial Data (Monographs on Statistics and Applied Probability) (Hardcover)
<a href=""http://rads.stackoverflow.com/amzn/click/158488410X"" rel=""noreferrer"">http://www.amazon.com/gp/product/158488410X</a></p>

<p>Data Analysis Using Regression and Multilevel/Hierarchical Models (Paperback)
<a href=""http://rads.stackoverflow.com/amzn/click/052168689X"" rel=""noreferrer"">http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=pd_sim_b_1</a></p>

<p>Bayesian Computation with R (Use R) (Paperback)
<a href=""http://rads.stackoverflow.com/amzn/click/0387922970"" rel=""noreferrer"">http://www.amazon.com/Bayesian-Computation-R-Use/dp/0387922970/ref=pd_bxgy_b_img_c</a></p>

<p>Hierarchical Modelling for the Environmental Sciences: Statistical Methods and Applications (Oxford Biology) (Paperback) (I'm assuming this one has R code as both authors use R extensively)</p>

<p>I know some python books dabble in multivariate analysis (Collective Intelligence, for example) but I haven't seen any that really delve into bayesian or hierarchical modeling.</p>
"
1192066,76235,2009-07-28T05:26:44Z,1177919,2,FALSE,"<p>Of the many silly little helper functions I've written, this gets used every time I load R. It just makes a list of the column names and indices but I use it constantly. </p>

<pre><code>##creates an object from a data.frame listing the column names and location
namesind=function(df){

    temp1=names(df)
    temp2=seq(1,length(temp1))
    temp3=data.frame(temp1,temp2)
    names(temp3)=c(""VAR"",""COL"")
    return(temp3)
    rm(temp1,temp2,temp3)
}

ni &lt;- namesind
</code></pre>

<p>Use ni to see your column numbers. (ni is just an alias for namesind, I never use namesind but thought it was a better name originally) Then if you want insert your column in say, position 12, and your data.frame is named bob with 20 columns, it would be</p>

<p>bob2 &lt;- data.frame(bob[,1:11],newcolumn, bob[,12:20] </p>

<p>though I liked the add at the end and rearrange answer from Hadley as well.</p>
"
1192135,76235,2009-07-28T05:55:28Z,736514,5,FALSE,"<p>Interpretability is kinda tough with Random Forests. While RF is an extremely robust classifier it makes its predictions democratically. By this I mean you build hundreds or thousands of trees by taking a random subset of your variables and a random subset of your data and build a tree. Then make a prediction for all the non-selected data and save the prediction. Its robust because it deals well with the vagaries of your data set, (ie it smooths over randomly high/low values, fortuitous plots/samples, measuring the same thing 4 different ways, etc). However if you have some highly correlated variables, both may seem important as they are not both always included in each model. </p>

<p>One potential approach with random forests may be to help whittle down your predictors then switch to regular CART or try the PARTY package for inference based tree models. However then you must be wary about data mining issues, and making inferences about parameters.</p>
"
1192198,143476,2009-07-28T06:19:07Z,1177919,1,FALSE,"<p>I always thought something like append() [though unfortunate the name is] should be a generic function</p>

<pre><code>## redefine append() as generic function                                        
append.default &lt;- append
append &lt;- `body&lt;-`(args(append),value=quote(UseMethod(""append"")))
append.data.frame &lt;- function(x,values,after=length(x))
  `row.names&lt;-`(data.frame(append.default(x,values,after)),
                row.names(x))

## apply the function                                                           
d &lt;- (if( !""LastName"" %in% names(d) )
      append(d,values=list(LastName=c(""Flim"",""Flom"",""Flam"")),after=2) else d)
</code></pre>
"
1195935,143305,2009-07-28T18:37:13Z,1195826,30,FALSE,"<p>It is a known issue, and one possible remedy is provided by <code>drop.levels()</code> in the <a href=""http://cran.r-project.org/web/packages/gdata/index.html"" rel=""noreferrer"">gdata</a> package where your example becomes</p>

<pre><code>&gt; drop.levels(subdf)
  letters numbers
1       a       1
2       b       2
3       c       3
&gt; levels(drop.levels(subdf)$letters)
[1] ""a"" ""b"" ""c""
</code></pre>

<p>There is also the <code>dropUnusedLevels</code> function in the <a href=""http://cran.r-project.org/web/packages/Hmisc/index.html"" rel=""noreferrer"">Hmisc</a> package. However, it only works by altering the subset operator <code>[</code> and is not applicable here.</p>

<p>As a corollary, a direct approach on a per-column basis is a simple <code>as.factor(as.character(data))</code>:</p>

<pre><code>&gt; levels(subdf$letters)
[1] ""a"" ""b"" ""c"" ""d"" ""e""
&gt; subdf$letters &lt;- as.factor(as.character(subdf$letters))
&gt; levels(subdf$letters)
[1] ""a"" ""b"" ""c""
</code></pre>
"
1195996,143319,2009-07-28T18:44:32Z,1195826,7,FALSE,"<p>This is obnoxious.  This is how I usually do it, to avoid loading other packages:</p>

<pre><code>levels(subdf$letters)&lt;-c(""a"",""b"",""c"",NA,NA)
</code></pre>

<p>which gets you:</p>

<pre><code>&gt; subdf$letters
[1] a b c
Levels: a b c
</code></pre>

<p>Note that the new levels will replace whatever occupies their index in the old levels(subdf$letters), so something like:</p>

<pre><code>levels(subdf$letters)&lt;-c(NA,""a"",""c"",NA,""b"")
</code></pre>

<p>won't work.</p>

<p>This is obviously not ideal when you have lots of levels, but for a few, it's quick and easy.</p>
"
1196932,146704,2009-07-28T21:45:12Z,1191689,3,FALSE,"<p>There are a few hierarchical models in <a href=""http://mcmcpack.wustl.edu"" rel=""nofollow noreferrer"">MCMCpack</a> for R, which to my knowledge is the fastest sampler for many common model types. (I wrote the [hierarchical item response][2] model in it.)</p>

<p>[RJAGS][3] does what its name sounds like. Code up a jags-flavored .bug model, provide data in R, and call Jags from R.</p>
"
1197154,143476,2009-07-28T22:41:31Z,1195826,292,TRUE,"<p>All you should have to do is to apply factor() to your variable again after subsetting:</p>

<pre><code>&gt; subdf$letters
[1] a b c
Levels: a b c d e
subdf$letters &lt;- factor(subdf$letters)
&gt; subdf$letters
[1] a b c
Levels: a b c
</code></pre>

<p><strong>EDIT</strong></p>

<p>From the factor page example:</p>

<pre><code>factor(ff)      # drops the levels that do not occur
</code></pre>

<p>For dropping levels from all factor columns in a dataframe, you can use:</p>

<pre><code>subdf &lt;- subset(df, numbers &lt;= 3)
subdf[] &lt;- lapply(subdf, function(x) if(is.factor(x)) factor(x) else x)
</code></pre>
"
1197383,16632,2009-07-28T23:53:43Z,1195826,29,FALSE,"<p>If you don't want this behaviour, don't use factors, use character vectors instead.  I think this makes more sense than patching things up afterwards. Try the following before loading your data with <code>read.table</code> or <code>read.csv</code>:  </p>

<pre><code>options(stringsAsFactors = FALSE)
</code></pre>

<p>The disadvantage is that you're restricted to alphabetical ordering.  (reorder is your friend for plots)</p>
"
1197460,37213,2009-07-29T00:20:11Z,1197434,3,TRUE,"<p>Yes, <a href=""http://cran.r-project.org/doc/manuals/R-data.html"" rel=""nofollow noreferrer"">here's how</a>:</p>

<blockquote>
  <p>Windows users can use odbcConnectExcel in package RODBC. This can select rows and columns from any of the sheets in an Excel spreadsheet file (at least from Excel 97–2003, depending on your ODBC drivers: by calling odbcConnect directly versions back to Excel 3.0 can be read). The version odbcConnectExcel2007 will read the Excel 2007 formats as well as earlier ones (provided the drivers are installed: see RODBC).</p>
</blockquote>
"
1197488,143305,2009-07-29T00:33:03Z,1197434,3,FALSE,"<p>Another (and even portable) option is <code>read.xls</code> in the <a href=""http://cran.r-project.org/web/packages/gdata/index.html"" rel=""nofollow noreferrer"">gdata</a> package. It is available cross-platform as it relies on underlying Perl code to parse, read, ... data from the xls file into a csv file that is then read. The required Perl packages are all provided by <a href=""http://cran.r-project.org/web/packages/gdata/index.html"" rel=""nofollow noreferrer"">gdata</a>.</p>

<p>An Octave package could easily be created using the same trick.</p>
"
1197550,2611,2009-07-29T00:51:34Z,1197434,3,FALSE,"<p>Your question has already been answered.  In case this helps for some future case, a really useful resource for these types of questions is the <a href=""http://cran.r-project.org/doc/manuals/R-data.html"" rel=""nofollow noreferrer"">R Data Import/Export document</a> which covers a lot of useful API, packages, tips, etc for accessing data from some common databases and file formats.  </p>

<p>For example, there's an <a href=""http://cran.r-project.org/doc/manuals/R-data.html#Reading-Excel-spreadsheets"" rel=""nofollow noreferrer"">Excel section</a>, which covers a lot of the ground in the answers already provided. </p>

<p>Another useful tip is to try the R help search system.  For example, try either of the following from the R prompt:</p>

<pre><code>&gt; ??xls
&gt; ??excel
</code></pre>

<p>EDIT: BTW, <code>??xls</code> is short for <code>help.search(""xls"")</code>.</p>
"
1197662,142477,2009-07-29T01:36:14Z,1197434,1,FALSE,"<p>The easiest way is for your data to be in the form of a CSV file, and then use <code>read.csv()</code> to read in the data.  Be aware that when you read in the data, that R will read in character strings and convert them into factors.  This can become an issue when you try to subset, the data, etc.</p>

<p>Alternatively, if you would like to read it in as a native Excel file, then you can use the <code>RODBC</code> package.</p>

<pre><code>library('RODBC')
yourData &lt;- sqlFetch(odbcConnectExcel(""yourData.xls""), 
  sqtable = ""nameOfSheet"",
  na.strings = ""NA"", 
  as.is = TRUE)                    
odbcCloseAll()
</code></pre>
"
1197766,143377,2009-07-29T02:19:51Z,1191689,0,FALSE,"<p>The lme4 package, which estimates hierarchical models using frequentist methods, has a function called mcmcsamp that allows you to sample from the posterior distribution of the model using MCMC. This currently  works only for linear models, quite unfortunately.</p>
"
1197945,2611,2009-07-29T03:40:37Z,1195826,9,FALSE,"<p>Here's another way, which I believe is equivalent to the <code>factor(..)</code> approach:</p>

<pre><code>&gt; df &lt;- data.frame(let=letters[1:5], num=1:5)
&gt; subdf &lt;- df[df$num &lt;= 3, ]

&gt; subdf$let &lt;- subdf$let[ , drop=TRUE]

&gt; levels(subdf$let)
[1] ""a"" ""b"" ""c""
</code></pre>
"
1198147,73568,2009-07-29T05:08:53Z,1198116,2,TRUE,"<p>Once you have set the connection to the file, you can use following statement:</p>

<pre><code>select [columnname] from [sheetname$] where [columnname] = 'somevalue'
</code></pre>

<p>Not sure about the row index thing. But you can make use of where clause if each row in the file has serial number or any such unique value.</p>
"
1198177,2611,2009-07-29T05:26:13Z,1198116,3,FALSE,"<p>Here's a sample query:</p>

<pre><code>SELECT [sheet1$.col1], [sheet1$.col2], [sheet2$.col1] 
FROM   [sheet1$], [sheet2$] 
WHERE  [sheet1$.col1] = [sheet2$.col2]
</code></pre>

<p>This assumes an excel document with 2 sheets (<code>sheet1</code> and <code>sheet2</code>).  Each sheet has 2 columns, with the first row as headers (<code>col1</code> and <code>col2</code> in each sheet).</p>

<p>Here's the complete code:</p>

<pre><code>&gt; library(RODBC)
&gt; conn &lt;- odbcConnectExcel('c:/tmp/foo.xls')
&gt; query &lt;- ""select [sheet1$.col1], [sheet1$.col2], [sheet2$.col1] 
            from [sheet1$], [sheet2$] 
            where [sheet1$.col1] = [sheet2$.col2];""
&gt; result &lt;- sqlQuery(conn, query)
&gt; odbcClose(conn)
&gt; result
  col1 col2 col1.1
1    1    3      5
2    2    4      6
3    3    5      7
</code></pre>

<p>I've never found a way to deal with row numbers.  I just create an extra column and fill down sequentially.  Not sure if that works for you.</p>
"
1198541,144157,2009-07-29T07:24:45Z,1197434,1,FALSE,"<p>An even simpler solution for Windows is to use the <a href=""http://treetron.googlepages.com/xlsreadwrite.htm"" rel=""nofollow noreferrer"">xlsReadWrite</a> package. No need for Perl, just install as an R package and you can read and write Excel files to your heart's content.</p>
"
1199155,NA,2009-07-29T09:59:55Z,659725,8,TRUE,"<p>The short answer is that for analytic data, a column store will tend to be faster, with less tuning required.</p>

<p>A row store, the traditional database architecture, is good at inserting small numbers of rows, updating rows in place, and querying small numbers of rows. In a row store, these operations can be done with one or two disk block I/Os.</p>

<p>Analytic databases typically load thousands of records at a time; sometimes, as in your case, they reload everything. They tend to be denormalized, so have a lot of columns. And at query time, they often read a high proportion of the rows in the table, but only a few of these columns. So, it makes sense from an I/O standpoint to store values of the same column together.</p>

<p>Turns out that this gives the database a huge opportunity to do value compression. For instance, if a string column has an average length of 20 bytes but has only 25 distinct values, the database can compress to about 5 bits per value. Column store databases can often operate without decompressing the data.</p>

<p>Often in computer science there is an I/O versus CPU time tradeoff, but in column stores the I/O improvements often improve locality of reference, reduce cache paging activity, and allow greater compression factors, so that CPU gains also.</p>

<p>Column store databases also tend to have other analytic-oriented features like bitmap indexes (yet another case where better organization allows better compression, reduces I/O, and allows algorithms that are more CPU-efficient), partitions, and materialized views.</p>

<p>The other factor is whether to use a massively parallel (MMP) database. There are MMP row-store and column-store databases. MMP databases can scale up to hundreds or thousands of nodes, and allow you to store humungous amounts of data, but sometimes have compromises like a weaker notion of transactions or a not-quite-SQL query language.</p>

<p>I'd recommend that you give LucidDB a try. (Disclaimer: I'm a committer to LucidDB.) It is open-source column store database, optimized for analytic applications, and also has other features such as bitmap indexes. It currently only runs on one node, but utilizes several cores effectively and can handle reasonable volumes of data with not much effort.</p>
"
1200402,143305,2009-07-29T13:47:39Z,659725,2,FALSE,"<p>4 million rows times 20 columns times 8 bytes for a double is 640 mb. Following the rule of thumb that R creates three temporary copies for every object, we get to around 2 gb.  That is not a lot by today's standard.  </p>

<p>So this should be doable in memory on a suitable 64-bit machine with a 'decent' amount of ram (say 8 gb or more).  Installing Ubuntu or Debian (possibly in the server version) can be done in a few minutes.</p>
"
1203881,143305,2009-07-30T00:46:26Z,1203662,2,TRUE,"<p>I cannot help you with the Java aspect, besides noting that on my Linux machine, simply calling <code>make</code> does the trick:</p>

<pre><code>/tmp/java-new$ make
javac -d . -source 1.4 -target 1.4 MutableREXP.java REngineException.java REngine.java REXPDouble.java REXPEnvironment.java REXPExpressionVector.java REXPFactor.java REXPGenericVector.java REXPInteger.java REXP.java REXPLanguage.java REXPList.java REXPLogical.java REXPMismatchException.java REXPNull.java REXPRaw.java REXPReference.javaREXPS4.java REXPString.java REXPSymbol.java REXPUnknown.java REXPVector.java RFactor.java RList.java
jar fc REngine.jar org
rm -rf org
javac -d . -cp REngine.jar Rserve/RConnection.java Rserve/RFileInputStream.java Rserve/RFileOutputStream.java Rserve/RserveException.java Rserve/RSession.java Rserve/protocol/jcrypt.java Rserve/protocol/REXPFactory.java Rserve/protocol/RPacket.java Rserve/protocol/RTalk.java
Note: Rserve/protocol/REXPFactory.java uses unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
jar fc Rserve.jar org
rm -rf org
</code></pre>

<p>Likewise with the example directory you mentioned -- it builds fine on my Linux machine just saying 'make'.  I do not use Eclipse and can't help with that aspect.</p>

<p>The C++ examples also run fine as well (though I needed to adjust the Makefile to build them).</p>

<p>You may need to ask to the Rosuda list for Rserv.</p>
"
1204294,NA,2009-07-30T03:19:26Z,1203662,1,FALSE,"<p>Yes, indeed. Compile the JAR just as Dirk indicated, then in Eclipse right click -> Add to Build Path, and the org.rosuda.* package gets created and can be imported as in the examples.</p>
"
1204916,2611,2009-07-30T07:09:28Z,103312,4,FALSE,"<p>Try checking the length of data returned by readBin:</p>

<pre><code>while (length(a &lt;- readBin(f, 'int', n=1)) &gt; 0) {
    # do something
}
</code></pre>
"
1206717,NA,2009-07-30T13:46:24Z,1203662,1,FALSE,"<p>FWIW the JAR files are also available directly from the Rserve website:
<a href=""http://www.rforge.net/Rserve/files/"" rel=""nofollow noreferrer"">http://www.rforge.net/Rserve/files/</a></p>

<p>And yes, ""make"" is all you need to run to create the JAR files from the sources (works on any platform including Windows).</p>
"
1208704,13251,2009-07-30T19:13:13Z,1208627,1,FALSE,"<p>There's a library for Clojure (not quite Java but still on the JVM) called <a href=""http://incanter.org/"" rel=""nofollow noreferrer"">Incanter</a>.</p>

<p>It is built on Colt and JFreeChart. It may be possible that you can just use <a href=""http://sites.google.com/site/piotrwendykier/software/parallelcolt"" rel=""nofollow noreferrer"">Colt</a> directly, but I wouldn't know.</p>
"
1208732,51445,2009-07-30T19:17:47Z,1208627,5,FALSE,"<p>I've had success with visualizing graphs with a commercial tool called <a href=""http://www.yworks.com/en/products_yfiles_about.html"" rel=""noreferrer"">yFiles</a>. For more general purpose statistical data visualization, you could try <a href=""http://rosuda.org/mondrian/"" rel=""noreferrer"">Mondrian</a>, which is GPL licensed. These both are Java libraries.</p>
"
1209332,NA,2009-07-30T20:58:21Z,1208627,6,FALSE,"<p>might want to check out <a href=""http://processing.org/"" rel=""noreferrer"">http://processing.org/</a></p>
"
1209567,148121,2009-07-30T21:42:04Z,1208627,3,FALSE,"<p>JFreeChart is a good Option. I have tried it, and is easy to set up with Eclipse too. BIRT is also nice, but if you are using it for reporting that is.</p>
"
1209823,2611,2009-07-30T22:49:21Z,1208627,3,FALSE,"<p>Prefuse is a pretty nice library for Java, and even lets you publish animations in Flash using a layer called Flare:</p>

<ul>
<li><a href=""http://prefuse.org/"" rel=""nofollow noreferrer"">prefuse visualization toolkit</a></li>
<li><a href=""http://prefuse.org/gallery/"" rel=""nofollow noreferrer"">prefuse visualization gallery</a></li>
</ul>

<p>On the R side, you only need to browse the gallery to see what it's capable of:</p>

<ul>
<li><a href=""http://addictedtor.free.fr/graphiques/"" rel=""nofollow noreferrer"">R Graphics Gallery</a></li>
</ul>
"
1213910,143305,2009-07-31T17:42:32Z,1213783,1,FALSE,"<p>I am not sure about <code>update.formula()</code>, but I have used the approach you take here of pasting text and converting it via <code>as.formula</code> in the past with success.  My reading of <code>help(update.formula)</code> does not make me think you can substitute the left-hand side as you desire.</p>

<p>Lastly, trust the dispatching mechanism.  If you object is of type formula, just call <code>update</code> which is preferred over the explicit <code>update.formula</code>.</p>
"
1214401,16632,2009-07-31T19:25:47Z,1213783,5,TRUE,"<p>Here's one approach:</p>

<pre><code>right &lt;- ~ a + b + c
left &lt;- ~ y 
left_2 &lt;- substitute(left ~ ., list(left = left[[2]]))

update(right, left_2)
</code></pre>

<p>But I think you'll have to either paste text strings together, or use substitute.  To the best of my knowledge, there are no functions to create one two sided formula from two one-sided formulas (or similar equivalents).</p>
"
1214432,16632,2009-07-31T19:30:45Z,1169539,45,FALSE,"<p>Here's an approach using the <a href=""http://had.co.nz/plyr"" rel=""noreferrer"">plyr</a> package:</p>

<pre><code>d &lt;- data.frame(
  state = rep(c('NY', 'CA'), 10),
  year = rep(1:10, 2),
  response= rnorm(20)
)

library(plyr)
# Break up d by state, then fit the specified model to each piece and
# return a list
models &lt;- dlply(d, ""state"", function(df) 
  lm(response ~ year, data = df))

# Apply coef to each model and return a data frame
ldply(models, coef)

# Print the summary of each model
l_ply(models, summary, .print = TRUE)
</code></pre>
"
1214471,16632,2009-07-31T19:37:52Z,520810,23,TRUE,"<p>No, but you can write it yourself:</p>

<pre><code>q &lt;- function(...) {
  sapply(match.call()[-1], deparse)
}
</code></pre>

<p>And just to show it works:</p>

<pre><code>&gt; q(a, b, c)
[1] ""a"" ""b"" ""c""
</code></pre>
"
1218085,148801,2009-08-02T03:33:24Z,1177919,2,FALSE,"<p>Dirk Eddelbuettel's answer works, but you don't need to indicate row numbers or specify entries in the lastname column. This code should do it for a data frame named <code>df</code>:</p>

<pre><code>if(!(""LastName"" %in% names(df))){
    df &lt;- cbind(df[1:2],LastName=NA,df[3:length(df)])
}
</code></pre>

<p>(this defaults <code>LastName</code> to <code>NA</code>, but you could just as easily use ""<code>LastName='Smith'</code>"")</p>
"
1219674,148801,2009-08-02T19:51:57Z,1169426,0,FALSE,"<p>In my experience, igraph is my favorite package for large, graph-theoretic work. It is memory efficient and has some very good algorithms. igraph uses an internal edgelist-like data structure.<br>
For simpler/smaller things I tend to use the 'sna' package (""social network analysis""). It's great for interactive work and plotting of smaller networks. sna uses more of an adjacency-matrix data structure.<br></p>
"
1219781,143305,2009-08-02T20:35:28Z,1219480,7,TRUE,"<p>In case you not familiar with R and packages, start with</p>

<pre><code>install.packages(mediation)
</code></pre>

<p>to download and install the package from CRAN. Then do</p>

<pre><code>library(help=mediation)
</code></pre>

<p>for a high-level view of the package, and available help files. Then use</p>

<pre><code>library(mediation)
help(mediate)
</code></pre>

<p>to load the package and read the help page.  The example can be run via</p>

<pre><code>example(mediate)
</code></pre>

<p>and you can run the other example for sensitivity analysis via</p>

<pre><code>example(medsens)
</code></pre>
"
1220721,45563,2009-08-03T05:05:08Z,498932,3,FALSE,"<p>You can simply drag/drop the folder in Windows Explorer into the File System view of your Installer vdproj. All the files and folders in the hierarchy will be added to your setup project.</p>

<p>Tip: If the folders are in SVN or similar source control, delete all the hidden folders before you do this! If you have PowerShell, check out</p>

<pre><code>get-childitem . -include _svn -force -recurse | foreach ($_) {remove-item -recurse -force $_.fullname}
</code></pre>

<p>Or you can always use Windows Search to find all hidden directories in the hierarchy and delete them from the Results Window.</p>
"
1220799,148801,2009-08-03T05:39:57Z,495744,1,TRUE,"<p>So you're given a character vector like <code>c(""08:00-08:15"",08:15-08:30)</code> and you want to convert to an internal R data type for consistency? Check out the help files for POSIXt and strftime.<br>
How about a function like this:</p>

<pre><code>importTimes &lt;- function(t){
    t &lt;- strsplit(t,""-"")
    return(lapply(t,strptime,format=""%H:%M:%S""))
}
</code></pre>

<p>This will take a character vector like you described, and return a list of the same length, each element of which is a POSIXt 2-vector giving the start and end times (on today's date). If you want you could add a <code>paste(""1970-01-01"",x)</code> somewhere inside the function to standardize the date you're looking at if it's an issue.<p>
Does that help at all?</p>
"
1223948,143305,2009-08-03T18:42:32Z,1223904,1,TRUE,"<p>Start by installing <a href=""http://cran.r-project.org/web/packages/RExcelInstaller/index.html"" rel=""nofollow noreferrer"">RExcelInstaller</a> from <a href=""http://cran.r-project.org"" rel=""nofollow noreferrer"">CRAN</a> via </p>

<pre><code>&gt; install.packages(""RExcelInstaller"")
</code></pre>

<p>and take it from there.</p>
"
1232609,142651,2009-08-05T11:17:04Z,1231195,11,FALSE,"<p>I can think of two options. The first option is to use an editor that allows to block comment and uncomment (eg. Eclipse). The second option is to use an if statement. But that will only allow you to 'comment' correct R syntax.  Hence a good editor is the prefered workaround.</p>

<pre><code>if(FALSE){
     #everything in this case is not executed

}
</code></pre>
"
1233157,143305,2009-08-05T13:09:36Z,1231195,38,TRUE,"<p>This does come up on the mailing list fairly regularly, see <a href=""http://thread.gmane.org/gmane.comp.lang.r.general/156893"" rel=""noreferrer"">for example this recent thread on r-help</a>.  The consensus answer usually is the one shown above: that given that the language has no direct support, you have to either</p>

<ul>
<li>work with an editor that has region-to-comment commands, and most advanced R editors do </li>
<li>use the <code>if (FALSE)</code> constructs suggested earlier but note that it still requires complete parsing and must hence be syntactically correct</li>
</ul>
"
1234018,143305,2009-08-05T15:32:26Z,1233948,1,FALSE,"<p>It would appear that you cannot: <code>oneway_test()</code> has no argument <code>conf.level</code> whereas <code>wilcox_test</code> and <code>normal_test</code> do. This is all documented, see <code>help(oneway_test)</code>.</p>
"
1236652,25282,2009-08-06T02:15:43Z,1236620,52,FALSE,"<p><a href=""http://www.dangoldstein.com/search_r.html"" rel=""noreferrer"">Dan Goldstein</a> provides a search engine for R that gives you fast answers to questions like setting global variables.
For global variables there's a solution in a <a href=""http://tolstoy.newcastle.edu.au/R/help/05/11/16539.html"" rel=""noreferrer"">mailinglist posting</a>:</p>

<pre><code>a &lt;- ""old""
test &lt;- function () {
   assign(""a"", ""new"", envir = .GlobalEnv)
}
test()
a  # display the new value
</code></pre>
"
1236721,143305,2009-08-06T02:42:29Z,1236620,106,TRUE,"<p>Use one post per main question.</p>

<ol>
<li><p>As the first answer with assign() showed you, there is a way to assign in the global environment.  A simpler, shorter (but not better ... stick with assign) way is to use the <code>&lt;&lt;-</code> operator, ie   </p>

<pre><code>a &lt;&lt;- ""new"" 
</code></pre>

<p>inside the function.</p></li>
<li><p>For your plots, use  <code>main=""My title here""</code>   for each plot.  Use something like <code>par(mar=c(3,3,3,1))</code> to give sufficient spacing.</p></li>
</ol>
"
1238815,16632,2009-08-06T13:16:50Z,1236620,8,FALSE,"<p>Why are you trying to create global variables from inside a function?  It is very unlikely that this is a good idea and you should post more details about what you are trying to achieve.</p>
"
1238981,143305,2009-08-06T13:46:20Z,1238933,11,TRUE,"<p>First off, a lot of that 'loops are bad' chatter stems from the dark ages when loops where in fact less efficiently implemented, in particular in some versions of S-Plus. </p>

<p>That said, and while your comment about the need for a large index object is correct, you could also use</p>

<ul>
<li><p>functions from the <code>apply</code> family such as <code>sapply</code>, <code>lapply</code> or <code>tapply</code> to unroll your structures</p></li>
<li><p>the relatively new <a href=""http://cran.r-project.org/web/packages/iterators/index.html"" rel=""noreferrer"">iterators</a> package which also avoids the large vector you mentioned as a memory constraint</p></li>
<li><p>the <a href=""http://www.milbo.users.sonic.net/ra/"" rel=""noreferrer"">Ra 'accelerated R' variant and its jit package</a> which can significantly accelerate simple loops.</p></li>
</ul>

<p>As added bonus, options one and two give a path towards parallel execution of the loops on suitable systems using tools from the CRAN packages snow, multicore, or NWS just to name a few.</p>
"
1241463,144278,2009-08-06T21:06:43Z,1238250,1,TRUE,"<p>Check out the documentation for that function using <code>?MAP</code>.</p>

<p>If there is nothing there, probably send an email to the author, listed at the package's CRAN page:
<a href=""http://cran.r-project.org/web/packages/psych/index.html"" rel=""nofollow noreferrer"">CRAN Page for Psych</a></p>

<p>That package is made by the folks at this site. They probably have a mailing list:
<a href=""http://personality-project.org/r"" rel=""nofollow noreferrer"">PersonalityProject.org</a></p>
"
1241558,37751,2009-08-06T21:20:48Z,1241184,10,TRUE,"<p>The most obvious methods that come to my mind are to use either Lattice or ggplot2. Here's an example using lattice:</p>

<pre><code> library(lattice)
 depthgroup&lt;-equal.count(quakes$depth, number=3, overlap=0)
 magnitude&lt;-equal.count(quakes$mag, number=2, overlap=0)
 xyplot(lat ~ long | depthgroup*magnitude,
 data=quakes,
 main=""Fiji Earthquakes"",
 ylab=""latitude"", xlab=""longitude"",
 pch=""."",
 scales=list(x=list(alternating=c(1,1,1))),
 between=list(y=1),
 par.strip.text=list(cex=0.7),
 par.settings=list(axis.text=list(cex=0.7)))
</code></pre>

<p>In lattice you would change the main= parameter. </p>

<p>The above example was lifted from <a href=""http://osiris.sunderland.ac.uk/~cs0her/Statistics/UsingLatticeGraphicsInR.htm"" rel=""nofollow noreferrer"">here</a>. </p>

<p>I don't have a good ggplot2 example, but there are a metricasston of examples with ggpolot2 over at the <a href=""http://learnr.wordpress.com/"" rel=""nofollow noreferrer"">learn r blog</a>. </p>

<p>One option might be <a href=""http://learnr.wordpress.com/2009/05/18/ggplot2-three-variable-time-series-panel-chart/"" rel=""nofollow noreferrer"">this example</a> where they use ggplot2 and </p>

<pre><code>opts (title = ""RSS and NINO3.4 Temperature Anomalies \nand SATO Index Trends Since 1980"")
</code></pre>

<p>But you would have to have all three graphs created in gg2plot, naturally.</p>

<p>I think you should be fine with either lattice or ggplot2. </p>
"
1241966,64253,2009-08-06T22:51:49Z,1238250,2,FALSE,"<p>@Ryan Rosario</p>

<p>I finally figured it out:</p>

<pre><code>install.packages(""psych"")
library(""psych"")
</code></pre>

<p>I ran the sample steps</p>

<pre><code>my.VSS &lt;- VSS(test.data,title=""VSS of 24 mental tests"")
</code></pre>

<p>and</p>

<pre><code>VSS(sim.circ(nvar=24),fm=""mle"", title=""VSS of 24 circumplex variables"")
</code></pre>

<p>and</p>

<pre><code>VSS(sim.item(nvar=24),fm=""mle"", title=""VSS of 24 circumplex variables"")
</code></pre>

<p>you get something like this as output (for the last input):</p>

<p>Very Simple Structure of  VSS of 24 circumplex variables 
Call: VSS(x = sim.item(nvar = 24), fm = ""mle"", title = ""VSS of 24 circumplex variables"")
VSS complexity 1 achieves a maximimum of 0.84  with  <strong>3  factors</strong>
VSS complexity 2 achieves a maximimum of 0.87  with  <strong>8  factors</strong></p>

<p>The Velicer MAP criterion achieves a minimum of 0.05  with  <strong>2  factors</strong></p>

<p>Velicer MAP
<a href=""http://bm2.genes.nig.ac.jp/RGM2/R_current/library/psych/man/VSS.html"" rel=""nofollow noreferrer"">1</a> 0.05 0.01 0.01 0.01 0.01 0.02 0.02 0.02</p>

<p>Very Simple Structure Complexity 1
<a href=""http://bm2.genes.nig.ac.jp/RGM2/R_current/library/psych/man/VSS.html"" rel=""nofollow noreferrer"">1</a> 0.44 0.84 0.84 0.80 0.75 0.76 0.80 0.80</p>

<p>Very Simple Structure Complexity 2
<a href=""http://bm2.genes.nig.ac.jp/RGM2/R_current/library/psych/man/VSS.html"" rel=""nofollow noreferrer"">1</a> 0.00 0.85 0.85 0.85 0.86 0.86 0.86 0.87</p>

<p>as the <a href=""http://bm2.genes.nig.ac.jp/RGM2/R_current/library/psych/man/VSS.html"" rel=""nofollow noreferrer"">documentation says</a> (highlighting above is mine):</p>

<blockquote>
  <p>""Wayne Velicer's MAP criterion has
  been added as an additional test for
  the optimal number of components to
  extract. Note that VSS and MAP will
  not always agree as to the optimal
  number.""</p>
</blockquote>

<p>In this case VSS with complexity of 1 and 2 gives an answer of 3 and 8 factors respectively while Velicer's MAP criterion gives 2.</p>
"
1242534,143476,2009-08-07T01:59:40Z,1241184,13,FALSE,"<p>Using the traditional graphics system, here are two ways:</p>

<p>(1)</p>

<pre><code>par(mfrow=c(2,2))
for( i in 1:4 ) plot(1:10)
mtext(""Title"",side=3,outer=TRUE,padj=3)
</code></pre>

<p>(2)</p>

<pre><code>par(mfrow=c(2,2))
for( i in 1:4 ) plot(1:10)
par(mfrow=c(1,1),mar=rep(0,4),oma=rep(0,4))
plot.window(0:1,0:1)
text(.5,.98,""Title"")
</code></pre>
"
1245337,143305,2009-08-07T15:14:13Z,1245273,50,TRUE,"<p>A histogram is a poor-man's density estimate.  Note that in your call to <code>hist()</code> using default arguments, you get <em>frequencies</em> not probabilities -- add  <code>,prob=TRUE</code> to the call if you want probabilities.  </p>

<p>As for the log axis problem, don't use 'x' if you do not want the x-axis transformed:</p>

<pre><code>plot(mydata_hist$count, log=""y"", type='h', lwd=10, lend=2)
</code></pre>

<p>gets you bars on a log-y scale -- the look-and-feel is still a little different but can probably be tweaked.</p>

<p>Lastly, you can also do  <code>hist(log(x), ...)</code>  to get a histogram of the log of your data.</p>
"
1246028,446918,2009-08-07T17:32:18Z,1208627,1,FALSE,"<p>The Weka project might be able to help you. It's an open source library of data mining algorithms written in Java. If they don't have the visualization themselves, it might be in their ""related projects"" section.</p>

<p><a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow noreferrer"">http://www.cs.waikato.ac.nz/ml/weka/</a></p>
"
1246489,144642,2009-08-07T19:11:45Z,1246244,6,TRUE,"<p>You want:</p>

<pre><code>normalize &lt;- apply(hist_data, 2, function(x) pnorm(x, mean=mean(x), sd=sd(x)))
</code></pre>

<p>The problem is that you're passing in the individual column into <code>pnorm</code>, but the entire <code>hist_data</code> into both the mean &amp; the sd.</p>

<p>As I mentioned on twitter, I'm no stats guy so I can't answer anything about what you're actually trying to do :)</p>
"
1246610,69051,2009-08-07T19:37:51Z,1208627,4,FALSE,"<p>Have to mention <a href=""http://www.piccolo2d.org/"" rel=""nofollow noreferrer"">Piccolo2D</a> -- using it very frequently. It does not provide the ready to use data structures as Prefuse does, but it surely scales and provides a great degree of freedom in how one would want to visualise the data.</p>
"
1249628,143305,2009-08-08T18:56:06Z,1249548,17,FALSE,"<p>Yes, methinks you need to arrange your data appropriately. One way would be this:</p>

<pre><code>X &lt;- data.frame(x=rep(x,2),
                y=c(3*x+eps, 2*x+eps),
                case=rep(c(""first"",""second""), each=100))

qplot(x, y, data=X, facets = . ~ case) + geom_smooth()
</code></pre>

<p>I am sure there are better tricks in plyr or reshape -- I am still not really up to speed 
on all these powerful packages by Hadley.</p>
"
1251236,142651,2009-08-09T11:50:11Z,1245273,31,FALSE,"<p>Another option would be to use the package <code>ggplot2</code>.</p>

<pre><code>ggplot(mydata, aes(x = V3)) + geom_histogram() + scale_x_log10()
</code></pre>
"
1252684,143305,2009-08-10T00:32:48Z,1252546,31,TRUE,"<p>Properly formatted your data looks like this</p>

<pre><code>862 2006-05-19 6.241603 5.774208 
863 2006-05-20 NA       NA 
864 2006-05-21 NA       NA 
865 2006-05-22 6.383929 5.906426 
866 2006-05-23 6.782068 6.268758 
867 2006-05-24 6.534616 6.013767 
868 2006-05-25 6.370312 5.856366 
869 2006-05-26 6.225175 5.781617 
870 2006-05-27 NA       NA
</code></pre>

<p>and is of a time-series nature.  So I would load into an object of class <code>zoo</code> (from the <a href=""http://cran.r-project.org/package=zoo"" rel=""noreferrer""><strong>zoo</strong></a> package) as that allows you to pick a number of strategies -- see below.  Which one you pick depends on the nature of your data and application. In general, the field of 'figuring missing data out' is called <em>data imputation</em>
and there is a rather large literature.</p>

<pre><code>R&gt; x &lt;- zoo(X[,3:4], order.by=as.Date(X[,2]))
R&gt; x
               x     y
2006-05-19 6.242 5.774
2006-05-20    NA    NA
2006-05-21    NA    NA
2006-05-22 6.384 5.906
2006-05-23 6.782 6.269
2006-05-24 6.535 6.014
2006-05-25 6.370 5.856
2006-05-26 6.225 5.782
2006-05-27    NA    NA
R&gt; na.locf(x)  # last observation carried forward
               x     y
2006-05-19 6.242 5.774
2006-05-20 6.242 5.774
2006-05-21 6.242 5.774
2006-05-22 6.384 5.906
2006-05-23 6.782 6.269
2006-05-24 6.535 6.014
2006-05-25 6.370 5.856
2006-05-26 6.225 5.782
2006-05-27 6.225 5.782
R&gt; na.approx(x)  # approximation based on before/after values
               x     y
2006-05-19 6.242 5.774
2006-05-20 6.289 5.818
2006-05-21 6.336 5.862
2006-05-22 6.384 5.906
2006-05-23 6.782 6.269
2006-05-24 6.535 6.014
2006-05-25 6.370 5.856
2006-05-26 6.225 5.782
R&gt; na.spline(x)   # spline fit ...
               x     y
2006-05-19 6.242 5.774
2006-05-20 5.585 5.159
2006-05-21 5.797 5.358
2006-05-22 6.384 5.906
2006-05-23 6.782 6.269
2006-05-24 6.535 6.014
2006-05-25 6.370 5.856
2006-05-26 6.225 5.782
2006-05-27 5.973 5.716
R&gt; 
</code></pre>
"
1253999,142651,2009-08-10T09:52:11Z,1249548,15,FALSE,"<p>Using the reshape package you can do something like this.</p>

<pre><code>library(ggplot2)
wide &lt;- data.frame(x = rnorm(100), eps = rnorm(100, 0, .2))
wide$first &lt;- with(wide, 3 * x + eps)
wide$second &lt;- with(wide, 2 * x + eps)
long &lt;- melt(wide, id.vars = c(""x"", ""eps""))
ggplot(long, aes(x = x, y = value)) + geom_smooth() + geom_point() + facet_grid(.~ variable)
</code></pre>
"
1256560,143305,2009-08-10T18:50:39Z,1256347,6,TRUE,"<p>Read your data, and convert it  into a <a href=""http://cran.r-project.org/package=zoo"" rel=""nofollow noreferrer""><strong>zoo</strong></a> object:</p>

<pre><code>R&gt; X &lt;- read.csv(""/tmp/so.csv"")
R&gt; X &lt;- zoo(X$Count, order.by=as.POSIXct(as.character(X[,1])))
</code></pre>

<p>Note that this will show warnings because of non-unique timestamps.</p>

<p>Task 1 using <code>aggregate</code> with <code>length</code> to count:</p>

<pre><code>R&gt; aggregate(X, force, length)
2009-07-20 16:30:45 2009-07-20 16:30:46 2009-07-20 16:30:47 
                  2                   3                   1 
</code></pre>

<p>Task 2 using <code>aggregate</code>:</p>

<pre><code>R&gt; aggregate(X, force, mean)
2009-07-20 16:30:45 2009-07-20 16:30:46 2009-07-20 16:30:47 
             12.500               7.333              20.000 
</code></pre>

<p>Task 3 can be done the same way by aggregating up to higher-order indices.  You can call <code>plot</code> on the result from aggregate:</p>

<pre><code>plot(aggregate(X, force, mean))
</code></pre>
"
1256566,142651,2009-08-10T18:51:22Z,1256347,2,FALSE,"<p>Averaging the data is easy with the plyr package.</p>

<pre><code>library(plyr)
Second &lt;- ddply(dataset, ""Timestamp"", function(x){
    c(Average = mean(x$Count), N = nrow(x))
})
</code></pre>

<p>To do the same thing by minute or hour, then you need to add fields with that info.</p>

<pre><code>library(chron)
dataset$Minute &lt;- minutes(dataset$Timestamp)
dataset$Hour &lt;- hours(dataset$Timestamp)
dataset$Day &lt;- dates(dataset$Timestamp)
#aggregate by hour
Hour &lt;- ddply(dataset, c(""Day"", ""Hour""), function(x){
    c(Average = mean(x$Count), N = nrow(x))
})
#aggregate by minute
Minute &lt;- ddply(dataset, c(""Day"", ""Hour"", ""Minute""), function(x){
    c(Average = mean(x$Count), N = nrow(x))
})
</code></pre>
"
1257851,16632,2009-08-10T23:55:52Z,1245273,7,FALSE,"<p>It's not entirely clear from your question whether you want a logged x-axis or a logged y-axis.  A logged y-axis is not a good idea when using bars because they are anchored at zero, which becomes negative infinity when logged.  You can work around this problem by using a frequency polygon or density plot.</p>
"
1259268,142651,2009-08-11T09:21:18Z,1233948,4,TRUE,"<p>The alpha levels are hardcoded and fixed at 0.99 If you want to change that, then you have to download the package source, change the levels and compile the package. The levels are coded in the Methods.R file. Search for binom.test or conf.level</p>

<p>You could ask the package author to change the package so you can set the level yourself. But bear in mind that the package author is not obliged to do that!</p>
"
1260077,143305,2009-08-11T12:37:03Z,1259867,1,FALSE,"<p>Here is a hack-ish way. Hadley may come with something more elegant:</p>

<p>To start, we simple concatenate the <code>by</code> output:</p>

<pre><code> R&gt; do.call(c,byOutput)
A1 A2 A3 A4 A5 B1 B2 B3 B4 B5 C1 C2 C3 C4 C5 
 1  2  2  1  1  1  1  2  1  2  1  2  1  1  2 
</code></pre>

<p>and what matters that we get the factor levels 1 and 2 here which we can use to re-index a new factor with those levels:</p>

<pre><code>R&gt; c(""Below"",""Above"")[do.call(c,byOutput)]
 [1] ""Below"" ""Above"" ""Above"" ""Below"" ""Below"" ""Below"" ""Below"" ""Above"" 
 [8] ""Below"" ""Above"" ""Below"" ""Above"" ""Below"" ""Below"" ""Above""
R&gt; as.factor(c(""Below"",""Above"")[do.call(c,byOutput)])
[1] Below Above Above Below Below Below Below Above Below Above 
[11] Below Above Below Below Above
Levels: Above Below
</code></pre>

<p>which we can then assign into the <code>data.frame</code> you wanted to modify:</p>

<pre><code>R&gt; myDataFrame$FactorLevelMedianSplit &lt;- 
      as.factor(c(""Below"",""Above"")[do.call(c,byOutput)])
</code></pre>

<p><strong>Update</strong>:  Never mind, we'd need to reindex myDataFrame to be sorted A A ... A B ... B C ... C as well before we add the new column.  Left as an exercise...</p>
"
1260696,142651,2009-08-11T14:22:44Z,1259867,3,TRUE,"<p>Here is a solution using the plyr package.</p>

<pre><code>myDataFrame &lt;- data.frame(myData=runif(15),myFactor=rep(c(""A"",""B"",""C""),5))
library(plyr)
ddply(myDataFrame, ""myFactor"", function(x){
    x$Median &lt;- median(x$myData)
    x$FactorLevelMedianSplit &lt;- factor(x$myData &lt;= x$Median, levels = c(TRUE, FALSE), labels = c(""Below"", ""Above""))
    x
})
</code></pre>
"
1261156,154039,2009-08-11T15:31:53Z,1260965,3,FALSE,"<p>The R Graphics Gallery has a very <a href=""http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=113"" rel=""nofollow noreferrer"">similar map</a> which should make for a good starting point. The code is here: www.ai.rug.nl/~hedderik/R/US2004 . You'd need to add a legend with the legend() function.</p>
"
1261205,143591,2009-08-11T15:41:36Z,1260965,4,FALSE,"<p>Take a look at the PBSmapping package (see borh the vignette/manual and the demo) and
<a href=""http://oreilly.com/catalog/9780596804770/"" rel=""nofollow noreferrer"">this</a> O'Reilly <em>Data  Mashups in R</em> article (unfortunately it is not free of charge but it worth 4.99$ to download, according <a href=""http://blog.revolution-computing.com/2009/06/data-mashups-in-r.html"" rel=""nofollow noreferrer"">Revolutions blog</a> ).</p>
"
1261288,143377,2009-08-11T15:54:00Z,1260965,59,TRUE,"<p>The following code has served me well. Customize it a little and you are done.
<a href=""http://files.eduardoleoni.com/map.png"">alt text http://files.eduardoleoni.com/map.png</a></p>

<pre><code>library(maptools)
substitute your shapefiles here
state.map &lt;- readShapeSpatial(""BRASIL.shp"")
counties.map &lt;- readShapeSpatial(""55mu2500gsd.shp"")
## this is the variable we will be plotting
counties.map@data$noise &lt;- rnorm(nrow(counties.map@data))
</code></pre>

<p>heatmap function</p>

<pre><code>plot.heat &lt;- function(counties.map,state.map,z,title=NULL,breaks=NULL,reverse=FALSE,cex.legend=1,bw=.2,col.vec=NULL,plot.legend=TRUE) {
  ##Break down the value variable
  if (is.null(breaks)) {
    breaks=
      seq(
          floor(min(counties.map@data[,z],na.rm=TRUE)*10)/10
          ,
          ceiling(max(counties.map@data[,z],na.rm=TRUE)*10)/10
          ,.1)
  }
  counties.map@data$zCat &lt;- cut(counties.map@data[,z],breaks,include.lowest=TRUE)
  cutpoints &lt;- levels(counties.map@data$zCat)
  if (is.null(col.vec)) col.vec &lt;- heat.colors(length(levels(counties.map@data$zCat)))
  if (reverse) {
    cutpointsColors &lt;- rev(col.vec)
  } else {
    cutpointsColors &lt;- col.vec
  }
  levels(counties.map@data$zCat) &lt;- cutpointsColors
  plot(counties.map,border=gray(.8), lwd=bw,axes = FALSE, las = 1,col=as.character(counties.map@data$zCat))
  if (!is.null(state.map)) {
    plot(state.map,add=TRUE,lwd=1)
  }
  ##with(counties.map.c,text(x,y,name,cex=0.75))
  if (plot.legend) legend(""bottomleft"", cutpoints, fill = cutpointsColors,bty=""n"",title=title,cex=cex.legend)
  ##title(""Cartogram"")
}
</code></pre>

<p>plot it</p>

<pre><code>plot.heat(counties.map,state.map,z=""noise"",breaks=c(-Inf,-2,-1,0,1,2,Inf))
</code></pre>
"
1263432,154039,2009-08-11T22:56:40Z,1107605,1,TRUE,"<p>There isn't, but you can force your Rscript into R editing mode by making the second line of the file look like this:</p>

<pre><code># -*- mode: R -*-
</code></pre>

<p>(More information <a href=""http://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html"" rel=""nofollow noreferrer"">here</a>.)</p>
"
1265157,23929,2009-08-12T09:27:23Z,1265129,19,TRUE,"<p>The dput command writes an ASCII representation. If instead of a filename you put """" it will write it to the console</p>

<pre><code>&gt; dput(site.data,"""")
structure(list(site = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 
3L, 3L), .Label = c(""ALBEN"", ""ALDER"", ""AMERI""), class = ""factor""), 
    year = c(5L, 10L, 20L, 5L, 10L, 20L, 5L, 10L, 20L), peak = c(101529.6, 
    117483.4, 132960.9, 6561.3, 7897.1, 9208.1, 43656.5, 51475.3, 
    58854.4)), .Names = c(""site"", ""year"", ""peak""), row.names = c(1L, 
2L, 3L, 8L, 9L, 10L, 15L, 16L, 17L), class = ""data.frame"")
</code></pre>

<p>Just copy the structure and put it after ""site.data="" in your example code and people will be able to recreate the data frame exactly as you have it.</p>
"
1266045,142762,2009-08-12T13:03:56Z,1262911,2,FALSE,"<p>Without a sample of the data, it's hard to say. But one small ""gotcha"" is that <code>#</code> is the default <code>comment.char</code> in <code>read.table()</code>. Try to set <code>comment.char = """"</code> and see if that fixes it.</p>
"
1266400,143305,2009-08-12T14:09:44Z,1266279,60,TRUE,"<p>The standard answer is to use packages -- see the <a href=""http://cran.r-project.org/doc/manuals/R-exts.pdf"" rel=""nofollow noreferrer"">Writing R Extensions</a> manual as well as different tutorials on the web.</p>

<p>It gives you</p>

<ul>
<li>a quasi-automatic way to organize your code by topic </li>
<li>strongly encourages you to write a help file, making you think about the interface</li>
<li>a lot of sanity checks via <code>R CMD check</code></li>
<li>a chance to add regression tests</li>
<li>as well as a means for namespaces.</li>
</ul>

<p>Just running <code>source()</code> over code works for really short snippets. Everything else should be in a package -- even if you do not plan to publish it as you can write internal packages for internal repositories. </p>

<p>As for the 'how to edit' part, the <a href=""http://cran.r-project.org/doc/manuals/R-ints.html"" rel=""nofollow noreferrer"">R Internals</a> manual has excellent <em>R coding standards</em> in Section 6.  Otherwise, I tend to use defaults in <a href=""http://ess.r-project.org/"" rel=""nofollow noreferrer"">Emacs' ESS mode</a>.</p>

<p><em>Update 2008-Aug-13:</em>  David Smith just blogged about the <a href=""https://google.github.io/styleguide/Rguide.xml"" rel=""nofollow noreferrer"">Google R Style Guide</a>.</p>
"
1266995,143591,2009-08-12T15:39:14Z,1266279,17,FALSE,"<p>I agree with Dirk advice! IMHO, organizing your programs from simple scripts to documented packages is, for Programming in R, like switching from Word to TeX/LaTeX for writing.
I recommend to take a look at the very useful <em><a href=""http://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf"" rel=""noreferrer"">Creating R Packages: A Tutorial</a></em> by Friedrich Leisch.</p>
"
1267323,169947,2009-08-12T16:40:41Z,1265129,6,FALSE,"<p>Actually, in your original example, the way you've pasted your data in column format works just fine.  I just copied your text from the web page, and did this (using OS X so I have the nice ""paste"" command):</p>

<pre><code>&gt; site.data &lt;- read.table(pipe(""pbpaste""))
</code></pre>

<p>For toy data like something posted as a test case, this is often the best approach.  To be extra-precise, dput() is good, as dggoldst says.</p>
"
1267574,148022,2009-08-12T17:31:32Z,1266279,4,FALSE,"<p>I also agree. Use the package.skeleton() function to get started.  Even if you think your code may never be run again, it may help motivate you to create more general code that could save you time later. </p>

<p>As for accessing the global environment, that is easy with the &lt;&lt;- operator, though it is discouraged. </p>
"
1268276,143476,2009-08-12T19:41:03Z,1266279,10,FALSE,"<p>I've been meaning to figure out how to write packages but haven't invested the time. For each of my mini-projects I keep all of my low-level functions in a folder called 'functions/', and source them into a separate namespace that I explicitly create.</p>

<p>The following lines of code will create an environment named ""myfuncs"" on the search path if it doesn't already exist (using attach), and populate it with the functions contained in the .r files in my 'functions/' directory (using sys.source). I usually put these lines at the top of my main script meant for the ""user interface"" from which high-level functions (invoking the low-level functions) are called.</p>

<pre><code>if( length(grep(""^myfuncs$"",search()))==0 )
  attach(""myfuncs"",pos=2)
for( f in list.files(""functions"",""\\.r$"",full=TRUE) )
  sys.source(f,pos.to.env(grep(""^myfuncs$"",search())))
</code></pre>

<p>When you make changes you can always re-source it with the same lines, or use something like</p>

<pre><code>evalq(f &lt;- function(x) x * 2, pos.to.env(grep(""^myfuncs$"",search())))
</code></pre>

<p>to evaluate additions/modifications in the environment you created.</p>

<p>It's kludgey I know, but avoids having to be too formal about it (but if you get the chance I do encourage the package system - hopefully I will migrate that way in the future).</p>

<p>As for coding conventions, this is the only thing I've seen regarding aesthetics (I like them and loosely follow but I don't use too many curly braces in R):</p>

<p><a href=""http://www1.maths.lth.se/help/R/RCC/"" rel=""noreferrer"">http://www1.maths.lth.se/help/R/RCC/</a></p>

<p>There are other ""conventions"" regarding the use of [,drop=FALSE] and &lt;- as the assignment operator suggested in various presentations (usually keynote) at the useR! conferences, but I don't think any of these are strict (though the [,drop=FALSE] is useful for programs in which you are not sure of the input you expect).</p>
"
1268283,25188,2009-08-12T19:42:45Z,1266279,-6,FALSE,"<p>R is OK for interactive use and small scripts, but I wouldn't use it for a large program.  I'd use a mainstream language for most of the programming and wrap it in an R interface.</p>
"
1268495,76235,2009-08-12T20:19:17Z,1266279,3,FALSE,"<p>Having not learned how to write packages yet, I have always organized by sourcing sub scripts. Its similar to writing classes but not as involved. Its not programatically elegant but I find I build up analyses over time. Once I have a big section that works I often move it to a different script and just source it since it will use the workspace objects. Perhaps I need to import data from several sources, sort all of them and find the intersections. I might put that section into an additional script. However, if you want to distribute your ""application"" for other people, or it uses some interactive input, a package is probably a good route. As a researcher I rarely need to distribute my analysis code but I OFTEN need to augment or tweak it.</p>
"
1269716,2611,2009-08-13T02:22:34Z,1269624,2,FALSE,"<p>Try:</p>

<pre><code>&gt; d &lt;- data.frame(a=1:3, b=4:6, c=7:9)

&gt; d
  a b c
1 1 4 7
2 2 5 8
3 3 6 9

&gt; d[1, ]
  a b c
1 1 4 7

&gt; d[1, ]['a']
  a
1 1
</code></pre>
"
1269719,143319,2009-08-13T02:23:36Z,1269624,100,TRUE,"<pre><code>x[r,]
</code></pre>

<p>where r is the row you're interested in.  Try this, for example:</p>

<pre><code>#Add your data
x &lt;- structure(list(A = c(5,    3.5, 3.25, 4.25,  1.5 ), 
                    B = c(4.25, 4,   4,    4.5,   4.5 ),
                    C = c(4.5,  2.5, 4,    2.25,  3   )
               ),
               .Names    = c(""A"", ""B"", ""C""),
               class     = ""data.frame"",
               row.names = c(NA, -5L)
     )

#The vector your result should match
y&lt;-c(A=5, B=4.25, C=4.5)

#Test that the items in the row match the vector you wanted
x[1,]==y
</code></pre>

<p><a href=""http://www.mayin.org/ajayshah/KB/R/html/b6.html"" rel=""noreferrer"">This page</a> (from <a href=""http://www.mayin.org/ajayshah/KB/R/index.html"" rel=""noreferrer"">this useful site</a>) has good information on indexing like this.</p>
"
1269808,2611,2009-08-13T03:02:17Z,1266279,31,FALSE,"<p>This might sound a little obvious especially if you're a programmer, but here's how I think about logical and physical units of code.</p>

<p>I don't know if this is your case, but when I'm working in R, I rarely start out with a large complex program in mind.   I usually start in one script and separate code into logically separable units, often using functions.  Data manipulation and visualization code get placed in their own functions, etc.  And such functions are grouped together in one section of the file (data manipulation at the top, then visualization, etc).  Ultimately you want to think about how to make it easier for you to maintain your script and lower the defect rate.</p>

<p>How fine/coarse grained you make your functions will vary and there are various rules of thumb: e.g. 15 lines of code, or ""a function should be responsible for doing one task which is identified by its name"", etc.  Your mileage will vary.  Since R doesn't support call-by-reference, I'm usually vary of making my functions too fine grained when it involves passing data frames or similar structures around.  But this may be overcompensation for some silly performance mistakes when I first started out with R.  </p>

<p>When to extract logical units into their own physical units (like source files and bigger groupings like packages)?  I have two cases.  First, if the file gets too large and scrolling around among logically unrelated units is an annoyance.  Second, if I have functions that can be reused by other programs.  I usually start out by placing some grouped unit, say data manipulation functions, into a separate file.  I can then source this file from any other script.  </p>

<p>If you're going to deploy your functions, then you need to start thinking about packages.  I don't deploy R code in production or for re-use by others for various reasons (briefly: org culture prefers other langauges, concerns about performance, GPL, etc).  Also, I tend to constantly refine and add to my collections of sourced files, and I'd rather not deal with packages when I make a change.  So you should check out the other package related answers, like Dirk's, for more details on this front.</p>

<p>Finally, I think your question isn't necessarily particular to R.  I would really recommend reading Code Complete by Steve McConnell which contains a lot of wisdom about such issues and coding practices at large.</p>
"
1270645,142651,2009-08-13T07:56:23Z,1269624,2,FALSE,"<p>If you don't know the row number, but do know some values then you can use subset</p>

<pre><code>x &lt;- structure(list(A = c(5,    3.5, 3.25, 4.25,  1.5 ), 
                    B = c(4.25, 4,   4,    4.5,   4.5 ),
                    C = c(4.5,  2.5, 4,    2.25,  3   )
               ),
               .Names    = c(""A"", ""B"", ""C""),
               class     = ""data.frame"",
               row.names = c(NA, -5L)
     )

subset(x, A ==5 &amp; B==4.25 &amp; C==4.5)
</code></pre>
"
1272819,154039,2009-08-13T15:45:04Z,1169373,19,TRUE,"<p>Memory for deleted objects is not released immediately. R uses a technique called ""garbage collection"" to reclaim memory for deleted objects. Periodically, it cycles through the list of accessible objects (basically, those that have names and have not been deleted and can therefore be accessed by the user), and ""tags"" them for retention. The memory for any untagged objects is returned to the operating system after the garbage-collection sweep.</p>

<p>Garbage collection happens automatically, and you don't have any direct control over this process. But you can force a sweep by calling the command gc() from the command line.</p>

<p>Even then, on some operating systems garbage collection might not reclaim memory (as reported by the OS). Older versions of Windows, for example, could increase but not decrease the memory footprint of R. Garbage collection would only make space for new objects in the future, but would not reduce the memory use of R.</p>
"
1273412,NA,2009-08-13T17:22:36Z,1262911,1,FALSE,"<p>Thanks for all your help,</p>

<p>Yes, so initially there were some hashes and I was able to handle them using <code>comment.char = ''</code>.  The problem turned out to be that some of my URLs contained ' and "" characters.  The strangest thing about the situation is that it didn't return any errors.  After I removed these characters using tr, I had no issues with loading the data.</p>
"
1274222,142651,2009-08-13T19:57:17Z,1274171,15,FALSE,"<p>Why do you want default NA values? As far as I know matrices are only sparse if they have zero-cells. As NA is a non-zero value, you loose all the benefits from the sparse matrix. A classic matrix is even more efficient if the matrix has hardly any zeros. A classic matrix is like a vector that will be cut according to the dimensions. So it only has to store the data vector and the dimensions. The sparse matrix stores only the non-zero values, but also stores there location. This is an advantage if and only if you have enough zero values.</p>
"
1279020,143305,2009-08-14T17:10:57Z,1279003,26,FALSE,"<p>You do that in the device, e.g. </p>

<pre><code>x11(width=4, height=6)
</code></pre>

<p>and similarly for the file-based ones</p>

<pre><code>pdf(""/tmp/foo.pdf"", width=4, height=6)
</code></pre>

<p>You can read the physical size via <code>par(""cin"")</code> etc but not set it.</p>
"
1279136,2611,2009-08-14T17:34:36Z,1279003,6,TRUE,"<p>I usually set this at the start of my session with <code>windows.options</code>:</p>

<pre><code>windows.options(width=10, height=10)

# plot away
plot(...)
</code></pre>

<p>If you need to reset to ""factory settings"":</p>

<pre><code>dev.off()
windows.options(reset=TRUE)

# more plotting
plot(...)
</code></pre>
"
1284203,143813,2009-08-16T12:32:24Z,1266279,14,FALSE,"<p>My concise answer:</p>

<ol>
<li>Write your functions carefully, identifying general enough outputs and inputs;</li>
<li>Limit the use of global variables;</li>
<li>Use S3 objects and, where appropriate, S4 objects;</li>
<li>Put the functions in packages, especially when your functions are calling C/Fortran.</li>
</ol>

<p>I believe R is more and more used in production, so the need for reusable code is greater than before. I find the interpreter much more robust  than before. There is no doubt that R is 100-300x slower than C, but usually the bottleneck is concentrated around a few lines of code, which can be delegated to C/C++. I think it would be a mistake to delegate the strengths of R in data manipulation and statistical analysis to another language. In these instances, the performance penalty is low, and in any case well worth the savings in development effort. If execution time alone were the matter, we'd be all writing assembler.</p>
"
1296673,84458,2009-08-18T21:37:22Z,1296646,52,FALSE,"<p>With <a href=""https://web.archive.org/web/20131108055534/http://rwiki.sciviews.org/doku.php?id=tips%3adata-frames%3asort"" rel=""noreferrer"">this (very helpful) function by Kevin Wright</a>, posted in the tips section of the R wiki, this is easily achieved.</p>

<pre><code>sort(dd,by = ~ -z + b)
#     b x y z
# 4 Low C 9 2
# 2 Med D 3 1
# 1  Hi A 8 1
# 3  Hi A 9 1
</code></pre>
"
1296745,143305,2009-08-18T21:51:22Z,1296646,1260,TRUE,"<p>You can use the <a href=""http://stat.ethz.ch/R-manual/R-devel/library/base/html/order.html"" rel=""noreferrer""><code>order()</code></a> function directly without resorting to add-on tools -- see this simpler answer which uses a trick right from the top of the <code>example(order)</code> code:</p>

<pre><code>R&gt; dd[with(dd, order(-z, b)), ]
    b x y z
4 Low C 9 2
2 Med D 3 1
1  Hi A 8 1
3  Hi A 9 1
</code></pre>

<p><em>Edit some 2+ years later:</em>  It was just asked how to do this by column index. The answer is to simply pass the desired sorting column(s) to the <code>order()</code> function:</p>

<pre><code>R&gt; dd[ order(-dd[,4], dd[,1]), ]
    b x y z
4 Low C 9 2
2 Med D 3 1
1  Hi A 8 1
3  Hi A 9 1
R&gt; 
</code></pre>

<p>rather than using the name of the column (and <code>with()</code> for easier/more direct access).</p>
"
1297459,144642,2009-08-19T01:56:32Z,1266279,6,FALSE,"<p>Count me as another person in favor of packages.  I'll admit to being pretty poor on writing man pages and vignettes until if/when I have to (ie being released), but it makes for a real handy way to bundle source doe.  Plus, if you get serious about maintaining your code, the points that Dirk brings up all come into plya.</p>
"
1297752,143305,2009-08-19T04:01:42Z,1297698,2,FALSE,"<p>I am not sure you can get this into lattice easily.  What you have in <code>fits</code> is a an S4 object containing a .Data slot with a list of <em>standard</em> <code>lm</code> objects:</p>

<pre><code>R&gt; class(fits)
[1] ""lmList""
attr(,""package"")
[1] ""lme4""
R&gt; class(fits@.Data)
[1] ""list""
R&gt; class(fits@.Data[[1]])
[1] ""lm""
R&gt; op &lt;- par(mfrow=c(2,4))
R&gt; invisible(lapply(fits@.Data, plot))
</code></pre>

<p>This last plot simply plots you the standard 2x2 plot for <code>lm</code> objects twice, once for each element of the list of fitted objects.  Use the <code>which</code> argument to <code>plot</code> to select subsets of these  or for other regression diagnostics.</p>

<p>If you actually want <code>lattice</code> plots of predicted vs actual, you may have to program this.</p>
"
1299539,16632,2009-08-19T12:14:46Z,1297698,6,TRUE,"<p>Instead of using <code>lmList</code>, I'd recommend the more general plyr package.  </p>

<pre><code>library(plyr)

d &lt;- data.frame(
 state = rep(c('NY', 'CA'), c(10, 10)), 
 year = rep(1:10, 2), 
 response = c(rnorm(10), rnorm(10))
)

# Create a list of models
# dlply = data frame -&gt; list
models &lt;- dlply(d, ~ state, function(df) { 
  lm(response ~ year, data = df)
})

# Extract the coefficients in a useful form
# ldply = list -&gt; data frame
ldply(models, coef)

# We can get the predictions in a similar way, but we need
# to cast to a data frame so the numbers come out as rows,
# not columns.
predictions &lt;- ldply(models, as.data.frame(predict))
</code></pre>

<p><code>predictions</code> is a regular R data frame and so is easy to plot.</p>
"
1300615,37751,2009-08-19T15:15:10Z,1299871,64,FALSE,"<p>There are some good examples of doing this over at the <a href=""http://wiki.r-project.org/rwiki/doku.php?id=tips:data-frames:merge"" rel=""noreferrer"">R Wiki</a>. I'll steal a couple here:</p>

<p><strong>Merge Method</strong></p>

<p>Since your keys are named the same the short way to do an inner join is merge():</p>

<pre><code>merge(df1,df2)
</code></pre>

<p>a full inner join (all records from both tables) can be created with the ""all"" keyword:</p>

<pre><code>merge(df1,df2, all=TRUE)
</code></pre>

<p>a left outer join of df1 and df2:</p>

<pre><code>merge(df1,df2, all.x=TRUE)
</code></pre>

<p>a right outer join of df1 and df2:</p>

<pre><code>merge(df1,df2, all.y=TRUE)
</code></pre>

<p>you can flip 'em, slap 'em and rub 'em down to get the other two outer joins you asked about :)</p>

<p><strong>Subscript Method</strong></p>

<p>A left outer join with df1 on the left using a subscript method would be:</p>

<pre><code>df1[,""State""]&lt;-df2[df1[ ,""Product""], ""State""]
</code></pre>

<p>The other combination of outer joins can be created by mungling the left outer join subscript example. (yeah, I know that's the equivalent of saying ""I'll leave it as an exercise for the reader..."")</p>
"
1300618,143319,2009-08-19T15:15:41Z,1299871,847,TRUE,"<p>By using the <code>merge</code> function and its optional parameters:</p>

<p><strong><em>Inner join:</em></strong> <code>merge(df1, df2)</code> will work for these examples because R automatically joins the frames by common variable names, but you would most likely want to specify <code>merge(df1, df2, by = ""CustomerId"")</code> to make sure that you were matching on only the fields you desired.  You can also use the <code>by.x</code> and <code>by.y</code> parameters if the matching variables have different names in the different data frames.</p>

<p><strong><em>Outer join:</em></strong> <code>merge(x = df1, y = df2, by = ""CustomerId"", all = TRUE)</code></p>

<p><strong><em>Left outer:</em></strong> <code>merge(x = df1, y = df2, by = ""CustomerId"", all.x = TRUE)</code></p>

<p><strong><em>Right outer:</em></strong> <code>merge(x = df1, y = df2, by = ""CustomerId"", all.y = TRUE)</code></p>

<p><strong><em>Cross join:</em></strong> <code>merge(x = df1, y = df2, by = NULL)</code></p>

<p><strike>Just as with the inner join, you would probably want to explicitly pass ""CustomerId"" to R as the matching variable.</strike>  I think it's almost always best to explicitly state the identifiers on which you want to merge; it's safer if the input data.frames change unexpectedly and easier to read later on.</p>
"
1300949,143305,2009-08-19T16:00:11Z,1300575,6,TRUE,"<p>You can use <code>model.matrix()</code> and <code>model.frame()</code> to evaluate the formula:</p>

<pre><code>lm1 &lt;- lm(log(Volume) ~ log(Girth) + log(Height), data=trees)
print(lm1)

form &lt;- log(Volume) ~ log(Girth) + log(Height)

# use model.matrix
mm &lt;- model.matrix(form, trees)
lm2 &lt;- lm.fit(as.matrix(mm), log(trees[,""Volume""]))
print(coefficients(lm2))

# use model.frame, need to add intercept by hand
mf &lt;- model.frame(form, trees)
lm3 &lt;- lm.fit(as.matrix(data.frame(""Intercept""=1, mf[,-1])), mf[,1])
print(coefficients(lm3))
</code></pre>

<p>which yields </p>

<pre><code>Call: lm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)

Coefficients: (Intercept)   log(Girth) log(Height)
      -6.63         1.98         1.12

(Intercept)  log(Girth) log(Height)
     -6.632       1.983       1.117  
Intercept  log.Girth. log.Height.
     -6.632       1.983       1.117
</code></pre>
"
1302334,143377,2009-08-19T20:05:46Z,1301759,3,TRUE,"<p>There are several ways to do this (it is R, after all) but I think the most clear is creating an index. We need a function that creates a sequential index (starting at one and ending with the number of observations).</p>

<pre><code>seq_len(3) 
&gt; [1] 1 2 3
</code></pre>

<p>But we need to calculate this index within each grouping variable (state). For this we can use R's <code>ave</code> function. It takes a numeric as the first argument, then the grouping factors, and finally the function to be applied in each group. </p>

<pre><code>s1$index &lt;- with(s1,ave(value1,state,FUN=seq_len))
s2$index &lt;- with(s2,ave(value2,state,FUN=seq_len))
</code></pre>

<p>(Note the use of <code>with</code>, which tells R to search for the variables within the environment/dataframe. This is better practice than using s1$value1, s2$value2, etc.)</p>

<p>Now we can simply merge (join) the two data frames (by the variables present in the both data frames:  state and index). </p>

<pre><code>merge(s1,s2)
</code></pre>

<p>which gives</p>

<pre><code>   state index value1 value2
1    IA     1      1      6
2    IA     2      2      7
3    IA     3      3      8
4    IL     1      4      3
5    IL     2      5      4
6    IL     3      6      5
</code></pre>

<p>For this to work, there should be the same number of observations by state in each of the data frames.</p>

<p>[Edit: commented the code for clarity.]
[Edit: Used seq_len instead of creating a new function as suggested by hadley.]</p>
"
1307824,2002705,2009-08-20T17:54:49Z,1299871,156,FALSE,"<p>I would recommend checking out <a href=""http://cran.r-project.org/web/packages/sqldf/index.html"" rel=""noreferrer"">Gabor Grothendieck's sqldf package</a>, which allows you to express these operations in SQL.</p>

<pre><code>library(sqldf)

## inner join
df3 &lt;- sqldf(""SELECT CustomerId, Product, State 
              FROM df1
              JOIN df2 USING(CustomerID)"")

## left join (substitute 'right' for right join)
df4 &lt;- sqldf(""SELECT CustomerId, Product, State 
              FROM df1
              LEFT JOIN df2 USING(CustomerID)"")
</code></pre>

<p>I find the SQL syntax to be simpler and more natural than its R equivalent (but this may just reflect my RDBMS bias).</p>

<p>See <a href=""https://github.com/ggrothendieck/sqldf#example-4-join"" rel=""noreferrer"">Gabor's sqldf GitHub</a> for more information on joins. </p>
"
1308393,160314,2009-08-20T19:43:30Z,1296646,24,FALSE,"<p>Alternatively, using the package Deducer</p>

<pre><code>library(Deducer)
dd&lt;- sortData(dd,c(""z"",""b""),increasing= c(FALSE,TRUE))
</code></pre>
"
1309127,23929,2009-08-20T22:12:04Z,1169248,51,FALSE,"<p>The <strong>any()</strong> function makes for readable code</p>

<pre><code>&gt; w &lt;- c(1,2,3)
&gt; any(w==1)
[1] TRUE

&gt; v &lt;- c('a','b','c')
&gt; any(v=='b')
[1] TRUE

&gt; any(v=='f')
[1] FALSE
</code></pre>
"
1309562,143305,2009-08-21T00:41:30Z,1309263,22,FALSE,"<p>I have looked at R's <code>src/library/stats/src/Trunmed.c</code> a few times as I wanted something similar too in a standalone C++ class / C subroutine. Note that this are actually two implementations in one, see <code>src/library/stats/man/runmed.Rd</code> (the source of the help file) which says</p>

<pre><code>\details{
  Apart from the end values, the result \code{y = runmed(x, k)} simply has
  \code{y[j] = median(x[(j-k2):(j+k2)])} (k = 2*k2+1), computed very
  efficiently.

  The two algorithms are internally entirely different:
  \describe{
    \item{""Turlach""}{is the Härdle-Steiger
      algorithm (see Ref.) as implemented by Berwin Turlach.
      A tree algorithm is used, ensuring performance \eqn{O(n \log
        k)}{O(n * log(k))} where \code{n &lt;- length(x)} which is
      asymptotically optimal.}
    \item{""Stuetzle""}{is the (older) Stuetzle-Friedman implementation
      which makes use of median \emph{updating} when one observation
      enters and one leaves the smoothing window.  While this performs as
      \eqn{O(n \times k)}{O(n * k)} which is slower asymptotically, it is
      considerably faster for small \eqn{k} or \eqn{n}.}
  }
}
</code></pre>

<p>It would be nice to see this re-used in a more standalone fashion. Are you volunteering?  I can help with some of the R bits.</p>

<p><em>Edit 1</em>: Besides the link to the older version of Trunmed.c above, here are current SVN copies of</p>

<ul>
<li><a href=""http://svn.r-project.org/R/trunk/src/library/stats/src/Srunmed.c"" rel=""noreferrer""><code>Srunmed.c</code></a> (for the Stuetzle version)</li>
<li><a href=""http://svn.r-project.org/R/trunk/src/library/stats/src/Trunmed.c"" rel=""noreferrer""><code>Trunmed.c</code></a> (for the Turlach version)</li>
<li><a href=""http://svn.r-project.org/R/trunk/src/library/stats/R/runmed.R"" rel=""noreferrer""><code>runmed.R</code></a> for the R function calling these</li>
</ul>

<p><em>Edit 2</em>: Ryan Tibshirani has some C and Fortran code on <a href=""http://www.stat.cmu.edu/~ryantibs/median/"" rel=""noreferrer"">fast median binning</a> which may be a suitable starting point for a windowed approach.</p>
"
1309786,10897,2009-08-21T02:13:07Z,1309263,-4,FALSE,"<p>If you just require a smoothed average a quick/easy way is to multiply the latest value by x and the average value by (1-x) then add them. This then becomes the new average.</p>

<p>edit: Not what the user asked for and not as statistically valid but good enough for a lot of uses.<br>
I'll leave it in here (in spite of the downvotes) for search!</p>
"
1309815,19410,2009-08-21T02:26:48Z,1309263,1,FALSE,"<p>If you have the ability to reference values as a function of points in time, you could sample values with replacement, applying <a href=""http://www.uvm.edu/~dhowell/StatPages/Resampling/BootstMedians/bootstrapping_medians.html"" rel=""nofollow noreferrer"">bootstrapping</a> to generate a bootstrapped median value within confidence intervals. This may let you calculate an approximated median with greater efficiency than constantly sorting incoming values into a data structure.</p>
"
1310332,144157,2009-08-21T06:27:50Z,1310247,3,FALSE,"<p>I prefer not to use <code>attach()</code>, as it is far too easy to run a batch of code several times each time calling <code>attach()</code>. The data frame is added to the search path each time, extending it unnecessarily. Of course, good programming practice is to also <code>detach()</code> at the end of the block of code, but that is often forgotten.</p>

<p>Instead, I use xxx$y or xxx[,""y""]. It's more transparent. </p>

<p>Another possibility is to use the data argument available in many functions which allows individual variables to be referenced within the data frame. e.g., <code>lm(z ~ y, data=xxx)</code>.</p>
"
1310592,142651,2009-08-21T07:55:32Z,1310247,8,FALSE,"<p>The main problem with attach is that it can result in unwanted behaviour. Suppose you have an object with name xyz in your workspace. Now you attach dataframe abc which has a column named xyz. If your code reference to xyz, can you guarantee that is references to the object or the dataframe column? If you don't use attach then it is easy. just xyz refers to the object. abc$xyz refers to the column of the dataframe.</p>

<p>One of the main reasons that attach is used frequently in textbooks is that it shortens the code.</p>
"
1311613,143305,2009-08-21T12:17:47Z,1310247,12,FALSE,"<p>I much prefer to use <code>with</code> to obtain the equivalent of <code>attach</code> on a single command:</p>

<pre><code> with(someDataFrame,  someFunction(...))
</code></pre>

<p>This also leads naturally to a form where <code>subset</code> is the first argument:</p>

<pre><code> with(subset(someDataFrame,  someVar &gt; someValue),
      someFunction(...))
</code></pre>

<p>which makes it pretty clear that we operate on a selection of the data.  And while many modelling function have both <code>data</code> and <code>subset</code> arguments, the use above is more consistent as it also applies to those functions who do not have  <code>data</code> and <code>subset</code> arguments.</p>
"
1311620,143377,2009-08-21T12:19:47Z,1310247,24,TRUE,"<p>I never use attach. <code>with</code> and <code>within</code> are your friends.</p>

<p>Example code:</p>

<pre><code>&gt; N &lt;- 3
&gt; df &lt;- data.frame(x1=rnorm(N),x2=runif(N))
&gt; df$y &lt;- with(df,{
   x1+x2
 })
&gt; df
          x1         x2          y
1 -0.8943125 0.24298534 -0.6513271
2 -0.9384312 0.01460008 -0.9238312
3 -0.7159518 0.34618060 -0.3697712
&gt; 
&gt; df &lt;- within(df,{
   x1.sq &lt;- x1^2
   x2.sq &lt;- x2^2
   y &lt;- x1.sq+x2.sq
   x1 &lt;- x2 &lt;- NULL
 })
&gt; df
          y        x2.sq     x1.sq
1 0.8588367 0.0590418774 0.7997948
2 0.8808663 0.0002131623 0.8806532
3 0.6324280 0.1198410071 0.5125870
</code></pre>

<p>Edit: hadley mentions transform in the comments. here is some code:</p>

<pre><code> &gt; transform(df, xtot=x1.sq+x2.sq, y=NULL)
       x2.sq       x1.sq       xtot
1 0.41557079 0.021393571 0.43696436
2 0.57716487 0.266325959 0.84349083
3 0.04935442 0.004226069 0.05358049
</code></pre>
"
1312211,143305,2009-08-21T14:13:49Z,1311920,8,FALSE,"<p>Use a proper <code>class</code> for your objects; base R has <code>ts</code> which has a <code>lag()</code> function to operate on.  Note that these <code>ts</code> objects came from a time when 'delta' or 'frequency' where constant:  monthly or quarterly data as in macroeconomic series.</p>

<p>For irregular data such as (business-)daily, use the <a href=""http://cran.r-project.org/package=zoo"" rel=""noreferrer"">zoo</a> or <a href=""http://cran.r-project.org/package=xts"" rel=""noreferrer"">xts</a> packages which can also deal (very well!) with lags.  To go further from there, you can use packages like <a href=""http://cran.r-project.org/package=dynlm"" rel=""noreferrer"">dynlm</a> or <a href=""http://cran.r-project.org/package=dlm"" rel=""noreferrer"">dlm</a> allow for dynamic regression models with lags.</p>

<p>The Task Views on Time Series, Econometrics, Finance all have further pointers.  </p>
"
1312929,16632,2009-08-21T16:24:01Z,1312865,5,TRUE,"<p>Have a look at the <code>layout</code> parameter. </p>

<p>Maybe you want something like:</p>

<pre><code>xyplot(Predicted_value + Actual_value ~ x_value | State_CD, data=dd, layout = c(4,5))
</code></pre>
"
1312933,158065,2009-08-21T16:24:46Z,1311920,2,FALSE,"<p>The <code>running</code> function in the <code>gtools</code> package does more or less what you want:</p>

<pre><code>&gt; require(""gtools"")
&gt; running(1:4, fun=I, width=3, allow.fewer=TRUE)

$`1:1`
[1] 1

$`1:2` 
[1] 1 2

$`1:3` 
[1] 1 2 3

$`2:4` 
[1] 2 3 4
</code></pre>
"
1313568,2002705,2009-08-21T18:29:57Z,1311920,21,TRUE,"<p>You can achieve this using the built-in <code>embed()</code> function, where its second 'dimension' argument is equivalent to what you've called 'lag':</p>

<pre><code>x &lt;- c(NA,NA,1,2,3,4)
embed(x,3)

## returns
     [,1] [,2] [,3]
[1,]    1   NA   NA
[2,]    2    1   NA
[3,]    3    2    1
[4,]    4    3    2
</code></pre>

<p><code>embed()</code> was discussed in a <a href=""https://stackoverflow.com/questions/1169376/cumulative-sums-moving-averages-and-sql-group-by-equivalents-in-r/1172367#1172367"">previous answer</a> by Joshua Reich.  (Note that I prepended x with NAs to replicate your desired output). </p>

<p>It's not particularly well-named but it is quite useful and powerful for operations involving sliding windows, such as rolling sums and moving averages.</p>
"
1313993,37751,2009-08-21T20:11:49Z,1313954,1,FALSE,"<p>well after posting the question I ran across <a href=""http://www.nabble.com/A-beginner%27s-question-about-ggplot-td23336793.html"" rel=""nofollow noreferrer"">this R Help thread</a> that may have helped me. It looks like I can do this:</p>

<pre><code> pg + geom_line(data=dd,aes(x_value, Actual_value,group=State_CD), colour=""green"") 
</code></pre>

<p>is that a good way of doing things? It odd to me because adding the second item has a totally different syntax than the first. </p>
"
1314082,160314,2009-08-21T20:35:27Z,1313954,2,FALSE,"<p>you might just want to change the form of your data a little bit, so that you have one y-axis variable, with an additional factor variable indicating whether it is a predicted or actual variable.</p>

<p>Is this something like what you are trying to do?</p>

<pre><code>dd&lt;-data.frame(type=rep(c(""Predicted_value"",""Actual_value""),20),y_value=rnorm(40),
                x_value=rnorm(40),State_CD=rnorm(40)&gt;0)
qplot(x_value,y_value,data=dd,colour=type,facets=.~State_CD)
</code></pre>
"
1314341,143319,2009-08-21T21:36:23Z,1313954,6,FALSE,"<p><strong>Update</strong>: several years on now, I almost always use Jonathan's method (via the <a href=""https://blog.rstudio.org/2014/07/22/introducing-tidyr/"" rel=""nofollow noreferrer"">tidyr package</a>) with ggplot2. My answer below works in a pinch, but gets tedious fast when you have 3+ variables.</p>

<hr>

<p>I'm sure Hadley will have a better answer, but - the syntax is different because the <code>ggplot(dd,aes())</code> syntax is (I think) primarily intended for plotting just one variable.  For two, I would use:</p>

<pre><code>ggplot() + 
geom_point(data=dd, aes(x_value, Actual_value, group=State_CD), colour=""green"") + 
geom_point(data=dd, aes(x_value, Predicted_value, group=State_CD), shape = 2) + 
facet_wrap(~ State_CD) + 
theme(aspect.ratio = 1)
</code></pre>

<p>Pulling the first set of points out of the ggplot() gives it the same syntax as the second.  I find this easier to deal with because the syntax is the same and it emphasizes the ""Grammar of Graphics"" that is at the core of ggplot2.</p>
"
1314342,158065,2009-08-21T21:36:39Z,1313954,18,TRUE,"<p>Just following up on what Ian suggested: for ggplot2 you really want all the y-axis stuff in one column with another column as a factor indicating how you want to decorate it.  It is easy to do this with <code>melt</code>.  To wit:</p>

<pre><code>qplot(x_value, value, 
      data = melt(dd, measure.vars=c(""Predicted_value"", ""Actual_value"")), 
      colour=variable) + facet_wrap(~State_CD)
</code></pre>

<p>Here's what it looks like for me:
<a href=""http://www.cs.princeton.edu/~jcone/example.png"">alt text http://www.cs.princeton.edu/~jcone/example.png</a></p>

<p>To get an idea of what <code>melt</code> is actually doing, here's the head:</p>

<pre><code>&gt; head(melt(dd, measure.vars=c(""Predicted_value"", ""Actual_value"")))
     x_value State_CD        variable      value
1  1.2898779        A Predicted_value  1.0913712
2  0.1077710        A Predicted_value -2.2337188
3 -0.9430190        A Predicted_value  1.1409515
4  0.3698614        A Predicted_value -1.8260033
5 -0.3949606        A Predicted_value -0.3102753
6 -0.1275037        A Predicted_value -1.2945864
</code></pre>

<p>You see, it ""melts"" Predicted_value and Actual_value into one column called <code>value</code> and adds another column called <code>variable</code> letting you know what column it originally came from.</p>
"
1319759,83761,2009-08-23T22:31:18Z,1319698,6,FALSE,"<p>Is using <code>sprintf</code> any more convenient for you?</p>

<p>Barring that, how about this little sleight of hand:</p>

<pre><code>'%+%' &lt;- paste

'and' %+% 'now' %+% 'for'%+% 'something' %+% 'completely' %+% 'different'    
# [1] ""and now for something completely different""
</code></pre>
"
1319786,86684,2009-08-23T22:42:54Z,1266279,46,FALSE,"<p>I like putting different functionality in their own files.</p>

<p>But I don't like R's package system.  It's rather hard to use.</p>

<p>I prefer a lightweight alternative, to place a file's functions inside an environment (what every other language calls a ""namespace"") and attach it.  For example, I made a 'util' group of functions like so:</p>

<pre><code>util = new.env()

util$bgrep = function [...]

util$timeit = function [...]

while(""util"" %in% search())
  detach(""util"")
attach(util)
</code></pre>

<p>This is all in a file <a href=""http://github.com/brendano/dlanalysis/tree/master/util.R"" rel=""noreferrer"">util.R</a>.  When you source it, you get the environment 'util' so you can call <code>util$bgrep()</code> and such; but furthermore, the <code>attach()</code> call makes it so just <code>bgrep()</code> and such work directly.  If you didn't put all those functions in their own environment, they'd pollute the interpreter's top-level namespace (the one that <code>ls()</code> shows).</p>

<p>I was trying to simulate Python's system, where every file is a module.  That would be better to have, but this seems OK.</p>
"
1321476,161921,2009-08-24T10:01:50Z,1274171,12,TRUE,"<p>Yes,  Thierry's answer is definitely true I can say as co-author of the 'Matrix' package...</p>

<p>To your other question: Why is accessing ""M"" slower than ""Y""? 
The main answer is that ""M"" is much much sparser than ""Y"" hence much smaller and -- depending on the sizes envolved and the RAM of your platform -- the access time is faster for much smaller objects, notably for indexing into them.</p>
"
1321491,161921,2009-08-24T10:05:07Z,1319698,23,TRUE,"<p>@ Dirk:  For once, you're not quite right. It's not the parser.
One <em>can</em> write methods in R for ""+"" -- help(""+"") goes to ""Arithmetic operators"" and mentions
that these are generic and you can write methods for them ... and of course many package writers do, e.g., we do for the 'Matrix' package, and I also do for the ""Rmpfr"" package, e.g.
But Dirk is also right (of course!) that you cannot do it in R currently,
by just defining a method for ""+.character"".</p>

<p>About three years ago, I had started a thread on R-devel (the R mailing list on R development; very much recommended if you are interested in these topics; you can also access through Gmane if you don't want to subscribe) :<a href=""https://www.stat.math.ethz.ch/pipermail/r-devel/2006-August/038991.html"" rel=""noreferrer"">r-devel archived msg</a></p>

<p>It came to an interesting discussion with quite a few pros and cons,
notably John Chambers (""the father of S and hence R"") pretty strongly opposing to use ""+"" for an operation that is <strong>not</strong> commutative, 
and also <a href=""https://www.stat.math.ethz.ch/pipermail/r-devel/2006-August/039012.html"" rel=""noreferrer"">r-devel archived msg2</a> (by another R-core member), supporting the view that we (R Core) should not adopt / support the idea; and if people **really* wanted it, they could define
%+%  for that.</p>
"
1326095,143319,2009-08-25T04:44:01Z,1325974,7,TRUE,"<p>I can see a couple of potential problems here - rcom has a weird installation.</p>

<ol>
<li><p>Did you load the package - that is, run <code>library(rcom)</code> after installing it?  I know, I know - but I forget that step all the time, and it would definitely give you the ""could not find function"" error.</p></li>
<li><p>Did you run the <code>installstatconnDCOM()</code> command?  I almost didn't notice it telling me to do this.</p></li>
</ol>
"
1328932,23855,2009-08-25T15:14:22Z,1328903,1,FALSE,"<p>Based on my quick look at the <a href=""http://cran.r-project.org/doc/manuals/R-lang.html#Function-calls"" rel=""nofollow noreferrer"">manual</a> it may be a user defined infix operator, so, it's hard to tell what the actual meaning would be...</p>

<p>I would think binary addition.  </p>
"
1329037,143305,2009-08-25T15:28:15Z,1328903,15,FALSE,"<p>There is no generally defined <code>%+%</code>.  Maybe you looked at <a href=""https://stackoverflow.com/questions/1319698/why-doesnt-operate-on-characters-in-r"">this question from yesterday</a> where </p>

<pre><code>R&gt; '%+%' &lt;- paste
R&gt; ""foo"" %+% ""bar""
[1] ""foo bar""
R&gt; 
</code></pre>

<p>and ad-hoc string concatenation function was defined.    Generally, the 'percent-operator-percent' syntax is open for user-defined functions of two arguments, but there is (AFAIK) no generally accepted version for <code>%+%</code> that you can expect to be present everywhere.</p>
"
1329946,84458,2009-08-25T18:04:01Z,1329940,105,TRUE,"<p>One option is to use <code>do.call()</code>: </p>

<pre><code> &gt; do.call(rbind, a)
      [,1] [,2] [,3] [,4] [,5] [,6]
 [1,]    1    1    2    3    4    5
 [2,]    2    1    2    3    4    5
 [3,]    3    1    2    3    4    5
 [4,]    4    1    2    3    4    5
 [5,]    5    1    2    3    4    5
 [6,]    6    1    2    3    4    5
 [7,]    7    1    2    3    4    5
 [8,]    8    1    2    3    4    5
 [9,]    9    1    2    3    4    5
[10,]   10    1    2    3    4    5
</code></pre>
"
1331219,84458,2009-08-25T21:48:39Z,1331203,1,FALSE,"<p>Using factors:</p>

<pre><code>&gt; A$id &lt;- as.factor(A$id)
&gt; A$id.new &lt;- as.numeric(A$id)
&gt; head(A)
       id          x id.new
1 4566144  1.5164706      4
2 9404670 -1.5487528     10
3 5281052  0.5846137      5
4  455565  0.1238542      1
5 7883051  0.2159416      7
6 5514346  0.3796395      6
</code></pre>

<p>Suppose x is the old ID and you want the new one.</p>

<pre><code>&gt; x &lt;- 7883051
&gt; as.numeric(which(levels(A$id)==x))
[1] 7
</code></pre>

<p>Suppose y is the new ID and you want the old one.</p>

<pre><code>&gt; as.numeric(as.character(A$id[which(as.integer(A$id)==y)[1]]))
[1] 5281052
</code></pre>

<p>(The above finds the first value of id at which the internal code for the factor is 5.  Are there better ways?)</p>
"
1331234,84458,2009-08-25T21:52:48Z,1331203,0,FALSE,"<p>One option is to use the <code>hash</code> package:</p>

<pre><code>&gt; library(hash)
&gt; sn &lt;- sort(unique(A$id))
&gt; g &lt;- hash(1:length(sn),sn)
&gt; h &lt;- hash(sn,1:length(sn))
&gt; A$id.new &lt;- .get(h,A$id)
&gt; head(A)
       id          x id.new
1 4566144  1.5164706      4
2 9404670 -1.5487528     10
3 5281052  0.5846137      5
4  455565  0.1238542      1
5 7883051  0.2159416      7
6 5514346  0.3796395      6
</code></pre>

<p>Suppose x is the old ID and you want the new one.</p>

<pre><code>&gt; x &lt;- 7883051
&gt; .get(h,as.character(x))
7883051 
      7 
</code></pre>

<p>Suppose y is the new ID and you want the old one.</p>

<pre><code>&gt; y &lt;- 5
&gt; .get(g,as.character(y))
      5 
5281052
</code></pre>

<p>(This can sometimes be more convenient/transparent than using factors.)</p>
"
1331243,144157,2009-08-25T21:54:26Z,1331203,1,TRUE,"<p>Try this:</p>

<pre><code>A$id.new &lt;- match(A$id,unique(A$id))
</code></pre>

<p><strong>Additional comment:</strong>
To get the table of values:</p>

<pre><code>rbind(unique(A$id.new),unique(A$id))
</code></pre>
"
1331254,143305,2009-08-25T21:57:14Z,1331203,1,FALSE,"<p>You can use factor() / ordered() here:</p>

<pre><code>R&gt; set.seed(123)
R&gt; ids &lt;- sample(1:1e7,10)
R&gt; A &lt;- data.frame(id=sample(ids,100,replace=TRUE), x=rnorm(100))
R&gt; A$id.new &lt;- as.ordered(as.character(A$id))
R&gt; table(A$id.new)

2875776 4089769  455565 4566144 5281052 5514346 7883051 8830172 8924185 9404670 
      6      10       6       8      12      10      13      10      10      15 
</code></pre>

<p>And you can then use as.numeric() to map to 1 to 10:</p>

<pre><code>R&gt; A$id.new &lt;- as.numeric(A$id.new)
R&gt; summary(A)
       id                x               id.new     
 Min.   : 455565   Min.   :-2.3092   Min.   : 1.00  
 1st Qu.:4566144   1st Qu.:-0.6933   1st Qu.: 4.00  
 Median :5514346   Median :-0.0634   Median : 6.00  
 Mean   :6370243   Mean   :-0.0594   Mean   : 6.07  
 3rd Qu.:8853675   3rd Qu.: 0.5575   3rd Qu.: 8.25  
 Max.   :9404670   Max.   : 2.1873   Max.   :10.00  
R&gt; 
</code></pre>
"
1331400,158065,2009-08-25T22:36:04Z,1330989,581,TRUE,"<p>Change the last line to </p>

<pre><code>q + theme(axis.text.x = element_text(angle = 90, hjust = 1))
</code></pre>

<p>By default, the axes are aligned at the center of the text, even when rotated.  When you rotate +/- 90 degrees, you usually want it to be aligned at the edge instead:</p>

<p><img src=""https://learnr.files.wordpress.com/2009/03/immigration_b4.png?w=416&amp;h=415"" alt=""alt text""></p>

<p>The image above is from <a href=""http://learnr.wordpress.com/2009/03/17/ggplot2-barplots/"" rel=""noreferrer"">this blog post</a>.</p>
"
1332708,143591,2009-08-26T06:30:54Z,1329940,11,FALSE,"<p>Not straightforward, but it works:</p>

<pre><code>&gt; t(sapply(a, unlist))
      [,1] [,2] [,3] [,4] [,5] [,6]
 [1,]    1    1    2    3    4    5
 [2,]    2    1    2    3    4    5
 [3,]    3    1    2    3    4    5
 [4,]    4    1    2    3    4    5
 [5,]    5    1    2    3    4    5
 [6,]    6    1    2    3    4    5
 [7,]    7    1    2    3    4    5
 [8,]    8    1    2    3    4    5
 [9,]    9    1    2    3    4    5
[10,]   10    1    2    3    4    5
</code></pre>
"
1335871,31000,2009-08-26T16:13:52Z,1335830,58,TRUE,"<p>The documentation for ifelse states:</p>

<blockquote>
  <p><code>ifelse</code> returns a value with the same
  shape as <code>test</code> which is filled with
  elements selected from either <code>yes</code> or
  <code>no</code> depending on whether the element
  of <code>test</code> is <code>TRUE</code> or <code>FALSE</code>.</p>
</blockquote>

<p>Since you are passing test values of length 1, you are getting results of length 1. If you pass longer test vectors, you will get longer results:</p>

<pre><code>&gt; ifelse(c(TRUE, FALSE), c(1, 2), c(3, 4))
[1] 1 4
</code></pre>
"
1336333,10897,2009-08-26T17:28:26Z,1336271,2,FALSE,"<p>Excellent review of <a href=""http://paulbourke.net/papers/conrec/"" rel=""nofollow noreferrer"">contouring algorithm</a>, you might need to mesh the surface first to interpolate onto a grid.</p>
"
1336384,64253,2009-08-26T17:40:28Z,1336271,1,FALSE,"<p>maybe you can use:</p>

<ul>
<li><a href=""http://cran.r-project.org/web/packages/GEOmap/index.html"" rel=""nofollow noreferrer"">GEOMap</a></li>
<li><a href=""http://cran.r-project.org/web/packages/geomapdata/index.html"" rel=""nofollow noreferrer"">geomapdata</a></li>
<li><a href=""http://cran.r-project.org/web/packages/gtm/index.html"" rel=""nofollow noreferrer"">gtm</a></li>
</ul>

<p>with</p>

<ul>
<li><a href=""http://cran.r-project.org/web/packages/Matrix/index.html"" rel=""nofollow noreferrer"">Matrix</a></li>
<li><a href=""http://cran.r-project.org/web/packages/SparseM/index.html"" rel=""nofollow noreferrer"">SparseM</a></li>
<li><a href=""http://cran.r-project.org/web/packages/slam/index.html"" rel=""nofollow noreferrer"">slam</a></li>
</ul>

<p><a href=""http://cran.r-project.org/web/packages/"" rel=""nofollow noreferrer"">in R</a></p>
"
1336437,153430,2009-08-26T17:51:44Z,1336271,3,FALSE,"<p><a href=""http://en.wikipedia.org/wiki/Kriging"" rel=""nofollow noreferrer"">Kriging interpolation</a> may be of some use for smoothly interpolating your sparse samples.</p>
"
1343529,143305,2009-08-27T20:16:34Z,1343442,6,FALSE,"<p>Many of us use ESS / Emacs for this very reason.  Saving old sessions with extension '.Rt' even gives you mode-specific commands for re-running parts of your session.</p>
"
1344178,143377,2009-08-27T22:51:46Z,1343442,4,FALSE,"<p>Greg Snow wrote recently on the R-help list (a very valuable resource, SO R people!): </p>

<p>""You may also want to look at ?TeachingDemos::txtStart as an alternative to sink, one advantage is that the commands as well as the output can be included.  With a little more work you can also include graphical output into a transcript file.""</p>

<p><a href=""http://www.nabble.com/Output-to-screen-and-file-at-the-same-time-td24954441.html"" rel=""nofollow noreferrer"">r-help</a></p>
"
1344238,144157,2009-08-27T23:13:04Z,1343442,1,FALSE,"<p>Check out the <a href=""http://stat.ethz.ch/R-manual/R-patched/library/utils/html/savehistory.html"" rel=""nofollow noreferrer"">savehistory() command</a></p>
"
1349177,86684,2009-08-28T20:30:22Z,1335830,7,FALSE,"<p>yeah, I think ifelse() is really designed for when you have a big long vector of tests and want to map each to one of two options.  For example, I often do colors for plot() in this way:</p>

<pre><code>plot(x,y, col = ifelse(x&gt;2,  'red', 'blue'))
</code></pre>

<p>If you had a big long vector of tests but wanted pairs for outputs, you could use <code>sapply()</code> or <code>plyr</code>'s <code>llply()</code> or something, perhaps.</p>
"
1351962,143305,2009-08-29T17:41:33Z,1351937,5,TRUE,"<p>A few comments to your questions:</p>

<ol>
<li>Look at <code>help(Startup)</code>. You are probably running <code>$R_HOME/etc/Rprofile</code> [ which on Ubuntu and Debian is also available as <code>/etc/R/Rprofile</code> as we created softlinks to connect them ] and you have both an empty <code>$R_HOME/etc/Rprofile.site</code> as well as <code>~/.Rprofile</code> to play with.</li>
<li>For starters, do <code>ls -l</code> on the lockfile and see it is there.  If so, and if you have no other R process running, remove. Re-start the package install.</li>
<li>RODBC needs development headers for an ODBC library and all that -- again, for Debian / Ubuntu you just say <code>sudo apt-get install r-cran-rodbc</code>. If you run another distro, see if they have RODBC prepackaged.</li>
<li>Regarding <code>~/.Rprofile</code> there are few possible tips and tricks you can learn from <a href=""https://stackoverflow.com/questions/1189759/expert-r-users-whats-in-your-rprofile"">this earlier StackOverflow question</a>.</li>
</ol>
"
1352386,NA,2009-08-29T21:11:02Z,1343442,0,FALSE,"<p>Emacs is good, but for those of us with a vi preference there's the vim-r plugin at:</p>

<p><a href=""http://www.uft.uni-bremen.de/chemie/ranke/index.php?page=vim_R_linux"" rel=""nofollow noreferrer"">http://www.uft.uni-bremen.de/chemie/ranke/index.php?page=vim_R_linux</a></p>

<p>It works brilliantly and has a tiny memory footprint.</p>
"
1353119,158065,2009-08-30T04:51:14Z,1352863,9,TRUE,"<p>You need to <code>print</code> the plot if you want it to display:</p>

<pre><code>print(ggplot(country.df, aes(x = value.v51)) + geom_histogram())
</code></pre>

<p>By default, ggplot commands return a plot object but the command itself does not actually display the plot; that is done with the <code>print</code> command.  Note that when you run code interactively, results of commands get printed which is why you often don't need the explicit print.  But when wrapping in a <code>foreach</code>, you need to explicitly print since the results of the commands in the body will not be echoed.</p>
"
1353998,165406,2009-08-30T14:28:17Z,1351937,-3,FALSE,"<p>Thanks for the reply. That really helps indeed!. I got my RODBC package installed using sudo apt-get. That was painless. However, I am still a bit confused regarding which Rprofile is being run. I am indeed running Ubuntu Jaunty. </p>

<p>When I do a whereis R command, I get the following.</p>

<p>R: /usr/bin/R /etc/R /usr/lib/R /usr/local/lib/R /usr/share/R /usr/share/man/man1/R.1.gz</p>

<p>Under /etc/R I have</p>

<p>ldpaths  Makeconf  Renviron  Renviron.site  repositories  Rprofile.site</p>

<p>If I cd to /usr/lib/R/etc, I have</p>

<p>ldpaths  Makeconf  Renviron  Renviron.site  repositories  Rprofile.site</p>

<p>1) My question is, which of these is being used to run R?</p>

<p>2) When I do a sudo apt-get install r-cran-rodbc, where is the package being installed?</p>

<p>3) I have a home folder for R at /home/rama/R/i486-pc-linux-gnu-library/2.9 where my bioconductor packages are all located. How do I get the sudo apt-get packages to install here? (? .Rprofile) Is it even necessary?</p>
"
1355408,143305,2009-08-31T01:09:33Z,1355355,4,FALSE,"<p>You almost had it. It <em>really</em> is just a matter of </p>

<ol>
<li>using one of the <code>*apply</code> functions to loop over your existing list, I often start with <code>lapply</code> and sometimes switch to <code>sapply</code></li>
<li>add an anonymous function that operates on one of the list elements at a time</li>
<li>you already knew it was <code>strsplit(string, splitterm)</code> and that you need the odd <code>[[1]][1]</code> to pick off the first term of the answer</li>
<li>just put it all together, starting with a preferred variable namne (as we stay clear of <code>t</code> or <code>c</code> and friends)</li>
</ol>

<p>which gives</p>

<pre><code>&gt; tlist &lt;- c(""bob_smith"",""mary_jane"",""jose_chung"",""michael_marx"",""charlie_ivan"") 
&gt; fnames &lt;- sapply(tlist, function(x) strsplit(x, ""_"")[[1]][1]) 
&gt; fnames 
  bob_smith    mary_jane   jose_chung michael_marx charlie_ivan   
      ""bob""       ""mary""       ""jose""    ""michael""    ""charlie"" 
&gt;
</code></pre>
"
1355409,143383,2009-08-31T01:10:16Z,1355355,3,FALSE,"<p>You could use <code>unlist()</code>:</p>

<pre><code>&gt; tsplit &lt;- unlist(strsplit(t,""_""))
&gt; tsplit
 [1] ""bob""     ""smith""   ""mary""    ""jane""    ""jose""    ""chung""   ""michael""
 [8] ""marx""    ""charlie"" ""ivan""   
&gt; t_out &lt;- tsplit[seq(1, length(tsplit), by = 2)]
&gt; t_out
[1] ""bob""     ""mary""    ""jose""    ""michael"" ""charlie""
</code></pre>

<p>There might be a better way to pull out only the odd-indexed entries, but in any case you won't have a loop.</p>
"
1355414,143319,2009-08-31T01:12:14Z,1355355,7,FALSE,"<p>I doubt this is the most elegant solution, but it beats looping:</p>

<pre><code>t.df &lt;- data.frame(tsplit)
t.df[1, ]
</code></pre>

<p>Converting lists to data frames is about the only way I can get them to do what I want.  I'm looking forward to reading answers by people who actually understand how to handle lists.</p>
"
1355420,144565,2009-08-31T01:16:25Z,1355355,26,TRUE,"<p>You can use <code>apply</code> (or <code>sapply</code>)</p>

<pre><code>t &lt;- c(""bob_smith"",""mary_jane"",""jose_chung"",""michael_marx"",""charlie_ivan"")
f &lt;- function(s) strsplit(s, ""_"")[[1]][1]
sapply(t, f)

bob_smith    mary_jane   jose_chung michael_marx charlie_ivan 

       ""bob""       ""mary""       ""jose""    ""michael""    ""charlie"" 
</code></pre>

<p>See: <a href=""http://nsaunders.wordpress.com/2010/08/20/a-brief-introduction-to-apply-in-r/"" rel=""noreferrer"">A brief introduction to “apply” in R</a></p>
"
1355576,165787,2009-08-31T02:33:51Z,1355355,9,FALSE,"<p>How about:</p>

<p><code>tlist &lt;- c(""bob_smith"",""mary_jane"",""jose_chung"",""michael_marx"",""charlie_ivan"")</code><br>
<code>fnames &lt;- gsub(""(_.*)$"", """", tlist)</code><br>
<code># _.* matches the underscore followed by a string of characters</code><br>
<code># the $ anchors the search at the end of the input string</code><br>
<code># so, underscore followed by a string of characters followed by the end of the input string</code>  </p>

<p>for the RegEx approach?</p>
"
1355624,165787,2009-08-31T02:56:25Z,1355355,2,FALSE,"<p>And one other approach, based on brentonk's unlist example...</p>

<p><code>tlist &lt;- c(""bob_smith"",""mary_jane"",""jose_chung"",""michael_marx"",""charlie_ivan"")</code><br>
<code>tsplit &lt;- unlist(strsplit(tlist,""_""))</code><br>
<code>fnames &lt;- tsplit[seq(1:length(tsplit))%%2 == 1]</code></p>
"
1355660,16632,2009-08-31T03:20:05Z,1355355,43,FALSE,"<p>And one more approach:</p>

<pre><code>t &lt;- c(""bob_smith"",""mary_jane"",""jose_chung"",""michael_marx"",""charlie_ivan"")
pieces &lt;- strsplit(t,""_"")
sapply(pieces, ""["", 1)
</code></pre>

<p>In words, the last line extracts the first element of each component of the list and then simplifies it into a vector.</p>

<p>How does this work? Well, you need to realise an alternative way of writing <code>x[1]</code> is <code>""[""(x, 1)</code>, i.e. there is a function called <code>[</code> that does subsetting. The <code>sapply</code> call applies calls this function once for each element of the original list, passing in two arguments, the list element and 1.</p>

<p>The advantage of this approach over the others is that you can extract multiple elements from the list without having to recompute the splits.  For example, the last name would be <code>sapply(pieces, ""["", 2)</code>.  Once you get used to this idiom, it's pretty easy to read.</p>
"
1358164,16632,2009-08-31T16:09:59Z,1358003,153,FALSE,"<p>Ensure you record your work in a reproducible script.  From time-to-time, reopen R, then <code>source()</code> your script.  You'll clean out anything you're no longer using, and as an added benefit will have tested your code.</p>
"
1358340,11834,2009-08-31T16:59:06Z,1358238,1,FALSE,"<p>Check out <a href=""http://www.gnu.org/software/octave/doc/interpreter/"" rel=""nofollow noreferrer"">GNU Octave</a> - between its polyfit() and the nonlinear constraints solver it ought to be possible to construct something suitable for your problem.</p>
"
1358356,143305,2009-08-31T17:03:07Z,1358238,8,FALSE,"<p>Your first model is actually <em>linear</em> in the three parameters and can be fit in R using </p>

<pre><code> fit &lt;- lm(y ~ x + I(x^2), data=X)
</code></pre>

<p>which will get you your three parameters.</p>

<p>The second model can also be fit using <code>nls()</code> in R with the usual caveats of having to provide starting values etc.  The <em>statistical</em> issues in optimization are not necessarily the same as the <em>numerical</em> issues -- you cannot just optimize any functional form no matter which language you choose.</p>
"
1358360,153430,2009-08-31T17:04:36Z,1358238,1,FALSE,"<p>You're probably not going to find a single routine with the flexibility implied in your examples (polynomials and rational functions using the same routine), let alone one that will parse a string to figure out what sort of equation to fit.</p>

<p>A least-squares polynomial fitter would be appropriate for your first example. (It's up to you what degree polynomial to use -- quadradic, cubic, quartic, etc.).   For a rational function like your second example, you might have to ""roll your own"" if you can't find a suitable library.  Also, keep in mind that a sufficiently high-degree polynomial can be used to approximate your ""real"" function, as long as you don't need to extrapolate beyond the limits of the data set you're fitting to.</p>

<p>As others have noted, there are other, more generalized parameter estimation algorithms which might also prove useful.  But those algorithms aren't quite ""plug and play"": they usually require you to write some helper routines, and supply a list of initial values for the model parameters.  It's possible for these sorts of algorithms to diverge, or get stuck in a local minimum or maximum for an unlucky choice of initial parameter estimates.   </p>
"
1358423,36093,2009-08-31T17:21:17Z,1358238,1,FALSE,"<p>In R, this is quite easy.</p>

<p>The built in method is called optim().  It takes as arguments a starting vector of potential parameters, then a function.  You have to go build your own error function, but that's really simple.</p>

<p>Then you call it like out = optim( 1 , err_fn)</p>

<p>where err_fn is</p>

<pre><code>err_fn = function(A) {
    diff = 0;
    for(i in 1:data_length){
      x = eckses[i];
      y = data[i];
      model_y = A*x;
      diff = diff + ( y - model_y )^2
    }
    return(diff);
}
</code></pre>

<p>This just assumes you have a vector of x and y values in eckses and data.  Change the model_y line as you see fit, even add more parameters.</p>

<p>It works on nonlinear just fine, I use it for four dimensional e^x curves and it is very fast.  The output data includes the error value at the end of the fitting, which is a measure of how well it fits, given as a sum of squared differences (in my err_fn).</p>

<p>EDIT:
If you NEED to take in the model as a string, you can have your user interface construct this whole model fitting process as an R script and load it in to run.  R can take text from STDIN or from a file, so it shouldn't be too hard to craft this function's string equivalent, and have it run optim automatically.  </p>
"
1358937,163053,2009-08-31T19:08:45Z,1358003,25,FALSE,"<p>That's a good trick.  </p>

<p>One other suggestion is to use memory efficient objects wherever possible: for instance, use a matrix instead of a data.frame.</p>

<p>This doesn't really address memory management, but one important function that isn't widely known is memory.limit().  You can increase the default using this command, memory.limit(size=2500), where the size is in MB.  As Dirk mentioned, you need to be using 64-bit in order to take real advantage of this.  </p>
"
1359545,76235,2009-08-31T21:37:51Z,1358003,24,FALSE,"<p>I never save an R workspace. I use import scripts and data scripts and output any especially large data objects that I don't want to recreate often to files. This way I always start with a fresh workspace and don't need to clean out large objects. That is a very nice function though.</p>
"
1359679,143305,2009-08-31T22:26:15Z,1358003,21,TRUE,"<p>To further illustrate the common strategy of frequent restarts, we can use <a href=""http://dirk.eddelbuettel.com/code/littler.html"" rel=""noreferrer"">littler</a> which allows us to run simple expressions directly from the command-line. Here is an example I sometimes use to time different BLAS for a simple crossprod.</p>

<pre><code> r -e'N&lt;-3*10^3; M&lt;-matrix(rnorm(N*N),ncol=N); print(system.time(crossprod(M)))'
</code></pre>

<p>Likewise,</p>

<pre><code> r -lMatrix -e'example(spMatrix)'
</code></pre>

<p>loads the Matrix package (via the --packages | -l switch)  and runs the examples of the spMatrix function.  As r always starts 'fresh', this method is also a good test during package development.</p>

<p>Last but not least r also work great for automated batch mode in scripts using the '#!/usr/bin/r' shebang-header.   Rscript is an alternative where littler is unavailable (e.g. on Windows).</p>
"
1360196,158151,2009-09-01T01:52:54Z,1358238,1,FALSE,"<p>if you have constraints on your coefficients, and you know that there is a specific type of function you'd want to fit to your data and that function is a messy one where standard regression methods or other curve fitting methods won't work, have you considered genetic algorithms?</p>

<p>they're not my first choice, but if you are trying to find the coefficients of the second function you mentioned, then perhaps GAs would work --- especially if you are using non-standard metrics to evaluate best fit. for example, if you wanted to find the coefficients of ""(A+Bx+Cx^2)/(Dx+Ex^2)"" such that the sum of square differences between your function and data is minimal <em>and</em> that there be some constraint on the arclength of the resulting function, then a stochastic algorithm might be a good way to approach this.</p>

<p>some caveats: 1) stochastic algorithms won't guarantee the <em>best</em> solution, but they will often be very close. 2) you have to be careful about the stability of the algorithm.</p>

<p>on a longer note, if you are at the stage where you want to find a function from some space of functions that best fits your data (e.g., you are not going to impose, say, the second model on your data), then genetic programming techniques may also help.</p>
"
1364521,86684,2009-09-01T20:37:36Z,1195826,4,FALSE,"<p>I wrote utility functions to do this.  Now that I know about gdata's drop.levels, it looks pretty similar.  Here they are (from <a href=""http://github.com/brendano/dlanalysis/tree/master/util.R"" rel=""nofollow noreferrer"">here</a>):</p>

<pre><code>present_levels &lt;- function(x) intersect(levels(x), x)

trim_levels &lt;- function(...) UseMethod(""trim_levels"")

trim_levels.factor &lt;- function(x)  factor(x, levels=present_levels(x))

trim_levels.data.frame &lt;- function(x) {
  for (n in names(x))
    if (is.factor(x[,n]))
      x[,n] = trim_levels(x[,n])
  x
}
</code></pre>
"
1367198,143377,2009-09-02T11:42:09Z,1366853,11,TRUE,"<p>Try using ggplot2:</p>

<pre><code>dnow &lt;- read.table(""http://dpaste.com/88561/plain/"")
library(ggplot2)
qplot(V1, colour=factor(V2), data=dnow, geom=""density"")
</code></pre>
"
1367575,143591,2009-09-02T13:03:11Z,1366853,3,FALSE,"<p>Using base graphics in a spaghetti code fashion:</p>

<pre><code>plot.multi.dens &lt;- function(s)
{
junk.x = NULL
junk.y = NULL
for(i in 1:length(s))
{
junk.x = c(junk.x, density(s[[i]])$x)
junk.y = c(junk.y, density(s[[i]])$y)
}
xr &lt;- range(junk.x)
yr &lt;- range(junk.y)
plot(density(s[[1]]), xlim = xr, ylim = yr, main = """")
for(i in 1:length(s))
{
lines(density(s[[i]]), xlim = xr, ylim = yr, col = i)
}
}
dnow &lt;- read.table(""http://dpaste.com/88561/plain/"")
library(sqldf)
x &lt;- unlist(sqldf(""select V1 from dnow where V2==0""))
y &lt;- unlist(sqldf(""select V1 from dnow where V2==1""))
z &lt;- unlist(sqldf(""select V1 from dnow where V2==2""))
plot.multi.dens(list(x,y,z))
library(Hmisc)
le &lt;- largest.empty(x,y,.1,.1)
legend(le,legend=c(""x"",""y"",""z""), col=(1:3), lwd=2, lty = 1)
</code></pre>
"
1367912,83761,2009-09-02T14:05:33Z,1366853,1,FALSE,"<p>I found myself needing to do this a lot when looking at microarray data, so I rolled this up as part of a library of utility code that I keep on github: <a href=""http://github.com/lianos/ARE.utils/tree/master"" rel=""nofollow noreferrer"">ARE.utils</a>, specifically the <a href=""https://github.com/lianos/ARE.utils/blob/master/R/visuals.R"" rel=""nofollow noreferrer"">plot.densities</a> function.</p>

<p>It uses base graphics so you can take inspiration from that function to create your own, or just take it whole-sale (but it relies on some other functions in that library):</p>

<ol>
<li><a href=""https://github.com/lianos/ARE.utils/blob/master/R/datasets.R"" rel=""nofollow noreferrer"">create.densities</a>, which converts a list/matrix/etc of data to a list of densities; and</li>
<li><a href=""https://github.com/lianos/ARE.utils/blob/master/R/general.R"" rel=""nofollow noreferrer"">match.dim</a> function (which converts dimension ""names"" into numeric axes).</li>
</ol>

<p>(You can, optionally, install the entire package, but I make no promises that I the functions in there won't change in some backwards incompatible way).</p>

<p>It's not hard to write your own such function, but just make sure you have the function pick the correct range on the axes and stuff. Anyway, you would then use the code like this:</p>

<pre><code>library(ARE.utils)
# Create a matrix dataset with separate observations in columns
dat &lt;- matrix(c(rnorm(100), rnorm(100, mean=3), 
                rnorm(100, mean=3, sd=2)),
              ncol=3)
# Plot them
plot.densities(dat, along='cols')
</code></pre>

<p>That would create three different density plots on the same axis with their own colors.</p>
"
1371124,164965,2009-09-03T02:19:29Z,1366853,4,FALSE,"<p>You can also solve this using the lattice package.</p>

<pre><code>require(lattice)
dnow &lt;- read.table('http://dpaste.com/88561/plain/')
densityplot(~V1, groups=V2, data=dnow)
</code></pre>
"
1371144,164965,2009-09-03T02:26:06Z,1343442,1,FALSE,"<p>I'm not sure yet how to answer an answer, but there is an updated version of Ranke's vim r-plugin called r-plugin2 available <a href=""http://www.vim.org/scripts/script.php?script_id=2628"" rel=""nofollow noreferrer"">here</a>. It seems more user-friendly and robust than the original.</p>
"
1372767,143476,2009-09-03T11:04:49Z,1358238,8,TRUE,"<p>To answer your question in a general sense (regarding parameter estimation in R) without considering the specifics of the equations you pointed out, I think you are looking for nls() or optim()... 'nls' is my first choice as it provides error estimates for each estimated parameter and when it fails I use 'optim'. If you have your x,y variables:</p>

<pre><code>out &lt;- tryCatch(nls( y ~ A+B*x+C*x*x, data = data.frame(x,y), 
                start = c(A=0,B=1,C=1) ) ,
                error=function(e) 
                optim( c(A=0,B=1,C=1), function(p,x,y)  
                      sum((y-with(as.list(p),A + B*x + C*x^2))^2), x=x, y=y) )
</code></pre>

<p>to get the coefficients, something like</p>

<pre><code>getcoef &lt;- function(x) if(class(x)==""nls"") coef(x) else x$par
getcoef(out)
</code></pre>

<p>If you want the standard errors in the case of 'nls', </p>

<pre><code>summary(out)$parameters
</code></pre>

<p>The help files and r-help mailing list posts contain many discussions regarding specific minimization algorithms implemented by each (the default used in each example case above) and their appropriateness for the specific form of the equation at hand. Certain algorithms can handle box constraints, and another function called constrOptim() will handle a set of linear constraints. This website may also help:</p>

<p><a href=""http://cran.r-project.org/web/views/Optimization.html"" rel=""noreferrer"">http://cran.r-project.org/web/views/Optimization.html</a></p>
"
1374740,143305,2009-09-03T17:10:01Z,1374609,3,FALSE,"<p>Have a look at Max Kuhn's <a href=""http://cran.r-project.org/package=caret"" rel=""nofollow noreferrer""><strong>caret</strong></a> which is designed to support exactly this: <em>Classification and Regression Training</em> as per its title.  </p>

<p>It wraps around Random Forest as well as numerous other packages, and has ample documentation, including this <a href=""http://www.jstatsoft.org/v28/i05/"" rel=""nofollow noreferrer"">JSS paper</a>.</p>

<p>Besides caret, you can of course just use the <code>predict</code> method on the model you get returned as this example from the help page suggests:</p>

<pre><code> data(iris)
 set.seed(111)
 ind &lt;- sample(2, nrow(iris), replace = TRUE, prob=c(0.8, 0.2))
 iris.rf &lt;- randomForest(Species ~ ., data=iris[ind == 1,])
 iris.pred &lt;- predict(iris.rf, iris[ind == 2,])
 table(observed = iris[ind==2, ""Species""], predicted = iris.pred)
</code></pre>

<p>Instead of a random sample using <code>ind</code>, just subset the data yourself into training and validation sets.</p>
"
1377014,143377,2009-09-04T03:04:08Z,1376967,3,FALSE,"<p>I think you need to provide more information. This seems to work:</p>

<pre><code> pg &lt;- ggplot(dd, aes(Predicted_value)) ## need aesthetics in the ggplot
 pg &lt;- pg + geom_density() 
 ## gotta provide the arguments of the dnorm
 pg &lt;- pg + stat_function(fun=dnorm, colour='red',            
            args=list(mean=mean(dd$Predicted_value), sd=sd(dd$Predicted_value)))
 ## wrap it!
 pg &lt;- pg + facet_wrap(~State_CD)
 pg
</code></pre>

<p>We are providing the same mean and sd parameter for every panel. Getting panel specific means and standard deviations is left as an exercise to the reader* ;) </p>

<p>'*' In other words, not sure how it can be done...</p>
"
1377025,143305,2009-09-04T03:11:53Z,1377003,8,FALSE,"<p>dnorm(0, mean=4, sd=10) does <em>not</em> give you thr probability of such a point occurring. To quote Wikipedia on <a href=""http://en.wikipedia.org/wiki/Probability_density_function"" rel=""nofollow noreferrer"">probability density function</a></p>

<blockquote>
  <p>In probability theory, a probability
  density function (pdf)—often referred
  to as a probability distribution
  function<a href=""http://en.wikipedia.org/wiki/Probability_density_function"" rel=""nofollow noreferrer"">1</a>—or density, of a random
  variable is a function that describes
  the density of probability at each
  point in the sample space. The
  probability of a random variable
  falling within a given set is given by
  the integral of its density over the
  set.</p>
</blockquote>

<p>and the probability you mention is </p>

<pre><code>R&gt; pnorm(0, 4, 10)
[1] 0.3446
</code></pre>

<p>or a 34.46% chance of getting a value equal to or smaller than 0 from a N(4, 10) distribution.</p>

<p>As for your Perl question:  If you know how to do it in R, but need it from Perl, maybe you need to write a Perl extension based on R's libRmath (provided in Debian by the package r-mathlib) to get those functions to Perl?  This does not require the R interpreter.</p>

<p>Otherwise, you could try the GNU GSL or the Cephes libraries for access to these special functions. </p>
"
1377047,143377,2009-09-04T03:19:36Z,1377003,3,TRUE,"<p>Why not something along these lines (I am writing in R, but it could be done in  perl with  Statistics::Distribution):</p>

<pre><code>dn &lt;- function(x=0 # value
               ,mean=0 # mean 
               ,sd=1 # sd
               ,sc=10000 ## scale the precision
               ) {
  res &lt;- (pnorm(x+1/sc, mean=mean, sd=sd)-pnorm(x, mean=mean, sd=sd))*sc
  res
}
&gt; dn(0,4,10,10000)
0.03682709
&gt; dn(2.02,2,.24)
1.656498
</code></pre>

<p>[edit:1] I should mention that this approximation can get pretty horrible at the far tails. it might  or might not matter depending on your application.</p>

<p>[edit:2] @foolishbrat Turned the code into a function. The result should always be positive. Perhaps you are forgetting that in the perl module you mention the function returns the upper probability 1-F, and R returns F?</p>

<p>[edit: 3] fixed a copy and paste error.</p>
"
1377254,160314,2009-09-04T04:41:14Z,1376967,1,FALSE,"<p>I think your best bet is to draw the line manually with geom_line.</p>

<pre><code>dd&lt;-data.frame(matrix(rnorm(144, mean=2, sd=2),72,2),c(rep(""A"",24),rep(""B"",24),rep(""C"",24)))
colnames(dd) &lt;- c(""x_value"", ""Predicted_value"",  ""State_CD"")
dd$Predicted_value&lt;-dd$Predicted_value*as.numeric(dd$State_CD) #make different by state

##Calculate means and standard deviations by level
means&lt;-as.numeric(by(dd[,2],dd$State_CD,mean))
sds&lt;-as.numeric(by(dd[,2],dd$State_CD,sd))

##Create evenly spaced evaluation points +/- 3 standard deviations away from the mean
dd$vals&lt;-0
for(i in 1:length(levels(dd$State_CD))){
    dd$vals[dd$State_CD==levels(dd$State_CD)[i]]&lt;-seq(from=means[i]-3*sds[i], 
                            to=means[i]+3*sds[i],
                            length.out=sum(dd$State_CD==levels(dd$State_CD)[i]))
}
##Create normal density points
dd$norm&lt;-with(dd,dnorm(vals,means[as.numeric(State_CD)],
                        sds[as.numeric(State_CD)]))


pg &lt;- ggplot(dd, aes(Predicted_value)) 
pg &lt;- pg + geom_density() 
pg &lt;- pg + geom_line(aes(x=vals,y=norm),colour=""red"") #Add in normal distribution
pg &lt;- pg + facet_wrap(~State_CD,scales=""free"")
pg
</code></pre>
"
1377280,143476,2009-09-04T04:50:48Z,1377248,2,FALSE,"<ol>
<li><p>You can use write.table() instead of write() to use with the arguments specified above. The latter is suited for printing matrices (but may require specifying ncol or transposing the input matrix) but the former is more general and I use it for both matrices and data frames.</p></li>
<li><p>You can replace</p>

<p>err_prob &lt;- pnorm(dat$V4,mean=meanl, sd=stdevl)</p>

<p>problist &lt;- cbind(problist,err_prob)</p></li>
</ol>

<p>with</p>

<pre><code>assign(sprintf(""err_prob%d"",lmer),pnorm(dat$V4,mean=meanl, sd=stdevl))
problist &lt;- eval(parse(text=sprintf(""cbind(problist,err_prob%d)"", lmer)))
</code></pre>

<p>The last line parses the character string as an expression and then evaluates it. You can alternatively do </p>

<pre><code>colnames(problist) &lt;- sprintf(""err_prob%d"",1:10)
</code></pre>

<p>a posteriori</p>
"
1377306,160314,2009-09-04T05:02:16Z,1377248,4,TRUE,"<p>First off, no need for the semi-colons, R knows that the end of the line is a break.</p>

<p><pre><code>
for (lmer in 1:10){
    meanl &lt;- lmer
    stdevl &lt;- (0.17*sqrt(lmer))
    err_prob &lt;- pnorm(dat$V4,mean=meanl, sd=stdevl)
    problist &lt;- cbind(problist,err_prob)
}
colnames(problist)&lt;-paste(""errorprob"",1:10,sep="""")
dat &lt;- cbind(dat,problist)
write.table(dat, file=""output.txt"", sep=""\t"",append=F)
</pre></code></p>

<ol>
<li><p>I believe that you are looking for the write.table function</p></li>
<li><p>Use the colnames function</p></li>
</ol>
"
1377678,149223,2009-09-04T07:21:28Z,1374842,0,TRUE,"<p>In case anyone else encounters this problem I'm posting my own workaround here. </p>

<p>I define targets in my Makevars and copy the libraries directly (ie answer 1). The variable R_LIBRARY_DIR provides the temporary location where the package is being built.</p>

<p>My Makevars now looks something like this</p>

<pre><code>OBJECTS = 

LIBSINSTDIR=$(R_LIBRARY_DIR)/myPackage/libs/

#ARCHFLAG is set in the configure script to i386 or ppc as appropriate
JNIINSTDIR=$(LIBSINSTDIR)/@ARCHFLAG@/

.PHONY: all

all: $(SHLIB) jnilib 

jnilib: object1.o object2.o
   $(CXX) -bundle $(JAVA_LIBS) $(JAVA_CPPFLAGS) -o libmyPackage.jnilib object1.o object2.o
   mkdir -p $(JNIINSTDIR)
   cp libmyPackage.jnilib $(JNIINSTDIR)
</code></pre>
"
1377841,13164,2009-09-04T08:04:26Z,1377003,0,FALSE,"<p>Here's how you can do the same thing you're doing with R in Perl using the <a href=""http://search.cpan.org/dist/Math-SymbolicX-Statistics-Distributions/lib/Math/SymbolicX/Statistics/Distributions.pm"" rel=""nofollow noreferrer"">Math::SymbolicX::Statistics::Distributions</a> module from CPAN:</p>

<pre><code>use strict; use warnings;

use Math::SymbolicX::Statistics::Distributions qw/normal_distribution/;

my $norm = normal_distribution(qw/mean sd/);
print $norm-&gt;value(mean =&gt; 4, sd =&gt; 10, x =&gt; 0), ""\n"";

# curry it with the parameter values
$norm-&gt;implement(mean =&gt; 4, sd =&gt; 10);
print $norm-&gt;value(x =&gt; 0),""\n""; # prints the same as above
</code></pre>

<p>The normal_distribution() function from that module is a generator for functions. $norm will be a <a href=""http://search.cpan.org/dist/Math-Symbolic/lib/Math/Symbolic.pm"" rel=""nofollow noreferrer"">Math::Symbolic</a> (::Operator) object that you can modify. For example with <em>implement</em>, which, in the above example, replaces the two parameter variables with constants.</p>

<p>Note, however as Dirk pointed out, that you probably want the cumulative function of the normal distribution. Or more generally the integral in a certain range.</p>

<p>Unfortunately, Math::Symbolic can't do integration symbolically. Therefore, you'd have to resort to numerical integration with the likes of <a href=""http://search.cpan.org/dist/Math-Integral-Romberg/lib/Math/Integral/Romberg.pm"" rel=""nofollow noreferrer"">Math::Integral::Romberg</a>. (Alternatively, search CPAN for an implementation of the error function.) This may be slow, but it's still easy to do. Add this to the above snippet:</p>

<pre><code>use Math::Integral::Romberg 'integral';

my ($int_sub) = $norm-&gt;to_sub(); # compile to a faster Perl sub
print $int_sub-&gt;(0),""\n"";  # same number as above

print ""p="" . integral($int_sub, -100., 0) . ""\n"";
# -100 is an arbitrary, small number
</code></pre>

<p>This should give you the ~0.344578258389676 from Dirk's answer.</p>
"
1379074,16632,2009-09-04T12:56:59Z,1376967,31,TRUE,"<p><code>stat_function</code> is designed to overlay the same function in every panel.  (There's no obvious way to match up the parameters of the function with the different panels).</p>

<p>As Ian suggests, the best way is to generate the normal curves yourself, and plot them as a <em>separate</em> dataset (this is where you were going wrong before - merging just doesn't make sense for this example and if you look carefully you'll see that's why you're getting the strange sawtooth pattern). </p>

<p>Here's how I'd go about solving the problem:</p>

<pre><code>dd &lt;- data.frame(
  predicted = rnorm(72, mean = 2, sd = 2),
  state = rep(c(""A"", ""B"", ""C""), each = 24)
) 

grid &lt;- with(dd, seq(min(predicted), max(predicted), length = 100))
normaldens &lt;- ddply(dd, ""state"", function(df) {
  data.frame( 
    predicted = grid,
    density = dnorm(grid, mean(df$predicted), sd(df$predicted))
  )
})

ggplot(dd, aes(predicted))  + 
  geom_density() + 
  geom_line(aes(y = density), data = normaldens, colour = ""red"") +
  facet_wrap(~ state) 
</code></pre>
"
1379899,143305,2009-09-04T15:17:05Z,1379549,9,TRUE,"<p>A few other R users I talked to use a 'one-directory-per-project' setup, and a simple Makefile.  As you suspected, that works well with Emacs/ESS.  </p>

<p>I tend to just call a simple shell script <code>sweave</code> which I wrote before before 'R CMD Sweave' was added (as I find re-creating or copying the Makefile unappealing, YMMV).  I also use Emacs and an auto-refreshing pdf viewer (like  <code>okular</code> or <code>kpdf</code>).  Emacs23 can preview pdf files directly too but I have yet to switch my work flow to that.</p>

<pre><code>edd@ron:~$ cat bin/sweave
#!/bin/bash -e

function errorexit () {
    echo ""Error: $1""
    exit 1
}

function filetest () {
    if [ ! -f $1 ]; then
       errorexit ""File $1 not found""
    fi
    return 0
}


if [ ""$#"" -lt 1 ]; then
    errorexit ""Need to specify argument file""
fi


BASENAME=$(basename $1 .Rnw)

RNWFILE=$BASENAME.Rnw
filetest $RNWFILE
echo ""library(tools); Sweave(\""$RNWFILE\"")"" \
      | R --no-save --no-restore --slave

LATEXFILE=$BASENAME.tex
filetest $LATEXFILE &amp;&amp; pdflatex $LATEXFILE
</code></pre>
"
1380476,119314,2009-09-04T17:08:30Z,1377130,1,FALSE,"<p>I also question the problem with masked arrays.  Here are a couple of examples:</p>

<pre><code>import numpy as np
data = np.ma.masked_array(np.arange(10))
data[5] = np.ma.masked # Mask a specific value

data[data&gt;6] = np.ma.masked # Mask any value greater than 6

# Same thing done at initialization time
init_data = np.arange(10)
data = np.ma.masked_array(init_data, mask=(init_data &gt; 6))
</code></pre>
"
1380530,2140,2009-09-04T17:22:16Z,1377130,1,FALSE,"<p>Masked arrays are the anwswer, as DpplerShift describes. For quick and dirty use, you can use fancy indexing with boolean arrays:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; data = np.arange(10)
&gt;&gt;&gt; valid_idx = data % 2 == 0 #pretend that even elements are missing

&gt;&gt;&gt; # Get non-missing data
&gt;&gt;&gt; data[valid_idx]
array([0, 2, 4, 6, 8])
</code></pre>

<p>You can now use valid_idx as a quick mask on other data as well</p>

<pre><code>&gt;&gt;&gt; comparison = np.arange(10) + 10
&gt;&gt;&gt; comparison[valid_idx]
array([10, 12, 14, 16, 18])
</code></pre>
"
1380976,163053,2009-09-04T18:59:31Z,1379549,6,FALSE,"<p>You can do everything that you suggest there with the StatET plugin for Eclipse.  That's what I use for Sweave development; it understands both latex and R very well, including syntax highlighting, etc.</p>

<p>You can get it here: <a href=""http://www.walware.de/goto/statet"" rel=""nofollow noreferrer"">http://www.walware.de/goto/statet</a>.</p>

<p>Longhow Lam has written a nice guide: <a href=""http://www.splusbook.com/Rintro/R_Eclipse_StatET.pdf"" rel=""nofollow noreferrer"">http://www.splusbook.com/Rintro/R_Eclipse_StatET.pdf</a>.</p>

<p><a href=""http://www.statalgo.com/?p=93"" rel=""nofollow noreferrer"">http://www.statalgo.com/?p=93</a></p>
"
1381350,160314,2009-09-04T20:16:35Z,1380694,7,TRUE,"<p>using the nlme library...</p>

<p>Answering your stated question, you can create a random intecept mixed effect model using the following code:</p>

<pre><code>&gt; library(nlme)
&gt; m1 &lt;- lme(Score ~ Condition + Time + Condition*Time,
+ data = myDat, random = ~ 1 | Subject)
&gt; summary(m1)
Linear mixed-effects model fit by REML
 Data: myDat 
       AIC      BIC    logLik
  31.69207 37.66646 -9.846036

Random effects:
 Formula: ~1 | Subject
         (Intercept)  Residual
StdDev: 5.214638e-06 0.3151035

Fixed effects: Score ~ Condition + Time + Condition * Time 
                   Value Std.Error DF  t-value p-value
(Intercept)    0.6208333 0.2406643 14 2.579666  0.0218
Condition      0.7841667 0.3403507  6 2.303996  0.0608
Time           0.9900000 0.1114059 14 8.886423  0.0000
Condition:Time 0.0637500 0.1575517 14 0.404629  0.6919
 Correlation: 
               (Intr) Condtn Time  
Condition      -0.707              
Time           -0.926  0.655       
Condition:Time  0.655 -0.926 -0.707

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-1.5748794 -0.6704147  0.2069426  0.7467785  1.5153752 

Number of Observations: 24
Number of Groups: 8 
</code></pre>

<p>The intercept variance is basically 0, indicating no within subject effect, so this model is not capturing the between time relationship well. A random intercept model is rarely the type of model you want for a repeated measures design.  A random intercept model assumes that the correlations between all time points are equal. i.e. the correlation between time 1 and time 2 is the same as between time 1 and time 3. Under normal circumstances (perhaps not those generating your fake data) we would expect the later to be less than the former. An auto regressive structure is usually a better way to go. </p>

<pre><code>&gt; m2&lt;-gls(Score ~ Condition + Time + Condition*Time,
+ data = myDat, correlation = corAR1(form = ~ Time | Subject))
&gt; summary(m2)
Generalized least squares fit by REML
  Model: Score ~ Condition + Time + Condition * Time 
  Data: myDat 
       AIC      BIC    logLik
  25.45446 31.42886 -6.727232

Correlation Structure: AR(1)
 Formula: ~Time | Subject 
 Parameter estimate(s):
       Phi 
-0.5957973 

Coefficients:
                   Value Std.Error   t-value p-value
(Intercept)    0.6045402 0.1762743  3.429543  0.0027
Condition      0.8058448 0.2492895  3.232566  0.0042
Time           0.9900000 0.0845312 11.711652  0.0000
Condition:Time 0.0637500 0.1195452  0.533271  0.5997

 Correlation: 
               (Intr) Condtn Time  
Condition      -0.707              
Time           -0.959  0.678       
Condition:Time  0.678 -0.959 -0.707

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-1.6850557 -0.6730898  0.2373639  0.8269703  1.5858942 

Residual standard error: 0.2976964 
Degrees of freedom: 24 total; 20 residual
</code></pre>

<p>Your data is showing a -.596 between time point correlation, which seems odd. normally there should, at a minimum be a positive correlation between time points. How was this data generated?</p>

<p>addendum:</p>

<p>With your new data we know that the data generating process is equivalent to a random intercept model (though that is not the most realistic for a longitudinal study. The visualization shows that the effect of time seems to be fairly linear, so we should feel comfortable treating it as a numeric variable.</p>

<pre><code>&gt; library(nlme)
&gt; m1 &lt;- lme(Score ~ Condition + as.numeric(Time) + Condition*as.numeric(Time),
+ data = myDat, random = ~ 1 | Subject)
&gt; summary(m1)
Linear mixed-effects model fit by REML
 Data: myDat 
       AIC      BIC    logLik
  38.15055 44.12494 -13.07527

Random effects:
 Formula: ~1 | Subject
        (Intercept)  Residual
StdDev:   0.2457355 0.3173421

Fixed effects: Score ~ Condition + as.numeric(Time) + Condition * as.numeric(Time) 
                                  Value Std.Error DF   t-value p-value
(Intercept)                    1.142500 0.2717382 14  4.204415  0.0009
ConditionYes                   1.748333 0.3842958  6  4.549447  0.0039
as.numeric(Time)               0.575000 0.1121974 14  5.124898  0.0002
ConditionYes:as.numeric(Time) -0.197500 0.1586710 14 -1.244714  0.2337
 Correlation: 
                              (Intr) CndtnY as.(T)
ConditionYes                  -0.707              
as.numeric(Time)              -0.826  0.584       
ConditionYes:as.numeric(Time)  0.584 -0.826 -0.707

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-1.44560367 -0.65018585  0.01864079  0.52930925  1.40824838 

Number of Observations: 24
Number of Groups: 8 
</code></pre>

<p>We see a significant Condition effect, indicating that the 'yes' condition tends to have higher scores (by about 1.7), and a significant time effect, indicating that both groups go up over time. Supporting the plot, we find no differential effect of time between the two groups (the interaction). i.e. the slopes are the same.</p>
"
1381503,16632,2009-09-04T20:54:36Z,1380694,6,FALSE,"<p>It's not an answer to your question, but you might find this visualisation of your data informative.</p>

<pre><code>library(ggplot2)
qplot(Time, Score, data = myDat, geom = ""line"", 
  group = Subject, colour = factor(Condition))
</code></pre>

<p><img src=""https://i.stack.imgur.com/Anndj.png"" alt=""Data visulation""></p>
"
1382334,158065,2009-09-05T03:02:51Z,1382288,11,TRUE,"<p>Look at the parameters <code>xaxs</code> and <code>yaxs</code> (under <code>?par</code>).  By default the axis contains the data range plus 4% on either side.  If you override the parameter, then you can get an axis exactly equal to the data range.  Cf:</p>

<pre><code>&gt; curve(x^2)

&gt; curve(x^2, xaxs=""i"", yaxs=""i"")
</code></pre>
"
1382912,164965,2009-09-05T09:53:31Z,1379549,1,FALSE,"<p>I use the ""one-directory-per-project"" and  Makefile approach as well. I also include commands to create output in HTML, which can then be converted to OOo and MS Word, using tth. This is important for me since a lot of my collaborators are MS Office users and are resistant to using the PDF output. I learned a lot about how to do this from Frank Harrell's twiki at Vanderbilt.</p>

<p>Personally I use gvim as my editor of choice and running <em>make</em> from there is quite simple, as it is from Emacs. </p>
"
1383070,26575,2009-09-05T11:13:00Z,1377003,1,FALSE,"<p>As others have pointed out, you probably want the cumulative distribution function. This can be obtained via the <a href=""http://en.wikipedia.org/wiki/Error_function"" rel=""nofollow noreferrer"">error function</a> (shifted by the mean and scaled by the standard deviation of your normal distribution), which exists in the standard math library and is made accessible in Perl by <a href=""http://search.cpan.org/perldoc?Math::Libm"" rel=""nofollow noreferrer"">Math::Libm</a>.</p>
"
1384349,158065,2009-09-05T21:54:29Z,1384025,5,TRUE,"<p><code>expression(lab)</code> doesn't actually evaluate <code>lab</code> so you end up with all the labels being <code>lab</code>.  Instead, you could change that line to:</p>

<pre><code>text(0.5, 0.5, parse(text=lab), cex=cex, font=font)
</code></pre>

<p>which will do what you want.  Note that the <code>pairs</code> function also accepts a labels argument, so this would work too:</p>

<pre><code>pairs(dat, labels=c(expression(alpha), expression(beta), expression(gamma)))
</code></pre>
"
1385926,16632,2009-09-06T15:28:50Z,1385873,3,FALSE,"<p>One approach is to use the reshape package to create a data.frame with years in columns and names in rows:</p>

<pre><code>library(reshape)
cast(d, name ~ year, value = ""numbers"")
</code></pre>

<p>You could then use <code>complete.cases</code> to extract the rows of interest.</p>
"
1385930,16632,2009-09-06T15:31:17Z,1385873,2,FALSE,"<p>If there is only one record per year, just count up the number of times each person appears in the dataset:</p>

<pre><code>counts &lt;- as.data.frame(table(name = d$name))
</code></pre>

<p>Then look for everyone who appeared twice:</p>

<pre><code>subset(counts, Freq == 2)
</code></pre>
"
1386053,83761,2009-09-06T16:26:30Z,1385873,1,FALSE,"<p>Here's another solution that uses just base R and doesn't make any assumptions about the number of records a person has per year:</p>

<pre><code>d &lt;- data.frame(cbind(numbers = rnorm(10), 
                      year = rep(c(2008, 2009), 5),
                      name = c(""john"", ""David"", ""Tom"", ""Kristin"",
                               ""Lisa"",""Eve"",""David"",""Tom"",""Kristin"",
                               ""Lisa"")))
# split data into 2 data.frames (1 for each year)
by.year &lt;- split(d, d$year, drop=T)

# find the names that appear in both years
keep &lt;- intersect(by.year[['2008']]$name, by.year[['2009']]$name)
# Or, if you had several years, use Reduce as a more general solution:
keep &lt;- Reduce(intersect, lapply(by.year, '[[', 'name'))

# show the rows of the original dataset only if their $name field
# is in our 'keep' vector
d[d$name %in% keep,]
</code></pre>
"
1386227,168747,2009-09-06T17:43:09Z,1385873,11,TRUE,"<p>Simple way:</p>

<pre><code>subset(
    d,
    name %in% intersect(name[year==2008], name[year==2009])
)
</code></pre>
"
1386804,143305,2009-09-06T21:43:57Z,1386767,19,FALSE,"<p>The <a href=""http://ess.r-project.org"" rel=""noreferrer""><strong>ESS</strong></a> mode for Emacs has the following to say in its manual:</p>

<h1>13.7 Rdired</h1>

<p>Ess-rdired provides a dired-like buffer for viewing, editing and
plotting objects in your current R session.  If you are used to using
the dired (directory editor) facility in Emacs, this mode gives you
similar functionality for R objects.</p>

<p>To get started, first make sure you can load ess-rdired.  Add the
following to your .emacs and then restart emacs.  </p>

<pre><code> (autoload 'ess-rdired ""ess-rdired""  
   ""View *R* objects in a dired-like buffer."" t)  
</code></pre>

<p>Start an R session with `M-x R' and then store a few variables, such
as:</p>

<pre><code> s &lt;- sin(seq(from=0, to=8*pi, length=100))
 x &lt;- c(1, 4, 9)
 y &lt;- rnorm(20)
 z &lt;- TRUE
</code></pre>

<p>Then use `M-x ess-rdired' to create a buffer listing the objects in
your current environment and display it in a new window:</p>

<pre><code>             mode length
   s      numeric    100
   x      numeric      3
   y      numeric     20
   z      logical      1
</code></pre>

<p>Type <code>C-h m</code> or <code>?</code> to get a list of the keybindings for this mode.
For example, with your point on the line of a variable, <code>p</code> will plot
the object, <code>v</code> will view it, and <code>d</code> will mark the object for deletion
(<code>x</code> will actually perform the deletion).</p>
"
1386816,NA,2009-09-06T21:48:36Z,1386767,5,FALSE,"<p><a href=""http://stat.ethz.ch/R-manual/R-patched/library/utils/html/str.html"" rel=""noreferrer"">str()</a> is very useful. Specifying <code>give.attr=FALSE</code> hides attributes.</p>

<pre><code>&gt; str(diamonds)
'data.frame':   53940 obs. of  10 variables:
 $ carat  : num  0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...
 $ cut    : Factor w/ 5 levels ""Fair"",""Good"",..: 5 4 2 4 2 3 3 3 1 3 ...
 $ color  : Factor w/ 7 levels ""D"",""E"",""F"",""G"",..: 2 2 2 6 7 7 6 5 2 5 ...
 $ clarity: Factor w/ 8 levels ""I1"",""SI2"",""SI1"",..: 2 3 5 4 2 6 7 3 4 5 ...
 $ depth  : num  61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...
 $ table  : num  55 61 65 58 58 57 57 55 61 61 ...
 $ price  : int  326 326 327 334 335 336 336 337 337 338 ...
 $ x      : num  3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...
 $ y      : num  3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...
 $ z      : num  2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...
</code></pre>
"
1386909,143305,2009-09-06T22:37:39Z,1386767,16,TRUE,"<p>The <code>lsos()</code> function shown in <a href=""https://stackoverflow.com/questions/1358003/tricks-to-manage-the-available-memory-in-an-r-session"">this SO questions</a> is also a primitive object browser:</p>

<pre><code>R&gt; lsos()
               Type  Size Rows Columns
ls.objects function 11792   NA      NA
lsos       function  1112   NA      NA
s           numeric   824  100      NA
y           numeric   184   20      NA
x           numeric    56    3      NA
z           logical    32    1      NA
R&gt; 
</code></pre>
"
1387407,160588,2009-09-07T03:11:09Z,1386767,3,FALSE,"<p>The rkward R IDE has an inbuilt object browser/editor which seems quite useful, however I haven't used it much myself</p>

<p>screenshots <a href=""http://sourceforge.net/apps/mediawiki/rkward/index.php?title=Screenshots#Browsing_.2F_Editing_Data"" rel=""nofollow noreferrer"">here</a></p>
"
1389153,28169,2009-09-07T12:21:25Z,1389123,1,FALSE,"<p><a href=""http://www.revolution-computing.com/products/windows-64bit.php"" rel=""nofollow noreferrer"">These folks</a> claimed they ""ported"" 1,500 packages to create their 64-bit version of R. If that's true, then the level of effort required might be part of the explanation why there is no free version available.</p>

<p>If you're not asking for a free download, the same source seems to be offering it, although commercially.</p>
"
1389480,143305,2009-09-07T13:39:36Z,1389123,5,TRUE,"<p><em>Update</em></p>

<p>Current versions of R for Windows (since R 2.11) do come with 64-bit executables.</p>

<p>In the past no suitable free compiler was available. Quoting from the old R-on-Windows FAQ (taken from the sources and hence with texinfo markup):</p>

<blockquote>
  <p>To build a 64-bit version on 64-bit Windows you will need a suitable 
  compiler, and experiments with the Mingw64 port of @code{binutils} and
  @code{gcc} have failed to produce a working version on R.  You are of
  course welcome to try a commercial compiler@footnote{such as those from
  Intel and PGI: there is no Fortran compiler in the Microsoft set, but
  @code{f2c} could be used.}, and pre-compiled versions using such
  compilers are available from R redistributors.</p>
</blockquote>

<p>REvolution Computing had a Windows 64 bit version in their commercial / enterprise product before the official R gained 64-bit support.</p>
"
1389522,143305,2009-09-07T13:51:31Z,1389428,7,FALSE,"<p>Yes, while there is no 'time' type, you can use an offset time:</p>

<pre><code>R&gt; now &lt;- Sys.time()
R&gt; now
[1] ""2009-09-07 08:40:32 CDT""
R&gt; class(now)
[1] ""POSIXt""  ""POSIXct""
R&gt; later &lt;- now + 5*60
R&gt; later
[1] ""2009-09-07 08:45:32 CDT""
R&gt; class(later)
[1] ""POSIXt""  ""POSIXct""
R&gt; tdelta &lt;- difftime(later, now)
R&gt; tdelta
Time difference of 5 mins
R&gt; class(tdelta)
[1] ""difftime""
R&gt; 
</code></pre>

<p>When you use the <a href=""http://cran.r-project.org/package=zoo"" rel=""noreferrer""><strong>zoo</strong></a> package, you are using standard <code>POSIXt</code> types for your times indices.  Both zoo and the newer and also highly-recommended <a href=""http://cran.r-project.org/package=xts"" rel=""noreferrer""><strong>xts</strong></a> package can use <code>POSIXt</code>, and especially the compact <code>POSIXct</code> type, for indexing.  </p>

<p>The xts package has a lot more indexing functionality, and Jeff recently added parsing of intervals according to the ISO8601-2004(e) specification and gives these references for 
<a href=""http://en.wikipedia.org/wiki/ISO_8601"" rel=""noreferrer"">ISO8601</a> and a <a href=""http://www.iso.org/iso/support/faqs/faqs_widely_used_standards/widely_used_standards_other/date_and_time_format.htm"" rel=""noreferrer"">FAQ for widely used standars for date and time formats</a>. To use this xts version, you may need to switch the the <a href=""http://r-forge.r-project.org/projects/xts"" rel=""noreferrer"">xts development snapshot on r-forge</a></p>

<p>[<em>Edit:</em>] Also, regarding the question on 'conversion': this is easy once you have objects 
of class POSIXt / POSIXct as <code>as.numeric()</code> will convert POSIXct to (fractional) seconds since the epoch. R goes further than the POSIX standard and uses a double here, so you get millisecond precision:</p>

<pre><code>R&gt; options(""digits.secs""=6)   ## needed so that fractional seconds are printed
R&gt; now &lt;- Sys.time(); difftime(Sys.time(), now)
Time difference of 0.000149 secs
</code></pre>

<p>and</p>

<pre><code>R&gt; print(as.numeric(now), digits=15)   ## print with digits for sub-second time
[1] 1252374404.22975
R&gt; 
</code></pre>
"
1390408,26575,2009-09-07T18:00:09Z,1328903,24,TRUE,"<p>The ultimate reason is that if you do both general-purpose programming and numerical computations, it is useful to have a large complement of binary operators available. For example, if you store numbers in two-dimensional arrays, you may want to multiply the arrays elementwise, or you may want to compute the matrix product of two arrays. In Matlab these two operators are <code>.*</code> and <code>*</code>; in R they are <code>*</code> and <code>%*%</code>. Python has <a href=""http://www.python.org/dev/peps/pep-0211/"" rel=""noreferrer"">resisted</a> <a href=""http://www.python.org/dev/peps/pep-0225/"" rel=""noreferrer"">attempts</a> to add new operators, and so <a href=""http://numpy.scipy.org/"" rel=""noreferrer"">numpy</a> differentiates between the two kinds of product by having two classes: the array class is multiplied elementwise, the matrix class is multiplied in the linear-algebra sense.</p>

<p>Another example from Python is that for lists, plus means concatenation: <code>[1,2,3]+[4,5] == [1,2,3,4,5]</code>. But for numpy arrays, plus means elementwise addition: <code>array([1,2]) + array([4,5]) == array([5,7])</code>. If your code needs to do both, you have to convert between classes or use function notation, which can lead to cumbersome-looking code, especially where mathematics is involved.</p>

<p>So it would sometimes be convenient to have more operators available for use, and you might not know in advance what sorts of operators a particular application calls for. Therefore, the implementors of R have chosen to treat as operators anything named like <code>%foo%</code>, and several examples exist: <code>%in%</code> is set membership, <code>%x%</code> is Kronecker product, <code>%o%</code> is outer product. For an example of a language that has taken this to the extreme, see <a href=""https://web.archive.org/web/20110716162714/http://projectfortress.sun.com/"" rel=""noreferrer"">Fortress</a> (section 16 of the specification starts with the rules for operator names).</p>

<p>In the blog post you mentioned, the author is using the <a href=""http://had.co.nz/ggplot2/"" rel=""noreferrer"">ggplot2</a> graphing package, which defines <code>%+%</code> to mean some kind of combination of two plot elements. Really it seems to add a method to the bare <code>+</code> (which is a generic function so you can define what it means for user-defined objects), but it also defines <code>%+%</code> so that you can use the ggplot2 meaning of <code>+</code> (whatever it is) for other objects. If you install ggplot2, type <code>require(ggplot2)</code> and <code>?&#x60;%+%&#x60;</code> to see the documentation of that operator, and <code>methods(&#x60;+&#x60;)</code> to see that a new definition has been added to <code>+</code>.</p>
"
1390743,163053,2009-09-07T19:48:45Z,1389428,4,TRUE,"<p>As Dirk points out, there is an object called ""difftime"", but it can't be added/subtracted. </p>

<pre><code>&gt; as.difftime(5, units=""mins"")
Time difference of 5 mins

&gt; d &lt;- seq(from=as.POSIXct(""2003-01-01""), to=as.POSIXct(""2003-01-04""), by=""days"")
&gt; d
[1] ""2003-01-01 GMT"" ""2003-01-02 GMT"" ""2003-01-03 GMT"" ""2003-01-04 GMT""

&gt; d + as.difftime(5, units=""mins"")
[1] ""2003-01-01 00:00:05 GMT"" ""2003-01-02 00:00:05 GMT""
[3] ""2003-01-03 00:00:05 GMT"" ""2003-01-04 00:00:05 GMT""
Warning message:
Incompatible methods (""+.POSIXt"", ""Ops.difftime"") for ""+"" 
</code></pre>

<p>Seems you can now do this:</p>

<pre><code>  &gt; as.difftime(5, units='mins')
    Time difference of 5 mins
    &gt; d &lt;- seq(from=as.POSIXct(""2003-01-01""), to=as.POSIXct(""2003-01-04""), by=""days"")
    &gt; d 
    [1] ""2003-01-01 GMT"" ""2003-01-02 GMT"" ""2003-01-03 GMT"" ""2003-01-04 GMT""
    &gt; d + as.difftime(5, unit='mins')
    [1] ""2003-01-01 00:05:00 GMT"" ""2003-01-02 00:05:00 GMT""
    [3] ""2003-01-03 00:05:00 GMT"" ""2003-01-04 00:05:00 GMT""
    &gt; d + as.difftime(5, unit='secs')
    [1] ""2003-01-01 00:00:05 GMT"" ""2003-01-02 00:00:05 GMT""
    [3] ""2003-01-03 00:00:05 GMT"" ""2003-01-04 00:00:05 GMT""
   &gt;
</code></pre>

<p>This is with the recently released R 2.15.0</p>
"
1393611,143305,2009-09-08T12:04:42Z,1393432,2,TRUE,"<p>Per Barry Rowlingson on r-help (where you seem to have cross-posted):</p>

<blockquote>
  <p>?read.fwf</p>
  
  <p>Read Fixed Width Format Files</p>
  
  <p>Description:</p>

<pre><code> Read a table of *f*ixed *w*idth *f*ormatted data into a
 'data.frame'.
</code></pre>
  
  <p>Usage:</p>

<pre><code> read.fwf(file, widths, header = FALSE, sep = ""\t"",
          skip = 0, row.names, col.names, n = -1,
          buffersize = 2000, ...)
</code></pre>
</blockquote>
"
1393669,149223,2009-09-08T12:15:15Z,1392868,1,FALSE,"<p>Try running R CMD CHECK RSPython_0.7-1.tar.gz
That should produce at least produce bunch of logs in a RSPython.Rcheck folder</p>

<p>You might get some clues in there. </p>

<p>Update --- 
If you can get one of the other packages to work I'd recommend it.  On my system (R 2.9.1 using system python (2.6) in /usr/bin/python), install works but then RSPython fails to run due to problems inside its .First.lib function. I expect you would need to hack the sources considerably to get it to work.</p>
"
1393789,141826,2009-09-08T12:40:38Z,1392868,0,FALSE,"<p>I found the <a href=""http://rpy.sourceforge.net/"" rel=""nofollow noreferrer"">rpy</a> and <a href=""http://rpy.sourceforge.net/"" rel=""nofollow noreferrer"">rpy2</a> packages and going to try those as an alternative to the older RSPython.</p>

<p>rpy2 is includes in fink's unstable distribution ... well, and it just works fine.</p>
"
1393805,26575,2009-09-08T12:44:32Z,1392868,1,FALSE,"<p>To debug this, simply unpack the tar file yourself (<code>tar xzvf RSPython_0.7-1.tar.gz</code>) and run <code>./configure</code> in the directory created. You should get a config.log file that you can examine.</p>
"
1395143,154039,2009-09-08T17:11:57Z,1395117,11,TRUE,"<p>First, convert the London time to a <code>POSIXct</code> object:</p>

<pre><code>pb.txt &lt;- ""2009-06-03 19:30""
pb.date &lt;- as.POSIXct(pb.txt, tz=""Europe/London"")
</code></pre>

<p>Then use <code>format</code> to print the date in another time zone:</p>

<pre><code>&gt; format(pb.date, tz=""America/Los_Angeles"",usetz=TRUE)
[1] ""2009-06-03 11:30:00 PDT""
</code></pre>

<p>There are some tricks to finding the right time zone identifier to use. More details in this post at the Revolutions blog: <a href=""http://blog.revolutionanalytics.com/2009/06/converting-time-zones.html"" rel=""nofollow noreferrer"">Converting time zones in R: tips, tricks and pitfalls</a></p>
"
1395148,143305,2009-09-08T17:12:45Z,1395118,1,FALSE,"<p>So store the value of the then-current seed and start from there.</p>
"
1395181,143305,2009-09-08T17:19:59Z,1395136,2,FALSE,"<p>The first goal should to get <em>working</em> code. You are there.  Then try some simple optmizations.   E.g. </p>

<pre><code>retVal &lt;- matrix(NA, ni, nj)     # assuming your result is scalar
for (i in 1:ni)
   for (j in 1:nj)
       retVal[i][j] &lt;- *some function of yours*
</code></pre>

<p>will already run <em>much</em> faster as you do not reallocate memory for each i,j combination.</p>

<p>As for the looping, you can start by replacing the inner loop with something from the <code>apply</code> family.  I am not aware of something fully general to answer your question -- it depends what arguments your function takes and what type of return object it produces.</p>
"
1395196,143305,2009-09-08T17:23:34Z,1395174,5,TRUE,"<p>You could use Jeff Horner's excellent <a href=""http://biostat.mc.vanderbilt.edu/rapache/"" rel=""noreferrer""><strong>rapache</strong></a>. </p>

<p>Other options that are CGI-alike are mentioned in the <a href=""http://cran.r-project.org/doc/FAQ/R-FAQ.html#R-Web-Interfaces"" rel=""noreferrer"">section 4 of the R FAQ which discusses Web interfaces</a>.</p>
"
1395210,160314,2009-09-08T17:26:04Z,1395147,16,TRUE,"<p>The effects package has good ploting methods for visualizing the predicted values of regressions.</p>

<pre><code>thedata&lt;-data.frame(x=rnorm(20),f=rep(c(""level1"",""level2""),10))
thedata$y&lt;-rnorm(20,,3)+thedata$x*(as.numeric(thedata$f)-1)

library(effects)
model.lm &lt;- lm(formula=y ~ x*f,data=thedata)
plot(effect(term=""x:f"",mod=model.lm,default.levels=20),multiline=TRUE)
</code></pre>
"
1395223,136862,2009-09-08T17:27:54Z,1395189,1,FALSE,"<p>If you want to remove rows that contain NA's you can use apply() to apply a quick function to check each row. E.g., if your matrix is x,</p>

<pre><code>goodIdx &lt;- apply(x, 1, function(r) !any(is.na(r)))
newX &lt;- x[goodIdx,]
</code></pre>
"
1395231,154039,2009-09-08T17:29:18Z,1395191,14,FALSE,"<p>You can use <code>isplit</code> (from the ""iterators"" package) to create an iterator object that loops over the blocks defined by the <code>site</code> column:</p>

<pre><code>require(iterators)
site.data &lt;- read.table(""isplit-data.txt"",header=T) 
sites &lt;- isplit(site.data,site.data$site)
</code></pre>

<p>Then you can use <code>foreach</code> (from the ""foreach"" package) to create a plot within each block:</p>

<pre><code>require(foreach)
foreach(site=sites) %dopar% {
 pdf(paste(site$key[[1]],"".pdf"",sep=""""))
 plot(site$value$year,site$value$peak,main=site$key[[1]])
 dev.off()
}
</code></pre>

<p>As a bonus, if you have a multiprocessor machine and call <code>registerDoMC()</code> first (from the ""doMC"" package), the loops will run in parallel, speeding things up. More details in this Revolutions blog post: <a href=""http://blog.revolution-computing.com/2009/08/blockprocessing-a-data-frame-with-isplit.html"" rel=""noreferrer"">Block-processing a data frame with isplit</a></p>
"
1395237,143383,2009-09-08T17:30:24Z,1395233,1,FALSE,"<p>My solution was to just change the default argument:</p>

<pre><code>R&gt; formals(foo)$y &lt;- call(""max"", as.name(""m""))
R&gt; foo(1)
[1] 11
</code></pre>
"
1395245,170308,2009-09-08T17:30:58Z,1395189,5,FALSE,"<p>I think <em>na.rm</em> usually only works within functions, say for the <em>mean</em> function. I would go with <em>complete.cases</em>: <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/complete.cases.htm"" rel=""noreferrer"">http://stat.ethz.ch/R-manual/R-patched/library/stats/html/complete.cases.htm</a></p>

<p>let's say you have the following 3x3-matrix:</p>

<pre><code>x &lt;- matrix(c(1:8, NA), 3, 3)

&gt; x
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6   NA
</code></pre>

<p>then you can get the complete cases of this matrix with</p>

<pre><code>y &lt;- x[complete.cases(x),]

&gt; y
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
</code></pre>

<p>The <em>complete.cases</em>-function returns a vector of truth values that says whether or not a case is complete:</p>

<pre><code>&gt; complete.cases(x)
[1]  TRUE  TRUE FALSE
</code></pre>

<p>and then you index the rows of matrix <em>x</em> and add the "","" to say that you want all columns.</p>
"
1395248,143319,2009-09-08T17:31:32Z,1395147,3,FALSE,"<p>Huh - still trying to wrap my brain around <code>expand.grid()</code>.  Just for comparison's sake, this is how I'd do it (using ggplot2):</p>

<pre><code>thedata &lt;- data.frame(predict(thelm), thelm$model$x, thelm$model$f)

ggplot(thedata, aes(x = x, y = yhat, group = f, color = f)) + geom_line()
</code></pre>

<p>The ggplot() logic is pretty intuitive, I think - group and color the lines by f.  With increasing numbers of groups, not having to specify a layer for each is increasingly helpful.</p>
"
1395249,143305,2009-09-08T17:31:52Z,1395115,9,TRUE,"<p>Use the serialization feature to turn any R object into a (raw or character) string, then store that string. See <code>help(serialize)</code>.</p>

<p>Reverse this for retrieval:  get the string, then <code>unserialize()</code> into a R object.</p>
"
1395255,143305,2009-09-08T17:33:30Z,1395229,6,FALSE,"<ol>
<li>Buy more ram</li>
<li>Switch to a 64-bit OS. Combine with point 1.</li>
</ol>
"
1395256,38426,2009-09-08T17:33:36Z,1395229,44,TRUE,"<p>From: </p>

<p><a href=""http://gking.harvard.edu/zelig/docs/How_do_I2.html"" rel=""noreferrer"">http://gking.harvard.edu/zelig/docs/How_do_I2.html</a> (<a href=""https://web.archive.org/web/20091208054821/http://gking.harvard.edu/zelig/docs/How_do_I2.html"" rel=""noreferrer"">mirror</a>)</p>

<blockquote>
  <p>Windows users may get the error that R
  has run out of memory. </p>
  
  <p>If you have R already installed and
  subsequently install more RAM, you may
  have to reinstall R in order to take
  advantage of the additional capacity. </p>
  
  <p>You may also set the amount of
  available memory manually. Close R,
  then right-click on your R program
  icon (the icon on your desktop or in
  your programs directory). Select
  ``Properties'', and then select the
  ``Shortcut'' tab. Look for the
  ``Target'' field and after the closing
  quotes around the location of the R
  executible, add </p>
  
  <p>--max-mem-size=500M</p>
  
  <p>as shown in the figure below. You may
  increase this value up to 2GB or the
  maximum amount of physical RAM you
  have installed. </p>
  
  <p>If you get the error that R cannot
  allocate a vector of length x, close
  out of R and add the following line to
  the ``Target'' field: </p>
  
  <p>--max-vsize=500M</p>
  
  <p>or as appropriate.  You can always
  check to see how much memory R has
  available by typing at the R prompt </p>
</blockquote>

<pre><code>memory.limit()
</code></pre>

<p>which gives you the amount of available memory in MB.  In previous versions of R you needed to use: round(memory.limit()/2^20, 2)</p>
"
1395257,136862,2009-09-08T17:33:45Z,1395158,6,FALSE,"<p>You can quote the column name, e.g.</p>

<pre><code>alltime$'CPT Desc'
</code></pre>
"
1395282,136862,2009-09-08T17:38:51Z,1395156,3,FALSE,"<p>How about:</p>

<pre><code> out &lt;- cbind(myDF, t(apply(myDF, 1, getNumberInfo)))
 colnames(out) &lt;- c('Value', 'Evenness', 'Positivity', 'Log')
</code></pre>

<p>Which gives you: <pre>
  Value Evenness  Positivity               Log
1    -2     Even NonPositive              NA
2    -1      Odd NonPositive              NA
3     0     Even NonPositive              NA
4     1      Odd    Positive                 0
5     2     Even    Positive 0.693147180559945
</pre></p>
"
1395285,143319,2009-09-08T17:39:15Z,1395189,6,FALSE,"<p><code>na.omit()</code> will take matrices (and data frames) and return only those rows with no NA values whatsoever - it takes <code>complete.cases()</code> one step further by deleting the FALSE rows for you.</p>

<pre><code>&gt; x &lt;- data.frame(c(1,2,3), c(4, NA, 6))
&gt; x
  c.1..2..3. c.4..NA..6.
1          1           4
2          2          NA
3          3           6
&gt; na.omit(x)
  c.1..2..3. c.4..NA..6.
1          1           4
3          3           6
</code></pre>
"
1395293,84458,2009-09-08T17:41:31Z,1395191,12,TRUE,"<p>Another choice is use the <code>ddply</code> function from the <code>ggplot2</code> library.  But you mention you mostly want to do a plot of peak vs. year, so you could also just use <code>qplot</code>:</p>

<pre><code>A &lt;- read.table(""example.txt"",header=TRUE)
library(ggplot2)
qplot(peak,year,data=A,colour=site,geom=""line"",group=site)
ggsave(""peak-year-comparison.png"")
</code></pre>

<p><a href=""http://i32.tinypic.com/16nuza.png"">alt text http://i32.tinypic.com/16nuza.png</a></p>

<p>On the other hand, I do like David Smith's solution that allows the applying of the function to be run across several processors.</p>
"
1395326,161640,2009-09-08T17:48:00Z,1395229,2,FALSE,"<p>Microsoft Windows accepts any memory request from processes if it could be done.</p>

<p>There is no limit for the memory that can be provided to a process, except the Virtual Memory Size.</p>

<p>Virtual Memory Size is 4GB in 32bit systems for any processes, no matter how many applications you are running. Any processes can allocate up to 4GB memory in 32bit systems.</p>

<p>In practice, Windows automatically allocates some parts of allocated memory from RAM or page-file depending on processes requests and paging file mechanism.</p>

<p>But another limit is the size of paging file. If you have a small paging-file, you cannot allocated large memories. You could increase the size of paging file according to <a href=""http://support.microsoft.com/kb/826513"" rel=""nofollow noreferrer"">Microsoft</a> to have more memory space.</p>
"
1395339,37751,2009-09-08T17:50:28Z,1395309,7,FALSE,"<p>I hear tell that <a href=""http://www.revolution-computing.com/campaign/100-percent-r-and-more.php?gclid=adgroup1&amp;gclid=CPDv-7zJ4pwCFQi_sgodFVXPIg"" rel=""nofollow noreferrer"">REvolution R</a> supports better multi-threading then the typical CRAN version of R and REvolution also supports 64 bit R in windows. I have been considering buying a copy but I found their pricing opaque. There's no price list on their web site. Very odd. </p>
"
1395342,163053,2009-09-08T17:51:12Z,1395229,33,FALSE,"<p>Use memory.limit(). You can increase the default using this command, memory.limit(size=2500), where the size is in MB. You need to be using 64-bit in order to take real advantage of this.</p>

<p>One other suggestion is to use memory efficient objects wherever possible: for instance, use a matrix instead of a data.frame.</p>
"
1395360,84458,2009-09-08T17:54:01Z,1395284,12,TRUE,"<p>Here's one possibility with the <code>ggplot2</code> package.</p>

<pre><code>N &lt;- 10
labs &lt;- factor(1:N,labels=paste(""This is \n observation"",1:N))
dnow &lt;- data.frame(x=1:N, y=runif(N), labels=labs)
qplot(labels,y,data=dnow) + 
      opts(axis.text.x=theme_text(angle=-45,hjust=0))
</code></pre>

<p><a href=""http://i28.tinypic.com/k024p3.png"">alt text http://i28.tinypic.com/k024p3.png</a></p>

<p>I'm looking forward to seeing the base package examples, too!</p>
"
1395365,143305,2009-09-08T17:54:33Z,1395309,29,FALSE,"<p>The <a href=""http://cran.r-project.org/web/views/HighPerformanceComputing.html"" rel=""noreferrer"">CRAN Task View on High-Performance Compting with R</a> lists several options. XP is a restriction, but you still get something like <a href=""http://cran.r-project.org/package=snow"" rel=""noreferrer""><strong>snow</strong></a> to work using sockets within minutes.</p>
"
1395384,84458,2009-09-08T17:57:49Z,1395301,3,FALSE,"<p>If you're using Emacs/ESS, this isn't a problem.  I navigate to the directory where my R script is located, open it, then start an R ESS process.  An R console pops up with the current directory as R's working directory.</p>

<p>If you haven't converted to Emacs/ESS, I recommend it.  (Though to prevent a flame war, I also note there are similar options for Vi users.)</p>

<p>Hope that helps.</p>
"
1395405,149223,2009-09-08T18:01:25Z,1395102,3,FALSE,"<p>Interesting question. </p>

<p>In principle the only difference is that gls can't fit models with random effects, whereas lme can. So the commands</p>

<pre><code>fm1 &lt;- gls(follicles ~ sin(2*pi*Time)+cos(2*pi*Time),Ovary,
           correlation=corAR1(form=~1|Mare))
</code></pre>

<p>and</p>

<pre><code>lm1 &lt;- lme(follicles~sin(2*pi*Time)+cos(2*pi*Time),Ovary,
           correlation=corAR1(form=~1|Mare))
</code></pre>

<p>ought to give the same result but they don't. The fitted parameters differ slightly. </p>
"
1395423,16632,2009-09-08T18:05:25Z,1395136,4,TRUE,"<p>The <a href=""http://had.co.nz/plyr"" rel=""nofollow noreferrer"">plyr</a> package provides a set of general tools for replacing looping constructs when you're work with a big data structure by breaking it into pieces, processing each piece independently and then joining the results back together.</p>
"
1395424,16632,2009-09-08T18:06:22Z,1395158,9,TRUE,"<p>Also see <code>make.names</code></p>
"
1395426,84458,2009-09-08T18:06:56Z,1395294,2,FALSE,"<p>First I'll generate some example data:</p>

<pre><code>&gt; set.seed(123)
&gt; x &lt;- 1:10
&gt; a &lt;- 3
&gt; b &lt;- 5
&gt; fit &lt;- c()
&gt; for (i in 1:10) {
+   y &lt;- a + b*x + rnorm(10,0,.3)
+   fit[[i]] &lt;- lm(y ~ x)
+ }
</code></pre>

<p>Here's one option for grabbing the estimates from each fit:</p>

<pre><code>&gt; t(sapply(fit, function(x) coef(x)))
      (Intercept)        x
 [1,]    3.157640 4.975409
 [2,]    3.274724 4.961430
 [3,]    2.632744 5.043616
 [4,]    3.228908 4.975946
 [5,]    2.933742 5.011572
 [6,]    3.097926 4.994287
 [7,]    2.709796 5.059478
 [8,]    2.766553 5.022649
 [9,]    2.981451 5.020450
[10,]    3.238266 4.980520
</code></pre>

<p>As you mention, other quantities concerning the fit are available.  Above I only grabbed the coefficients with the <code>coef()</code> function.  Check out the following command for more:</p>

<pre><code>names(summary(fit[[1]]))
</code></pre>
"
1395431,37751,2009-09-08T18:08:56Z,1395294,3,TRUE,"<p>I use the <code>plyr</code> package and then if my list of objects was called <code>modelOutput</code> and I want to get out all the predicted values I would do this:</p>

<pre><code>modelPredictions &lt;- ldply(modelOutput, as.data.frame(predict))
</code></pre>

<p>if I want all the coefficients I do this:</p>

<pre><code>modelCoef &lt;- ldply(modelOutput, as.data.frame(coef))
</code></pre>

<p>Hadley originally showed me how to do this <a href=""https://stackoverflow.com/questions/1169539/linear-regression-and-group-by-in-r/1214432#1214432"">in a previous question</a>. </p>
"
1395437,16363,2009-09-08T18:10:13Z,1395410,31,FALSE,"<p>Not sure I understand.  </p>

<p>Appending to same file (one plot per page):</p>

<pre><code>pdf(""myOut.pdf"")
for (i in 1:10){
  plot(...)
}
dev.off()
</code></pre>

<p>New file for each loop:</p>

<pre><code>for (i in 1:10){
  pdf(paste(""myOut"",i,"".pdf"",sep=""""))
  plot(...)
  dev.off()
}
</code></pre>
"
1395443,143305,2009-09-08T18:11:21Z,1395410,50,TRUE,"<p>Did you look at help(pdf) ?</p>

<blockquote>
  <p>Usage:</p>

<pre><code> pdf(file = ifelse(onefile, ""Rplots.pdf"", ""Rplot%03d.pdf""),
     width, height, onefile, family, title, fonts, version,
     paper, encoding, bg, fg, pointsize, pagecentre, colormodel,
     useDingbats, useKerning)
</code></pre>
  
  <p>Arguments:</p>

<pre><code>file: a character string giving the name of the file. For use with
      'onefile=FALSE' give a C integer format such as
      '""Rplot%03d.pdf""' (the default in that case). (See
      'postscript' for further details.)
</code></pre>
</blockquote>

<p>For 1), you keep onefile at the default value of TRUE. Several plots go into the same file.</p>

<p>For 2), you set onefile to FALSE and choose a filename with the C integer format and R will create a set of files.</p>
"
1395520,170352,2009-09-08T18:26:15Z,1386767,3,FALSE,"<p>What about <a href=""http://rattle.togaware.com/"" rel=""nofollow noreferrer"">Rattle</a>? </p>

<p>Rattle stands for <strong>R</strong> <strong>A</strong>nalytical <strong>T</strong>ool <strong>T</strong>o <strong>L</strong>earn <strong>E</strong>asily. According to the website Rattle <em>''is a popular GUI for data mining using R. It presents statistical and visual summaries of data, transforms data that can be readily modelled, builds both unsupervised and supervised models from the data, presents the performance of models graphically, and scores new datasets.''</em></p>
"
1395553,136407,2009-09-08T18:31:24Z,1395105,15,FALSE,"<p>You can generate tikz code from R: 
<a href=""http://r-forge.r-project.org/projects/tikzdevice/"" rel=""noreferrer"">http://r-forge.r-project.org/projects/tikzdevice/</a></p>
"
1395557,84458,2009-09-08T18:32:05Z,1395105,28,FALSE,"<p>As stolen from <a href=""http://tolstoy.newcastle.edu.au/R/e7/help/09/06/0676.html"" rel=""noreferrer"">here</a>, the following command correctly uses LaTeX to draw the title:</p>

<pre><code>plot(1, main=expression(beta[1]))
</code></pre>

<p>See <code>?plotmath</code> for more details.</p>
"
1395579,163053,2009-09-08T18:35:23Z,1395301,6,FALSE,"<p>You could use an environmental variable.  This can work with <code>Sys.getenv()</code> and <code>Sys.setenv()</code>.  For instance:</p>

<pre><code>&gt; Sys.setenv(R_TEST=""testit"")
&gt; Sys.getenv(""R_TEST"")
  R_TEST 
""testit"" 
</code></pre>

<p>If you sent the variable in your script, you should be able to access it from within, and then call <code>setwd()</code> on that output.</p>
"
1395583,84458,2009-09-08T18:36:10Z,1395271,10,TRUE,"<p>This seems to work:</p>

<pre><code>&gt; groupAcres &lt;- ddply(mydata, c(""state""), function(df) c(myName=sum(df$acres)))
&gt; groupAcres
  state   myName
1     A 56.87973
2     B 57.84451
3     C 52.82415
</code></pre>
"
1395638,37751,2009-09-08T18:43:58Z,1395577,47,FALSE,"<p>I think you want to first suppress the labels on the x axis with the xaxt=""n"" option:</p>

<pre><code>plot(flow~factor(month),xlab=""Month"",ylab=""Total Flow per Month"",ylim=c(0,55000), xaxt=""n"")  
</code></pre>

<p>then use the axis command to add in your own labels. This example assumes the labels are in an object called month.name</p>

<pre><code>axis(1, at=1:12, labels=month.name) 
</code></pre>

<p>I had to look up how to do this and <a href=""http://www.nabble.com/replacing-default-axis-labels-on-a-plot-td23538822.html"" rel=""noreferrer"">I stole the example from here</a>.</p>
"
1395653,170364,2009-09-08T18:47:44Z,1395504,1,TRUE,"<p>This will download the file to the working directory, but not directly into memory.  </p>

<p><code>download.file('http://user2010.org/pics/useR-large.png', 'anImage.png')</code></p>

<p>The Rcurl package may also do what you want. (link not posted because of SO restrictions)</p>
"
1395720,163053,2009-09-08T18:59:00Z,1395622,22,FALSE,"<p>If you wrap your inner function with a try() statement, you get more information:</p>

<pre><code>&gt; sapply(x, function(x) try(1/x))
Error in 1/x : non-numeric argument to binary operator
[1] ""-0.5""                                                    
[2] ""Error in 1/x : non-numeric argument to binary operator\n""
[3] ""Inf""                                                     
[4] ""1""                                                       
[5] ""0.5""
</code></pre>

<p>In this case, you can see which index fails.</p>
"
1395728,169947,2009-09-08T19:00:13Z,1395156,8,TRUE,"<p>You might want to change your <code>getNumberInfo</code> function to return a list rather than a vector, so that the values can have different types.  As it is, they're all being cast to strings, which probably isn't what you want for <code>logX</code>.</p>

<pre><code>getNumberInfo &lt;- function(x) {
  if(x %% 2 ==0) evenness = ""Even"" else evenness=""Odd""
  if(x &gt; 0) positivity = ""Positive"" else positivity = ""NonPositive""
  if (positivity == ""Positive"") logX = log(x) else logX=NA
  list(evenness,positivity,logX)
}
</code></pre>

<p>Furthermore, you can use the names to a somewhat better effect so that you don't have to repeat them:</p>

<pre><code>getNumberInfo &lt;- function(x) {
  list(evenness = if(x %% 2 ==0) ""Even"" else ""Odd"",
       positivity = if(x &gt; 0) ""Positive"" else ""NonPositive"",
       logX = if(x &gt; 0) log(x) else NA)
}
</code></pre>

<p>Then the solution becomes simple:</p>

<pre><code>&gt; cbind(myDF, t(sapply(myDF$Value, getNumberInfo)))
  Value evenness  positivity      logX
1    -2     Even NonPositive        NA
2    -1      Odd NonPositive        NA
3     0     Even NonPositive        NA
4     1      Odd    Positive         0
5     2     Even    Positive 0.6931472
</code></pre>

<p>Finally, if you use <code>ifelse</code> (which can work on vectors) instead of <code>if</code>, it gets even simpler because you don't have to call <code>apply</code>:</p>

<pre><code>getNumberInfo &lt;- function(x) {
  list(evenness = ifelse(x %% 2 ==0, ""Even"", ""Odd""),
       positivity = ifelse(x &gt; 0, ""Positive"", ""NonPositive""),
       logX = ifelse(x &gt; 0, log(x), NA))
}

&gt; cbind(myDF, getNumberInfo(myDF$Value))
  Value evenness  positivity      logX
1    -2     Even NonPositive        NA
2    -1      Odd NonPositive        NA
3     0     Even NonPositive        NA
4     1      Odd    Positive 0.0000000
5     2     Even    Positive 0.6931472
</code></pre>

<p>That last solution emits a warning, because it's actually computing the log of every element, not just those with <code>x&gt;0</code>.  Not sure the most elegant way to deal with that.</p>
"
1395760,84458,2009-09-08T19:04:28Z,1395157,1,FALSE,"<p>One option: represent each person's movement as a directed edge.  Create an aggregate graph such that each edge has a weight corresponding to the number of times that edge occurred.  Those edges with large weights will be ""typical"" 1-paths.</p>

<p>Of course, it gets more interesting to find common k-paths or explore how paths vary among individuals.  The naive approach for 2-paths would be to create N additional nodes that correspond to nodes when visited in the middle of the 2-path.  For example, if you have nodes a_1, ..., a_N you would create nodes b_1, ..., b_N.  The aggregate network might have an edge (a_3, b_5, 10) and an edge (b_5, a_7, 10); this would represent the two-path (a_3, b_5, a_7) occurring 10 times.  The task you're interested in corresponds to finding those two-paths with large weights.</p>

<p>Both the <code>igraph</code> and <code>network</code> packages would suffice for this sort of analysis.</p>

<p>If you have some bound on <code>k</code> (ie. only 6-paths occur in your dataset), I might also suggest enumerating all the paths that are taken and computing the histogram of each unique path.  I don't know of any functions that do this automagically for you.</p>
"
1395763,170364,2009-09-08T19:05:00Z,1395191,4,FALSE,"<p>There are two handy built in functions for dealing with these kind of situations.  ?aggregate and ?by.  In this case because you want a plot and aren't returning a scalar, use by()</p>

<p><code>data &lt;- read.table(""example.txt"",header=TRUE)</code></p>

<p><code>by(data[, c('year', 'peak')], data$site, plot)</code></p>

<p>The output says <code>NULL</code> because that's what plot returns.  You might want to set the graphics device to pdf to capture all the output.</p>
"
1395792,36093,2009-09-08T19:11:42Z,1395191,6,FALSE,"<p>Here's what I would do, although it looks like you guys have it handled by library functions.</p>

<pre><code>for(i in 1:length(unique(data$site))){
  constrainedData = data[data$site==data$site[i]];
  doSomething(constrainedData);
}
</code></pre>

<p>This kind of code is more direct and might be less efficient, but I prefer to be able to read what it is doing than learn some new library function for the same thing.  makes this feel more flexible too, but in all honesty this is just the way I figured it out as a novice.  </p>
"
1395908,143377,2009-09-08T19:39:22Z,1395284,4,FALSE,"<p>This is what I cooked up (before my <code>ggplot2</code> days) using base graphics:</p>

<pre><code>## data
N &lt;- 10
dnow &lt;- data.frame(x=1:N, y=runif(N), labels=paste(""This is \nobservation "",1:N))
## make margins wide
par(mfrow=c(1,1), mar=c(10,10,6,4))
## plot without axix labels or ticks
with(dnow, plot(x,y, xaxt=""n"", xlab=""""))
## the positions we ant to plot
atn &lt;- seq(1,N,3)
## the label for these positions
lab &lt;- dnow$labels[atn]
## plot the axis, but do not plot labels
axis(1, at=atn, labels=FALSE)
## plot labels
text(atn, ## x position
     par(""usr"")[3]-.05, ## position of the low axis
     srt=45, ## angle
     labels=lab, ##labels
     xpd=TRUE, ## allows plotting outside the region 
     pos=2)
## par(""usr"")[3]
</code></pre>
"
1395960,151019,2009-09-08T19:49:24Z,1395499,1,FALSE,"<p>Store datetimes as UTC - or in this case convert to UTC subtract a day and convert back to local time. (This ignores leap seconds) </p>
"
1396006,160314,2009-09-08T19:57:47Z,1395945,9,FALSE,"<p>check out the ggplot documentation for scale_brewer <a href=""http://www.had.co.nz/ggplot2/scale_brewer.html"" rel=""noreferrer"">http://www.had.co.nz/ggplot2/scale_brewer.html</a></p>

<p>some examples:</p>

<pre><code>#see available pallets:

library(RColorBrewer)
display.brewer.all(5)

#scatter plot
dsamp &lt;- diamonds[sample(nrow(diamonds), 1000), ] 
d &lt;- qplot(carat, price, data=dsamp, colour=clarity)
dev.new()
d
dev.new()
d + scale_colour_brewer(palette=""Set1"")
dev.new()
d + scale_colour_brewer(palette=""Blues"")
</code></pre>
"
1396014,84458,2009-09-08T19:59:03Z,1395945,12,FALSE,"<p>Here's a small dataset:</p>

<pre><code>dat &lt;- data.frame(x=1:20,
                  y=rnorm(20,0,10),
                  v=20:1)
</code></pre>

<p>Suppose I want my points colored using the value v.  I can change the way in which the coloring is performed using the <a href=""http://had.co.nz/ggplot2/scale_gradient.html"" rel=""noreferrer"">scale_colour_gradient()</a> function. </p>

<pre><code>library(ggplot2)
qplot(x,y,data=dat,colour=color,size=4) + 
  scale_colour_gradient(low=""black"", high=""white"")
</code></pre>

<p><a href=""http://i29.tinypic.com/amxtu0.png"" rel=""noreferrer"">alt text http://i29.tinypic.com/amxtu0.png</a></p>

<p>This example should just get you started.  For more, check out the <code>scale_brewer()</code> mentioned in the other post.</p>
"
1396049,168643,2009-09-08T20:06:19Z,1260965,10,FALSE,"<p>Check out the packages </p>

<pre><code>library(sp)
library(rgdal)
</code></pre>

<p>which are nice for geodata, and</p>

<pre><code>library(RColorBrewer)  
</code></pre>

<p>is useful for colouring. <a href=""http://picasaweb.google.no/lh/photo/FtqMZz1wTsrcRE9gFX3U-g?feat=directlink"" rel=""nofollow noreferrer"">This map</a> is made with the above packages and this code:</p>

<pre><code>VegMap &lt;- readOGR(""."", ""VegMapFile"")
Veg9&lt;-brewer.pal(9,'Set2')
spplot(VegMap, ""Veg"", col.regions=Veg9,
 +at=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5),
 +main='Vegetation map')
</code></pre>

<p><code>""VegMapFile""</code> is a shapefile and <code>""Veg""</code> is the variable displayed. Can probably be done better with a little work. I don`t seem to be allowed to upload image, here is an link to the image:</p>
"
1396235,163053,2009-09-08T20:41:36Z,1395267,3,FALSE,"<p>Not totally clear on your requirements: can you be sure that the user will have R installed (e.g. can you run a script on their desktops to install everything first)?  Does it have to run over the web?</p>

<ul>
<li>The animation package (<a href=""http://cran.r-project.org/web/packages/animation/"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/packages/animation/</a>) isn't interactive, but it can create moving images.</li>
<li>The iplots package is useful, although it requires R: <a href=""http://rosuda.org/iPlots/iplots.html"" rel=""nofollow noreferrer"">http://rosuda.org/iPlots/iplots.html</a></li>
<li>Similarly, rggobi is extremely useful for interactive graphics, but it also requires R.  You can read more <a href=""http://www.jstatsoft.org/v30/b07/paper"" rel=""nofollow noreferrer"">http://www.jstatsoft.org/v30/b07/paper</a> and <a href=""http://www.ggobi.org/rggobi/"" rel=""nofollow noreferrer"">http://www.ggobi.org/rggobi/</a>.  </li>
<li>A last example is biplotgui: <a href=""http://r-forge.r-project.org/projects/biplotgui/"" rel=""nofollow noreferrer"">http://r-forge.r-project.org/projects/biplotgui/</a></li>
</ul>

<p>I heard that there's a project in development to create Flash output from R, but I can't find anything about it.</p>
"
1396250,19410,2009-09-08T20:44:39Z,1395267,0,FALSE,"<p><em>Can I keep this whole process within R?</em></p>

<p>Check out <a href=""http://www.ggobi.org/"" rel=""nofollow noreferrer"">GGobi</a>:</p>

<blockquote>
  <p><em>GGobi is an open source visualization program for exploring high-dimensional data. It provides highly dynamic and interactive graphics such as tours, as well as familiar graphics such as the scatterplot, barchart and parallel coordinates plots. Plots are interactive and linked with brushing and identification.</em></p>
</blockquote>
"
1396280,144642,2009-09-08T20:53:15Z,1395622,0,FALSE,"<p>You can debug() the function, or put a browser() inside the body.  This is only particularly useful if you don't have a gajillion iterations to work through. </p>

<p>Also, I've not personally done this, but I suspect you could put a browser() in as part of a tryCatch(), such that when the error is generated you can use the browser() interface.</p>
"
1396410,168747,2009-09-08T21:23:25Z,1395301,15,TRUE,"<p>You should copy shortcut to R (R.lnk file) to desire folder. Then in ""Properties"" (right mouse button -> last option) delete anything in field ""Start in..."" in second tab (""Shortcut""?). If you start R with this shortcut working directory will be that one where the shortcut is.</p>

<p>I don't have english version of Windows so I'm not sure about field names, but they should be easy to find.</p>

<p>Similar questions were in R-windows-faq:</p>

<p><a href=""http://cran.r-project.org/bin/windows/base/rw-FAQ.html#How-do-I-run-it_003f"" rel=""noreferrer"">2.5 How do I run it?</a></p>

<p><a href=""http://cran.r-project.org/bin/windows/base/rw-FAQ.html#How-can-I-keep-workspaces-for-different-projects-in-different-directories_003f"" rel=""noreferrer"">2.10 How can I keep workspaces for different projects in different directories?</a></p>

<p><a href=""http://cran.r-project.org/bin/windows/base/rw-FAQ.html#What-are-HOME-and-working-directories_003f"" rel=""noreferrer"">2.14 What are HOME and working directories?</a> </p>

<p>In 2.14 is mentioned that </p>

<blockquote>
  <p>The working directory is the directory from which Rgui or Rterm was launched, unless a shortcut was used when it is given by the `Start in' field of the shortcut's properties. </p>
</blockquote>
"
1396421,NA,2009-09-08T21:28:03Z,1395266,3,FALSE,"<p>Clojure compiles to Java byte code, so you should be able to do what you want using <code>rJava</code>. The slightly ugly part would be figuring out what the method signatures are, since <code>rJava</code> requires the JNI-style method signature.</p>
"
1396596,144157,2009-09-08T22:09:06Z,1395301,5,FALSE,"<p>Save your workspace to the desired directory and thereafter you just open the workspace from Windows explorer.</p>
"
1396773,168643,2009-09-08T22:58:36Z,1395301,3,FALSE,"<p>Just a detail: instead of reversing the slashes as you say, just add another backslash. Two of these \\ works the same way as one of these /. That makes it at least a little easier.</p>
"
1396924,16632,2009-09-08T23:48:10Z,1395622,8,FALSE,"<p>Use the plyr package, with <code>.inform = TRUE</code>:</p>

<pre><code>library(plyr)
laply(x, function(x) 1/x, .inform = TRUE)
</code></pre>
"
1396938,16632,2009-09-08T23:52:29Z,1395271,19,FALSE,"<p>Use summarise (or summarize):</p>

<pre><code>  groupAcres &lt;- ddply(mydata, ""state"", summarise, 
     myName = sum(acres))
</code></pre>
"
1397791,23813,2009-09-09T06:03:47Z,1395206,1,FALSE,"<p>This doesn't exactly solve your problem, but may point in a helpful direction. I'm disregarding the interplay between changing <code>adjtarget</code> and comparing to it, and show a similar problem, where we compare to the constant <code>target</code>.  Then it's possible to change the <code>if</code> in the loop to a vector comparison:</p>

<pre><code>lv &lt;- but.last(subject) &gt; but.last(target)
ind &lt;- which(lv)
</code></pre>

<p>Prepare the result vector (I'll call it <code>x</code>, as it won't be the same result as your <code>adjtarget</code>) as a shifted copy of <code>target</code> and assign the changes to it:</p>

<pre><code>x &lt;- c(target[1], but.last(target))  # corresponds to the true branch of the `if`
x[ind+1] &lt;- target[ind] - perweek    # corresponds to the false branch
</code></pre>

<p>Alternatively,</p>

<pre><code>x &lt;- c(target[1], but.last(target) - (!lv)*perweek
</code></pre>

<p>As I said, this doesn't solve your problem, but perhaps we could start from here.</p>
"
1398183,160588,2009-09-09T07:57:02Z,1397823,9,FALSE,"<p>A step-by-step tutorial to this kind of plotting is in the <a href=""http://rwiki.sciviews.org/doku.php?id=tips%3Agraphics-misc%3Atranslucency"" rel=""nofollow noreferrer"">R-wiki</a></p>
"
1399003,134830,2009-09-09T11:04:36Z,1395504,1,FALSE,"<p>A simple solution if to set 'n' to be reasonably large, read the file, check for possible overflow, and try again if necessary.</p>

<pre><code>N &lt;- 1e7
repeat
{
   anImage &lt;- readBin(filename, 'raw', n=N)
   if(length(anImage) == N) N &lt;- 5 * N else break
}
</code></pre>
"
1399351,143305,2009-09-09T12:18:43Z,1399217,4,TRUE,"<p>You found the <code>ts</code> type for time series which is suitable for ARIMA-modeling and series with fixed 'delta t' such as monhtly or quarterly series.</p>

<p>But R is good at working with dates in general. Try experimenting with keeping your data in a <code>data.frame</code>, but convert your x-axis data to either type <code>Date</code> or <code>POSIXt</code>. The <code>plot()</code> will call an axis-formatting function that knows about time and you get better defaults which you can still override.</p>

<p>Better still is use of packages <a href=""http://cran.r-project.org/package=zoo"" rel=""nofollow noreferrer""><strong>zoo</strong></a> or <a href=""http://cran.r-project.org/package=xts"" rel=""nofollow noreferrer""><strong>xts</strong></a> which give you additional control as well as bells and whistles:</p>

<pre><code> &gt; X &lt;- data.frame(x=seq(Sys.Date()-11*31,Sys.Date(),by=""1 months""),  
                    y=cumsum(rnorm(12)))
 &gt; plot(X)                   # plot as a data.frame with Date column
 &gt; library(zoo)
 &gt; plot(zoo(X[,-1], X[,1]))  # plot as a zoo object
 &gt; library(xts)
 &gt; plot(xts(X[,-1], X[,1]))  # plot as an xts object
</code></pre>

<p><em>Edit:</em>  I forgot that if your data is already a <code>ts</code> object, you have easier converters
<code>as.zoo()</code> and <code>as.xts()</code>.  And the <code>plot.zoo</code> help page has examples for custom formatting of the time axis.</p>
"
1399467,134830,2009-09-09T12:44:49Z,1395528,45,FALSE,"<pre><code>library(RCurl)
library(XML)

# Download page using RCurl
# You may need to set proxy details, etc.,  in the call to getURL
theurl &lt;- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""
webpage &lt;- getURL(theurl)
# Process escape characters
webpage &lt;- readLines(tc &lt;- textConnection(webpage)); close(tc)

# Parse the html tree, ignoring errors on the page
pagetree &lt;- htmlTreeParse(webpage, error=function(...){})

# Navigate your way through the tree. It may be possible to do this more efficiently using getNodeSet
body &lt;- pagetree$children$html$children$body 
divbodyContent &lt;- body$children$div$children[[1]]$children$div$children[[4]]
tables &lt;- divbodyContent$children[names(divbodyContent)==""table""]

#In this case, the required table is the only one with class ""wikitable sortable""  
tableclasses &lt;- sapply(tables, function(x) x$attributes[""class""])
thetable  &lt;- tables[which(tableclasses==""wikitable sortable"")]$table

#Get columns headers
headers &lt;- thetable$children[[1]]$children
columnnames &lt;- unname(sapply(headers, function(x) x$children$text$value))

# Get rows from table
content &lt;- c()
for(i in 2:length(thetable$children))
{
   tablerow &lt;- thetable$children[[i]]$children
   opponent &lt;- tablerow[[1]]$children[[2]]$children$text$value
   others &lt;- unname(sapply(tablerow[-1], function(x) x$children$text$value)) 
   content &lt;- rbind(content, c(opponent, others))
}

# Convert to data frame
colnames(content) &lt;- columnnames
as.data.frame(content)
</code></pre>

<p><strong>Edited to add:</strong></p>

<p>Sample output</p>

<pre><code>                     Opponent Played Won Drawn Lost Goals for Goals against  % Won
    1               Argentina     94  36    24   34       148           150  38.3%
    2                Paraguay     72  44    17   11       160            61  61.1%
    3                 Uruguay     72  33    19   20       127            93  45.8%
    ...
</code></pre>
"
1399674,163053,2009-09-09T13:25:28Z,1399217,2,FALSE,"<p>Just to add to what Dirk said:</p>

<p>Once you're using a proper date type (Date or POSIXt), then you can use the format() command to choose how you want it to look in your plot:</p>

<pre><code>&gt; format(seq(Sys.Date()-11*31,Sys.Date(),by=""1 months""), ""%b-%y"")
 [1] ""Oct-08"" ""Nov-08"" ""Dec-08"" ""Jan-09"" ""Feb-09"" ""Mar-09"" ""Apr-09"" ""May-09""
 [9] ""Jun-09"" ""Jul-09"" ""Aug-09"" ""Sep-09""
</code></pre>

<p>Look at the help for strptime for more examples of formatting options.</p>

<pre><code>?strptime
</code></pre>
"
1399992,143377,2009-09-09T14:17:09Z,1163640,2,FALSE,"<p>In this discussion at the R-list <a href=""http://www.nabble.com/PDF-Compression-td24734941.html"" rel=""nofollow noreferrer"">link text</a> I learned about <a href=""http://ttp://www.accesspdf.com/pdftk/"" rel=""nofollow noreferrer"">pdftk</a>. With <code>n= 1e5</code> reduced the pdf size from 6mb to 600k. Pretty neat!</p>
"
1400428,168139,2009-09-09T15:34:46Z,1386767,3,FALSE,"<p>I use <a href=""http://sourceforge.net/projects/tinn-r/"" rel=""nofollow noreferrer"">Tinn-R</a> which has a wonderful R explorer window which shows a list of objects. One can also chosose the view in which details of the objects are displayed. Tinn-r is a great script editor (which is its primary purpose) and has some shortcuts such as dataframe.name$[ctrl-shift-D] which brings up a list of column names in dataframe.name so that the programmer does not need to remember them and their exact spelling. </p>
"
1400560,168139,2009-09-09T15:58:17Z,1310247,6,FALSE,"<p>""Attach"" is an evil temptation. The <em>only place where it works well is in the classroom setting</em> where one is given a single dataframe and expected to write lines of code to do the analysis on that one dataframe. The user is unlikely to ever use that data again once the assignement is done and handed in.</p>

<p>However, <em>in the real world</em>, more data frames can be added to the collection of data in a particular project. Furthermore one often copies and pastes blocks of code to be used for something similar. Often one is borrowing from something one did a few months ago and cannot remember the nuances of what was being called from where. In these circumstances one gets drowned by the previous use of ""attach.""</p>
"
1400957,144529,2009-09-09T17:24:11Z,1400937,3,FALSE,"<p>It was my understanding that \10 would we understood as backreference 0 followed by a digit of 1. I think 9 is the max.</p>
"
1401009,123582,2009-09-09T17:33:10Z,1400937,1,FALSE,"<p>According to <a href=""http://www.regular-expressions.info/refreplace.html"" rel=""nofollow noreferrer"">this site</a>, back references \10 to \99 works on some languages, but not most.</p>

<p>Those that are reported to work are</p>

<ul>
<li><a href=""http://www.regular-expressions.info/jgsoft.html"" rel=""nofollow noreferrer"">JGSoft</a></li>
<li><a href=""http://www.regular-expressions.info/python.html"" rel=""nofollow noreferrer"">Python</a></li>
<li><a href=""http://www.regular-expressions.info/php.html#preg"" rel=""nofollow noreferrer"">PHP preg</a></li>
<li><a href=""http://www.regular-expressions.info/realbasic.html"" rel=""nofollow noreferrer"">REAL basic</a></li>
</ul>
"
1401030,53114,2009-09-09T17:37:14Z,1400937,7,TRUE,"<p>See <a href=""http://www.regular-expressions.info/rlanguage.html"" rel=""nofollow noreferrer"">Regular Expressions with The R Language</a>:</p>

<blockquote>
  <p>You can use the backreferences <code>\1</code> through <code>\9</code> in the replacement text to reinsert text matched by a <a href=""http://www.regular-expressions.info/brackets.html"" rel=""nofollow noreferrer"">capturing group</a>. There is no replacement text token for the overall match. Place the entire regex in a capturing group and then use <code>\1</code>.</p>
</blockquote>

<p>But with PCRE you should be able to use <a href=""http://www.regular-expressions.info/named.html"" rel=""nofollow noreferrer"">named groups</a>. So try <code>(?P&lt;</code><em><code>name</code></em><code>&gt;</code><em><code>regex</code></em><code>)</code> for groupd naming and <code>(?P=</code><em><code>name</code></em><code>)</code> as backreference.</p>
"
1401367,168168,2009-09-09T18:43:11Z,1395528,23,FALSE,"<p>Another option using Xpath.</p>

<pre><code>library(RCurl)
library(XML)

theurl &lt;- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""
webpage &lt;- getURL(theurl)
webpage &lt;- readLines(tc &lt;- textConnection(webpage)); close(tc)

pagetree &lt;- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)

# Extract table header and contents
tablehead &lt;- xpathSApply(pagetree, ""//*/table[@class='wikitable sortable']/tr/th"", xmlValue)
results &lt;- xpathSApply(pagetree, ""//*/table[@class='wikitable sortable']/tr/td"", xmlValue)

# Convert character vector to dataframe
content &lt;- as.data.frame(matrix(results, ncol = 8, byrow = TRUE))

# Clean up the results
content[,1] &lt;- gsub(""Â "", """", content[,1])
tablehead &lt;- gsub(""Â "", """", tablehead)
names(content) &lt;- tablehead
</code></pre>

<p>Produces this result</p>

<pre><code>&gt; head(content)
   Opponent Played Won Drawn Lost Goals for Goals against % Won
1 Argentina     94  36    24   34       148           150 38.3%
2  Paraguay     72  44    17   11       160            61 61.1%
3   Uruguay     72  33    19   20       127            93 45.8%
4     Chile     64  45    12    7       147            53 70.3%
5      Peru     39  27     9    3        83            27 69.2%
6    Mexico     36  21     6    9        69            34 58.3%
</code></pre>
"
1401598,16632,2009-09-09T19:30:46Z,1400937,4,FALSE,"<p>Use <code>strsplit</code> instead:</p>

<pre><code>test &lt;- ""abcdefghijklmnop""
strsplit(test, """")[[1]][c(5, 7, 9, 10, 15)]
</code></pre>
"
1401668,158065,2009-09-09T19:42:35Z,1397823,7,TRUE,"<p>I'm not 100% sure what you have in mind, but I think first you want to load and plot an image in R.  You can do that with the <code>ReadImages</code> package:</p>

<pre><code>picture &lt;- read.jpeg(""avatar.jpg"")
plot(picture)
</code></pre>

<p>Then you can do a scatter plot on top of it:</p>

<pre><code>points(runif(50,0, 128), runif(50,0,128))
</code></pre>

<p>This yields (when run on your avatar picture)<a href=""http://www.cs.princeton.edu/~jcone/picture_demo.png"">alt text http://www.cs.princeton.edu/~jcone/picture_demo.png</a>:</p>
"
1401956,NA,2009-09-09T20:40:00Z,1395622,21,TRUE,"<p>Use the standard R debugging techniques to stop exactly when the error occurs:</p>

<pre><code>options(error = browser) 
</code></pre>

<p>or</p>

<pre><code>options(error = recover)
</code></pre>

<p>When done, revert to standard behaviour:</p>

<pre><code>options(error = NULL)
</code></pre>
"
1401997,168168,2009-09-09T20:47:10Z,1395156,3,FALSE,"<p>Another alternative:</p>

<pre><code>&gt; library(plyr)
&gt; df &lt;- mdply(myDF, getNumberInfo)
&gt; names(df) &lt;- c('Value', 'Evenness', 'Positivity', 'Log')
&gt; df
  Value Evenness  Positivity       Log
1    -2     Even NonPositive        NA
2    -1      Odd NonPositive        NA
3     0     Even NonPositive        NA
4     1      Odd    Positive 0.0000000
5     2     Even    Positive 0.6931472
</code></pre>
"
1402080,161808,2009-09-09T21:02:10Z,1395622,0,FALSE,"<p>I've faced the same problem and have tended to make my calls with (l)(m)(s)(t)apply to be functions that I can debug().</p>

<p>So, instead of blah&lt;-sapply(x,function(x){ x+1 })</p>

<p>I'd say, </p>

<pre><code> myfn&lt;-function(x){x+1}
 blah&lt;-sapply(x,function(x){myfn(x)})
</code></pre>

<p>and use debug(myfn) with options(error=recover).</p>

<p>I also like the advice about sticking print() lines here and there to see what is happening.</p>

<p>Even better is to design a test of myfn(x) that it has to pass and to be sure it passes said test before subjecting it to sapply. I only have patience to to this about half the time.</p>
"
1402101,168542,2009-09-09T21:05:40Z,1395622,1,FALSE,"<p>Like geoffjentry said:</p>

<pre><code>&gt; sapply(x, function(x) {
  res &lt;- tryCatch(1 / x,
                  error=function(e) {
                          cat(""Failed on x = "", x, ""\n"", sep="""") ## browser()
                          stop(e)
                        })
})
</code></pre>

<p>Also, your for loop could be rewritten to be much cleaner (possibly a little slower):</p>

<pre><code>&gt; y &lt;- NULL
&gt; for (xi in x)
    y &lt;- c(y, 1 / xi)

Error in 1/xi : non-numeric argument to binary operator
</code></pre>

<p>For loops are slow in R, but unless you really need the speed I'd go with a simple iterative approach over a confusing list comprehension.</p>

<p>If I need to figure out some code on the fly, I'll always go:</p>

<pre><code>sapply(x, function(x) {
  browser()
  ...
})
</code></pre>

<p>And write the code from inside the function so I see what I'm getting.</p>

<p>-- Dan</p>
"
1402103,143305,2009-09-09T21:05:42Z,1401904,23,FALSE,"<p>Two quick suggestions:</p>

<ol>
<li><p>Use Gabor's <a href=""http://code.google.com/p/batchfiles/"" rel=""noreferrer"">batchfiles</a> which are said to comprise tools helping with e.g. this bulk library relocations.  Caveat: I have not used them.</p></li>
<li><p>Don't install libraries within the 'filetree' of the installed R version. On Windows, I may put R into C:/opt/R/R-$version but place all libraries into C:/opt/R/library/ using the following snippet as it alleviates the problem in the first place:</p></li>
</ol>

<p></p>

<pre><code>$ cat .Renviron         # this is using MSys/MinGW which looks like Cygwin  
## Example .Renviron on Windows    
R_LIBS=""C:/opt/R/library""
</code></pre>
"
1402104,161808,2009-09-09T21:06:07Z,1395191,10,FALSE,"<p>I seem to recall that plain old <code>split()</code> has a method for data.frames, so that <code>split(data,data$site)</code> would produce a list of blocks. You could then operate on this list using <code>sapply</code>/<code>lapply</code>/<code>for</code>.</p>

<p><code>split()</code> is also nice because of <code>unsplit()</code>, which will create a vector the same length as the original data and in the correct order.</p>
"
1402109,168168,2009-09-09T21:07:05Z,1329940,3,FALSE,"<pre><code>&gt; library(plyr)
&gt; as.matrix(ldply(a))
      V1 V2 V3 V4 V5 V6
 [1,]  1  1  2  3  4  5
 [2,]  2  1  2  3  4  5
 [3,]  3  1  2  3  4  5
 [4,]  4  1  2  3  4  5
 [5,]  5  1  2  3  4  5
 [6,]  6  1  2  3  4  5
 [7,]  7  1  2  3  4  5
 [8,]  8  1  2  3  4  5
 [9,]  9  1  2  3  4  5
[10,] 10  1  2  3  4  5
</code></pre>
"
1402184,168542,2009-09-09T21:23:58Z,1395309,2,FALSE,"<p>On Windows I believe the best way to do this would probably be with foreach and snow as David Smith said.</p>

<p>However, Unix/Linux based systems can compute using multiple processes with the 'multicore' package.  It provides a high-level function, 'mclapply', that performs a list comprehension across multiple cores.  An advantage of the 'multicore' package is that each processor gets a private copy of the Global Environment that it may modify.  Initially, this copy is just a pointer to the Global Environment, making the sharing of variable extremely quick if the Global Environment is treated as read-only.</p>

<p>Rmpi requires that the data be explicitly transferred between R processes instead of working with the 'multicore' closure approach.</p>

<p>-- Dan</p>
"
1402284,158065,2009-09-09T21:44:18Z,1401894,2,FALSE,"<p>If you want to apply a function (such as <code>var</code>) across a factor such as <code>Run</code> or <code>Rep</code>, you can use <code>tapply</code>:</p>

<pre><code>&gt; with(variance, tapply(Value, Run, var))
          1           2           3           4 
0.005833333 0.310000000 0.610000000 0.043333333 
&gt; with(variance, tapply(Value, Rep, var))
          1          2          3 
0.48562500 0.88729167 0.05583333 
</code></pre>
"
1402326,19410,2009-09-09T21:50:18Z,1401894,6,FALSE,"<p>You have four groups of three observations:</p>

<pre><code>&gt; run1 = c(9.85, 9.95, 10.00)
&gt; run2 = c(9.90, 8.80, 9.50)
&gt; run3 = c(11.20, 11.10, 9.80)
&gt; run4 = c(9.70, 10.10, 10.00)
&gt; runs = c(run1, run2, run3, run4)
&gt; runs
 [1]  9.85  9.95 10.00  9.90  8.80  9.50 11.20 11.10  9.80  9.70 10.10 10.00
</code></pre>

<p>Make some labels:</p>

<pre><code>&gt; n = rep(3, 4)
&gt; group = rep(1:4, n)
&gt; group
 [1] 1 1 1 2 2 2 3 3 3 4 4 4
</code></pre>

<p>Calculate within-run stats:</p>

<pre><code>&gt; withinRunStats = function(x) c(sum = sum(x), mean = mean(x), var = var(x), n = length(x))
&gt; tapply(runs, group, withinRunStats)
$`1`
         sum         mean          var            n 
29.800000000  9.933333333  0.005833333  3.000000000 

$`2`
  sum  mean   var     n 
28.20  9.40  0.31  3.00 

$`3`
  sum  mean   var     n 
32.10 10.70  0.61  3.00 

$`4`
        sum        mean         var           n 
29.80000000  9.93333333  0.04333333  3.00000000 
</code></pre>

<p>You can do some ANOVA here:</p>

<pre><code>&gt; data = data.frame(y = runs, group = factor(group))
&gt; data
       y group
1   9.85     1
2   9.95     1
3  10.00     1
4   9.90     2
5   8.80     2
6   9.50     2
7  11.20     3
8  11.10     3
9   9.80     3
10  9.70     4
11 10.10     4
12 10.00     4

&gt; fit = lm(runs ~ group, data)
&gt; fit

Call:
lm(formula = runs ~ group, data = data)

Coefficients:
(Intercept)       group2       group3       group4  
  9.933e+00   -5.333e-01    7.667e-01   -2.448e-15 

&gt; anova(fit)
Analysis of Variance Table

Response: runs
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
group      3 2.57583 0.85861  3.5437 0.06769 .
Residuals  8 1.93833 0.24229                  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

&gt; degreesOfFreedom = anova(fit)[, ""Df""]
&gt; names(degreesOfFreedom) = c(""treatment"", ""error"")
&gt; degreesOfFreedom
treatment     error 
        3         8
</code></pre>

<p>Error or within-group variance:</p>

<pre><code>&gt; anova(fit)[""Residuals"", ""Mean Sq""]
[1] 0.2422917
</code></pre>

<p>Treatment or between-group variance:</p>

<pre><code>&gt; anova(fit)[""group"", ""Mean Sq""]
[1] 0.8586111
</code></pre>

<p>This should give you enough to do confidence intervals.</p>
"
1402406,143319,2009-09-09T22:10:44Z,1401894,3,FALSE,"<p>I'm going to take a crack at this when I have more time, but meanwhile, here's the <code>dput()</code> for Kiar's data structure:</p>

<pre><code>structure(list(Run = c(1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4), Rep = c(1, 
2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3), Value = c(9.85, 9.95, 10, 9.9, 
8.8, 9.5, 11.2, 11.1, 9.8, 9.7, 10.1, 10)), .Names = c(""Run"", 
""Rep"", ""Value""), row.names = c(NA, -12L), class = ""data.frame"")
</code></pre>

<p>... in case you'd like to take a quick shot at it.</p>
"
1402655,163053,2009-09-09T23:24:30Z,1402634,15,TRUE,"<p>You can use <code>grep()</code> with <code>colnames()</code>:</p>

<pre><code>survey[,grep(""bern"", colnames(survey))]
</code></pre>
"
1402739,144157,2009-09-09T23:50:35Z,1401872,3,FALSE,"<p>There are two automatic methods in the <a href=""http://robjhyndman.com/software/forecast"" rel=""nofollow noreferrer"">forecast package</a>: <code>auto.arima()</code> which will handle automatic modelling using ARIMA models, and <code>ets()</code> which will automatically select the best model from the exponential smoothing family (including trend and seasonality where appropriate). The AIC is used in both cases for model selection. Neither handles ARCH/GARCH models though. The package is described in some detail in this JSS article: <a href=""http://www.jstatsoft.org/v27/i03"" rel=""nofollow noreferrer"">http://www.jstatsoft.org/v27/i03</a></p>

<p>Further to your question:</p>

<blockquote>
  <p><em>When will it be possible to use
  forecast package functions, especially
  ets function, with high dimensional
  data(weekly data, for example)?</em></p>
</blockquote>

<p>Probably early next year. The paper is written (see robjhyndman.com/working-papers/complex-seasonality) and we are working on the code now. </p>
"
1403567,171249,2009-09-10T05:49:11Z,1395622,1,FALSE,"<p>Using debug or browser isn't a good idea in this case, because it will stop your code so frequently.  Use Try or TryCatch instead, and deal with the situation when it arises.</p>
"
1404365,74658,2009-09-10T09:54:19Z,1401894,1,TRUE,"<p>I've been looking at a similar problem.  I've found reference to caluclating confidence intervals by Burdick and Graybill (Burdick, R. and Graybill, F. 1992, Confidence Intervals on variance components, CRC Press)</p>

<p>Using some code I've been trying I get these values</p>

<pre><code>

> kiaraov = aov(Value~Run+Error(Run),data=kiar)

> summary(kiaraov)

Error: Run
    Df  Sum Sq Mean Sq
Run  3 2.57583 0.85861

Error: Within
          Df  Sum Sq Mean Sq F value Pr(>F)
Residuals  8 1.93833 0.24229               
> confint = 95

> a = (1-(confint/100))/2

> grandmean = as.vector(kiaraov$""(Intercept)""[[1]][1]) # Grand Mean (I think)

> within = summary(kiaraov)$""Error: Within""[[1]]$""Mean Sq""  # S2^2Mean Square Value for Within Run

> dfRun = summary(kiaraov)$""Error: Run""[[1]]$""Df""

> dfWithin = summary(kiaraov)$""Error: Within""[[1]]$""Df""

> Run = summary(kiaraov)$""Error: Run""[[1]]$""Mean Sq"" # S1^2Mean Square for between Run

> between = (Run-within)/((dfWithin/(dfRun+1))+1) # (S1^2-S2^2)/J

> total = between+within

> between # Between Run Variance
[1] 0.2054398

> within # Within Run Variance
[1] 0.2422917

> total # Total Variance
[1] 0.4477315

> betweenCV = sqrt(between)/grandmean * 100 # Between Run CV%

> withinCV = sqrt(within)/grandmean * 100 # Within Run CV%

> totalCV = sqrt(total)/grandmean * 100 # Total CV%

> #within confidence intervals

> withinLCB = within/qf(1-a,8,Inf) # Within LCB

> withinUCB = within/qf(a,8,Inf) # Within UCB

> #Between Confidence Intervals

> n1 = dfRun

> n2 = dfWithin

> G1 = 1-(1/qf(1-a,n1,Inf)) # According to Burdick and Graybill this should be a

> G2 = 1-(1/qf(1-a,n2,Inf))

> H1 = (1/qf(a,n1,Inf))-1  # and this should be 1-a, but my results don't agree

> H2 = (1/qf(a,n2,Inf))-1

> G12 = ((qf(1-a,n1,n2)-1)^2-(G1^2*qf(1-a,n1,n2)^2)-(H2^2))/qf(1-a,n1,n2) # again, should be a, not 1-a

> H12 = ((1-qf(a,n1,n2))^2-H1^2*qf(a,n1,n2)^2-G2^2)/qf(a,n1,n2) # again, should be 1-a, not a

> Vu = H1^2*Run^2+G2^2*within^2+H12*Run*within

> Vl = G1^2*Run^2+H2^2*within^2+G12*within*Run

> betweenLCB = (Run-within-sqrt(Vl))/J # Betwen LCB

> betweenUCB = (Run-within+sqrt(Vu))/J # Between UCB

> #Total Confidence Intervals

> y = (Run+(J-1)*within)/J

> totalLCB = y-(sqrt(G1^2*Run^2+G2^2*(J-1)^2*within^2)/J) # Total LCB

> totalUCB = y+(sqrt(H1^2*Run^2+H2^2*(J-1)^2*within^2)/J) # Total UCB

> result = data.frame(Name=c(""within"", ""between"", ""total""),CV=c(withinCV,betweenCV,totalCV),LCB=c(sqrt(withinLCB)/grandmean*100,sqrt(betweenLCB)/grandmean*100,sqrt(totalLCB)/grandmean*100),UCB=c(sqrt(withinUCB)/grandmean*100,sqrt(betweenUCB)/grandmean*100,sqrt(totalUCB)/grandmean*100))

> result
     Name       CV      LCB      UCB
1  within 4.926418 3.327584  9.43789
2 between 4.536327      NaN 19.73568
3   total 6.696855 4.846030 20.42647
</code></pre>

<p>Here the lower confidence interval for between run CV is less than zero, so reported as NaN.</p>

<p>I'd love to have a better way to do this.  If I get time I might try to create a function to do this.</p>

<p>Paul.</p>

<p>--</p>

<p>Edit: I did eventually write a function, here it is (caveat emptor)</p>

<pre><code>#' avar Function
#' 
#' Calculate thewithin, between and total %CV of a dataset by ANOVA, and the
#' associated confidence intervals
#' 
#' @param dataf - The data frame to use, in long format 
#' @param afactor Character string representing the column in dataf that contains the factor
#' @param aresponse  Charactyer string representing the column in dataf that contains the response value
#' @param aconfidence What Confidence limits to use, default = 95%
#' @param digits  Significant Digits to report to, default = 3
#' @param debug Boolean, Should debug messages be displayed, default=FALSE
#' @returnType dataframe containing the Mean, Within, Between and Total %CV and LCB and UCB for each
#' @return 
#' @author Paul Hurley
#' @export
#' @examples 
#' #Using the BGBottles data from Burdick and Graybill Page 62
#' assayvar(dataf=BGBottles, afactor=""Machine"", aresponse=""weight"")
avar&lt;-function(dataf, afactor, aresponse, aconfidence=95, digits=3, debug=FALSE){
    dataf&lt;-subset(dataf,!is.na(with(dataf,get(aresponse))))
    nmissing&lt;-function(x) sum(!is.na(x))
    n&lt;-nrow(subset(dataf,is.numeric(with(dataf,get(aresponse)))))
    datadesc&lt;-ddply(dataf, afactor, colwise(nmissing,aresponse))
    I&lt;-nrow(datadesc)
    if(debug){print(datadesc)}
    if(min(datadesc[,2])==max(datadesc[,2])){
        balance&lt;-TRUE
        J&lt;-min(datadesc[,2])
        if(debug){message(paste(""Dataset is balanced, J="",J,""I is "",I,sep=""""))}
    } else {
        balance&lt;-FALSE
        Jh&lt;-I/(sum(1/datadesc[,2], na.rm = TRUE))
        J&lt;-Jh
        m&lt;-min(datadesc[,2])
        M&lt;-max(datadesc[,2])
        if(debug){message(paste(""Dataset is unbalanced, like me, I is "",I,sep=""""))}
        if(debug){message(paste(""Jh is "",Jh, "", m is "",m, "", M is "",M, sep=""""))}
    }
    if(debug){message(paste(""Call afactor="",afactor,"", aresponse="",aresponse,sep=""""))}
    formulatext&lt;-paste(as.character(aresponse),"" ~ 1 + Error("",as.character(afactor),"")"",sep="""")
    if(debug){message(paste(""formula text is "",formulatext,sep=""""))}
    aovformula&lt;-formula(formulatext)
    if(debug){message(paste(""Formula is "",as.character(aovformula),sep=""""))}
    assayaov&lt;-aov(formula=aovformula,data=dataf)
    if(debug){
        print(assayaov)
        print(summary(assayaov))
    }
    a&lt;-1-((1-(aconfidence/100))/2)
    if(debug){message(paste(""confidence is "",aconfidence,"", alpha is "",a,sep=""""))}
    grandmean&lt;-as.vector(assayaov$""(Intercept)""[[1]][1]) # Grand Mean (I think)
    if(debug){message(paste(""n is"",n,sep=""""))}

    #This line commented out, seems to choke with an aov object built from an external formula
    #grandmean&lt;-as.vector(model.tables(assayaov,type=""means"")[[1]]$`Grand mean`) # Grand Mean (I think)
    within&lt;-summary(assayaov)[[2]][[1]]$""Mean Sq""  # d2e, S2^2 Mean Square Value for Within Machine = 0.1819
    dfRun&lt;-summary(assayaov)[[1]][[1]]$""Df""  # DF for within = 3
    dfWithin&lt;-summary(assayaov)[[2]][[1]]$""Df""  # DF for within = 8
    Run&lt;-summary(assayaov)[[1]][[1]]$""Mean Sq"" # S1^2Mean Square for Machine
    if(debug){message(paste(""mean square for Run ?"",Run,sep=""""))}
    #Was between&lt;-(Run-within)/((dfWithin/(dfRun+1))+1) but my comment suggests this should be just J, so I'll use J !
    between&lt;-(Run-within)/J # d2a (S1^2-S2^2)/J
    if(debug){message(paste(""S1^2 mean square machine is "",Run,"", S2^2 mean square within is "",within))}
    total&lt;-between+within
    between # Between Run Variance
    within # Within Run Variance
    total # Total Variance
    if(debug){message(paste(""between is "",between,"", within is "",within,"", Total is "",total,sep=""""))}

    betweenCV&lt;-sqrt(between)/grandmean * 100 # Between Run CV%
    withinCV&lt;-sqrt(within)/grandmean * 100 # Within Run CV%
    totalCV&lt;-sqrt(total)/grandmean * 100 # Total CV%
    n1&lt;-dfRun
    n2&lt;-dfWithin
    if(debug){message(paste(""n1 is "",n1,"", n2 is "",n2,sep=""""))}
    #within confidence intervals
    if(balance){
        withinLCB&lt;-within/qf(a,n2,Inf) # Within LCB
        withinUCB&lt;-within/qf(1-a,n2,Inf) # Within UCB
    } else {
        withinLCB&lt;-within/qf(a,n2,Inf) # Within LCB
        withinUCB&lt;-within/qf(1-a,n2,Inf) # Within UCB
    }
#Mean Confidence Intervals
    if(debug){message(paste(grandmean,""+/-(sqrt("",Run,""/"",n,"")*qt("",a,"",df="",I-1,""))"",sep=""""))} 
    meanLCB&lt;-grandmean+(sqrt(Run/n)*qt(1-a,df=I-1)) # wrong
    meanUCB&lt;-grandmean-(sqrt(Run/n)*qt(1-a,df=I-1)) # wrong
    if(debug){message(paste(""Grandmean is "",grandmean,"", meanLCB = "",meanLCB,"", meanUCB = "",meanUCB,aresponse,sep=""""))}
    if(debug){print(summary(assayaov))}
#Between Confidence Intervals
    G1&lt;-1-(1/qf(a,n1,Inf)) 
    G2&lt;-1-(1/qf(a,n2,Inf))
    H1&lt;-(1/qf(1-a,n1,Inf))-1  
    H2&lt;-(1/qf(1-a,n2,Inf))-1
    G12&lt;-((qf(a,n1,n2)-1)^2-(G1^2*qf(a,n1,n2)^2)-(H2^2))/qf(a,n1,n2) 
    H12&lt;-((1-qf(1-a,n1,n2))^2-H1^2*qf(1-a,n1,n2)^2-G2^2)/qf(1-a,n1,n2) 
    if(debug){message(paste(""G1 is "",G1,"", G2 is "",G2,sep=""""))
        message(paste(""H1 is "",H1,"", H2 is "",H2,sep=""""))
        message(paste(""G12 is "",G12,"", H12 is "",H12,sep=""""))
    }
    if(balance){
        Vu&lt;-H1^2*Run^2+G2^2*within^2+H12*Run*within
        Vl&lt;-G1^2*Run^2+H2^2*within^2+G12*within*Run
        betweenLCB&lt;-(Run-within-sqrt(Vl))/J # Betwen LCB
        betweenUCB&lt;-(Run-within+sqrt(Vu))/J # Between UCB
    } else {
        #Burdick and Graybill seem to suggest calculating anova of mean values to find n1S12u/Jh
        meandataf&lt;-ddply(.data=dataf,.variable=afactor, .fun=function(df){mean(with(df, get(aresponse)), na.rm=TRUE)})
        meandataaov&lt;-aov(formula(paste(""V1~"",afactor,sep="""")), data=meandataf)
        sumsquare&lt;-summary(meandataaov)[[1]]$`Sum Sq`
        #so maybe S12u is just that bit ?
        Runu&lt;-(sumsquare*Jh)/n1
        if(debug){message(paste(""n1S12u/Jh is "",sumsquare,"", so S12u is "",Runu,sep=""""))}
        Vu&lt;-H1^2*Runu^2+G2^2*within^2+H12*Runu*within
        Vl&lt;-G1^2*Runu^2+H2^2*within^2+G12*within*Runu
        betweenLCB&lt;-(Runu-within-sqrt(Vl))/Jh # Betwen LCB
        betweenUCB&lt;-(Runu-within+sqrt(Vu))/Jh # Between UCB
        if(debug){message(paste(""betweenLCB is "",betweenLCB,"", between UCB is "",betweenUCB,sep=""""))}
    }
#Total Confidence Intervals
    if(balance){
        y&lt;-(Run+(J-1)*within)/J
        if(debug){message(paste(""y is "",y,sep=""""))}
        totalLCB&lt;-y-(sqrt(G1^2*Run^2+G2^2*(J-1)^2*within^2)/J) # Total LCB
        totalUCB&lt;-y+(sqrt(H1^2*Run^2+H2^2*(J-1)^2*within^2)/J) # Total UCB
    } else {
        y&lt;-(Runu+(Jh-1)*within)/Jh
        if(debug){message(paste(""y is "",y,sep=""""))}
        totalLCB&lt;-y-(sqrt(G1^2*Runu^2+G2^2*(Jh-1)^2*within^2)/Jh) # Total LCB
        totalUCB&lt;-y+(sqrt(H1^2*Runu^2+H2^2*(Jh-1)^2*within^2)/Jh) # Total UCB
    }
    if(debug){message(paste(""totalLCB is "",totalLCB,"", total UCB is "",totalUCB,sep=""""))}
#   result&lt;-data.frame(Name=c(""within"", ""between"", ""total""),CV=c(withinCV,betweenCV,totalCV),
#           LCB=c(sqrt(withinLCB)/grandmean*100,sqrt(betweenLCB)/grandmean*100,sqrt(totalLCB)/grandmean*100),
#           UCB=c(sqrt(withinUCB)/grandmean*100,sqrt(betweenUCB)/grandmean*100,sqrt(totalUCB)/grandmean*100))
    result&lt;-data.frame(Mean=grandmean,MeanLCB=meanLCB, MeanUCB=meanUCB, Within=withinCV,WithinLCB=sqrt(withinLCB)/grandmean*100, WithinUCB=sqrt(withinUCB)/grandmean*100,
            Between=betweenCV, BetweenLCB=sqrt(betweenLCB)/grandmean*100, BetweenUCB=sqrt(betweenUCB)/grandmean*100,
            Total=totalCV, TotalLCB=sqrt(totalLCB)/grandmean*100, TotalUCB=sqrt(totalUCB)/grandmean*100)
    if(!digits==""NA""){
        result$Mean&lt;-signif(result$Mean,digits=digits)
        result$MeanLCB&lt;-signif(result$MeanLCB,digits=digits)
        result$MeanUCB&lt;-signif(result$MeanUCB,digits=digits)
        result$Within&lt;-signif(result$Within,digits=digits)
        result$WithinLCB&lt;-signif(result$WithinLCB,digits=digits)
        result$WithinUCB&lt;-signif(result$WithinUCB,digits=digits)
        result$Between&lt;-signif(result$Between,digits=digits)
        result$BetweenLCB&lt;-signif(result$BetweenLCB,digits=digits)
        result$BetweenUCB&lt;-signif(result$BetweenUCB,digits=digits)
        result$Total&lt;-signif(result$Total,digits=digits)
        result$TotalLCB&lt;-signif(result$TotalLCB,digits=digits)
        result$TotalUCB&lt;-signif(result$TotalUCB,digits=digits)
    }
    return(result)
}

assayvar&lt;-function(adata, aresponse, afactor, anominal, aconfidence=95, digits=3, debug=FALSE){
    result&lt;-ddply(adata,anominal,function(df){
                resul&lt;-avar(dataf=df,afactor=afactor,aresponse=aresponse,aconfidence=aconfidence, digits=digits, debug=debug)
                resul$n&lt;-nrow(subset(df, !is.na(with(df, get(aresponse)))))
                return(resul)
            })
    return(result)
}
</code></pre>
"
1405647,168542,2009-09-10T14:33:06Z,1405571,2,FALSE,"<p>Try</p>

<pre><code>&gt; f &lt;- textConnection(""test3"", ""w"")
&gt; cat(test, ""\n"", file=f)
&gt; test3
[1] ""V 1 x 1 2 3 y 3 5 8 V 2 x y V 3 y 7 2 1 V 4 x 9 3 7 y ""
&gt; close(f)
</code></pre>
"
1405649,158065,2009-09-10T14:33:16Z,1405571,12,TRUE,"<p>Instead of <code>cat</code>ing to a file, why not use the <code>paste</code> command to generate a string instead?</p>

<pre><code>&gt; paste(test, collapse=""\n"")
[1] ""V 1\nx\n1 2 3\ny\n3 5 8\nV 2\nx\ny\nV 3\ny\n7 2 1\nV 4\nx\n9 3 7\ny""
</code></pre>

<p>Now instead of doing a <code>cat</code> then <code>readlines</code> you can just pass this string directly into <code>strsplit</code>.</p>
"
1406170,160314,2009-09-10T16:00:06Z,1405571,24,FALSE,"<p>As a more general solution, you can use the capture output function. It results in a character vector with elements corresponding to each line of the output.</p>

<p>your example:</p>

<pre><code>test2&lt;-capture.output(cat(test))
</code></pre>

<p>here is a multi-line example:</p>

<pre><code>&gt; out&lt;-capture.output(summary(lm(hwy~cyl*drv,data=mpg)))
&gt; out
 [1] """"                                                               
 [2] ""Call:""                                                          
 [3] ""lm(formula = hwy ~ cyl * drv, data = mpg)""                      
 [4] """"                                                               
 [5] ""Residuals:""                                                     
 [6] ""    Min      1Q  Median      3Q     Max ""                       
 [7] ""-8.3315 -1.4139 -0.1382  1.6479 13.5861 ""                       
 [8] """"                                                               
 [9] ""Coefficients:""                                                  
[10] ""            Estimate Std. Error t value Pr(&gt;|t|)    ""           
[11] ""(Intercept)  32.1776     1.2410  25.930  &lt; 2e-16 ***""           
[12] ""cyl          -2.0049     0.1859 -10.788  &lt; 2e-16 ***""           
[13] ""drvf          8.4009     1.8965   4.430 1.47e-05 ***""           
[14] ""drvr          8.2509     6.4243   1.284    0.200    ""           
[15] ""cyl:drvf     -0.5362     0.3422  -1.567    0.119    ""           
[16] ""cyl:drvr     -0.5248     0.8379  -0.626    0.532    ""           
[17] ""---""                                                            
[18] ""Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ""
[19] """"                                                               
[20] ""Residual standard error: 2.995 on 228 degrees of freedom""       
[21] ""Multiple R-squared: 0.7524,\tAdjusted R-squared: 0.747 ""         
[22] ""F-statistic: 138.6 on 5 and 228 DF,  p-value: &lt; 2.2e-16 ""       
[23] """"    
</code></pre>
"
1406242,143305,2009-09-10T16:12:49Z,1406202,0,FALSE,"<p>If memory serves, Rcmdr already does this for you using rgl. That may be limited to the models that Rcmdr fits, though. </p>

<p>On the other hand, it gives you (fast !) scrolling, zooming, ... which lattice cannot do.</p>
"
1406337,163053,2009-09-10T16:30:57Z,1403355,2,TRUE,"<p>There are many ways to interact with plots from external programs if you have control of the plot from something like rJava.  </p>

<p>You might have a look at <a href=""http://wiki.r-project.org/rwiki/doku.php?id=tips:data-io:ms_windows#exchanging_data_between_r_and_ms_windows_apps_excel_etc"" rel=""nofollow noreferrer"">the R wiki entry on the subject</a>.  </p>
"
1407080,76235,2009-09-10T18:59:19Z,1405571,1,FALSE,"<p>There's also the assign statement which allows you to build a name and set an object to it. Very useful if you want to iterate a bunch of tests and name them with dynamic values.</p>

<p>assign(""mary"", paste(test,sep= ""\n""))</p>

<p>will assign the paste statement to mary. However say you were running a bunch of regressions and wanted your regression objects named by predictor. You could do something like</p>

<pre><code>assign(paste(""myRegression"",names(dataframe)[2],sep=""""), lm(dataframe$response~dataframe[,2]))
</code></pre>

<p>which would give you the object</p>

<p>myRegressionPredictorName as you linear model.</p>
"
1407151,76235,2009-09-10T19:10:37Z,1402634,3,FALSE,"<p>If you have a series of names you like to grab you can also use match. perhaps you often need variables ""pulse"", ""exercise"", ""height"", ""weight"" and ""age"", but they sometimes show up in different places or with other added variables. You can save the vector of common names then match them against the dataframe and have a new df of just your standard columns in the order you want.</p>

<pre><code>basenames &lt;- c(""pulse"", ""exercise"", ""height"", ""weight"", ""age"")
get.columns &lt;- match(basenames, names(dataframe))
new.df &lt;- dataframe[,get.columns]
</code></pre>
"
1407296,171659,2009-09-10T19:41:02Z,1407238,1,FALSE,"<p>I use search &amp; replace, but of course, it's not completely automatic and you have to take care not to replace ""\t"" or ""\n"".</p>
"
1407339,143476,2009-09-10T19:47:40Z,1407238,1,FALSE,"<p>Not exactly the answer you're looking for but R has its own shell scripting functions which I often use:</p>

<p>list.files(,full=TRUE) [returns full path with appropriate separators]</p>

<p>file.path() [joins with OS-specific separator]</p>

<p>and so on...</p>
"
1407531,143305,2009-09-10T20:25:38Z,1407449,11,FALSE,"<p>You can do this with <code>by()</code>. First set up some data:</p>

<pre><code>R&gt; set.seed(42)
R&gt; testdf &lt;- data.frame(var1=rnorm(100), var2=rnorm(100,2), var3=rnorm(100,3),  
                        group=as.factor(sample(letters[1:10],100,replace=T)),  
                        year=as.factor(sample(c(2007,2009),100,replace=T)))
R&gt; summary(testdf)
      var1              var2              var3          group      year   
 Min.   :-2.9931   Min.   :-0.0247   Min.   :0.30   e      :15   2007:50  
 1st Qu.:-0.6167   1st Qu.: 1.4085   1st Qu.:2.29   c      :14   2009:50  
 Median : 0.0898   Median : 1.9307   Median :2.98   f      :12            
 Mean   : 0.0325   Mean   : 1.9125   Mean   :2.99   h      :12            
 3rd Qu.: 0.6616   3rd Qu.: 2.4618   3rd Qu.:3.65   d      :11            
 Max.   : 2.2866   Max.   : 4.7019   Max.   :5.46   b      :10            
                                                    (Other):26  
</code></pre>

<p>Use <code>by()</code>:</p>

<pre><code>R&gt; by(testdf[,1:3], testdf$year, mean)
testdf$year: 2007
   var1    var2    var3 
0.04681 1.77638 3.00122 
--------------------------------------------------------------------- 
testdf$year: 2009
   var1    var2    var3 
0.01822 2.04865 2.97805 
R&gt; by(testdf[,1:3], list(testdf$group, testdf$year), mean)  
## longer answer by group and year suppressed
</code></pre>

<p>You still need to reformat this for your table but it does give you the gist of your answer in one line.</p>

<p><em>Edit:</em> Further processing can be had via</p>

<pre><code>R&gt; foo &lt;- by(testdf[,1:3], list(testdf$group, testdf$year), mean)  
R&gt; do.call(rbind, foo)
          var1   var2  var3
 [1,]  0.62352 0.2549 3.157
 [2,]  0.08867 1.8313 3.607
 [3,] -0.69093 2.5431 3.094
 [4,]  0.02792 2.8068 3.181
 [5,] -0.26423 1.3269 2.781
 [6,]  0.07119 1.9453 3.284
 [7,] -0.10438 2.1181 3.783
 [8,]  0.21147 1.6345 2.470
 [9,]  1.17986 1.6518 2.362
[10,] -0.42708 1.5683 3.144
[11,] -0.82681 1.9528 2.740
[12,] -0.27191 1.8333 3.090
[13,]  0.15854 2.2830 2.949
[14,]  0.16438 2.2455 3.100
[15,]  0.07489 2.1798 2.451
[16,] -0.03479 1.6800 3.099
[17,]  0.48082 1.8883 2.569
[18,]  0.32381 2.4015 3.332
[19,] -0.47319 1.5016 2.903
[20,]  0.11743 2.2645 3.452
R&gt; do.call(rbind, dimnames(foo))
     [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]   [,10] 
[1,] ""a""    ""b""    ""c""    ""d""    ""e""    ""f""    ""g""    ""h""    ""i""    ""j""   
[2,] ""2007"" ""2009"" ""2007"" ""2009"" ""2007"" ""2009"" ""2007"" ""2009"" ""2007"" ""2009""
</code></pre>

<p>You can play with the <code>dimnames</code> some more:</p>

<pre><code>R&gt; expand.grid(dimnames(foo))
   Var1 Var2
1     a 2007
2     b 2007
3     c 2007
4     d 2007
5     e 2007
6     f 2007
7     g 2007
8     h 2007
9     i 2007
10    j 2007
11    a 2009
12    b 2009
13    c 2009
14    d 2009
15    e 2009
16    f 2009
17    g 2009
18    h 2009
19    i 2009
20    j 2009
R&gt; 
</code></pre>

<p><em>Edit:</em> And with that, we can create a <code>data.frame</code> for the result without resorting to external packages using only base R:</p>

<pre><code>R&gt; data.frame(cbind(expand.grid(dimnames(foo)), do.call(rbind, foo)))
   Var1 Var2     var1   var2  var3
1     a 2007  0.62352 0.2549 3.157
2     b 2007  0.08867 1.8313 3.607
3     c 2007 -0.69093 2.5431 3.094
4     d 2007  0.02792 2.8068 3.181
5     e 2007 -0.26423 1.3269 2.781
6     f 2007  0.07119 1.9453 3.284
7     g 2007 -0.10438 2.1181 3.783
8     h 2007  0.21147 1.6345 2.470
9     i 2007  1.17986 1.6518 2.362
10    j 2007 -0.42708 1.5683 3.144
11    a 2009 -0.82681 1.9528 2.740
12    b 2009 -0.27191 1.8333 3.090
13    c 2009  0.15854 2.2830 2.949
14    d 2009  0.16438 2.2455 3.100
15    e 2009  0.07489 2.1798 2.451
16    f 2009 -0.03479 1.6800 3.099
17    g 2009  0.48082 1.8883 2.569
18    h 2009  0.32381 2.4015 3.332
19    i 2009 -0.47319 1.5016 2.903
20    j 2009  0.11743 2.2645 3.452
R&gt; 
</code></pre>
"
1407538,163053,2009-09-10T20:28:14Z,1407449,5,FALSE,"<p>First of all, you don't need to use cbind, and that's why everything is a factor.  This works:</p>

<pre><code>test_data &lt;- data.frame(
var0 = rnorm(100),
var1 = rnorm(100,1),
var2 = rnorm(100,2),
var3 = rnorm(100,3),
var4 = rnorm(100,4),
group = sample(letters[1:10],100,replace=T),
year = sample(c(2007,2009),100, replace=T))
</code></pre>

<p>Secondly, the best practice is to use ""."" instead of ""_"" in variable names. <a href=""https://google.github.io/styleguide/Rguide.xml"" rel=""nofollow noreferrer"">See the google style guide</a> (for instance). </p>

<p>Finally, you can use the Rigroup package; it's very fast.  Combine the igroupMeans() function with apply, and set the index <code>i=as.factor(paste(test_data$group,test_data$year,sep=""""))</code>.  I'll try to include an example of this later.</p>

<p><strong>EDIT 6/9/2017</strong></p>

<p>Rigroup package was removed from CRAN. See <a href=""http://r.789695.n4.nabble.com/Withdrawal-of-package-Rigroup-0-84-0-from-CRAN-td4664893.html"" rel=""nofollow noreferrer"">this</a></p>
"
1407623,163053,2009-09-10T20:45:11Z,1407238,1,FALSE,"<p>You could create a wrapper function around all path names:</p>

<pre><code>&gt; replace.slash &lt;- function(path.name) gsub(""\\\\"",""/"",path.name)
&gt; path.name &lt;- ""c:\\tmp\\""
&gt; replace.slash(path.name)
[1] ""c:/tmp/""
</code></pre>

<p><em>[Edit]:</em> Thanks Hadley.  I corrected the error there.  </p>

<p>Incidentally, I found this very <a href=""https://stat.ethz.ch/pipermail/r-help/2008-September/thread.html#174702"" rel=""nofollow noreferrer"">useful discussion on this subject</a>.</p>
"
1407727,142651,2009-09-10T21:07:29Z,1407680,2,FALSE,"<pre><code>library(ggplot2)
ggplot(result, aes(x = Name, y = CV, ymin = LCB, ymax = UCB)) + geom_errorbar() + geom_point()
ggplot(result, aes(x = Name, y = CV, ymin = LCB, ymax = UCB)) + geom_pointrange()
</code></pre>
"
1407732,143305,2009-09-10T21:07:52Z,1407680,7,FALSE,"<p>For the table aspect, the <a href=""http://cran.r-project.org/package=xtable"" rel=""noreferrer""><strong>xtable</strong></a> package comes to mind as it can produce LaTeX output (which you can use via Sweave for professional reports) as well as html.</p>

<p>If you combine that in Sweave with fancy graphs (see other questions for ggplot examples) you are almost there.</p>
"
1407770,163053,2009-09-10T21:15:53Z,1407680,8,TRUE,"<p>I would use <strong>xtable</strong>.  I usually use it with Sweave.</p>

<pre><code>library(xtable)
d &lt;- data.frame(letter=LETTERS, index=rnorm(52)) 
d.table &lt;- xtable(d[1:5,])
print(d.table,type=""html"")
</code></pre>

<p>If you want to use it in a Sweave document, you would use it like so:</p>

<pre><code>&lt;&lt;label=tab1,echo=FALSE,results=tex&gt;&gt;=
xtable(d, caption = ""Here is my caption"", label = ""tab:one"",caption.placement = ""top"")
@
</code></pre>
"
1407792,142651,2009-09-10T21:20:28Z,1407449,11,TRUE,"<p>Given the format you want for the result, the reshape package will be more efficient than plyr.</p>

<pre><code>test_data &lt;- data.frame(
var0 = rnorm(100),
var1 = rnorm(100,1),
var2 = rnorm(100,2),
var3 = rnorm(100,3),
var4 = rnorm(100,4),
group = sample(letters[1:10],100,replace=T),
year = sample(c(2007,2009),100, replace=T))

library(reshape)
Molten &lt;- melt(test_data, id.vars = c(""group"", ""year""))
cast(group + variable ~ year, data = Molten, fun = mean)
</code></pre>

<p>The result looks like this</p>

<pre><code>   group variable         2007         2009
1      a     var0  0.003767891  0.340989068
2      a     var1  2.009026385  1.162786943
3      a     var2  1.861061882  2.676524736
4      a     var3  2.998011426  3.311250399
5      a     var4  3.979255971  4.165715967
6      b     var0 -0.112883844 -0.179762343
7      b     var1  1.342447279  1.199554144
8      b     var2  2.486088196  1.767431740
9      b     var3  3.261451449  2.934903824
10     b     var4  3.489147597  3.076779626
11     c     var0  0.493591055 -0.113469315
12     c     var1  0.157424796 -0.186590644
13     c     var2  2.366594176  2.458204041
14     c     var3  3.485808031  2.817153628
15     c     var4  3.681576886  3.057915666
16     d     var0  0.360188789  1.205875725
17     d     var1  1.271541181  0.898973536
18     d     var2  1.824468264  1.944708165
19     d     var3  2.323315162  3.550719308
20     d     var4  3.852223640  4.647498956
21     e     var0 -0.556751465  0.273865769
22     e     var1  1.173899189  0.719520372
23     e     var2  1.935402724  2.046313047
24     e     var3  3.318669590  2.871462470
25     e     var4  4.374478734  4.522511874
26     f     var0 -0.258956555 -0.007729091
27     f     var1  1.424479454  1.175242755
28     f     var2  1.797948551  2.411030282
29     f     var3  3.083169793  3.324584667
30     f     var4  4.160641429  3.546527820
31     g     var0  0.189038036 -0.683028110
32     g     var1  0.429915866  0.827761101
33     g     var2  1.839982321  1.513104866
34     g     var3  3.106414330  2.755975622
35     g     var4  4.599340239  3.691478466
36     h     var0  0.015557352 -0.707257185
37     h     var1  0.933199148  1.037655156
38     h     var2  1.927442457  2.521369108
39     h     var3  3.246734239  3.703213646
40     h     var4  4.242387776  4.407960355
41     i     var0  0.885226638 -0.288221276
42     i     var1  1.216012653  1.502514588
43     i     var2  2.302815441  1.905731471
44     i     var3  2.026631277  2.836508446
45     i     var4  4.800676814  4.772964668
46     j     var0 -0.435661855  0.192703997
47     j     var1  0.836814185  0.394505861
48     j     var2  1.663523873  2.377640369
49     j     var3  3.489536343  3.457597835
50     j     var4  4.146020948  4.281599816
</code></pre>
"
1408049,143319,2009-09-10T22:24:05Z,1407680,0,FALSE,"<p>Can't comment on making a pretty table, but to set the significant figures, the easiest thing to do (for this sample data, mind you) would be to move Name to rownames and round the whole thing.</p>

<pre><code>#Set the rownames equal to Name - assuming all unique
rownames(result) &lt;- result$Name  
#Drop the Name column so that round() can coerce
#result.mat to a matrix
result.mat &lt;- result[ , -1]       
round(result.mat, 2) #Where 2 = however many sig digits you want.            
</code></pre>

<p>This is not a terribly robust solution - non-unique Name values would break it, I think, as would other non-numeric columns.  But for producing a table like your example, it does the trick.</p>
"
1408166,168747,2009-09-10T23:02:21Z,1407449,5,FALSE,"<p>It could be done with basic R function:</p>

<pre><code>n &lt;- 100
test_data &lt;- data.frame(
    var0 = rnorm(n),
    var1 = rnorm(n,1),
    var2 = rnorm(n,2),
    var3 = rnorm(n,3),
    var4 = rnorm(n,4),
    group = sample(letters[1:10],n,replace=TRUE),
    year = sample(c(2007,2009),n, replace=TRUE)
)

tapply(
    seq_len(nrow(test_data)),
    test_data$group,
    function(ind) sapply(
        c(""var0"",""var1"",""var2"",""var3"",""var4""),
        function(x_name) tapply(
            test_data[[x_name]][ind],
            test_data$year[ind],
            mean
        )
    )
)
</code></pre>

<p>Explanations:</p>

<ul>
<li>tip: when generating random data is usefull to define number of observations. Changing sample size is easier that way,</li>
<li>first tapply split row index 1:nrow(test_data) by groups,</li>
<li>then for each group sapply over variables</li>
<li>for fixed group and variable do simple tapply returnig mean of variable per year.</li>
</ul>

<p>In R 2.9.2 result is:</p>

<pre><code>$a
 var0.2007  var1.2007  var2.2007  var3.2007  var4.2007 
-0.3123034  0.8759787  1.9832617  2.7063034  4.1322758 

$b
            var0      var1     var2     var3     var4
2007  0.81366885 0.4189896 2.331256 3.073276 4.164639
2009 -0.08916257 1.5442126 3.008014 3.215019 4.398279

$c
          var0      var1     var2     var3     var4
2007 0.4232098 1.3657369 1.386627 2.808511 3.878809
2009 0.3245751 0.6672073 1.797886 1.752568 3.632318

$d
           var0      var1     var2     var3     var4
2007 -0.1335138 0.5925237 2.303543 3.293281 3.234386
2009  0.9547751 2.2111581 2.678878 2.845234 3.300512

$e
           var0      var1     var2     var3     var4
2007 -0.5958653 1.3535658 1.886918 3.036121 4.120889
2009  0.1372080 0.7215648 2.298064 3.186617 3.551147

$f
           var0      var1     var2     var3     var4
2007 -0.3401813 0.7883120 1.949329 2.811438 4.194481
2009  0.3012627 0.2702647 3.332480 3.480494 2.963951

$g
         var0       var1      var2     var3     var4
2007 1.225245 -0.3289711 0.7599302 2.903581 4.200023
2009 0.273858  0.2445733 1.7690299 2.620026 4.182050

$h
           var0     var1     var2     var3     var4
2007 -1.0126650 1.554403 2.220979 3.713874 3.924151
2009 -0.6187407 1.504297 1.321930 2.796882 4.179695

$i
            var0     var1     var2     var3     var4
2007  0.01697314 1.318965 1.794635 2.709925 2.899440
2009 -0.75790995 1.033483 2.363052 2.422679 3.863526

$j
           var0      var1     var2     var3     var4
2007 -0.7440600 1.6466291 2.020379 3.242770 3.727347
2009 -0.2842126 0.5450029 1.669964 2.747455 4.179531
</code></pre>

<p>With my random data there is problem with ""a"" group - only 2007 cases were present. If year will be factor (with levels 2007 and 2009) then results may look better (you will have two rows for each year, but there probably be NA).</p>

<p>Result is list, so you can use lapply to eg. convert to latex table, html table, print on screen transpose, etc.</p>
"
1408623,NA,2009-09-11T01:45:37Z,1402634,2,FALSE,"<p>The ""operators"" package allows some Perl-like syntax:</p>

<pre><code>library(operators)

survey[, colnames(survey) %~% ""bern""]
</code></pre>

<p>or</p>

<pre><code>subset(survey, select = colnames(survey) %~% ""bern"")
</code></pre>
"
1410038,26575,2009-09-11T09:51:38Z,1408897,3,FALSE,"<p>Maybe you need to run <code>require(RPostgreSQL)</code> before you can use <code>dbConnect</code>?</p>
"
1410737,143305,2009-09-11T13:04:26Z,1408897,9,FALSE,"<p>Just for completeness, you have two more options</p>

<ul>
<li><a href=""http://c"" rel=""noreferrer""><strong>RODBC</strong></a> which is very mature and feature-complete but doesn't correspond to the DBI framework as the PostgreSQL, MySQL, SQLite, Oracle, ... interfaces do. You also need to fiddle with ODBC setup files which can be tricky. But ODBC may be useful for other data access uses too.</li>
<li><a href=""http://www.bioconductor.org/packages/bioc/html/RdbiPgSQL.html"" rel=""noreferrer""><strong>RdbiPgSQL</strong></a> from the BioConductor project which is also mature but uses yet another protocol that was to compete with DBI and never took of. This PostgreSQL package is featureful though.</li>
</ul>

<p>But as a <a href=""http://cran.r-project.org/package=RPostgreSQL"" rel=""noreferrer""><strong>RPostgreSQL</strong></a> maintainer/co-author I am glad you found this one.  As the other poster suggested, try <code>library(RPostgreSQL)</code> before issueing commands.  If you encounter other problems, feel free to email me off-SO with a bug report.</p>

<p><em>Edit</em>: There is another option of embedding R inside PostgreSQL using Joe Conway's <a href=""http://www.joeconway.com/plr/"" rel=""noreferrer""><strong>PL/R</strong></a>.</p>
"
1411839,84458,2009-09-11T16:05:19Z,1411599,3,FALSE,"<p>While we're waiting for a better answer, I figured I should post the (suboptimal) solution you mentioned.  <code>dat</code> is the structure included in your question.</p>

<pre><code>d &lt;- data.frame(animal=factor(sapply(list(dat[2:length(dat)]),
                function(x) rep(names(x),x))))
cxc &lt;- ggplot(d, aes(x = animal)) +  geom_bar(width = 1, colour = ""black"") 
cxc + coord_polar() 
</code></pre>
"
1412327,142879,2009-09-11T17:41:46Z,1407238,0,FALSE,"<p>why not create a function that checks the OS and returns the proper file separator (the java solution i believe)?</p>

<pre><code>file_sep &lt;- function(){
ifelse(.Platform$OS.type == ""unix"", ""/"", ""//"")
}
file_sep()
</code></pre>

<p>you can pick a shorter name if you like. The big flaw here is that you have to paste together file paths, but it's still worth it long term if you're working on big projects.</p>
"
1412531,2192597,2009-09-11T18:21:37Z,1412473,1,FALSE,"<p>I think what you're asking is whether it is possible to count the number of unique types of group to which an individual belongs.</p>

<p>If so, then that is certainly possible.</p>

<p>I wouldn't be able to tell you how to do it in R since I don't know a lot of R, and I don't know what your data looks like.  But there's no reason why it wouldn't be possible.</p>

<p>Is this data coming from a database?  If so, then it might be easier to write a SQL query to compute the value you want, rather than to do it in R.  If you describe your schema, there should be lots of people here who could give you the query you need.</p>
"
1412607,84458,2009-09-11T18:36:52Z,1412582,26,TRUE,"<p>As pointed out <a href=""http://www.stats.uwo.ca/faculty/murdoch/software/debuggingR/pmd.shtml"" rel=""noreferrer"">here</a>, there's an easy way to do this, and I think this trick has the potential to change lives for the better.</p>

<p>First, call this: </p>

<pre><code>options(error=recover)
</code></pre>

<p>Now when we call <code>f(x,y)</code> we will have an option to choose an environment to recover.  Here I select option 1, which opens up a debugger and lets me play around with variables just before <code>lm()</code> is called.</p>

<pre><code>&gt; f(x,y)
Error in model.frame.default(formula = y ~ x, drop.unused.levels = TRUE) : 
  variable lengths differ (found for 'x')

Enter a frame number, or 0 to exit   

1: f(x, y)
2: lm(y ~ x)
3: eval(mf, parent.frame())
4: eval(expr, envir, enclos)
5: model.frame(formula = y ~ x, drop.unused.levels = TRUE)
6: model.frame.default(formula = y ~ x, drop.unused.levels = TRUE)

Selection: 1
Called from: eval(expr, envir, enclos)
Browse[1]&gt; x
[1] 1 2 3 4 5
Browse[1]&gt; y
[1] 1.6591197 0.5939368 4.3371049 4.4754027 5.9862130 1.0000000
</code></pre>
"
1412798,143377,2009-09-11T19:14:42Z,1412473,3,TRUE,"<pre><code>##simulate data
##individuals
n &lt;- 10
## groups
g &lt;- 5
## group types
gt &lt;- 3
## individuals*group membership
N &lt;- 20
## inidividuals data frame
di &lt;- data.frame(individual=sample(1:n,N,replace=TRUE),
                 group=sample(1:g,N, replace=TRUE))
## groups data frame
dg &lt;- data.frame(group=1:g, type=sample(1:gt,g,replace=TRUE))
## merge
dm &lt;- merge(di,dg)
## order - not necessary, but nice
dm &lt;- dm[order(dm$individual),]
## group type per individual
library(plyr)
dr &lt;- ddply(dm, ""individual"", function(x) length(unique(x$type)))

&gt; head(dm)
   group individual type
2      2          1    2
8      2          1    2
20     5          1    1
9      3          3    2
12     3          3    2
17     4          3    2

&gt; head(dr)
  individual V1
1          1  2
2          3  1
3          4  2
4          5  1
5          6  1
6          7  1
</code></pre>
"
1413274,37751,2009-09-11T21:06:03Z,1407449,8,FALSE,"<p><strong>EDIT:</strong> I wrote the following and then realized that Thierry had already written up almost EXACTLY the same answer. I somehow overlooked his answer. So if you like this answer, vote his up instead. I'm going ahead and posting since I spent the time typing it up. </p>

<hr>

<p>This sort of stuff consumes way more of my time than I wish it did! Here's a solution using the <a href=""http://had.co.nz/reshape/"" rel=""noreferrer"">reshape package</a> by Hadley Wickham. This example does not do <strong>exactly</strong> what you asked because the results are all in one big table, not a table for each group. </p>

<p>The trouble you were having with the numeric values showing up as factors was because you were using cbind and everything was getting slammed into a matrix of type character. The cool thing is you don't need cbind with data.frame. </p>

<pre><code>test_data &lt;- data.frame(
var0 = rnorm(100),
var1 = rnorm(100,1),
var2 = rnorm(100,2),
var3 = rnorm(100,3),
var4 = rnorm(100,4),
group = sample(letters[1:10],100,replace=T),
year = sample(c(2007,2009),100, replace=T))

library(reshape)
molten_data &lt;- melt(test_data, id=c(""group"", ""year"")))
cast(molten_data, group + variable ~ year, mean)
</code></pre>

<p>and this results in the following:</p>

<pre><code>    group variable        2007         2009
1      a     var0 -0.92040686 -0.154746420
2      a     var1  1.06603832  0.559765035
3      a     var2  2.34476321  2.206521587
4      a     var3  3.01652065  3.256580166
5      a     var4  3.75256699  3.907777127
6      b     var0 -0.53207427 -0.149144766
7      b     var1  0.75677714  0.879387608
8      b     var2  2.41739521  1.224854891
9      b     var3  2.63877431  2.436837719
10     b     var4  3.69640598  4.439047363
...
</code></pre>

<p>I wrote a <a href=""http://www.cerebralmastication.com/?p=339"" rel=""noreferrer"">blog post recently</a> about doing something similar with <a href=""http://had.co.nz/plyr/"" rel=""noreferrer"">plyr</a>. I should do a part 2 about how to do the same thing using the reshape package. Both plyr and reshape were written by Hadley Wickham and are crazy useful tools. </p>
"
1413705,144157,2009-09-11T23:25:48Z,1413697,1,FALSE,"<p>Use write.table(...,row.names=TRUE,col.names=TRUE,sep="","")</p>
"
1413776,143305,2009-09-11T23:54:32Z,1413697,1,FALSE,"<p>Note that there is an <a href=""http://cran.r-project.org/doc/manuals/R-data.pdf"" rel=""nofollow noreferrer""><strong>entire manual</strong></a> dedicated to questions like this, it really does not hurt to look at those once in a while.</p>
"
1413815,16632,2009-09-12T00:09:08Z,1413697,7,TRUE,"<p>Use <code>write.csv</code></p>
"
1413935,84458,2009-09-12T01:11:16Z,1395105,40,FALSE,"<p>Here's an example using <code>ggplot2</code>:</p>

<pre><code>q &lt;- qplot(cty, hwy, data = mpg, colour = displ)
q + xlab(expression(beta +frac(miles, gallon)))
</code></pre>

<p><a href=""http://i31.tinypic.com/10z7n7d.png"">alt text http://i31.tinypic.com/10z7n7d.png</a></p>
"
1413940,143305,2009-09-12T01:13:42Z,1413853,4,FALSE,"<p>I am just guessing but when you say</p>

<ul>
<li><p><code>source(""fns.R"")</code> you are not involving Emacs/ESS at all and the computing time is just the time R takes to slurp in the file and to digest it -- probably very little, whereas</p></li>
<li><p><code>Eval Function</code> passes a region to the Emacs interpreter which has to send it (presumably line-by-line) to the R engine which then digests it in piecemeal fashion.</p></li>
</ul>

<p>and that would make the second approach slower.</p>

<p>Yet, in the grand scheme of things, who cares?  I often send entire buffers or large regions, and that takes maybe a large part of a second?   I still think---just as you say---that the ability for the (rich) editor and the underlying language to interact that way is something extremely powerful.  </p>

<p>Kudos to the Emacs hackers and the ESS team.</p>
"
1413999,165787,2009-09-12T01:47:56Z,1395206,1,FALSE,"<p>Just for clarification, if I understand your code, this is the kind of result you're looking for...</p>

<pre><code>&gt; (goal &lt;- cbind(subject,target,adjtarget))

      subject target adjtarget
 [1,]     200    200       200
 [2,]     195    198       198
 [3,]     190    196       196
 [4,]     185    194       194
 [5,]     185    192       192
 [6,]     185    190       190
 [7,]     188    188       188
 [8,]     189    186       186
 [9,]     195    184       186
[10,]     200    182       186
[11,]     210    180       186
[12,]     210    178       186
</code></pre>

<p>If I'm right, then the challenge to vectorizing this is the repeated assignment of 186 in adjtarget. Vectorized code will evaluate the right hand side (RHS) before assigning it to the left hand side (LHS). So, the vectorized code won't see the updated value in adjtarget at row 9 until after the assignment is finished.</p>

<pre><code>&gt; y &lt;- ifelse(subject &gt; target, 1, 0) # matches TRUE case
&gt; x &lt;- target
&gt; x[ind+1] &lt;- target[ind]
&gt; cbind(goal, x, y)
      subject target adjtarget   x y
 [1,]     200    200       200 200 0
 [2,]     195    198       198 198 0
 [3,]     190    196       196 196 0
 [4,]     185    194       194 194 0
 [5,]     185    192       192 192 0
 [6,]     185    190       190 190 0
 [7,]     188    188       188 188 0
 [8,]     189    186       186 186 1
 [9,]     195    184       186 186 1 # assigned correctly (?)
[10,]     200    182       186 184 1 # incorrect x; should be 186
[11,]     210    180       186 182 1 # incorrect x; should be 186
[12,]     210    178       186 180 1 # incorrect x; should be 186
</code></pre>
"
1414844,143476,2009-09-12T10:43:37Z,1413853,1,FALSE,"<p>If you wanted to execute your whole buffer - if you are in Unix/Linux, you can also start off your script with a shebang:</p>

<pre><code>#!/usr/bin/Rscript
</code></pre>

<p>And make your file executable</p>

<pre><code>chmod 744 myscript.r
</code></pre>

<p>(I recall reading Google likes their r scripts to end in .R but oh well...) and you can execute it this way:</p>

<pre><code>./myscript.r
</code></pre>

<p>And, with arguments,</p>

<pre><code>./myscript.r arg1 arg2
</code></pre>

<p>(which I have actually used to invoke an R function from a Matlab system call) and in your R file you might use</p>

<pre><code>userargs = tail(commandArgs(),2) 
</code></pre>

<p>to get arg1 and arg2. You can also do without the shebang:</p>

<pre><code>R --no-save &lt; myscript.r arg1 arg2
</code></pre>

<p>and so on. With Windows I recall it was </p>

<pre><code>R CMD BATCH myscript.r
</code></pre>

<p>or something to that effect... I did notice a little delay when running commands through ESS (though I do love ESS <em>dearly</em>) so when I know I want to run the the whole buffer I sometimes start a shell in a window below the R script (where the R buffer would normally reside) and use the tricks above.</p>

<p>You can also use</p>

<pre><code>echo 'source(""myscript.r"")' | R --no-save
</code></pre>

<p>as well - the benefit of using these methods over running 'source(""myscript.r"")' directly in R or an R buffer is that you are starting out with a clear workspace (though you should be careful that your .Rprofile will not be loaded unless you call 'source(""~/.Rscript"")' explicitly in 'myscript.r') so you can be sure that your script is self-contained (it calls the proper libraries, your lexically-scoped functions aren't referencing unintended variables in the global space that you forgot to remove, and so on).</p>
"
1415197,163053,2009-09-12T14:05:05Z,1412582,3,FALSE,"<p>You could also just use the debug() function:</p>

<pre><code>&gt; debug(f)
&gt; f(x,y)
debugging in: f(x, y)
debug: {
    y &lt;- c(y, 1)
    lm(y ~ x)
}
Browse[1]&gt; 
debug: y &lt;- c(y, 1)
Browse[1]&gt; x
[1] 1 2 3 4 5
Browse[1]&gt; y
[1] 2.146553 2.610003 2.869081 2.758753 4.433881
</code></pre>
"
1415983,153440,2009-09-12T20:01:50Z,1401872,0,FALSE,"<p>Thanks useRs, I have tried the forecast package, that too as a composite of arima and ets, but not to much acclaim from aic or bic(sbc), so i am now tempted to treat each of the time series to its own svm(support vector machine) because of its better genralization adaptability and also being able to add other variables apart from lags and non linear kernel functions</p>

<p>Any premonitions?</p>
"
1416260,138470,2009-09-12T21:56:45Z,1407647,25,FALSE,"<p>From <code>?read.table</code>: The number of data columns is determined by looking at the first five lines of input (or the whole file if it has less than five lines), or from the length of col.names if it is specified and is longer. This could conceivably be wrong if fill or blank.lines.skip are true, so specify col.names if necessary.</p>

<p>So, perhaps your data file isn't clean.  Being more specific will help the data import:</p>

<pre><code>d = read.table(""foobar.txt"", 
               sep=""\t"", 
               col.names=c(""id"", ""name""), 
               fill=FALSE, 
               strip.white=TRUE)
</code></pre>

<p>will specify exact columns and <code>fill=FALSE</code> will force a two column data frame.</p>
"
1416265,NA,2009-09-12T22:00:17Z,1408897,0,FALSE,"<p>RODBC works great for me. You just have to set up a data source name (DSN) for the database you want to connect to. I find this nice because then the specific connection info does not have to be stored in R, and it may vary for your collaborators.</p>

<p>Also, yes, it sounds like you haven't loaded the RPostgresSQL package.</p>
"
1416630,138470,2009-09-13T01:44:56Z,1395499,0,FALSE,"<p>If you just want to count the days, you can use <code>trunc</code> to just move the day:</p>

<pre><code>&gt; trunc(Sys.time(), ""day"") + 86400
[1] ""2009-09-13 PDT""
&gt; trunc(Sys.time(), ""day"") - 86400
[1] ""2009-09-11 PDT""
</code></pre>
"
1416643,152948,2009-09-13T01:57:31Z,1395499,2,TRUE,"<p>Your code is doing exactly what you asked it to do, because you didn't ask it to add or subtract one day, you asked it to add or subtract <em>24 hours</em>. 24 hours (86400 seconds) before <code>2009-03-08 23:00:00 EDT</code> <em>is</em> <code>2009-03-07 22:00:00 EST</code>. I'm not familiar with the R libraries, but I am familiar with the POSIX functions that they wrap. If you take a POSIXct, decrease its <code>day</code> property by 1, and then ""re-cast"" it to <code>POSIXct</code> via <code>POSIXlt</code> ( to ensure that e.g. February -1st becomes January 31st) then you should be able to subtract one day reliably.</p>
"
1416690,16363,2009-09-13T02:34:11Z,1403355,2,FALSE,"<p>With Windows, you can also use <a href=""http://www.sciviews.org/_rgui/projects/RDcom.html"" rel=""nofollow noreferrer"">R-(D)COM</a>.</p>
"
1416912,143377,2009-09-13T05:14:35Z,1413853,8,TRUE,"<p>I think the folks at <a href=""https://stat.ethz.ch/mailman/listinfo/ess-help"" rel=""noreferrer"">ess list</a> have better answers for you. But if you evaluate invisibly, the processing is much much faster. Try putting this in your .emacs file:</p>

<pre><code>(setq ess-eval-visibly-p nil)
</code></pre>
"
1416936,170364,2009-09-13T05:35:38Z,1412582,3,FALSE,"<p><code>options(error=recover)</code></p>

<p>Probably answers the question best.  However, I wanted to mention another handy debugging tool, <code>traceback()</code>.  Calling this right after an error has occurred is often enough to pinpoint the bug.</p>
"
1417303,74658,2009-09-13T09:35:30Z,1417269,0,TRUE,"<p>It's not the best sounding solution, but If you might be running this script often like this, you could declare a boolean whether graphics are required (graphics_required=TRUE) and then enclose all your plot commands in if/then clauses based on your boolean, then just change the boolean to change the behaviour of the script</p>
"
1417800,16632,2009-09-13T13:53:37Z,1417269,3,FALSE,"<p>It's not a problem for ggplot2 or lattice graphics - you always have to explicitly <code>print</code> them when they are called in non-interactive settings (like from within a script).</p>
"
1418276,165787,2009-09-13T17:19:16Z,1417269,1,FALSE,"<p>Perhaps this might be of some help...</p>

<p>""A package that provides a null graphics device; includes a vignette, ""devNull"", that documents how to create a new graphics device as an add on package. ""</p>

<p>from <a href=""http://developer.r-project.org/"" rel=""nofollow noreferrer"">http://developer.r-project.org/</a></p>
"
1419153,143305,2009-09-13T23:55:21Z,1419136,5,FALSE,"<p>Among the possibilities are <code>grep</code>, <code>match</code> and <code>%in%</code>. Here is a solution using <code>grep</code>:</p>

<pre><code>R&gt; foo &lt;- data.frame(q1_1=1:4, q1_2=11:14, q2_1=21:24, q2_2=31:34, q3_1=41:44, q3_2=51:54)
R&gt; colnames(foo)
[1] ""q1_1"" ""q1_2"" ""q2_1"" ""q2_2"" ""q3_1"" ""q3_2""
R&gt; grep(""q3_"", colnames(foo))
[1] 5 6
R&gt; q3 &lt;- foo[, grep(""q3_"", colnames(foo))]
R&gt; q3
  q3_1 q3_2
1   41   51
2   42   52
3   43   53
4   44   54
R&gt; 
</code></pre>
"
1419156,144157,2009-09-13T23:57:04Z,1419136,2,FALSE,"<pre><code>q3 &lt;- a[,grep(""q3_"",colnames(a))]
</code></pre>
"
1419625,144157,2009-09-14T04:03:43Z,1419526,6,TRUE,"<pre><code>prop.test(c(X,Y), c(n,n), alternative=""greater"")
</code></pre>

<p>does everything you want.</p>
"
1420587,134830,2009-09-14T09:46:50Z,1417269,2,FALSE,"<p>Good practise for coding R means wrapping as much of your code as possible into functions.  (See, e.g., Chapter 5 of the <a href=""http://www.burns-stat.com/pages/Tutor/R_inferno.pdf"" rel=""nofollow noreferrer"">R Inferno</a>, pdf.)  If you place your plotting code inside a function, it need not be displayed when you source it.  Compare the following.</p>

<p>File foo.r contains</p>

<pre><code>plot(1:10)
</code></pre>

<p>When you call <code>source('foo.r')</code>, the plot is shown.</p>

<p>File bar.r contains</p>

<pre><code>bar &lt;- function() plot(1:20)
</code></pre>

<p>When you call <code>source('bar.r')</code>, the plot is not shown.  You can display it at your convenience by typing <code>bar()</code> at the command prompt.</p>
"
1422558,143377,2009-09-14T16:17:37Z,1422247,2,FALSE,"<p>This is just a mixture of normals. So why not something like:</p>

<pre><code>rmnorm &lt;- function(n,mean, sd,prob) {
    nmix &lt;- length(mean)
    if (length(sd)!=nmix) stop(""lengths should be the same."")
    y &lt;- sample(1:nmix,n,prob=prob, replace=TRUE)
    mean.mix &lt;- mean[y]
    sd.mix &lt;- sd[y]
    rnorm(n,mean.mix,sd.mix)
}
plot(density(rmnorm(10000,mean=c(0,3), sd=c(1,2), prob=c(.5,.5))))
</code></pre>

<p>This should be fine if all you need are samples from this mixture distribution.</p>
"
1422636,16632,2009-09-14T16:34:01Z,1422247,9,FALSE,"<p>Alternative approach:</p>

<pre><code>sample(x, n, replace = TRUE)
</code></pre>
"
1423006,143305,2009-09-14T17:57:30Z,1422987,54,TRUE,"<p>Yes, there is:  <a href=""http://ess.r-project.org"" rel=""noreferrer""><strong>ESS</strong></a>.</p>

<p>And it is very highly recommended.  And it does <em>a lot</em> more than just syntax highlighting so please read the <a href=""http://ess.r-project.org/Manual/ess.html"" rel=""noreferrer"">docs</a>.</p>
"
1423390,148801,2009-09-14T19:19:13Z,1419136,0,FALSE,"<p>I'd just do something like:</p>

<p><code>
q3 &lt;- data.frame(a[paste('q3_',1:27,sep='')])
</code></p>

<p>You can still use <code>attach(a)</code> if you like (and if there's no element in '<code>a</code>' named '<code>a</code>'), but there is no need.</p>
"
1423423,148801,2009-09-14T19:24:34Z,1408897,1,FALSE,"<p>My guess is you need to install the DBI package (most database packages depend on it).</p>

<p>If you use <code>install.packages('RPpostgreSQL',dep=TRUE)</code> from within R it should take care of any dependency issues.</p>
"
1423462,148801,2009-09-14T19:32:52Z,1395309,6,FALSE,"<p>I believe the <code>multicore</code> package works on XP. It gives some basic multi-process capability, especially through offering a drop-in replacement for <code>lapply()</code> and a simple way to evaluate an expression in a new thread (<code>mcparallel()</code>).</p>
"
1423467,143319,2009-09-14T19:33:28Z,1407238,2,FALSE,"<p><a href=""http://personal.vsnl.com/sureshms/utilities.html"" rel=""nofollow noreferrer"">ClipPath</a> adds right-click menu options to choose which kind of slash you want to paste.</p>

<p>Via <a href=""http://gettinggeneticsdone.blogspot.com/2009/09/clippath-copies-file-path-from-explorer.html"" rel=""nofollow noreferrer"">Getting Genetics Done</a>, which looks like it could be a useful resource for R users in general.</p>
"
1423767,160314,2009-09-14T20:33:04Z,1423472,10,TRUE,"<pre><code>&gt; summary(assayaov)[1][[1]][[1]][[3]]
[1] 15.49
&gt; summary(bgaov)[1][[1]][[1]][[3]]
[1] 1.776475
</code></pre>
"
1423985,143305,2009-09-14T21:20:30Z,1423902,3,FALSE,"<p>I fear that you are getting a few things muddled up and confused, as you did in your recent post on the r-help list:</p>

<ol>
<li>Think about it from the bottom up.</li>
<li>You need a mechanism for parallel computing, whether or not it involves several machines.</li>
<li>The <a href=""http://cran.r-project.org/package=snow"" rel=""nofollow noreferrer"">snow</a> package does that for you, which is one of the reasons we highlighted it in the <a href=""http://www.jstatsoft.org/v31/i01"" rel=""nofollow noreferrer"">JSS survey paper</a></li>
<li>You can start with snow, which in the simplest case works a) on Windows and b) does not require MPI or PVM -- just use sockets</li>
<li>Once you have that figured out, move 'up' to higher levels of abstraction of using snow with the <a href=""http://cran.r-project.org/package=doSNOW"" rel=""nofollow noreferrer"">doSNOW</a> backend to support <a href=""http://cran.r-project.org/package=foreach"" rel=""nofollow noreferrer"">foreach</a>.</li>
<li>The <a href=""http://cran.r-project.org/package=multicore"" rel=""nofollow noreferrer"">multicore</a> package does not (yet?) work on Windows and is of no (current?) use to you, so ignore it at least for now. Likewise for its <a href=""http://cran.r-project.org/package=doMC"" rel=""nofollow noreferrer"">doMC</a> backend.</li>
</ol>

<p>Working examples exist for each step along the way. I always fall back to verifying by test cases what works and what doesn't.</p>
"
1424274,142651,2009-09-14T22:47:28Z,1424189,4,TRUE,"<p>You could add the numbers to the legend.</p>

<pre><code>library(ggplot2)
critters &lt;- structure(list(Zoo = ""Omaha"", Animals = 50, Bears = 10, PolarBears = 3), .Names = c(""Zoo"", ""Animals"", ""Bears"", ""PolarBears""), row.names = c(NA, -1L), class = ""data.frame"")
d &lt;- data.frame(animal=factor(c(rep(""Animals"", critters$Animals),
       rep(""Bears"", critters$Bears), rep(""PolarBears"", critters$PolarBears)),
       levels = c(""PolarBears"", ""Bears"", ""Animals""), ordered= TRUE))
levels(d$animal) &lt;- apply(data.frame(table(d$animal)), 1, paste, collapse = "": "")
ggplot(d, aes(x = factor(1), fill = factor(animal))) +  geom_bar() +
  coord_polar() + labs(x = NULL, fill = NULL) +
  scale_fill_manual(values = c(""firebrick2"", ""yellow2"", ""green3"")) +
  opts(title = paste(""Animals, Bears and Polar Bears:\nOmaha Zoo"", sep=""""))
</code></pre>
"
1424310,160314,2009-09-14T23:00:49Z,1424189,5,FALSE,"<p>you can also add it directly to the plot:</p>

<pre><code>grr &lt;- ggplot(d, aes(x = factor(1), fill = factor(animal))) +  geom_bar() +
coord_polar() + labs(x = NULL, fill = NULL) +
scale_fill_manual(values = c(""firebrick2"", ""yellow2"", ""green3"")) +
opts(title = paste(""Animals, Bears and Polar Bears:\nOmaha Zoo"", sep=""""))+
geom_text(y=c(3,10,50)-3,label=c(""3"",""10"",""50""),size=4)
grr
</code></pre>
"
1424549,126042,2009-09-15T00:28:37Z,1424351,7,TRUE,"<p>The <a href=""http://projetos.inpa.gov.br/i3geo/pacotes/r/win/library/utils/html/normalizePath.html"" rel=""nofollow noreferrer"">normalizePath</a> function will turn short names into long names:</p>

<blockquote>
  <p>This converts relative paths to absolute paths, and converts short names to long names.</p>
</blockquote>
"
1426020,143476,2009-09-15T09:02:19Z,1406202,6,FALSE,"<p>I do love rgl! But there are times when 3-D plots in lattice are useful too - you can write your own function which you can pass to the 'panel' argument to lattice functions. For instance,</p>

<pre><code>mypanel &lt;- function(x,y,z,...) {
  panel.wireframe(x,y,z,...)
  panel.cloud(x,y,z,...)
}
wireframe(myGrid$z ~ myGrid$x * myGrid$y, xlab=""X"", ylab=""Y"", zlab=""Z"",
          panel=mypanel)
</code></pre>

<p>The last function you call can be wireframe() or cloud(); in either case since panel.wireframe() and panel.cloud() are called within the panel function the result should be the same. </p>

<p>Edit: Thanks for pointing that out, Aaron, then probably you can pass z2 as another variable:</p>

<pre><code>mypanel &lt;- function(x,y,z,z2,...) {
  panel.wireframe(x,y,z,...)
  panel.cloud(x,y,z2,...)
}
wireframe(z ~ x * y, data=myGrid, xlab=""X"", ylab=""Y"", zlab=""Z"",
          panel=mypanel, z2=myGrid$z2)
</code></pre>
"
1428249,37751,2009-09-15T16:34:49Z,1428174,2,FALSE,"<p>I'm not quite grokking what you are doing so I'll just throw out something that may, or may not help.</p>

<p>Here's what I think of as the Cartesian product of the two columns:</p>

<pre><code>expand.grid(x[,1],x[,2])
</code></pre>
"
1428258,143305,2009-09-15T16:36:29Z,1428174,7,TRUE,"<p>The <code>expand.grid()</code> function useful for this:</p>

<pre><code>R&gt; GG &lt;- expand.grid(1:10,1:10)
R&gt; GG &lt;- GG[GG[,1]&gt;=GG[,2],]     # trim it to your 55 pairs
R&gt; dim(GG)
[1] 55  2
R&gt; head(GG)
  Var1 Var2
1    1    1
2    2    1
3    3    1
4    4    1
5    5    1
6    6    1
R&gt; 
</code></pre>

<p>Now you have the 'n*(n+1)/2' subsets and you can simple index your original matrix.</p>
"
1428288,163053,2009-09-15T16:41:17Z,1428174,2,FALSE,"<p>You can also try the ""relations"" package.  <a href=""http://cran.r-project.org/web/packages/relations/vignettes/relations.pdf"" rel=""nofollow noreferrer"">Here is the vignette.</a>  It should work like this:</p>

<pre><code>relation_table(x %&gt;&lt;% x)
</code></pre>
"
1428325,136862,2009-09-15T16:47:31Z,1428174,1,FALSE,"<p>Using Dirk's answer:</p>

<pre><code>idx &lt;- expand.grid(1:nrow(x), 1:nrow(x))
idx&lt;-idx[idx[,1] &gt;= idx[,2],]
N &lt;- cbind(x[idx[,2],], x[idx[,1],])

&gt; all(M == N)
[1] TRUE
</code></pre>

<p>Thanks everyone!</p>
"
1429513,163053,2009-09-15T20:42:58Z,1429476,5,TRUE,"<p>Check out the ""Rlibstree"" package on omegahat: <a href=""http://www.omegahat.org/Rlibstree/"" rel=""noreferrer"">http://www.omegahat.org/Rlibstree/</a>.  </p>

<p>This uses <a href=""http://www.icir.org/christian/libstree/"" rel=""noreferrer"">http://www.icir.org/christian/libstree/</a>.  </p>
"
1429579,149772,2009-09-15T20:57:52Z,1429476,0,FALSE,"<p>I don't know R, but I used to implement Hirschberg's algorithm which is fast and don't consume too much space.</p>

<p>As I remember it is only 2 or 3 recursively called short functions.</p>

<p>Here is a link:
<a href=""http://wordaligned.org/articles/longest-common-subsequence"" rel=""nofollow noreferrer"">http://wordaligned.org/articles/longest-common-subsequence</a></p>

<p>So don't hesitate to implement it in R, it worths the effort since it is a very interesting algorithm. </p>
"
1430013,143319,2009-09-15T22:44:02Z,1429907,7,FALSE,"<p>I use <a href=""http://www.stat.uni-muenchen.de/~leisch/Sweave/"" rel=""noreferrer"">Sweave</a> for the report-producing side of this, but I've also been hearing about the <a href=""http://cran.r-project.org/web/packages/brew/index.html"" rel=""noreferrer"">brew</a> package - though I haven't yet looked into it.</p>

<p>Essentially, I have a number of surveys for which I produce summary statistics.  Same surveys, same reports every time.  I built a Sweave template for the reports (which takes a bit of work).  But once the work is done, I have a separate R script that lets me point out the new data.  I press ""Go"", Sweave dumps out a few score .tex files, and I run a little Python script to pdflatex them all.  My predecessor spent ~6 weeks each year on these reports; I spend about 3 days (mostly on cleaning data; escape characters are hazardous).</p>

<p>It's very possible that there are better approaches now, but if you do decide to go this route, let me know - I've been meaning to put up some of my Sweave hacks, and that would be a good kick in the pants to do so.</p>
"
1430129,168137,2009-09-15T23:18:29Z,1429907,5,FALSE,"<p>Agreed that Sweave is the way to go, with <a href=""http://cran.r-project.org/web/packages/xtable/index.html"" rel=""noreferrer"">xtable</a> for generating LaTeX tables. Although I haven't spent too much time working with them, the recently released <a href=""http://cran.r-project.org/web/packages/tikzDevice/index.html"" rel=""noreferrer"">tikzDevice</a> package looks really promising, particularly when coupled with <a href=""http://www.rforge.net/pgfSweave/"" rel=""noreferrer"">pgfSweave</a> (which, as far as I know is only available on rforge.net at this time -- there is a link to r-forge from there, but it's not responding for me at the moment). </p>

<p>Between the two, you'll get consistent formatting between text and figures (fonts, etc.). With brew, these might constitute the holy grail of report generation.</p>
"
1430185,153943,2009-09-15T23:40:17Z,1429907,7,FALSE,"<p>I'm going to suggest something in a different sort of direction from the other submitters, based on the fact that you asked specifically about <em>project workflow</em>, rather than <em>tools</em>. Assuming you're relatively happy with your document-production model, it sounds like your challenges really may be centered more around issues of version tracking, asset management, and review/publishing process.</p>

<p>If that sounds correct, I would suggest looking into an integrated ticketing/source management/documentation tool like <a href=""http://www.redmine.org/"" rel=""noreferrer"">Redmine</a>. Keeping related project artifacts such as pending tasks, discussion threads, and versioned data/code files together can be a great help even for projects well outside the traditional ""programming"" bailiwick.</p>
"
1430228,163053,2009-09-15T23:56:33Z,1429907,17,FALSE,"<p>I agree with the other responders: Sweave is excellent for report writing with R.  And rebuilding the report with updated results is as simple as re-calling the Sweave function.  It's completely self-contained, including all the analysis, data, etc.  And you can version control the whole file.</p>

<p>I use the StatET plugin for Eclipse for developing the reports, and Sweave is integrated (Eclipse recognizes latex formating, etc).  On Windows, <a href=""http://miktex.org/"" rel=""noreferrer"">it's easy to use MikTEX</a>.</p>

<p>I would also add, that <a href=""http://en.wikipedia.org/wiki/Beamer_(LaTeX)"" rel=""noreferrer"">you can create beautiful reports with Beamer</a>.  Creating a normal report is just as simple.  I included an example below that pulls data from Yahoo! and creates a chart and a table (using quantmod).  You can build this report like so:</p>

<pre><code>Sweave(file = ""test.Rnw"")
</code></pre>

<p>Here's the Beamer document itself:</p>

<pre><code>% 
\documentclass[compress]{beamer}
\usepackage{Sweave}
\usetheme{PaloAlto} 
\begin{document}

\title{test report}
\author{john doe}
\date{September 3, 2009} 

\maketitle

\begin{frame}[fragile]\frametitle{Page 1: chart}

&lt;&lt;echo=FALSE,fig=TRUE,height=4, width=7&gt;&gt;=
library(quantmod)
getSymbols(""PFE"", from=""2009-06-01"")
chartSeries(PFE)
@

\end{frame}


\begin{frame}[fragile]\frametitle{Page 2: table}

&lt;&lt;echo=FALSE,results=tex&gt;&gt;=
library(xtable)
xtable(PFE[1:10,1:4], caption = ""PFE"")
@

\end{frame}

\end{document}
</code></pre>
"
1430334,143305,2009-09-16T00:42:53Z,1430313,2,FALSE,"<p>As I recall you have to turn 'recording' or 'History' on. This is platform-specific and I am not near a Windows machine.</p>
"
1430370,163053,2009-09-16T00:54:21Z,1430313,4,FALSE,"<p>Dirk is right.  </p>

<p>Another thing to look at are the following functions:</p>

<pre><code>x &lt;- recordPlot()
replayPlot(x)
</code></pre>

<p>From the R for Windows FAQ:</p>

<p>The graphics has a history mechanism. As README.R-2.9.2 says:</p>

<p>The History menu allows the recording of plots. When plots have been recorded they can be reviewed by  and , saved and replaced. Recording can be turned on automatically (the Recording item on the list) or individual plots can be added (Add or the  key). The whole plot history can be saved to or retrieved from an R variable in the global environment. The format of recorded plots may change between R versions. Recorded plots should not be used as a permanent storage format for R plots.
There is only one graphics history shared by all the windows devices.</p>

<p>The R console and graphics windows have configuration files stored in the RHOME\etc directory called Rconsole and Rdevga; you can keep personal copies in your HOME directory. They contain comments which should suffice for you to edit them to your preferences. For more details see ?Rconsole. There is a GUI preferences editor invoked from the Edit menu which can be used to edit the file Rconsole.</p>
"
1430569,16632,2009-09-16T02:09:41Z,1429907,88,FALSE,"<p>If you'd like to see some examples, I have a few small (and not so small) data cleaning and analysis projects available online.  In most, you'll find a script to download the data, one to clean it up, and a few to do exploration and analysis:</p>

<ul>
<li><a href=""http://github.com/hadley/data-baby-names"" rel=""noreferrer"">Baby names from the social security administration</a></li>
<li><a href=""http://github.com/hadley/data-fuel-economy"" rel=""noreferrer"">30+ years of fuel economy data from the EPI</a></li>
<li><a href=""http://github.com/hadley/data-housing-crisis"" rel=""noreferrer"">A big collection of data about the housing crisis</a></li>
<li><a href=""http://github.com/hadley/data-movies"" rel=""noreferrer"">Movie ratings from the IMDB</a></li>
<li><a href=""http://github.com/hadley/sfhousing"" rel=""noreferrer"">House sale data in the Bay Area</a></li>
</ul>

<p>Recently I have started numbering the scripts, so it's completely obvious in which order they should be run.  (If I'm feeling really fancy I'll sometimes make it so that the exploration script will call the cleaning script which in turn calls the download script, each doing the minimal work necessary - usually by checking for the presence of output files with <code>file.exists</code>.  However, most times this seems like overkill).</p>

<p>I use git for all my projects (a source code management system) so its easy to collaborate with others, see what is changing and easily roll back to previous versions.</p>

<p>If I do a formal report, I usually keep R and latex separate, but I always make sure that I can <code>source</code> my R code to produce all the code and output that I need for the report. For the sorts of reports that I do, I find this easier and cleaner than working with latex.</p>
"
1430589,163053,2009-09-16T02:14:05Z,1395499,1,FALSE,"<p>Thanks hobbs!  I need to do a little more work with it, but subtracting from the day slot works in POSIXlt:</p>

<pre><code>&gt; a &lt;- as.POSIXct('2009-03-08 23:00:00.000')
&gt; as.POSIXlt(a)
[1] ""2009-03-08 23:00:00 EDT""
&gt; a &lt;- as.POSIXlt(a)
&gt; a$mday &lt;- a$mday -1
&gt; a
[1] ""2009-03-07 23:00:00 EDT""
</code></pre>
"
1430603,1855677,2009-09-16T02:20:17Z,1428750,2,FALSE,"<p>?break</p>

<p>Only gets you out of loop.</p>

<p>?try</p>

<p>Lets you set up code that might fail and gracefully recover.</p>
"
1430950,136862,2009-09-16T04:31:08Z,1430557,4,TRUE,"<p>Not sure if this is what you are looking for, but to generate the list of questions:</p>

<pre><code>&gt; gsub('^', 'q', gsub(' ', '', 
    apply(expand.grid(1:10,letters),1,
           function(r) paste(r, sep='', collapse='')
         )))
  [1] ""q1a""  ""q2a""  ""q3a""  ""q4a""  ""q5a""  ""q6a""  ""q7a""  ""q8a""  ""q9a""  ""q10a""
 [11] ""q1b""  ""q2b""  ""q3b""  ""q4b""  ""q5b""  ""q6b""  ""q7b""  ""q8b""  ""q9b""  ""q10b""
 [21] ""q1c""  ""q2c""  ""q3c""  ""q4c""  ""q5c""  ""q6c""  ""q7c""  ""q8c""  ""q9c""  ""q10c""
 [31] ""q1d""  ""q2d""  ""q3d""  ""q4d""  ""q5d""  ""q6d""  ""q7d""  ""q8d""  ""q9d""  ""q10d""
 [41] ""q1e""  ""q2e""  ""q3e""  ""q4e""  ""q5e""  ""q6e""  ""q7e""  ""q8e""  ""q9e""  ""q10e""
 [51] ""q1f""  ""q2f""  ""q3f""  ""q4f""  ""q5f""  ""q6f""  ""q7f""  ""q8f""  ""q9f""  ""q10f""
 [61] ""q1g""  ""q2g""  ""q3g""  ""q4g""  ""q5g""  ""q6g""  ""q7g""  ""q8g""  ""q9g""  ""q10g""
 [71] ""q1h""  ""q2h""  ""q3h""  ""q4h""  ""q5h""  ""q6h""  ""q7h""  ""q8h""  ""q9h""  ""q10h""
 [81] ""q1i""  ""q2i""  ""q3i""  ""q4i""  ""q5i""  ""q6i""  ""q7i""  ""q8i""  ""q9i""  ""q10i""
 [91] ""q1j""  ""q2j""  ""q3j""  ""q4j""  ""q5j""  ""q6j""  ""q7j""  ""q8j""  ""q9j""  ""q10j""
 ...
</code></pre>

<p>And then you turn your inner part of the analysis into a function that takes the question prefix as a parameter:</p>

<pre><code>analyzeQuestion &lt;- function (prefix)
{
  q &lt;- d1[,grep(prefix,colnames(d1))] ## Pull in everything matching q4a_X
  q &lt;- melt(q) ## restructure data for post-hoc

  qaaov &lt;- aov(formula=value~variable,data=q4a) ## anova
  return (LTukey(q4aaov,which="""",conf.level=0.95)) ## Tukey's post-hoc
}
</code></pre>

<p>Now - I'm not sure where your 'q4a' variable is coming from (as used in the <code>aov(..., data=q4a)</code>- so not sure what to do about that bit. But hopefully this helps.</p>

<p>To put the two together you can use <code>sapply()</code> to apply the <code>analyzeQuestion</code> function to each of the prefixes that we automagically generated.</p>
"
1431016,26575,2009-09-16T04:53:55Z,1429907,4,FALSE,"<p>At a more ""meta"" level, you might be interested in the <a href=""http://www.crisp-dm.org"" rel=""nofollow noreferrer"">CRISP-DM</a> process model.</p>
"
1431092,86684,2009-09-16T05:20:43Z,1429907,2,FALSE,"<p>For writing a quick preliminary report or email to a colleague, I find that it can be very efficient to copy-and-paste plots into MS Word or an email or wiki page -- often best is a bitmapped screenshot (e.g. on mac, Apple-Shift-(Ctrl)-4).  I think this is an underrated technique.</p>

<p>For a more final report, writing R functions to easily regenerate all the plots (as files) is very important.  It does take more time to code this up.</p>

<p>On the larger workflow issues, I like Hadley's answer on enumerating the code/data files for the cleaning and analysis flow.  All of my data analysis projects have a similar structure.</p>
"
1431731,144157,2009-09-16T08:40:41Z,1431657,5,FALSE,"<p>I'm assuming you really want a bar chart rather than a histogram. In that case, <code>barplot</code> from standard graphics or <code>barchart</code> from the lattice package will both do it. Or use ggplot as shown in <a href=""http://had.co.nz/ggplot2/position_stack.html"" rel=""noreferrer"">this example</a> or <a href=""http://learnr.wordpress.com/2009/03/16/ggplot2-barplots/"" rel=""noreferrer"">here</a></p>
"
1432146,149223,2009-09-16T10:19:58Z,1431657,4,FALSE,"<p>For example;</p>

<pre><code>ht1=c(0.3,0.7)
ht2=c(0.4,0.6)
barplot(cbind(ht1,ht2))
</code></pre>
"
1432685,16632,2009-09-16T12:36:10Z,1430557,3,FALSE,"<p>I would recommend melting the entire dataset and then splitting <code>variable</code> into its component pieces.  Then you can more easily use subset to look at (e.g.) just question four: <code>subset(molten, q = 4)</code>.</p>
"
1432891,163053,2009-09-16T13:13:53Z,1432867,13,FALSE,"<p>The first plot was just covered in a <a href=""http://www.imachordata.com/?p=199"" rel=""noreferrer""> blog post  on imachordata.com</a>.  (hat tip to <a href=""http://blog.revolution-computing.com/2009/09/making-barplots-with-error-bars-in-r.html"" rel=""noreferrer"">David Smith on blog.revolution-computing.com</a>)  You can also <a href=""http://had.co.nz/ggplot2/geom_errorbar.html"" rel=""noreferrer"">read the related documentation from Hadley on ggplot2</a>.</p>

<p>Here's the example code:</p>

<pre><code>library(ggplot2)
data(mpg)

#create a data frame with averages and standard deviations
 hwy.avg&lt;-ddply(mpg, c(""class"", ""year""), function(df)
 return(c(hwy.avg=mean(df$hwy), hwy.sd=sd(df$hwy))))

#create the barplot component
 avg.plot&lt;-qplot(class, hwy.avg, fill=factor(year), data=hwy.avg, geom=""bar"", position=""dodge"")

#first, define the width of the dodge
dodge &lt;- position_dodge(width=0.9)

#now add the error bars to the plot
avg.plot+geom_linerange(aes(ymax=hwy.avg+hwy.sd, ymin=hwy.avg-hwy.sd), position=dodge)+theme_bw()
</code></pre>

<p>It ends up looking like this:
<a href=""http://www.imachordata.com/wp-content/uploads/2009/09/barplot.png"" rel=""noreferrer"">alt text http://www.imachordata.com/wp-content/uploads/2009/09/barplot.png</a></p>
"
1433824,149223,2009-09-16T15:45:10Z,1433523,6,TRUE,"<p>I think that disclaimer only applies if you use the $ operator to access your java objects.  As long as you stick with the .jcall function you won't incur the overhead.</p>

<p>In terms of experience using rJava, I've found it works exactly as advertised and for my package (farmR) it hasn't caused any performance problems.  I don't make a huge number of calls into java though, and I haven't used any of the java GUI toolkits.</p>
"
1433834,16632,2009-09-16T15:47:00Z,936748,28,TRUE,"<p>See <code>lockBinding</code>:</p>

<pre><code>a &lt;- 1
lockBinding(""a"", globalenv())
a &lt;- 2
Error: cannot change value of locked binding for 'a'
</code></pre>
"
1434118,143305,2009-09-16T16:35:22Z,1433523,7,FALSE,"<p>It is not the <em>only</em> one as the <a href=""http://www.omegahat.org"" rel=""nofollow noreferrer"">Omegahat</a> project also has the <a href=""http://www.omegahat.org/RSJava/"" rel=""nofollow noreferrer"">RSJava</a> package. But as many of the other brilliant innovations from Omegahat (which practically speaking is really just Duncan Temple Lang), this one may not build as easily or reliably.</p>

<p>The <a href=""http://cran.r-project.org/package=rJava"" rel=""nofollow noreferrer""><strong>rJava</strong></a> package on the other hand is used by almost thirty other packages </p>

<blockquote>
  <p>CADStat, Containers, Deducer, JGR,
  RFreak, RImageJ, RJDBC, RLadyBug,
  aCGH.Spline, ant, arulesNBMiner,
  colbycol, cshapes, dynGraph, farmR,
  gWidgetsrJava, glmulti,
  helloJavaWorld, iplots, rSymPy, rcdk,
  rcdklibs, scagnostics, spcosa, RKEA,
  RWeka, Snowball, openNLP, wordnet</p>
</blockquote>

<p>which I take as quite the endorsement.</p>
"
1434223,170364,2009-09-16T16:58:59Z,1430313,0,FALSE,"<p>On a mac, one can use the command key with the back arrow (and forward arrow) to cycle through plots.</p>

<p>Make sure you've plotted at least two plots to the same quartz device, then with the plotting window in focus, hit <code>⌘ ←</code> to see the previous plot.</p>
"
1434349,74658,2009-09-16T17:22:03Z,1429907,2,FALSE,"<p>I'll add my voice to sweave.  For complicated, multi-step analysis you can use a <a href=""http://ggorjan.blogspot.com/2009/01/using-makefile-to-ease-repeated.html"" rel=""nofollow noreferrer"">makefile</a> to specify the different parts.  Can prevent having to repeat the whole analysis if just one part has changed.</p>
"
1434424,136862,2009-09-16T17:34:34Z,1429907,177,TRUE,"<p>I generally break my projects into 4 pieces:</p>

<ol>
<li>load.R</li>
<li>clean.R</li>
<li>func.R</li>
<li>do.R</li>
</ol>

<p>load.R: Takes care of loading in all the data required. Typically this is a short file, reading in data from files, URLs and/or ODBC. Depending on the project at this point I'll either write out the workspace using <code>save()</code> or just keep things in memory for the next step.</p>

<p>clean.R: This is where all the ugly stuff lives - taking care of missing values, merging data frames, handling outliers. </p>

<p>func.R: Contains all of the functions needed to perform the actual analysis. <code>source()</code>'ing this file should have no side effects other than loading up the function definitions. This means that you can modify this file and reload it without having to go back an repeat steps 1 &amp; 2 which can take a long time to run for large data sets.</p>

<p>do.R: Calls the functions defined in func.R to perform the analysis and produce charts and tables.</p>

<p>The main motivation for this set up is for working with large data whereby you don't want to have to reload the data each time you make a change to a subsequent step. Also, keeping my code compartmentalized like this means I can come back to a long forgotten project and quickly read load.R and work out what data I need to update, and then look at do.R to work out what analysis was performed.</p>
"
1434906,84458,2009-09-16T19:08:28Z,1434897,7,TRUE,"<p>Here's one handy option:</p>

<pre><code>site.data &lt;- read.table(textConnection(
""        site year     peak
1  ALBEN    5 101529.6
2  ALBEN   10 117483.4
3  ALBEN   20 132960.9
8  ALDER    5   6561.3
9  ALDER   10   7897.1
10 ALDER   20   9208.1
15 AMERI    5  43656.5
16 AMERI   10  51475.3
17 AMERI   20  58854.4""))
</code></pre>
"
1434927,163053,2009-09-16T19:12:50Z,1434897,12,FALSE,"<p>That's a neat solution.  I'm guessing there's a way to do this with RCurl, <a href=""https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package"">as in this post which scraped off wikipedia</a>.</p>

<p>But as a more general point for discussion: why don't we just use data from the ""datasets"" package in R?  Then everyone will have the data by just calling the data() function, and there are datasets to cover most cases.</p>

<p><i>[Edit]:</i> I was able to do this.  It's clearly more work (i.e. impractical) than your solution.  :)</p>

<p><i>[Edit 2]:</i> I wrapped this into a function and tried it with another page.  </p>

<pre><code>getSOTable &lt;- function(url, code.block=2, raw=FALSE, delimiter=""code"") {
  require(RCurl)
  require(XML)

  webpage &lt;- getURL(url)
  webpage &lt;- readLines(tc &lt;- textConnection(webpage)); close(tc)
  pagetree &lt;- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)
  x &lt;- xpathSApply(pagetree, paste(""//*/"", delimiter, sep=""""), xmlValue)[code.block]  
  if(raw)
    return(strsplit(x, ""\n"")[[1]])
  else 
    return(read.table(textConnection(strsplit(x, ""\n"")[[1]][-1])))
}

getSOTable(""https://stackoverflow.com/questions/1434897/how-do-i-load-example-datasets-in-r"")
    site year     peak
1  ALBEN    5 101529.6
2  ALBEN   10 117483.4
3  ALBEN   20 132960.9
8  ALDER    5   6561.3
9  ALDER   10   7897.1
10 ALDER   20   9208.1
15 AMERI    5  43656.5
16 AMERI   10  51475.3
17 AMERI   20  58854.4

getSOTable(""https://stackoverflow.com/questions/1428174/quickly-generate-the-cartesian-product-of-a-matrix"", code.block=10)
   X1 X2 X3 X4
1   1 11  1 11
2   1 11  2 12
3   1 11  3 13
4   1 11  4 14
5   1 11  5 15
6   1 11  6 16
7   1 11  7 17
8   1 11  8 18
9   1 11  9 19
10  1 11 10 20
</code></pre>
"
1435232,170364,2009-09-16T20:19:21Z,1434411,4,FALSE,"<p>For part 1 I prefer <code>aggregate</code> because it keeps the data in a more R-like one observation per row format.</p>

<p><code>aggregate(var1, list(fac1, fac2), mean, na.rm=T)</code></p>
"
1435648,168139,2009-09-16T21:41:24Z,1407238,5,TRUE,"<p>I wrote a <strong>autohotkey</strong> script that is triggered by typing ""rfil "" - without the inverted commas. </p>

<pre><code>:O:rfil:: ;replaces backslashes with forward slashes in a file name that is stored on the clipboard
StringReplace,clipboard,clipboard,\,/,All
send %clipboard%
return
</code></pre>

<p>If anyone can tell me a quicker way than using the send command I would appreciate it.
I have an autohotkey script running all the time on all my computers so I did not have to download new software in order to run this script. I simply added it to my default script file.</p>

<p>I will be happy to explain what I did if you want me to.</p>
"
1436168,170364,2009-09-17T00:31:20Z,1435937,3,TRUE,"<p>Take a look at <code>?Encoding</code> to set the encoding for specific objects.</p>

<p>You might have luck with <code>options(encoding = )</code> see <code>?options</code>, (disclaimer, I don't have a windows machine)</p>

<p>As for editors, I haven't heard complaints about encoding issues with <a href=""http://www.crimsoneditor.com/"" rel=""nofollow noreferrer"">Crimson editor</a> which lists utf-8 support as a feature.  </p>
"
1436809,163053,2009-09-17T05:15:31Z,1429907,16,FALSE,"<p>I just wanted to add, in case anyone missed it, that <a href=""http://learnr.wordpress.com/2009/09/09/brew-creating-repetitive-reports/"" rel=""noreferrer"">there's a great post on the learnr blog about creating repetitive reports</a> with <a href=""http://cran.r-project.org/web/packages/brew/index.html"" rel=""noreferrer"">Jeffrey Horner's brew package</a>.  Matt and Kevin both mentioned brew above.  I haven't actually used it much myself.</p>

<p>The entries follows a nice workflow, so it's well worth a read:</p>

<ol>
<li>Prepare the data.</li>
<li>Prepare the report template.</li>
<li>Produce the report.</li>
</ol>

<p>Actually producing the report once the first two steps are complete is very simple:</p>

<pre><code>library(tools)
library(brew)
brew(""population.brew"", ""population.tex"")
texi2dvi(""population.tex"", pdf = TRUE)
</code></pre>
"
1437343,134830,2009-09-17T07:56:25Z,1395102,18,TRUE,"<p>From <a href=""http://books.google.co.uk/books?id=N3WeyHFbHLQC&amp;dq=pinheiro+bates&amp;printsec=frontcover#v=onepage&amp;q=&amp;f=false"" rel=""noreferrer"">Pinheiro &amp; Bates 2000</a>, Section 5.4, p250:</p>

<blockquote>
  <p>The <strong>gls</strong> function is used to fit the
  extended linear model, using either
  maximum likelihood, or restricted
  maximum likelihood.  It can be veiwed
  as an <strong>lme</strong> function without the
  argument <em>random</em>.</p>
</blockquote>

<p>For further details, it would be instructive to compare the <code>lme</code> analysis of the orthodont dataset (starting on p147 of the same book) with the <code>gls</code> analysis (starting on p250).  To begin, compare</p>

<hr>

<pre><code>orth.lme &lt;- lme(distance ~ Sex * I(age-11), data=Orthodont)
summary(orth.lme)

Linear mixed-effects model fit by REML
 Data: Orthodont 
       AIC     BIC    logLik
  458.9891 498.655 -214.4945

Random effects:
 Formula: ~Sex * I(age - 11) | Subject
 Structure: General positive-definite
                      StdDev    Corr                
(Intercept)           1.7178454 (Intr) SexFml I(-11)
SexFemale             1.6956351 -0.307              
I(age - 11)           0.2937695 -0.009 -0.146       
SexFemale:I(age - 11) 0.3160597  0.168  0.290 -0.964
Residual              1.2551778                     

Fixed effects: distance ~ Sex * I(age - 11) 
                          Value Std.Error DF  t-value p-value
(Intercept)           24.968750 0.4572240 79 54.60945  0.0000
SexFemale             -2.321023 0.7823126 25 -2.96687  0.0065
I(age - 11)            0.784375 0.1015733 79  7.72226  0.0000
SexFemale:I(age - 11) -0.304830 0.1346293 79 -2.26421  0.0263
 Correlation: 
                      (Intr) SexFml I(-11)
SexFemale             -0.584              
I(age - 11)           -0.006  0.004       
SexFemale:I(age - 11)  0.005  0.144 -0.754

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.96534486 -0.38609670  0.03647795  0.43142668  3.99155835 

Number of Observations: 108
Number of Groups: 27
</code></pre>

<hr>

<pre><code>orth.gls &lt;- gls(distance ~ Sex * I(age-11), data=Orthodont)
summary(orth.gls)

Generalized least squares fit by REML
  Model: distance ~ Sex * I(age - 11) 
  Data: Orthodont 
       AIC      BIC    logLik
  493.5591 506.7811 -241.7796

Coefficients:
                          Value Std.Error  t-value p-value
(Intercept)           24.968750 0.2821186 88.50444  0.0000
SexFemale             -2.321023 0.4419949 -5.25124  0.0000
I(age - 11)            0.784375 0.1261673  6.21694  0.0000
SexFemale:I(age - 11) -0.304830 0.1976661 -1.54214  0.1261

 Correlation: 
                      (Intr) SexFml I(-11)
SexFemale             -0.638              
I(age - 11)            0.000  0.000       
SexFemale:I(age - 11)  0.000  0.000 -0.638

Standardized residuals:
        Min          Q1         Med          Q3         Max 
-2.48814895 -0.58569115 -0.07451734  0.58924709  2.32476465 

Residual standard error: 2.256949 
Degrees of freedom: 108 total; 104 residual
</code></pre>

<hr>

<p>Notice that the estimates of the fixed effects are the same (to 6 decimal places), but the standard errors are different, as is the correlation matrix.</p>
"
1437399,134830,2009-09-17T08:10:03Z,1435937,0,FALSE,"<p><a href=""http://www.textpad.com/"" rel=""nofollow noreferrer"">TextPad</a> is a well featured editor <a href=""http://www.textpad.com/add-ons/synn2t.html"" rel=""nofollow noreferrer"">supporting R syntax</a> that allows you to specify the target platform for files (Win/UNIX/Mac/keep current encoding) when you save them.  The only problem with it is that some of the keyboard shortcuts are nonstandard (e.g. 'Find' is F5, not F3).</p>
"
1439530,143305,2009-09-17T15:18:23Z,1439348,7,TRUE,"<p>When you say </p>

<blockquote>
  <p>the function did do other things
  depending on the class of the object
  thrown at it</p>
</blockquote>

<p>you are already at the heart of the S3 dispatch mechanism! So
I would recommend reading a programming book on R as e.g. </p>

<ul>
<li>(classic but dated) Venables/Ripley ""S Programming"", </li>
<li>Gentleman ""Bioinformatics with R"", </li>
<li>Brown/Murdoch ""First Course in Statistical Programming with R"", </li>
<li>Chambers ""Software for Data Analysis: Programming with R"", </li>
</ul>

<p>or other resources from <a href=""https://stackoverflow.com/questions/192369/books-for-learning-the-r-language"">this SO question on R books</a> along with an example package or two from the rich set of CRAN packages.</p>
"
1439541,143305,2009-09-17T15:20:06Z,1439513,14,FALSE,"<p>Did you look at </p>

<pre><code>?LETTERS
</code></pre>

<p>and doesn't that do what you want?  Else there are <code>paste()</code> and related functions.</p>

<p><em>Edit:</em> Maybe the <code>collapse=</code> to paste is what you need:</p>

<pre><code>R&gt; replicate(5, paste(sample(LETTERS, 10, replace=TRUE), collapse=""""))
[1] ""OHZBIYEFMD"" ""UINBOFEIXN"" ""UORJZATYNT"" ""ZNPWNBFFXJ"" ""ZOKYMTCDKZ""
R&gt; 
</code></pre>
"
1439843,84458,2009-09-17T16:15:38Z,1439513,31,TRUE,"<p>Is this what you're looking for?</p>

<pre><code>&gt; paste(""This_"",letters,sep="""")

&gt; [1] ""This_a"" ""This_b"" ""This_c"" ""This_d"" ""This_e"" ""This_f"" ""This_g"" ""This_h""
  [9] ""This_i"" ""This_j"" ""This_k"" ""This_l"" ""This_m"" ""This_n"" ""This_o"" ""This_p""
  [17] ""This_q"" ""This_r"" ""This_s"" ""This_t"" ""This_u"" ""This_v"" ""This_w"" ""This_x""
  [25] ""This_y"" ""This_z""
</code></pre>
"
1439894,170352,2009-09-17T16:24:55Z,1439513,8,FALSE,"<p>Thanks guys! I figured it out! </p>

<pre><code>paste(letters[1:26])
</code></pre>

<p>Cheers!</p>
"
1441788,114917,2009-09-17T23:11:05Z,1441717,0,FALSE,"<p>Depending on your application, a long way around might be to use something like this:</p>

<p><a href=""http://googlemapsmania.blogspot.com/2006/07/new-google-maps-us-zip-code-mashups.html"" rel=""nofollow noreferrer"">http://googlemapsmania.blogspot.com/2006/07/new-google-maps-us-zip-code-mashups.html</a></p>

<p>To map your data. If that wasn't quite what you wanted, you can get raw zip code shapefiles from census.gov and do it manually, which is quite a pain.</p>

<p>Also, if you haven't seen it, this is a neat way to interact with similar data, and might offer some pointers:</p>

<p><a href=""http://benfry.com/zipdecode/"" rel=""nofollow noreferrer"">http://benfry.com/zipdecode/</a></p>
"
1441822,97160,2009-09-17T23:19:55Z,1441717,0,FALSE,"<p>Check out this excellent online visualization tool by IBM
<a href=""http://manyeyes.alphaworks.ibm.com/manyeyes/"" rel=""nofollow noreferrer"">http://manyeyes.alphaworks.ibm.com/manyeyes/</a></p>

<p><strong>EDIT</strong> FYI, ManyEyes uses the <a href=""http://prefuse.org/gallery/"" rel=""nofollow noreferrer"">Prefuse visualization toolkit</a> for some of its viz. Even though it is a java-based framework, they also provide a Flash/ActionScript tool for the web.</p>
"
1441833,165787,2009-09-17T23:21:25Z,1441717,3,FALSE,"<p>Someone may have something more direct for you, but I found O'Reilly's 'Data Mashups in R' very interesting... in part, it's a spatial mapping of home foreclosure auctions.</p>

<p><a href=""http://oreilly.com/catalog/9780596804770/"" rel=""nofollow noreferrer"">http://oreilly.com/catalog/9780596804770/</a></p>
"
1441862,144157,2009-09-17T23:31:39Z,1439348,4,FALSE,"<p>For example, plot() will do different things depending on the object. You can see the specific plot functions (called methods) using plot.ts(), plot.lm(), etc. i.e., plot() will call plot.ts() if a ts object is passed. In general, plot.xxx() is applied to objects of class xxx. If there is no specific method for the class, then plot.default() is used.</p>

<p>The function plot() is called a generic function because it can apply to many different classes. Other common generic functions are summary(), print() and predict().</p>

<p>As Dirk says, it is well worth reading the documentation on S3 methods and classes. </p>
"
1442241,NA,2009-09-18T01:48:30Z,1441717,1,FALSE,"<p>There is a rich and sophisticated series of packages in R to plot, do analysis, and other functions related to GIS.  One place to get started is the CRAN task view on <a href=""http://cran.r-project.org/web/views/Spatial.html"" rel=""nofollow noreferrer"">Spatial Data</a>:
This is a complex and sometimes arcane world, and takes some work to understand. </p>

<p>If you are looking for a free, very functional mapping application, may I suggest:</p>

<p>MapWindow ( mapwindow.com)</p>
"
1442252,143319,2009-09-18T01:50:41Z,1441717,1,FALSE,"<p><a href=""http://trends.techcrunch.com/"" rel=""nofollow noreferrer"">Daniel Levine at TechCrunch Trends</a> has done nice things with the <code>maps</code> package in R.  He has code available on his site, too.</p>

<p>Paul's suggestion of looking into Processing - which Ben Fry used to make zipdecode - is also a good one, if you're up for learning a (Java-like) new language.</p>
"
1442254,163053,2009-09-18T01:51:40Z,1441717,9,FALSE,"<p>There are many ways to do this in R (see the <a href=""http://cran.r-project.org/web/views/Spatial.html"" rel=""noreferrer"">spatial view</a>); many of these <a href=""http://cran.r-project.org/web/packages/maps/index.html"" rel=""noreferrer"">depend on the ""maps"" package</a>.</p>

<ul>
<li><p>Check out this <a href=""http://www.ai.rug.nl/~hedderik/R/US2004/"" rel=""noreferrer"">cool example of the US 2004 election</a>.  It ends up looking like this:<a href=""http://www.ai.rug.nl/~hedderik/R/US2004/US04Election-PopGraded.png"" rel=""noreferrer"">alt text http://www.ai.rug.nl/~hedderik/R/US2004/US04Election-PopGraded.png</a></p></li>
<li><p>Here's a slightly ugly example of a model <a href=""http://addictedtor.free.fr/graphiques/sources/source_146.R"" rel=""noreferrer"">that uses the ""maps"" package with ""lattice"".</a></p></li>
<li>Andrew Gelman made some very nice plots like this.  See, for instance, <a href=""http://www.stat.columbia.edu/~cook/movabletype/archives/2009/03/how_went_the_20.html"" rel=""noreferrer"">this blog post on red states/blue states</a> and <a href=""http://www.stat.columbia.edu/~cook/movabletype/archives/2009/05/discussion_and.html"" rel=""noreferrer"">this follow up post</a>.</li>
<li><p>Here's a very simple example <a href=""http://cran.r-project.org/web/packages/gmaps/index.html"" rel=""noreferrer"">using the ""gmaps"" package</a>, which shows a map of Arrests by state for arrests per 100,000 for Murder:</p>

<pre><code>require(gmaps)
data(USArrests)
attach(USArrests)
grid.newpage()
grid.frame(name=""map"")
grid.pack(""map"",USALevelPlot(states=rownames(USArrests),levels=Murder,col.fun=reds),height=unit(1,'null'))
grid.pack(""map"",gradientLegendGrob(at=quantile(Murder),col.fun=reds),side=""bottom"",height=unit(.2,'npc'))
detach(USArrests)
</code></pre></li>
</ul>
"
1444451,143305,2009-09-18T13:08:09Z,1444306,8,FALSE,"<p>""It's hard"". Much of this is context-dependent and you may have to embed this into your application:</p>

<ul>
<li>Does the data drift, trend, or cycle ?</li>
<li>Is the variability fixed or is it itself variable ?</li>
<li>Are there other series you can use for 'benchmarking' ?</li>
</ul>

<p>Other than the outliers packages there is also the <a href=""http://cran.r-project.org/web/packages/qcc/index.html"" rel=""noreferrer""><strong>qcc</strong></a> package as the quality control literature covers this area.</p>

<p>There are many other areas you could look at as e.g. the <a href=""http://cran.r-project.org/web/views/Robust.html"" rel=""noreferrer""><strong>robust statistics Task View</strong></a>.</p>
"
1444512,134830,2009-09-18T13:20:27Z,1439348,26,FALSE,"<p>My thought process for finding function code tends to be:</p>

<p>First, type the name of the function</p>

Case 1: It shows the code

<p>Great, you're done.  </p>

<p>Example: diag</p>

Case 2: The function comes up as a one line UseMethod statement

<p>You have an S3 method.
Type <code>methods(fnname)</code> to see available methods, 
then <code>getAnywhere(fnname.myclass)</code>.</p>

<p>Example: mean</p>

<pre><code>methods(mean)
getAnywhere(mean.default)
</code></pre>

Case 3: The function contains a .Internal or .Primitive statement

<p>The function is written in C, for improved performance.
Download a copy of the R source code and extract the tarball.
Search in the src directory for the function name.</p>

<p><strong>EDIT:</strong> You can also search for the file using Google or Yahoo site search.</p>

<pre><code>site:https://svn.r-project.org/R/trunk/src functionname
</code></pre>

<p><strong>End EDIT</strong></p>

<p>Example: qnorm</p>

<p>A simple windows search for ""qnorm"" in the src directory of the R source code reveals the file qnorm.c, which contains the function definition.</p>

<p><strong>EDIT:</strong> 
qnorm.c is also the top result from the search</p>

<pre><code>site:https://svn.r-project.org/R/trunk/src qnorm
</code></pre>

<p><strong>End EDIT</strong></p>

Case 4: Still can't find the function

<p>It's probably a method of an S4 class.</p>

<p>Type <code>class(myobj)</code> to find the class.</p>

<p>Type <code>showMethods(class=""myclass"")</code> to find available methods for that class.</p>

<p>Type <code>getMethods(""fnname"", ""myclass"")</code>.</p>

<p>Example: plot pixmap</p>

<p>This requires the pixmap package.</p>

<pre><code>library(pixmap)
pixie &lt;- pixmap(1:12, nrow=3, ncol=4)
class(pixie)   #""pixmap""
showMethods(class=""pixmap"")
getMethod(""plot"", ""pixmap"")
</code></pre>
"
1444548,74658,2009-09-18T13:28:15Z,1444306,22,TRUE,"<p>I agree with Dirk, It's hard.  I would recomend first looking at why you might have outliers.  An outlier is just a number that someone thinks is suspicious, it's not a concrete 'bad' value, and unless you can find a reason for it to be an outlier, you may have to live with the uncertainty.</p>

<p>One thing you didn't mention was what kind of outlier you're looking at.  Are your data clustered around a mean, do they have a particular distribution or is there some relationship between your data. </p>

<p>Here's some examples</p>

<p>First, we'll create some data, and then taint it with an outlier;</p>

<pre><code>&gt; testout&lt;-data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5),Y=rnorm(50,mean=200,sd=25))
&gt; #Taint the Data
&gt; testout$X1[10]&lt;-5
&gt; testout$X2[10]&lt;-5
&gt; testout$Y[10]&lt;-530

&gt; testout
         X1         X2        Y
1  44.20043  1.5259458 169.3296
2  40.46721  5.8437076 200.9038
3  48.20571  3.8243373 189.4652
4  60.09808  4.6609190 177.5159
5  50.23627  2.6193455 210.4360
6  43.50972  5.8212863 203.8361
7  44.95626  7.8368405 236.5821
8  66.14391  3.6828843 171.9624
9  45.53040  4.8311616 187.0553
10  5.00000  5.0000000 530.0000
11 64.71719  6.4007245 164.8052
12 54.43665  7.8695891 192.8824
13 45.78278  4.9921489 182.2957
14 49.59998  4.7716099 146.3090
&lt;snip&gt;
48 26.55487  5.8082497 189.7901
49 45.28317  5.0219647 208.1318
50 44.84145  3.6252663 251.5620
</code></pre>

<p>It's often most usefull to examine the data graphically (you're brain is much better at spotting outliers than maths is)</p>

<pre><code>&gt; #Use Boxplot to Review the Data
&gt; boxplot(testout$X1, ylab=""X1"")
&gt; boxplot(testout$X2, ylab=""X2"")
&gt; boxplot(testout$Y, ylab=""Y"")
</code></pre>

<p>Then you can use a test.  If the test returns a cut off value, or the value that might be an outlier, you can use ifelse to remove it</p>

<pre><code>&gt; #Use Outlier test to remove individual values
&gt; testout$newX1&lt;-ifelse(testout$X1==outlier(testout$X1),NA,testout$X1)
&gt; testout
         X1         X2        Y    newX1
1  44.20043  1.5259458 169.3296 44.20043
2  40.46721  5.8437076 200.9038 40.46721
3  48.20571  3.8243373 189.4652 48.20571
4  60.09808  4.6609190 177.5159 60.09808
5  50.23627  2.6193455 210.4360 50.23627
6  43.50972  5.8212863 203.8361 43.50972
7  44.95626  7.8368405 236.5821 44.95626 
8  66.14391  3.6828843 171.9624 66.14391 
9  45.53040  4.8311616 187.0553 45.53040
10  5.00000  5.0000000 530.0000       NA 
11 64.71719  6.4007245 164.8052 64.71719 
12 54.43665  7.8695891 192.8824 54.43665 
13 45.78278  4.9921489 182.2957 45.78278 
14 49.59998  4.7716099 146.3090 49.59998 
15 45.07720  4.2355525 192.9041 45.07720 
16 62.27717  7.1518606 186.6482 62.27717 
17 48.50446  3.0712422 228.3253 48.50446 
18 65.49983  5.4609713 184.8983 65.49983 
19 44.38387  4.9305222 213.9378 44.38387 
20 43.52883  8.3777627 203.5657 43.52883 
&lt;snip&gt;
49 45.28317  5.0219647 208.1318 45.28317 
50 44.84145  3.6252663 251.5620 44.84145
</code></pre>

<p>Or for more complicated examples, you can use stats to calculate critical cut off values, here using the Lund Test (See Lund, R. E. 1975, ""Tables for An Approximate Test for Outliers in Linear Models"", Technometrics, vol. 17, no. 4, pp. 473-476. and Prescott, P. 1975, ""An Approximate Test for Outliers in Linear Models"", Technometrics, vol. 17, no. 1, pp. 129-132.)</p>

<pre><code>&gt; #Alternative approach using Lund Test
&gt; lundcrit&lt;-function(a, n, q) {
+ # Calculates a Critical value for Outlier Test according to Lund
+ # See Lund, R. E. 1975, ""Tables for An Approximate Test for Outliers in Linear Models"", Technometrics, vol. 17, no. 4, pp. 473-476.
+ # and Prescott, P. 1975, ""An Approximate Test for Outliers in Linear Models"", Technometrics, vol. 17, no. 1, pp. 129-132.
+ # a = alpha
+ # n = Number of data elements
+ # q = Number of independent Variables (including intercept)
+ F&lt;-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
+ crit&lt;-((n-q)*F/(n-q-1+F))^0.5
+ crit
+ }

&gt; testoutlm&lt;-lm(Y~X1+X2,data=testout)

&gt; testout$fitted&lt;-fitted(testoutlm)

&gt; testout$residual&lt;-residuals(testoutlm)

&gt; testout$standardresid&lt;-rstandard(testoutlm)

&gt; n&lt;-nrow(testout)

&gt; q&lt;-length(testoutlm$coefficients)

&gt; crit&lt;-lundcrit(0.1,n,q)

&gt; testout$Ynew&lt;-ifelse(abs(testout$standardresid)&gt;crit,NA,testout$Y)

&gt; testout
         X1         X2        Y    newX1   fitted    residual standardresid
1  44.20043  1.5259458 169.3296 44.20043 209.8467 -40.5171222  -1.009507695
2  40.46721  5.8437076 200.9038 40.46721 231.9221 -31.0183107  -0.747624895
3  48.20571  3.8243373 189.4652 48.20571 203.4786 -14.0134646  -0.335955648
4  60.09808  4.6609190 177.5159 60.09808 169.6108   7.9050960   0.190908291
5  50.23627  2.6193455 210.4360 50.23627 194.3285  16.1075799   0.391537883
6  43.50972  5.8212863 203.8361 43.50972 222.6667 -18.8306252  -0.452070155
7  44.95626  7.8368405 236.5821 44.95626 223.3287  13.2534226   0.326339981
8  66.14391  3.6828843 171.9624 66.14391 148.8870  23.0754677   0.568829360
9  45.53040  4.8311616 187.0553 45.53040 214.0832 -27.0279262  -0.646090667
10  5.00000  5.0000000 530.0000       NA 337.0535 192.9465135   5.714275585
11 64.71719  6.4007245 164.8052 64.71719 159.9911   4.8141018   0.118618011
12 54.43665  7.8695891 192.8824 54.43665 194.7454  -1.8630426  -0.046004311
13 45.78278  4.9921489 182.2957 45.78278 213.7223 -31.4266180  -0.751115595
14 49.59998  4.7716099 146.3090 49.59998 201.6296 -55.3205552  -1.321042392
15 45.07720  4.2355525 192.9041 45.07720 213.9655 -21.0613819  -0.504406009
16 62.27717  7.1518606 186.6482 62.27717 169.2455  17.4027250   0.430262983
17 48.50446  3.0712422 228.3253 48.50446 200.6938  27.6314695   0.667366651
18 65.49983  5.4609713 184.8983 65.49983 155.2768  29.6214506   0.726319931
19 44.38387  4.9305222 213.9378 44.38387 217.7981  -3.8603382  -0.092354925
20 43.52883  8.3777627 203.5657 43.52883 228.9961 -25.4303732  -0.634725264
&lt;snip&gt;
49 45.28317  5.0219647 208.1318 45.28317 215.3075  -7.1756966  -0.171560291
50 44.84145  3.6252663 251.5620 44.84145 213.1535  38.4084869   0.923804784
       Ynew
1  169.3296
2  200.9038
3  189.4652
4  177.5159
5  210.4360
6  203.8361
7  236.5821
8  171.9624
9  187.0553
10       NA
11 164.8052
12 192.8824
13 182.2957
14 146.3090
15 192.9041
16 186.6482
17 228.3253
18 184.8983
19 213.9378
20 203.5657
&lt;snip&gt;
49 208.1318
50 251.5620
</code></pre>

<p>Edit: I've just noticed an issue in my code.  The Lund test produces a critical value that should be compared to the absolute value of the studantized residual (i.e. without sign)</p>
"
1444923,16632,2009-09-18T14:28:19Z,1444306,28,FALSE,"<p>If you're worried about outliers, instead on throwing them out, use a robust method. For example, instead of lm, use rlm. </p>
"
1446050,163053,2009-09-18T18:04:18Z,1445964,33,TRUE,"<p>This won't give you the line number, but it will tell you where the failure happens in the call stack which is very helpful:</p>

<pre><code>traceback()
</code></pre>

<p><i>[Edit:]</i> When running a script from the command line you will have to skip one or two calls, see <a href=""https://stackoverflow.com/questions/13116099/traceback-for-interactive-and-non-interactive-r-sessions"">traceback() for interactive and non-interactive R sessions</a></p>

<p>I'm not aware of another way to do this without the usual debugging suspects:</p>

<ol>
<li>debug()</li>
<li>browser()</li>
<li>options(error=recover) [followed by options(error = NULL) to revert it]</li>
</ol>

<p><a href=""https://stackoverflow.com/questions/1412582/getting-the-state-of-variables-after-an-error-occurs-in-r"">You might want to look at this related post.</a></p>

<p><i>[Edit:]</i> Sorry...just saw that you're running this from the command line.  In that case I would suggest working with the options(error) functionality.  Here's a simple example:</p>

<pre><code>options(error = quote({dump.frames(to.file=TRUE); q()}))
</code></pre>

<p>You can create as elaborate a script as you want on an error condition, so you should just decide what information you need for debugging.</p>

<p>Otherwise, if there are specific areas you're concerned about (e.g. connecting to a database), then wrap them in a tryCatch() function.</p>
"
1446122,143305,2009-09-18T18:17:49Z,1445964,10,FALSE,"<p>Support for this will be forthcoming in R 2.10 and later.  Duncan Murdoch just posted to r-devel on Sep 10 2009 about <a href=""http://www.nabble.com/findLineNum-and-setBreakpoint-added-td25381730.html"" rel=""nofollow noreferrer"">findLineNum and setBreapoint</a>:</p>

<pre><code>I've just added a couple of functions to R-devel to help with
debugging.  findLineNum() finds which line of which function corresponds
to a particular line of source code; setBreakpoint() takes the output of
findLineNum, and calls trace() to set a breakpoint there.

These rely on having source reference debug information in the code.
This is the default for code read by source(), but not for packages.  To
get the source references in package code, set the environment variable
R_KEEP_PKG_SOURCE=yes, or within R, set options(keep.source.pkgs=TRUE),
then install the package from source code.  Read ?findLineNum for
details on how to
tell it to search within packages, rather than limiting the search to
the global environment.

For example,

x &lt;- "" f &lt;- function(a, b) {
             if (a &gt; b)  {
                 a
             } else {
                 b
             }
         }""


eval(parse(text=x))  # Normally you'd use source() to read a file...

findLineNum(""&lt;text&gt;#3"")   # &lt;text&gt; is a dummy filename used by parse(text=)

This will print

 f step 2,3,2 in &lt;environment: R_GlobalEnv&gt;

and you can use

setBreakpoint(""&lt;text&gt;#3"")

to set a breakpoint there.

There are still some limitations (and probably bugs) in the code; I'll
be fixing thos
</code></pre>
"
1446144,143377,2009-09-18T18:21:07Z,1441717,36,TRUE,"<p>I am assuming you want static maps. </p>

<p><a href=""http://files.eduardoleoni.com/mapUS.png"" rel=""nofollow noreferrer"">alt text http://files.eduardoleoni.com/mapUS.png</a></p>

<p>1) Get the shapefiles of the <a href=""http://www.census.gov/geo/www/cob/zt_metadata.html"" rel=""nofollow noreferrer"">zip</a> boundaries and <a href=""http://www.census.gov/geo/www/cob/st2000.html"" rel=""nofollow noreferrer"">state</a> boundaries at census.gov: </p>

<p>2) Use the plot.heat function I posted in this <a href=""https://stackoverflow.com/questions/1260965/developing-geographic-thematic-maps-with-r"">SO question</a>.</p>

<p>For example (assumes you have the maryland shapefiles in the map subdirectory):</p>

<pre><code>library(maptools)
##substitute your shapefiles here
state.map &lt;- readShapeSpatial(""maps/st24_d00.shp"")
zip.map &lt;- readShapeSpatial(""maps/zt24_d00.shp"")
## this is the variable we will be plotting
zip.map@data$noise &lt;- rnorm(nrow(zip.map@data))
## put the lab point x y locations of the zip codes in the data frame for easy retrieval
labelpos &lt;- data.frame(do.call(rbind, lapply(zip.map@polygons, function(x) x@labpt)))
names(labelpos) &lt;- c(""x"",""y"")                        
zip.map@data &lt;- data.frame(zip.map@data, labelpos)
## plot it
png(file=""map.png"")
## plot colors
plot.heat(zip.map,state.map,z=""noise"",breaks=c(-Inf,-2,-1,0,1,2,Inf))
## plot text
with(zip.map@data[sample(1:nrow(zip.map@data), 10),] , text(x,y,NAME))
dev.off()
</code></pre>
"
1448619,126042,2009-09-19T14:14:20Z,1448600,5,FALSE,"<p>As for changing the prompt, the command you're looking for is <code>options</code>, with argument <code>prompt</code>, which I found <a href=""http://www.statmethods.net/interface/customizing.html"" rel=""noreferrer"">here</a>.</p>

<pre>
> options(prompt = ""# Customized R Prompt!\n"")
# Customized R Prompt!
1 + 5
[1] 6
# Customized R Prompt!
</pre>

<p>Setting the prompt to an empty string results in:</p>

<pre>
> options(prompt="""")
Error in options(prompt = """") : invalid value for 'prompt'
</pre>

<p>Which is why I used a comment.  As far as I can tell, R does not have  block comments, which is why  I made the prompt a line comment and put a newline at the end of it -- should be <em>no</em> problems if anyone copy-pastes your session that way.</p>

<p>I'm still looking on the output format for a bit here...  There's some code at <a href=""http://tolstoy.newcastle.edu.au/R/help/05/03/0006.html"" rel=""noreferrer"">this mailling list post</a> that seems to format the output without the [#] blocks, but it's sure not pretty.</p>
"
1448696,143305,2009-09-19T14:50:46Z,1448600,1,FALSE,"<p>You write that</p>

<blockquote>
  <p>one of the things that's always
  frustrated me is that I can't simply
  copy commands and output from R and
  paste them into another R session.</p>
</blockquote>

<p>and I presume you are on Windows with the standard R Windows binary.  I fear that what you have in mind may just not be doable.  But because what you want to do is actually very desirable, people have done in a different way.  From the <a href=""http://ess.r-project.org"" rel=""nofollow noreferrer"">ESS</a> manual:</p>

<blockquote>
  <h1>5 Manipulating saved transcript files</h1>
  
  <p>Inferior S mode records the transcript
  (the list of all commands executed,
  and their output) in the process
  buffer, which can be saved as a
  ""transcript file"", which should
  normally have the suffix `.St'.  The
  most obvious use for a transcript file
  is as a static record of the actions
  you have performed in a particular S
  session.  Sometimes, however, you may
  wish to re-execute commands recorded
  in the transcript file by submitting
  them to a running ESS process.  This
  is what Transcript Mode is for.</p>
  
  <p>If you load file a with the suffix
  `.St' into Emacs, it is placed in S
  Transcript Mode.  [...]</p>
</blockquote>

<p>Switching to Emacs/ESS may however not be compatible with your students. So for directly copying and pasting, your best bet may just be to wrap expressions into <code>dput()</code> first:</p>

<pre><code>R&gt; set.seed(42)
R&gt; x &lt;- rnorm(10)
R&gt; x
 [1]  1.37096 -0.56470  0.36313  0.63286  0.40427 -0.10612  1.51152 -0.09466  2.01842 -0.06271
R&gt; dput(x)
c(1.37095844714667, -0.564698171396089, 0.363128411337339, 0.63286260496104, 
0.404268323140999, -0.106124516091484, 1.51152199743894, -0.0946590384130976, 
2.01842371387704, -0.062714099052421)
R&gt; 
</code></pre>

<p>The last expression can be cut and pasted back to R.</p>
"
1448764,163053,2009-09-19T15:26:40Z,1448600,3,FALSE,"<p>I have two other suggestions:</p>

<p>1) You could write your code in a script file; then you can copy and paste the code without any trouble.  </p>

<p>From the standard R GUI, go File > New Script.  Enter all your code in there, then to run it, just highlight code and press CTRL-R.  Many other R GUI's have similar behavior.  You can still work interactively in this mode; the key difference is that you highlight code and run it rather than hitting ENTER.</p>

<p>2) Use the <code>history()</code> functions.  See the help: ?history.  You can save your console history with this:</p>

<p><code>savehistory(file = "".Rhistory"")</code></p>

<p>Then you can open it as a script file with this command:</p>

<p><code>edit(file="".Rhistory"")</code></p>

<p>You might also want to change the max.show, possibly even as the default in your own .Rprofile.  For example,</p>

<p><code>history(max.show = Inf, reverse = FALSE)</code></p>
"
1448823,161808,2009-09-19T15:58:34Z,1448600,23,FALSE,"<p>You might try:</p>

<p><code>options(prompt="" "", continue="" "")</code></p>

<p>Note the spaces in between the quotes.</p>

<p>The first option makes the prompt disappear. The second erases the ""+"" from appearing on long wrapping lines.</p>
"
1448825,134830,2009-09-19T15:59:46Z,1448600,1,FALSE,"<p>Not sure whether this is supported on all platforms, but on Windows, you can Copy, then right-click and select ""Paste commands only"", which does exactly what you need.  Unfortunately, there's no keyboard shortcut.</p>
"
1449292,143305,2009-09-19T19:23:37Z,1449266,4,FALSE,"<p>Use <code>text()</code>:</p>

<pre><code>plot(1:10, 1:10) 
text(5, 5, ""Foo"")
</code></pre>

<p>and see <code>help(text)</code> for options on placing the text. The function is vectorised so you can
also do something akin to </p>

<pre><code> text(1:10, 1:10, LETTERS[1:10])
</code></pre>

<p>if you have vectors of text and positions.</p>
"
1449300,147427,2009-09-19T19:28:51Z,1449266,23,TRUE,"<pre><code>b0 = 2.5; b1 = 2
n = 100
x = rnorm(n, 20, 15)
y = b0 + b1*x + rnorm(n, 0, 15)
plot(x, y)
plot(x, y, type='n')
text(x, y, round(y, 2), cex=0.45)
text(x, y, round(y, 2), cex=0.8)
text(x, y, paste(round(x, 2), round(y, 2), sep="", ""), cex=0.8) # for (x, y), but this gets cluttered. 
</code></pre>

<p><img src=""https://imgur.com/teajK.png"" alt=""alt text""></p>

<p>Use <code>cex</code> for character size (see help for text). And use <code>plot(x, y, type='n')</code> to set up your window correctly without actually plotting anything.</p>
"
1449418,168137,2009-09-19T20:15:55Z,1448600,1,FALSE,"<p>There was a <a href=""http://r.789695.n4.nabble.com/How-to-avoid-copy-paste-when-copying-code-from-this-list-td905551.html"" rel=""nofollow noreferrer"">recent discussion</a> on the r-help mailing list with several example functions to strip > from pasted code to mimic the ""Paste Commands Only"" command of the Windows R GUI.</p>
"
1449738,168747,2009-09-19T22:53:51Z,1448600,5,FALSE,"<p>Painful way is to regexp an original output. Suppose you have some code:</p>

<pre><code>x &lt;- rnorm(10)
  x

head(USArrests)

lm(y~x+z,
    data.frame(y=rnorm(10),z=runif(10),x=rbinom(10,2,.5))
)
</code></pre>

<p>You can save it to a file and then using readLines read to variable. I do the same, using  textConnection on copied output:</p>

<pre><code>to_edit &lt;- readLines(textConnection(""
&gt; x &lt;- rnorm(10)
&gt;   x
 [1] -0.43409069 -1.05399275  1.53440218  0.05812936  1.62713995 -1.20644184
 [7] -0.15698798 -2.36494897 -0.14440292  1.47182117
&gt; 
&gt; head(USArrests)
           Murder Assault UrbanPop Rape
Alabama      13.2     236       58 21.2
Alaska       10.0     263       48 44.5
Arizona       8.1     294       80 31.0
Arkansas      8.8     190       50 19.5
California    9.0     276       91 40.6
Colorado      7.9     204       78 38.7
&gt; 
&gt; lm(y~x+z,
+ data.frame(y=rnorm(10),z=runif(10),x=rbinom(10,2,.5))
+ )

Call:
lm(formula = y ~ x + z, data = data.frame(y = rnorm(10), z = runif(10),     x = rbinom(10, 2, 0.5)))

Coefficients:
(Intercept)            x            z  
    -0.6460       0.3678       0.3918  
""))
</code></pre>

<p>Now some edits:</p>

<pre><code>id_commands &lt;- grep(""^&gt; |^\\+ "",to_edit) # which are commands or its continuity
to_edit[id_commands] &lt;- sub(""^&gt; |^\\+ "","""",to_edit[id_commands]) # remove promt
to_edit[-id_commands] &lt;- paste(""#"",to_edit[-id_commands]) # comment output
</code></pre>

<p>And the result is:</p>

<pre><code>&gt; writeLines(to_edit) # you can specify file or write on screen
# 
x &lt;- rnorm(10)
  x
#  [1] -0.43409069 -1.05399275  1.53440218  0.05812936  1.62713995 -1.20644184
#  [7] -0.15698798 -2.36494897 -0.14440292  1.47182117

head(USArrests)
#            Murder Assault UrbanPop Rape
# Alabama      13.2     236       58 21.2
# Alaska       10.0     263       48 44.5
# Arizona       8.1     294       80 31.0
# Arkansas      8.8     190       50 19.5
# California    9.0     276       91 40.6
# Colorado      7.9     204       78 38.7

lm(y~x+z,
data.frame(y=rnorm(10),z=runif(10),x=rbinom(10,2,.5))
)
# 
# Call:
# lm(formula = y ~ x + z, data = data.frame(y = rnorm(10), z = runif(10),     x = rbinom(10, 2, 0.5)))
# 
# Coefficients:
# (Intercept)            x            z  
#     -0.6460       0.3678       0.3918  
# 
</code></pre>

<p>It works but as I said it's painful.</p>
"
1449873,160314,2009-09-20T00:08:43Z,1449266,1,FALSE,"<p>similar to Vince's answer except using ggplot2:</p>

<pre><code>b0 = 2.5; b1 = 2
n = 20
x = rnorm(n, 20, 15)
y = b0 + b1*x + rnorm(n, 0, 15)
dat&lt;-data.frame(x,y)
library(ggplot2)
ggplot(data=dat)+geom_text(aes(x=x,y=y),size=4,label=paste(round(x, 2), round(y, 2), sep="", ""))
</code></pre>

<p>character size can be changed by altering the size parameter.</p>
"
1450494,143476,2009-09-20T07:12:02Z,1449266,0,FALSE,"<pre><code>x &lt;- 1/3
plot(1,type=""none"",ann=FALSE)
## text and values only
text(mean(par(""usr"")[1:2]),mean(par(""usr"")[3:4])-par(""cxy"")[2]*2,
     paste(""z = "",round(x,2)))
## text, values, and mathematical expressions
text(mean(par(""usr"")[1:2]),mean(par(""usr"")[3:4]),
     bquote(x^2==.(round(x,2))))
text(mean(par(""usr"")[1:2]),mean(par(""usr"")[3:4])-par(""cxy"")[2],
     substitute(gamma==value,list(value=round(x,2))))
</code></pre>
"
1450659,142651,2009-09-20T09:33:08Z,1449266,2,FALSE,"<p>With ggplot2 you can add both the points and the labels. Putting the aes() in ggplot() has the benefit that this aes() will be the default for all geoms. Hence, in this case you only need to specify the x and values once, but they are used by both geom_point() and geom_text()</p>

<p>The modified code of Ian Fellows would look like this:</p>

<pre><code>b0 &lt;- 2.5
b1 &lt;- 2
n &lt;- 20
dat &lt;- data.frame(x = rnorm(n, 20, 15))
dat$y &lt;- b0 + b1*dat$x + rnorm(n, 0, 15)
dat$text &lt;- with(dat, paste(round(x, 2), round(y, 2), sep="", ""))
library(ggplot2)
ggplot(data=dat, aes(x = x, y = y, label = text)) + geom_point() + geom_text(size=4, hjust = 1, vjust = 1)
</code></pre>
"
1451246,165787,2009-09-20T15:14:32Z,1448600,15,TRUE,"<p>So, I very much like Jake and Marek's solutions. Jake's is straightforward, but doesn't address the output formatting part of the problem. Marek's was a bit cumbersome, so I wrapped it up in a function resulting in</p>

<pre><code>cleanCode &lt;- function() {
  if (.Platform$OS.type == ""unix"" &amp;&amp; .Platform$pkgType == ""mac.binary"") {
    to_edit &lt;- readLines(pipe(""pbpaste"")) # Mac ONLY solution
  } else {
    to_edit &lt;- readLines(""clipboard"") # Windows/Unix solution
  }
  opts &lt;- options()
  cmdPrompts &lt;- paste(""^"", opts$prompt, ""|^"", opts$continue, sep="""")

  # can someone help me here? how to escape the + to \\+, as well as other special chars

  id_commands &lt;- grep(""^&gt; |^\\+ "", to_edit) # which are command or continuation lines
  to_edit[id_commands] &lt;- sub(""^&gt; |^\\+ "", """", to_edit[id_commands]) # remove prompts
  to_edit[-id_commands] &lt;- paste(""  # "", to_edit[-id_commands]) # comment output
  writeLines(to_edit)
}
</code></pre>

<p>which lets me highlight and copy a portion of the interactive session. </p>

<p>So, for example, I can use this to copy</p>

<pre><code>&gt; x &lt;- rnorm(20)
&gt; plot(x)
&gt; summary(x)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-2.34000 -0.86010 -0.21940 -0.43340  0.04383  1.06400 
&gt; str(x)
 num [1:20] -1.568 -0.219 -1.951 1.064 0.768 ...
&gt; sd(x)
[1] 0.8932958
</code></pre>

<p>to the clipboard and with a simple call to</p>

<pre><code>&gt; cleanCode() 
</code></pre>

<p>produces output such as</p>

<pre><code>x &lt;- rnorm(20)
plot(x)
summary(x)
  #      Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
  #  -2.34000 -0.86010 -0.21940 -0.43340  0.04383  1.06400 
str(x)
  #   num [1:20] -1.568 -0.219 -1.951 1.064 0.768 ...
sd(x)
  #  [1] 0.8932958
</code></pre>

<p>which someone could quickly highlight and copy &amp; paste into an R session to execute the code and compare their output. Of course, in this case, they'll get different results, since I'm basing the example on random data.</p>

<p>Thank you Jake, Marek, and everyone else... all of the responses have been helpful!</p>
"
1452263,118402,2009-09-20T23:05:42Z,1452235,3,FALSE,"<p>I haven't used Garvin's package and don't know what is possible along those lines.  However:</p>

<p>Typically people just write computationally intensive functions directly in C/C++/Fortran, after <a href=""http://www.stat.berkeley.edu/~paciorek/computingTips/Profiling_in_R.html"" rel=""nofollow noreferrer"">profiling</a> to find the bottlenecks.  See the <a href=""http://dirk.eddelbuettel.com/code/rcpp.html"" rel=""nofollow noreferrer"">RCpp interface</a> or <a href=""http://gbi.agrsci.dk/~shd/misc/Rdocs/callC/call-C-from-R.pdf"" rel=""nofollow noreferrer"">Calling C functions from R using .C and .Call</a> for examples.  The <a href=""http://scythe.wustl.edu/"" rel=""nofollow noreferrer"">Scythe Statistical Library</a> is also very nice for R users since the syntax/function names are similar.</p>
"
1452330,143305,2009-09-20T23:40:50Z,1452235,17,FALSE,"<p>I had forgotten about the Rice project, it has been a while. I think the operational term here is stated at the top of the project page:  <em>Last Updated 3/8/06</em>.</p>

<p>And we all know R changes a lot. So I have only the standard few pointers for you:</p>

<ul>
<li><a href=""http://www.cs.uiowa.edu/~luke/"" rel=""noreferrer"">Luke Tierney</a>, who not only knows a lot about R internals but equally about byte compilers, has been working on such a project. Nothing ready yet, and it would still work in conjunction with the standard R engine. </li>
<li>Stephen Milborrow has the <a href=""http://www.milbo.users.sonic.net/ra/"" rel=""noreferrer"">Ra</a> extension to R that works with his just-in-time compiler package <a href=""http://cran.r-project.org/package=jit"" rel=""noreferrer"">jit</a></li>
<li>my Introduction to High-Performance Computing with R tutorials <a href=""http://dirk.eddelbuettel.com/papers/useR2009hpcTutorial.pdf"" rel=""noreferrer"">(most recent tutorial slides from UseR! 2009)</a> covers the profiling, compiling extentions, parallel computing with R, ... part, including 
<a href=""http://dirk.eddelbuettel.com/code/rcpp.html"" rel=""noreferrer"">Rcpp</a> and and a bit about
<a href=""http://dirk.eddelbuettel.com/code/rinside.html"" rel=""noreferrer"">RInside</a>.</li>
</ul>

<p>In short: there is no way have what you desire <em>specific ways to compile and deploy R code without installing R in advance</em>. Sorry. </p>

<p><em>Edit/Update (April 2011):</em> Luke's new <code>compiler</code> package will be part of R 2.13.0 (to be released April 2011) but not 'activated' by default which is expected for R 2.14.0 expected for October 2011. </p>

<p><em>Edit/Update (December 2011):</em>  Prof Tierney just release a <a href=""http://www.divms.uiowa.edu/~luke/R/compiler/compiler.pdf"" rel=""noreferrer"">massive 100+ page paper</a> on the byte-code compiler.</p>
"
1454340,172261,2009-09-21T12:46:52Z,1454211,55,TRUE,"<p>""not run"" encloses code that shouldn't be executed in the <code>example</code> function (e.g. time-consuming code parts, user-interaction, ...).</p>

<p>see e.g. <code>?example</code>:</p>

<p>As detailed in the manual <em>Writing R Extensions</em>, the author of
the help page can markup parts of the examples for two exception
rules</p>

<ul>
<li><p>'dontrun' encloses code that should not be run.</p></li>
<li><p>'dontshow' encloses code that is invisible on help pages, but will
 be run both by the package checking tools, and the
 'example()' function.  This was previously 'testonly', and
 that form is still accepted.</p></li>
</ul>
"
1454375,168747,2009-09-21T12:54:16Z,1454211,20,FALSE,"<p>In <a href=""http://cran.r-project.org/doc/manuals/R-exts.html#Documenting-functions"" rel=""nofollow noreferrer"">""Writing R Extensions""</a> manual, in section about \examples{...} is said that</p>

<blockquote>
  <p>You can use \dontrun{} for text that should only be shown, but not run, and \dontshow{} for extra commands for testing that should not be shown to users, but will be run by example()</p>
</blockquote>

<p>When you build a package then all code in \dontrun{} closure is visible in help as </p>

<pre><code>## Not run:
...
## End(**Not run**)
</code></pre>

<p>edit: <a href=""https://stackoverflow.com/questions/1454211/what-does-not-run-mean-in-r-help-pages/1454340#1454340"">This answer</a> was earlier.</p>
"
1454382,143305,2009-09-21T12:56:26Z,1453505,7,FALSE,"<p>Duncan Temple Lang has a package <a href=""http://www.omegahat.org/FlashMXML"" rel=""nofollow noreferrer"">FlashMXML</a> that it provides (and I quote):</p>

<ul>
<li>various graphics devices for R that generate the plot as ActionScript code that can be displayed within a Flash application, and</li>
<li>tools to generate MXML content from R data</li>
<li>an R function for compiling MXML files to SWF binaries to be run by a Flash Player in a Web browser or stand-alone.</li>
</ul>
"
1454383,143591,2009-09-21T12:56:34Z,1454211,4,FALSE,"<p>C &amp; p from  Chapter 5.4 (R Documentation Files) of the MUST-TO-READ <a href=""http://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf"" rel=""nofollow noreferrer"">Creating R Packages: A Tutorial</a> by Friedrich Leisch:</p>

<blockquote>
  <p>The examples section should contain
  executable R code, and automatically
  running the code  is part of checking
  a package. There are two special
  markup commands for the examples:</p>
  
  <p><strong>dontrun</strong>: Everything inside \dontrun{}
  is not executed by the tests or
  example(). This is useful,  e.g., for
  interactive functions, functions
  accessing the Internet etc.. Do not
  misuse it to make  life easier for you
  by giving examples which cannot be
  executed.</p>
</blockquote>
"
1454579,162832,2009-09-21T13:45:26Z,1449266,0,FALSE,"<p>Maybe this can help as well</p>

<pre><code># example data
dat &lt;- data.frame(name = sample(letters[1:4],20, replace=T), x1 = rnorm(20,2), x2 = 42+x1*rnorm(20,0,2))
# plot the data
plot(dat$x1,dat$x2)
# use identify to print name for each 'dot' that you click with left mouse
identify(dat$x1,dat$x2,labels=name)
# When done identifying point, click with right mousebutton.
</code></pre>

<p>I like this functionality for interactive purposes. Dont know how to achieve this in ggplot though</p>
"
1455556,163053,2009-09-21T16:55:21Z,1453505,3,TRUE,"<p>This doesn't directly address your need, but the <a href=""http://animation.yihui.name/animation:start"" rel=""nofollow noreferrer"">animation package also allows you to create flash output</a>, but just of static animations.  <a href=""http://www.r-project.org/conferences/useR-2008/slides/Xie.pdf"" rel=""nofollow noreferrer"">Here are the slides from UseR! last year</a>.</p>

<p><i>Edit:</i> You should also check out <a href=""http://blog.revolution-computing.com/2009/06/animate-r-graphics-with-flash.html"" rel=""nofollow noreferrer"">this blog post from David Smith</a>.</p>
"
1457268,142651,2009-09-21T22:41:54Z,1457216,5,FALSE,"<p>This is what I do. A darker colour indicates a more important combination of A and B.</p>

<pre><code>dataset &lt;- data.frame(A = sample(LETTERS[1:5], 200, prob = runif(5), replace = TRUE), B = sample(LETTERS[1:5], 200, prob = runif(5), replace = TRUE))
Counts &lt;- as.data.frame(with(dataset, table(A, B)))
library(ggplot2)
ggplot(Counts, aes(x = A, y = B, fill = Freq)) + geom_tile() + scale_fill_gradient(low = ""white"", high = ""black"")
</code></pre>

<p>Or if you prefer lines</p>

<pre><code>library(ggplot2)
dataset &lt;- data.frame(A = sample(letters[1:5], 200, prob = runif(5), replace = TRUE), B = sample(letters[1:5], 200, prob = runif(5), replace = TRUE))
Counts &lt;- as.data.frame(with(dataset, table(A, B)))
Counts$X &lt;- 0
Counts$Xend &lt;- 1
Counts$Y &lt;- as.numeric(Counts$A)
Counts$Yend &lt;- as.numeric(Counts$B)
ggplot(Counts, aes(x = X, xend = Xend, y = Y, yend = Yend, size = Freq)) +
geom_segment() + scale_x_continuous(breaks = 0:1, labels = c(""A"", ""B"")) + 
scale_y_continuous(breaks = 1:5, labels = letters[1:5])
</code></pre>

<p>This third options add labels to the data points using geom_text().</p>

<pre><code>library(ggplot2)
dataset &lt;- data.frame(
    A = sample(letters[1:5], 200, prob = runif(5), replace = TRUE), 
    B = sample(LETTERS[20:26], 200, prob = runif(7), replace = TRUE)
)
Counts &lt;- as.data.frame(with(dataset, table(A, B)))
Counts$X &lt;- 0
Counts$Xend &lt;- 1
Counts$Y &lt;- as.numeric(Counts$A)
Counts$Yend &lt;- as.numeric(Counts$B)
ggplot(Counts, aes(x = X, xend = Xend, y = Y, yend = Yend)) + 
geom_segment(aes(size = Freq)) + 
scale_x_continuous(breaks = 0:1, labels = c(""A"", ""B"")) + 
scale_y_continuous(breaks = -1) + 
geom_text(aes(x = X, y = Y, label = A), colour = ""red"", size = 7, hjust = 1, vjust = 1) + 
geom_text(aes(x = Xend, y = Yend, label = B), colour = ""red"", size = 7, hjust = 0, vjust = 0)
</code></pre>
"
1457599,158065,2009-09-22T00:42:09Z,1457216,7,FALSE,"<p>Since your data is bipartite, I would suggest plotting points in the first factor on one side, points in the other factor on the other, with lines between them, like so:</p>

<p><img src=""https://i.stack.imgur.com/2y2c2.png"" alt=""enter image description here""></p>

<p>The code I used to generate this was:</p>

<pre><code>## Make up data.
data &lt;- data.frame(X1=sample(state.region, 10),
                   X2=sample(state.region, 10))

## Set up plot window.
plot(0, xlim=c(0,1), ylim=c(0,1),
     type=""n"", axes=FALSE, xlab="""", ylab="""")

factor.to.int &lt;- function(f) {
  (as.integer(f) - 1) / (length(levels(f)) - 1)
}

segments(factor.to.int(data$X1), 0, factor.to.int(data$X2), 1,
         col=data$X1)
axis(1, at = seq(0, 1, by = 1 / (length(levels(data$X1)) - 1)),
     labels = levels(data$X1))
axis(3, at = seq(0, 1, by = 1 / (length(levels(data$X2)) - 1)),
     labels = levels(data$X2))
</code></pre>
"
1457832,143377,2009-09-22T02:17:10Z,1457821,8,TRUE,"<p>You want to use rowSums (see the indexing with a character vector.)</p>

<pre><code>tmp &lt;- data.frame(a=1:2,b=3:4,d=5:6)
rowSums(tmp[,c(""a"",""d"")])
</code></pre>

<p>or, more generally, apply:</p>

<pre><code>apply(tmp[,c(""a"",""d"")], 1, sum)
</code></pre>
"
1457841,168139,2009-09-22T02:20:23Z,1457821,1,FALSE,"<p>I just got the answer. I knew I want some sort of sum. I went to the R help to look up ""sum"". And there I found it.
The answer is to follow the link ""colSums"" to ""rowSums"".
So where metrics is a character vector of the relevant column names. The following line produces a vector where all the numbers are added across each row. </p>

<pre><code>rowSums(data.frame[metrics])
</code></pre>

<p>How would one do it if one wanted every value multiplied to each other? I do not see a rowProducts.</p>
"
1457890,163053,2009-09-22T02:44:10Z,1457821,4,FALSE,"<p>There are many ways to do this kind of operation (ie. apply a function across a row or column), but as Eduardo points out, apply is the most basic:</p>

<pre><code>tmp &lt;- data.frame(a=1:2,b=3:4,d=5:6)
apply(tmp, 1, prod)
</code></pre>

<p>This is a very flexible function.  For instance, you can do both operations at once with this call:</p>

<pre><code>apply(tmp, MARGIN=1, function(x) c(sum(x), prod(x)))
</code></pre>

<p>Performing the same analysis across columns is also simple (the MARGIN parameter describes whether you use rows or columns):</p>

<pre><code>apply(tmp, MARGIN=2, function(x) c(sum(x), prod(x)))
</code></pre>
"
1458691,168747,2009-09-22T07:53:47Z,1457821,3,FALSE,"<p>Answering to <a href=""https://stackoverflow.com/questions/1457821/adding-multiple-columns-transforming-with-multiple-variables/1457841#1457841"">Farrel answer</a>:</p>

<p>On <a href=""http://www.rseek.org/"" rel=""nofollow noreferrer"" title=""RSeek"">RSeek</a> for rowProd I found two packages - <a href=""http://cran.r-project.org/web/packages/matrixStats/index.html"" rel=""nofollow noreferrer"" title=""matrixStats"">matrixStats</a> and <a href=""http://cran.r-project.org/web/packages/fUtilities/index.html"" rel=""nofollow noreferrer"" title=""fUtilities"">fUtilities</a>. You could look on them.</p>

<p>Second solution is bit tricky. You can create you expression and evaluate them.</p>

<pre><code>X &lt;- structure(list(
    varA = c(0.98, 0.75, -0.56, -1.43, 0.65, -1.15, -1.52, 0.1, 0.06, 0.76),
    varB = c(-0.12, -0.6, 0.62, 0.9, -0.44, 0.37, 0.62, 0.76, -1.61, -0.26),
    varC = c(-0.5, -0.37, -0.43, -0.7, 0.83, -0.24, -0.57, 0.05, -1.31, 0.7),
    varD = c(-0.06, -0.11, 1.03, -1.76, -0.42, -1.21, -0.62, -1, -1.16, 2.13),
    varE = c(-1.96, 0.69, -1.85, -1.74, -1.47, 1.24, 0.29, -1.18, 0.89, 0.42),
    varF = c(0.29, -0.22, -1.29, 1.19, 0.38, -0.23, -0.5, -1.07, -1.83, 0.58),
    varG = c(0.59, -0.41, -1.37, 0.89, -0.75, 0.95, 0.95, -0.9, 0.71, -1.3)
  ),
  .Names = c(""varA"", ""varB"", ""varC"", ""varD"", ""varE"", ""varF"", ""varG""),
  row.names = c(NA, -10L), class = ""data.frame""
)

metrics &lt;- c(""varB"",""varC"",""varF"")

eval(
  parse( text = paste(metrics,collapse="" * "") ),
  envir = X
)
</code></pre>

<p>Some explanations:</p>

<ul>
<li>paste create a string looks like varB * varC * varF (collapse is for concatenating elements of vector)</li>
<li>parse is to convert text to expression</li>
<li>eval with envir=X is to execute expression within X</li>
</ul>

<p>For your original question you could use collapse=""+"".</p>

<p>edit: if your variables aren't in a data.frame then eval without envir is enough.</p>

<p>edit2: examples of using rowProds from mentioned packages:</p>

<pre><code>matrixStats::rowProds(as.matrix(X[,metrics])) # convert to a matrix is needed
fUtilities::rowProds(X[,metrics]) # without conversion
</code></pre>

<p>I digg in source this functions and:</p>

<ul>
<li>fUtilities use apply, so this is the same as apply(X,1,prod) (this is not efficient soulution)</li>
<li>matrixStats is smart and do something like exp(rowSums(log(X))), so should be faster.</li>
</ul>

<p>Speed tests:</p>

<pre><code>Xm &lt;- matrix(rnorm(50000*8),ncol=8)
Xd &lt;- as.data.frame(Xm)

require(fUtilities)
require(matrixStats)
system.time( matrixStats::rowProds(as.matrix(Xd)) ) 
#   user  system elapsed 
#   0.08    0.02    0.09 
system.time( matrixStats::rowProds(Xm) )
#   user  system elapsed 
#   0.08    0.00    0.08 
system.time( fUtilities::rowProds(Xd) )
#   user  system elapsed 
#   0.52    0.00    0.52 
</code></pre>

<p>Even with conversion to a matrix matrixStats version is faster.</p>
"
1461049,168747,2009-09-22T16:11:50Z,1457216,3,FALSE,"<p>Maybe mosaicplot:</p>

<pre><code>X &lt;- structure(list(
  ID = 1:50,
  A = structure(c(6L, 1L, 2L, 4L, 4L, 3L, 7L, 1L, 3L, 4L, 1L, 1L, 4L, 4L, 1L, 3L, 5L, 5L, 2L, 6L, 6L, 1L, 1L, 1L, 3L, 3L, 5L, 6L, 3L, 2L, 8L, 5L, 2L, 6L, 5L, 2L, 8L, 3L, 5L, 1L, 1L, 6L, 2L, 8L, 8L, 4L, 1L, 2L, 6L, 2L), .Label = c(""AA"",""BB"", ""CC"", ""DD"", ""FF"", ""GG"", ""HH"", ""II""), class = ""factor""),
  B = structure(c(3L, 2L, 6L, 2L, 3L, 6L, 8L, 3L, 1L, 8L, 6L, 3L, 2L, 6L, 7L, 8L, 2L, 6L, 5L, 5L, 2L, 1L, 3L, 2L, 5L, 3L, 1L, 4L, 3L, 8L, 3L, 1L, 2L, 7L, 1L, 5L, 1L, 7L, 5L, 6L, 8L, 5L, 4L, 4L, 2L, 2L, 4L, 5L, 3L, 3L), .Label = c(""RR"", ""SS"", ""TT"", ""UU"", ""VV"", ""XX"", ""YY"", ""ZZ""), class = ""factor"")
  ), .Names = c(""ID"", ""A"", ""B""), class = ""data.frame"", row.names = c(NA, -50L)
)

mosaicplot(with(X,table(A,B)))
</code></pre>

<p>For you example dataset:</p>

<p><img src=""https://imgur.com/fLPqS.png"" alt=""mosaicplot""></p>
"
1461239,177281,2009-09-22T16:50:06Z,1457216,2,FALSE,"<p>Thanks! I think that the connectivity between elements in each class is best visualized by the link graph examples given by both Jonathon and Thierry. Thierry's 2nd which shows the magnitude is definitely where i will start. </p>

<p><strong>update</strong>
thanks everyone for you ideas and tips!</p>

<p>I came acrossthe bipartite package that has functions to visualize this kind of data. I think its a clean visualization of the relationships I am trying to show.</p>

<p>did:</p>

<pre><code>    library(bipartite)
    dataset &lt;- data.frame(
         A = sample(letters[1:5], 200, prob = runif(5), replace = TRUE), 
         B = sample(LETTERS[20:26], 200, prob = runif(7), replace = TRUE)
     )
    datamat &lt;- as.matrix(table(dataset$A, dataset$B))
    visweb(datamat, text = ""interaction"", textsize = .8)
</code></pre>

<p>giving: 
<a href=""http://i34.tinypic.com/2d0o229.png"" rel=""nofollow noreferrer"">visweb result</a></p>

<p>couldnt put image in as a new user :(</p>
"
1463249,79513,2009-09-22T23:58:43Z,95007,48,TRUE,"<p>You're understandably confused.  That documentation is terrible.  I had to go back to the paper its based on (Hyndman, R.J.; Fan, Y. (November 1996). ""Sample Quantiles in Statistical Packages"". <em>American Statistician</em> 50 (4): 361–365. <a href=""http://dx.doi.org/10.2307%2F2684934"" rel=""nofollow noreferrer"">doi:10.2307/2684934</a>) to get an understanding.  Let's start with the first problem.</p>

<blockquote>
  <p>where 1 &lt;= i &lt;= 9, (j-m)/n &lt;= p &lt;  (j-m+1)/ n, x[j] is the jth order statistic, n is the sample size, and m is a constant determined by the sample quantile type. Here gamma depends on the fractional part of g = np+m-j.</p>
</blockquote>

<p>The first part comes straight from the paper, but what the documentation writers omitted was that <code>j = int(pn+m)</code>.  This means <code>Q[i](p)</code> only depends on the two order statistics closest to being <code>p</code> fraction of the way through the (sorted) observations.  (For those, like me, who are unfamiliar with the term, the ""order statistics"" of a series of observations is the sorted series.)</p>

<p>Also, that last sentence is just wrong.  It should read</p>

<blockquote>
  <p>Here gamma depends on the fractional part of np+m, g = np+m-j</p>
</blockquote>

<p>As for <code>m</code> that's straightforward.  <code>m</code> depends on which of the 9 algorithms was chosen.  So just like <code>Q[i]</code> is the quantile function, <code>m</code> should be considered <code>m[i]</code>.  For algorithms 1 and 2, <code>m</code> is 0, for 3, <code>m</code> is -1/2, and for the others, that's in the next part.</p>

<blockquote>
  <p>For the continuous sample quantile types (4 through 9), the sample quantiles can be obtained by linear interpolation between the kth order statistic and p(k):</p>
  
  <p>p(k) = (k - alpha) / (n - alpha - beta + 1), where α and β are constants determined by the type. Further, m = alpha + p(1 - alpha - beta), and gamma = g.</p>
</blockquote>

<p>This is really confusing.  What the documentation calls <code>p(k)</code> is not the same as the <code>p</code> from before.  <code>p(k)</code> is the <a href=""http://mathworld.wolfram.com/PlottingPosition.html"" rel=""nofollow noreferrer"">plotting position</a>.  In the paper, the authors write it as <code>p</code><sub><code>k</code></sub>, which helps.  Especially since in the expression for <code>m</code>, the <code>p</code> is the original <code>p</code>, and the <code>m = alpha + p * (1 - alpha - beta)</code>.  Conceptually, for algorithms 4-9, the points (<code>p</code><sub><code>k</code></sub>, <code>x[k]</code>) are interpolated to get the solution (<code>p</code>, <code>Q[i](p)</code>).  Each algorithm only differs in the algorithm for the <code>p</code><sub><code>k</code></sub>.</p>

<p>As for the last bit, R is just stating what S uses.</p>

<p>The original paper gives a list of 6 ""desirable properties for a sample quantile"" function, and states a preference for #8 which satisfies all by 1.  #5 satisfies all of them, but they don't like it on other grounds (it's more phenomenological than derived from principles).  #2 is what non-stat geeks like myself would consider the quantiles and is what's described in wikipedia.</p>

<p>BTW, in response to <a href=""https://stackoverflow.com/questions/95007/explain-the-quantile-function-in-r/397303#397303"">dreeves answer</a>, Mathematica does things significantly differently.  I think I understand the mapping.  While Mathematica's is easier to understand, (a) it's easier to shoot yourself in the foot with nonsensical parameters, and (b) it can't do R's algorithm #2.  (Here's <a href=""http://mathworld.wolfram.com/Quantile.html"" rel=""nofollow noreferrer"">Mathworld's Quantile page</a>, which states Mathematica can't do #2, but gives a simpler generalization of all the other algorithms in terms of four parameters.)</p>
"
1463463,176958,2009-09-23T01:11:02Z,1463444,0,FALSE,"<p>Well I tried the same, but the component they use in Windows GUI obviously does not have that feature. Where was the word wrapping option in the Mac menu ?</p>
"
1463475,126042,2009-09-23T01:15:01Z,1463444,2,FALSE,"<p>Shane points out that this does not work on the Windows R application, so I'm marking this as CW in case someone on another platform stumbles across this question.</p>

<hr>

<p>I don't have Windows handy to try it on, but you might be able to use <code>options(width=XXX)</code> to accomplish word-wrap.</p>

<pre><code>&gt; rnorm(20)
 [1]  1.5096142  2.5213651  1.6129801  1.2328282  0.1099109  0.7681205
 [7]  0.7408279  0.1853688  0.2679453 -1.4006292  0.5178583 -0.8838526
[13] -1.5162541 -1.5603825 -0.7217159  2.3466593  0.7382550  1.6618710
[19]  1.3201585  0.2872295
&gt; options(width=50)
&gt; rnorm(20)
 [1] -0.990605829 -1.479986280 -0.670011156
 [4]  1.545288381  1.749429922 -0.386976121
 [7]  0.152663018  0.537898605  0.307018436
[10] -1.214402678 -0.066987719 -0.003181806
[13]  0.775656734 -1.084597991  1.419298825
[16]  1.634812239 -0.234720361 -1.232159240
[19] -0.560096460  0.167267767
</code></pre>

<p>And here's the R help for options, for reference:</p>

<blockquote>
  <p>'width': controls the maximum number
  of columns on a line used in
   printing vectors, matrices and arrays, and when filling by
  'cat'.</p>
  
  <p>Columns are normally the same as characters except in CJK
  languages.</p>
  
  <p>You may want to change this if you re-size the window that R
  is running in.  Valid values are 10...10000 with default
  normally 80.  (The limits on valid values are in file
  'Print.h' and can be changed by re-compiling R.)  Some R
  consoles automatically change the value when they are
  resized.</p>
</blockquote>
"
1463622,163053,2009-09-23T02:05:08Z,1463444,3,TRUE,"<p>I think that the issue happens with long strings.  Here I create a character vector:</p>

<pre><code>&gt; z &lt;- ""zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz zzzzzzzzzzzzzzzzzzzzzzzzzzzzzz zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz""
</code></pre>

<p>Then, on Windows, depending on the display size, it will look like this:</p>

<pre><code>&gt; z
[1] ""zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz zzzzzzzzzzzzzzzzzzzzzzz$
</code></pre>

<p>With a $ sign at the end.  Setting the width option has no effect for a long character vector.  In fact, in the Windows GUI, by default it sets the option(width) automatically on resize.  </p>

<p>I don't believe that there are any global options to address this.  You might try using the strwrap() function.  </p>

<pre><code>&gt; strwrap(z, width=60)
[1] ""zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz""   ""zzzzzzzzzzzzzzzzzzzzzzzzzzzzzz""   
[3] ""zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz""   ""zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz""
</code></pre>
"
1467334,143305,2009-09-23T17:00:01Z,1467201,37,TRUE,"<p>""Probably.""  I do it too, and often even in a loop as in</p>

<pre><code>cleanMem &lt;- function(n=10) { for (i in 1:n) gc() }
</code></pre>

<p>Yet that does not, in my experience, restore memory to a pristine state. </p>

<p>So what I usually do is to keep the tasks at hand in script files and execute those using the 'r' frontend (on Unix, and from the 'littler' package).   Rscript is an alternative on that other OS.</p>

<p>That workflow happens to agree with</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/1429907/workflow-for-statistical-analysis-and-report-writing"">workflow-for-statistical-analysis-and-report-writing</a></li>
<li><a href=""https://stackoverflow.com/questions/1358003/tricks-to-manage-the-available-memory-in-an-r-session"">tricks-to-manage-the-available-memory-in-an-r-session</a></li>
</ul>

<p>which we covered here before.</p>
"
1467409,163053,2009-09-23T17:13:42Z,1467201,6,FALSE,"<p>""Maybe.""  I don't really have a definitive answer.  But the help file suggests that there are really only two reasons to call gc():</p>

<ol>
<li>You want a report of memory usage.</li>
<li>After removing a large object, ""it may prompt R to return memory to the operating system.""</li>
</ol>

<p>Since it can slow down a large simulation with repeated calls, I have tended to only do it after removing something large.  In other words, I don't think that it makes sense to systematically call it all the time unless you have good reason to.</p>
"
1468024,163053,2009-09-23T19:10:52Z,1467807,3,TRUE,"<p>It looks like you want to apply a number of different calculations to some data, grouping it by one field (in the example, by state)?  </p>

<p>There are many ways to do this.  See <a href=""https://stackoverflow.com/questions/1407449/for-each-group-summarise-means-for-all-variables-in-dataframe-ddply-split/"">this related question</a>.  </p>

<p>You could use Hadley Wickham's <code>reshape</code> package (see <a href=""http://had.co.nz/reshape/"" rel=""nofollow noreferrer"">reshape homepage</a>).  For instance, if you wanted the mean, sum, and count functions applied to some data grouped by a value (this is meaningless, but it uses the airquality data from reshape):</p>

<pre><code>&gt; library(reshape)
&gt; names(airquality) &lt;- tolower(names(airquality))
&gt; # melt the data to just include month and temp
&gt; aqm &lt;- melt(airquality, id=""month"", measure=""temp"", na.rm=TRUE)
&gt; # cast by month with the various relevant functions
&gt; cast(aqm, month ~ ., function(x) c(mean(x),sum(x),length(x)))
  month X1   X2 X3
1     5 66 2032 31
2     6 79 2373 30
3     7 84 2601 31
4     8 84 2603 31
5     9 77 2307 30
</code></pre>

<p>Or you can use the <code>by()</code> function.  Where the index will represent the states.  In your case, rather than apply one function (e.g. mean), you can apply your own function that will do multiple tasks (depending upon your needs): for instance, <code>function(x) { c(mean(x), length(x)) }</code>.  Then run <code>do.call(""rbind""</code> (for instance) on the output.</p>

<p>Also, you might give some consideration to using a reporting package such as Sweave (with xtable) or <a href=""http://cran.r-project.org/web/packages/brew/index.html"" rel=""nofollow noreferrer"">Jeffrey Horner's brew package</a>.  There is a <a href=""http://learnr.wordpress.com/2009/09/09/brew-creating-repetitive-reports/"" rel=""nofollow noreferrer"">great post on the learnr blog about creating repetitive reports</a> that shows how to use it.  </p>
"
1468767,16632,2009-09-23T22:02:19Z,1467201,7,FALSE,"<p>No.  If there is not enough memory available for an operation, R will run <code>gc()</code> automatically.</p>
"
1468881,135944,2009-09-23T22:32:39Z,1468856,0,FALSE,"<p>OK, I found one slightly obtuse answer, using the <em>plyr</em> package:</p>

<pre><code>&gt; Lobs[laply(names(Lobs), function(x) !identical(Lobs[[x]], Lexp[[x]]))]
$b
[1] ""ni""
</code></pre>

<p>So, it takes the names of the array from the observed function, uses double-bracket indexing and the identical() function to compare the sub-lists, then uses the binary array that results from laply() to index into the original observed function. </p>

<p>Anyone got a better/cleaner/sexier/faster way?</p>
"
1468933,142651,2009-09-23T22:52:55Z,1467807,1,FALSE,"<p>Another options is the plyr package.</p>

<pre><code>library(plyr)
names(airquality) &lt;- tolower(names(airquality))
ddply(airquality, ""month"", function(x){
    with(x, c(meantemp = mean(temp), maxtemp = max(temp), nonsense = max(temp) - min(solar.r)))
})
</code></pre>
"
1469346,135870,2009-09-24T01:09:25Z,1379549,6,FALSE,"<p>I use <a href=""http://www.uoregon.edu/~koch/texshop/"" rel=""nofollow noreferrer"">TeXShop</a> on OS X to produce all of my LaTeX and Sweave reports. For me, a new compilation pipeline is as simple as adding a file, called <code>Sweave.engine</code> to <code>~/Library/TeXShop/Engines/</code> which contains the following:</p>

<pre><code>#!/usr/bin/env Rscript
args &lt;- commandArgs(T)

fname &lt;- strsplit(args[1],'\\.')[[1]][2]

Sweave(paste(fname,'Rnw',sep='.'))

system(paste('pdflatex',paste(fname,'tex',sep='.')))
</code></pre>

<p>Sweave is now a selectable method of compiling a document inside TeXShop. I can set it to be the default for a document by adding the following TeX hash-bang to the top of the file:</p>

<pre><code>% !TEX TS-program = Sweave
</code></pre>

<p>Hitting Cmd-T will typeset the document- the pdf automatically pops up in a separate window. TeXShop also incorporates <a href=""http://www.tug.org/texworks/"" rel=""nofollow noreferrer"">SyncTeX</a> technology so a Cmd-Click in the Rnw source will highlight the corresponding output in the PDF window and a Cmd-Click in the PDF window will highlight the corresponding input in the Rnw source.</p>

<p>TeXShop is mac-only but a great Qt/poppler-based clone, <a href=""http://www.tug.org/texworks/"" rel=""nofollow noreferrer"">TeXworks</a>, is available for Linux, Windows and Mac and supports many of the same features-- including TeX hash-bangs and SyncTeX. TeXworks has reached a level of maturity where it is included in version 2.8 of the <a href=""http://miktex.org/"" rel=""nofollow noreferrer"">MikTeX</a> package for Windows.</p>
"
1469356,170364,2009-09-24T01:12:32Z,1468856,21,TRUE,"<p>At least in this case</p>

<p><code>Lobs[!(Lobs %in% Lexp)]</code></p>

<p>gives you what you want.</p>
"
1469498,163053,2009-09-24T02:21:34Z,1468962,7,TRUE,"<p>I admittedly know very little about this subject, but just to point you in a direction:</p>

<ul>
<li>Have you looked at the cluster package?  It has very good documentation.  In particular, look at help(agnes) for some suggestions.  <a href=""http://stat.ethz.ch/people/maechler"" rel=""noreferrer"">Martin Maechler</a> (a member of the R core team) created the package and has contributed to Stack Overflow discussions before, so hopefully he'll provide an answer here.</li>
<li>The hclust() function is part of the stats package.  In fact, I believe that there are plans to merge hclust() and agnes().</li>
<li>You may also find <a href=""https://wiki.brandeis.edu/twiki/bin/view/Bio/BioConductor#Coefficient_of_Correlation_Clust"" rel=""noreferrer"">this page from the Bioconductor project helpful</a>.</li>
<li>Otherwise, you may have some luck looking at other packages on the CRAN <b><a href=""http://cran.r-project.org/web/views/Cluster.html"" rel=""noreferrer"">Clustering</a></b>, <a href=""http://cran.r-project.org/web/views/NaturalLanguageProcessing.html"" rel=""noreferrer"">Natural Language Processing</a> or <a href=""http://cran.r-project.org/web/views/MachineLearning.html"" rel=""noreferrer"">Machine Learning</a> views.</li>
</ul>
"
1470317,178297,2009-09-24T07:58:00Z,1468962,7,FALSE,"<p>The standard approach would be one that involves <code>cor()</code>, <code>hclust()</code> and <code>plot.hclust()</code>.
I'd highly recommend heatmap.2 from the wonderful gplots package.</p>
"
1471844,134830,2009-09-24T13:55:33Z,1467201,16,FALSE,"<p>From the help page on <a href=""https://www.rdocumentation.org/packages/base/topics/gc"" rel=""nofollow noreferrer""><code>gc</code></a>:</p>

<blockquote>
  <p>A call of 'gc' causes a garbage
  collection to take place. This will
  also take place automatically without
  user intervention, and the primary
  purpose of calling 'gc' is for the
  report on memory usage.</p>
  
  <p>However, it can be useful to call 'gc'
  after a large object has been removed,
  as this may prompt R to return memory
  to the operating system.</p>
</blockquote>

<p>So it <em>can</em> be useful to do, but mostly you shouldn't have to.  My personal opinion is that it is code of last resort - you shouldn't be littering your code with <code>gc()</code> statements as a matter of course, but if your machine keeps falling over, and you've tried everything else, then it might be helpful.</p>

<p>By everything else, I mean things like</p>

<ol>
<li><p>Writing functions rather than raw scripts, so variables go out of scope.</p></li>
<li><p>Emptying your workspace if you go from one problem to another unrelated one.</p></li>
<li><p>Discarding data/variables that you aren't interested in.  (I frequently receive spreadsheets with dozens of uninteresting columns.)</p></li>
</ol>
"
1472125,76235,2009-09-24T14:36:44Z,1468962,1,FALSE,"<p>I went to <a href=""http://www.rseek.org/"" rel=""nofollow noreferrer"">http://www.rseek.org/</a> and entered agnes algorithm and found the CLUSTER package on CRAN has the following function details for function AGNES.</p>

<blockquote>
  <p>Details</p>
  
  <p>agnes is fully described in chapter 5
  of Kaufman and Rousseeuw (1990).
  Compared to other agglomerative
  clustering methods such as hclust,
  agnes has the following features: (a)
  it yields the agglomerative
  coefficient (see agnes.object) which
  measures the amount of clustering
  structure found; and (b) apart from
  the usual tree it also provides the
  banner, a novel graphical display (see
  plot.agnes).</p>
  
  <p>The agnes-algorithm constructs a
  hierarchy of clusterings. At first,
  each observation is a small cluster by
  itself. Clusters are merged until only
  one large cluster remains which
  contains all the observations. At each
  stage the two nearest clusters are
  combined to form one larger cluster.</p>
  
  <p>For method=""average"", the distance
  between two clusters is the average of
  the dissimilarities between the points
  in one cluster and the points in the
  other cluster. In method=""single"", we
  use the smallest dissimilarity between
  a point in the first cluster and a
  point in the second cluster (nearest
  neighbor method). When
  method=""complete"", we use the largest
  dissimilarity between a point in the
  first cluster and a point in the
  second cluster (furthest neighbor
  method).</p>
</blockquote>

<p>Clustering is a pretty huge topic and you'll find many packages for R that implement some form of it. When you have both attributes and covariates, combining clustering with ordination can sometimes yield more insight.</p>
"
1474125,163053,2009-09-24T20:59:32Z,1474081,307,FALSE,"<p>If you have the file locally, then use <code>install.packages()</code> and set the <code>repos=NULL</code>:  </p>

<pre><code>install.packages(path_to_file, repos = NULL, type=""source"")
</code></pre>

<p>Where <code>path_to_file</code> would represent the full path and file name:</p>

<ul>
<li>On Windows it will look something like this: <code>""C:\\RJSONIO_0.2-3.tar.gz""</code>.</li>
<li>On UNIX it will look like this: <code>""/home/blah/RJSONIO_0.2-3.tar.gz""</code>.</li>
</ul>
"
1474236,172261,2009-09-24T21:24:18Z,1474081,64,FALSE,"<p>Download the source package, open Terminal.app and execute</p>

<pre><code>R CMD INSTALL RJSONIO_0.2-3.tar.gz
</code></pre>
"
1474359,143377,2009-09-24T21:50:23Z,1474081,42,FALSE,"<p>You can install directly from the repository (note the <code>type=""source""</code>):</p>

<pre><code>install.packages(""RJSONIO"", repos = ""http://www.omegahat.org/R"", type=""source"")
</code></pre>
"
1475668,147427,2009-09-25T06:06:42Z,1475631,0,FALSE,"<p>I think (1) is the best option. I actually don't think this <em>isn't</em> elegant. I think it would be more computationally intensive to redraw every time you hit a point greater than xlim or ylim.</p>

<p>Also, I saw in Peter Hoff's book about Bayesian statistics a cool use of ts() instead of lines() for cumulative sums/means. It looks pretty spiffy:</p>

<p><img src=""https://imgur.com/gUJfz.png"" alt=""alt text""></p>
"
1475805,170364,2009-09-25T06:52:48Z,1475631,5,TRUE,"<p>I'm not sure if this is possible using base graphics, if someone has a solution I'd love to see it.  However graphics systems based on grid (lattice and ggplot2) allow the graphics object to be saved and updated.  It's insanely easy in ggplot2.</p>

<p><code>require(ggplot2)</code></p>

<p>make some data and get the range: </p>

<pre><code>foo &lt;- as.data.frame(cbind(data=rnorm(100), numb=seq_len(100)))
</code></pre>

<p>make an initial ggplot object and plot it:</p>

<pre><code>p &lt;- ggplot(as.data.frame(foo), aes(numb, data)) + layer(geom='line')
p
</code></pre>

<p>make some more data and add it to the plot</p>

<pre><code>foo &lt;- as.data.frame(cbind(data=rnorm(200), numb=seq_len(200)))

p &lt;- p + geom_line(aes(numb, data, colour=""red""), data=as.data.frame(foo))
</code></pre>

<p>plot the new object</p>

<pre><code>p
</code></pre>
"
1476280,142651,2009-09-25T09:14:23Z,1476185,28,TRUE,"<p>The easiest option is to use geom_smooth() and let ggplot2 fit the model for you.</p>

<pre><code>ggplot(calvarbyruno.1, aes(y = PAR, x = Nominal, weight=Nominal^calweight)) + 
    geom_smooth(method = ""lm"") + 
    geom_smooth(method = ""lm"", formula = y ~ poly(x, 2), colour = ""red"") + 
    geom_point() + 
    coord_flip()
</code></pre>

<p><img src=""https://i.stack.imgur.com/nzXHu.png"" alt=""Illustration using geom_smooth""></p>

<p>Or you can create a new dataset with the predicted values.</p>

<pre><code>newdata &lt;- data.frame(Nominal = pretty(calvarbyruno.1$Nominal, 100))
newdata$Linear &lt;- predict(callin.1, newdata = newdata)
newdata$Quadratic &lt;- predict(calquad.1, newdata = newdata)
require(reshape2)
newdata &lt;- melt(newdata, id.vars = ""Nominal"", variable.name = ""Model"")
ggplot(calvarbyruno.1, aes(x = PAR, y = Nominal, weight=Nominal^calweight)) + 
    geom_line(data = newdata, aes(x = value, colour = Model)) + 
    geom_point()
</code></pre>
"
1476301,178967,2009-09-25T09:17:00Z,1475360,0,FALSE,"<p>@Marek - lag(data) does do what I want, but I wanted to be able to use this as part of an ""apply"" construct to make the vector->matrix abstraction a little easier.</p>
"
1476839,163053,2009-09-25T11:43:05Z,1475360,3,FALSE,"<p>Here's some data:</p>

<pre><code> &gt; x &lt;- zoo(matrix(1:12, 4, 3), as.Date(""2003-01-01"") + 0:3)
 &gt; x
 2003-01-01 1 5  9
 2003-01-02 2 6 10
 2003-01-03 3 7 11
 2003-01-04 4 8 12
</code></pre>

<p>If you want to lag this multivariate time series, just call lag (i.e. no need for apply):</p>

<pre><code> &gt; lag(x)                 
 2003-01-01 2 6 10
 2003-01-02 3 7 11
 2003-01-03 4 8 12
</code></pre>

<p>If you want to apply a function across the rows, it needs to be sensible.  For instance, to get mean of the row values:</p>

<pre><code>&gt; apply(x, 1, mean)
2003-01-01 2003-01-02 2003-01-03 2003-01-04 
         5          6          7          8 
</code></pre>

<p>You can't apply a zoo object and get a zoo object back.  The output of apply is ""a vector or array or list of values"".  In the example above:</p>

<pre><code>&gt; class(apply(x, 1, mean))
[1] ""numeric""
</code></pre>

<p>You need to recreate it as a zoo object and then lag it:</p>

<pre><code>&gt; lag(zoo(apply(coredata(x), 1, mean), index(x)))
 2003-01-01 2003-01-02 2003-01-03 
         6          7          8 
</code></pre>

<p>You need to be slightly careful of the direction of your output.  But you can transpose it if necessary with the <code>t()</code> function.  For instance:</p>

<pre><code>&gt; zoo(t(apply(coredata(x), 1, quantile)), index(x))
           0% 25% 50% 75% 100%
2003-01-01  1   3   5   7    9
2003-01-02  2   4   6   8   10
2003-01-03  3   5   7   9   11
2003-01-04  4   6   8  10   12
</code></pre>

<p>You could also wrap this in a function.  Alternatively you can use one of the apply functions in the <code>xts</code> time series library (this will retain the time series object in the process):</p>

<pre><code>&gt; x &lt;- as.xts(x)
&gt; apply.daily(x, mean)
           [,1]
2003-01-01    5
2003-01-02    6
2003-01-03    7
2003-01-04    8
</code></pre>
"
1477938,37751,2009-09-25T15:19:59Z,1476185,9,FALSE,"<p>Earlier I asked a related question and Hadley had <a href=""https://stackoverflow.com/questions/1297698/plotting-regression-results-from-lme4-in-r-using-lattice-or-something-else/1299539#1299539"">this good answer</a>.  Using the predict function from that post you can add two columns to your data. One for each model:</p>

<pre><code>calvarbyruno.1$calQuad &lt;- predict(calquad.1)
calvarbyruno.1$callin &lt;- predict(callin.1)
</code></pre>

<p>Then it's a matter of plotting the point and adding each model in as a line:</p>

<pre><code>ggplot() + 
geom_point(data=calvarbyruno.1, aes(PAR, Nominal), colour=""green"") + 
geom_line(data=calvarbyruno.1, aes(calQuad, Nominal), colour=""red"" ) + 
geom_line(data=calvarbyruno.1, aes(callin, Nominal), colour=""blue"" ) + 
opts(aspect.ratio = 1)
</code></pre>

<p>And that results in this nice picture (yeah the colors could use some work):</p>

<p><a href=""http://www.cerebralmastication.com/wp-content/uploads/2009/09/ggplot2.png"" rel=""nofollow noreferrer"">alt text http://www.cerebralmastication.com/wp-content/uploads/2009/09/ggplot2.png</a></p>
"
1478578,147427,2009-09-25T17:22:28Z,1478532,5,FALSE,"<pre><code>&gt; y &lt;- rnorm(10)
&gt; b &lt;- as.factor(sample(1:4,10,replace=T))
&gt; qplot(1:10, y, shape=b)
&gt; qplot(1:10, y, pch=letters[1:10], cex=6)
</code></pre>

<p>Is this what you mean? I imagine you can use any of R's plotting characters...</p>

<p>This may not be a very 'ggplot' way of doing this though, but the man page does read ""You can use it like you'd use the 'plot' function."". :-)</p>

<p><img src=""https://imgur.com/N2YDt.png"" alt=""alt text""></p>
"
1478790,88358,2009-09-25T18:08:16Z,1478758,1,FALSE,"<p>Well if you need the entire sequence how fast it can be? assuming that the function is O(1), you cannot do better than O(n), and looping through will give you just that.</p>
"
1478839,135944,2009-09-25T18:21:08Z,1478758,1,FALSE,"<p>In general, the syntax x$y &lt;- f(z) will have to reallocate x every time, which would be very slow if x is a large object. But, it turns out that R has some tricks so that the list replacement function <code>[[&lt;-</code> doesn't reallocate the whole list every time. So I think you can reasonably efficiently do:</p>

<pre><code>x[[1]] &lt;- x1 
for (m in seq(2, n))
    x[[m]] &lt;- f(x[[m-1]])
</code></pre>

<p>The only wasteful aspect here is that you have to generate an array of length n-1 for the for loop, which isn't ideal, but it's probably not a giant issue. You could replace it by a while loop if you preferred. The usual vectorization tricks (lapply, etc.) won't work here...</p>

<p>(The double brackets give you a list element, which is what you probably want, rather than a singleton list.)</p>

<p>For more details, see Chambers (2008). Software for Data Analysis. p. 473-474.</p>
"
1478990,135944,2009-09-25T18:51:48Z,1478532,36,TRUE,"<p>The ggplot way to do it would be to use <code>scale_shape_manual</code> and provide the desired shapes in the <code>values</code> argument:</p>

<pre><code>qplot(1:10, y, shape=b) + scale_shape_manual(values = c(0, 5, 6, 15))
</code></pre>

<p><img src=""https://imgur.com/UAmPd.png"" alt=""result of above""></p>

<p>The shapes are the same as the usual 0-25 indexes: <a href=""http://yusung.blogspot.com/2008/11/plot-symbols-in-r.html"" rel=""noreferrer"">http://yusung.blogspot.com/2008/11/plot-symbols-in-r.html</a></p>
"
1479199,143305,2009-09-25T19:37:54Z,1478758,1,FALSE,"<p>You could consider writing it in C / C++ / Fortran and use the handy <a href=""http://cran.r-project.org/package=inline"" rel=""nofollow noreferrer""><strong>inline</strong></a> package to deal with the compiling, linking and loading for you. </p>

<p>Of course, your function <code>f()</code> may be a real constraint if that one needs to remain an R function.  There is a callback-from-C++-to-R example in <a href=""http://cran.r-project.org/package=Rcpp"" rel=""nofollow noreferrer""><strong>Rcpp</strong></a> but this requires a bit more work than just using inline.</p>
"
1479238,163053,2009-09-25T19:46:22Z,1478758,4,FALSE,"<p>In terms of the question of whether this can be fully ""vectorized"" in any way, I think the answer is probably ""no"".  The fundamental idea behind array programming is that operations apply to an entire set of values at the same time.  Similarly for questions of ""embarassingly parallel"" computation.  In this case, since your recursive algorithm depends on each prior state, there would be no way to gain speed from parallel processing: it must be run serially.  </p>

<p>That being said, the usual advice for speeding up your program applies.  For instance, do as much of the calculation outside of your recursive function as possible.  Sort everything.  Predefine your array lengths so they don't have to grow during the looping.  Etc.  <a href=""http://www.nabble.com/Recursion-is-slow-td25284189.html"" rel=""nofollow noreferrer"">See this question for a similar discussion</a>.  There is also a pseudocode example in <a href=""http://www.insightful.com/Hesterberg/articles/EfficientSplus.txt"" rel=""nofollow noreferrer"">Tim Hesterberg's article on efficient S-Plus Programming</a>.</p>
"
1479252,16632,2009-09-25T19:49:48Z,1478758,3,FALSE,"<p>Solve the recurrence relationship ;)</p>
"
1480749,160553,2009-09-26T08:14:28Z,1428750,4,FALSE,"<p><a href=""http://ess.r-project.org/Manual/ess.html#index-aborting-S-commands-110"" rel=""nofollow noreferrer"">According to the ESS manual</a>, this should work:
<code>C-c C-c</code> (comint-interrupt-subjob)
Sends a <code>Control-C</code> signal to the ESS process. This has the effect of aborting the current command. </p>

<p>John Fox has a website where he offers a configuration for ESS. In it, he has this function:</p>

<pre><code>(defun stop-R ()

""Interrupt R process in lower window.""

(interactive)

(select-window win2)

(comint-interrupt-subjob)

(select-window win1))
</code></pre>

<p>You should be able to add this function to the menu in XEmacs using:</p>

<pre><code>(defun R-menu ()

  ""Hook to install R menu and sub-menus""

  (add-menu-item '(""ESS"" ""R"") ""Interrupt computation"" 'stop-R
)
)
(add-hook 'ess-mode-hook 'R-menu)
</code></pre>

<p>You might check out the rest of his configuration file and documentation to see if it interests you. I haven't tried this yet, but I hope that it works for you!</p>

<p>Charlie</p>
"
1481205,149223,2009-09-26T13:27:33Z,1481032,4,FALSE,"<p>Seems like PBSmapping uses some crude heuristics to work out the projection from the .prj file.  (see help(importShapefile)). I personally don't understand all the stuff inside a prj file but using this website www.spatialreference.org I reckon your map matches </p>

<p><a href=""http://www.spatialreference.org/ref/epsg/26912/"" rel=""nofollow noreferrer"">http://www.spatialreference.org/ref/epsg/26912/</a></p>

<p>Whenever I get a new shape file I find it's projection system on this website and then look for the proj4 string, which in this case is 
""+proj=utm +zone=12 +ellps=GRS80 +datum=NAD83 +units=m +no_defs""</p>

<p>(Like I said I don't know PBSmapping, but you can read this in using maptools as follows)</p>

<pre><code>library(maptools)
sf=readShapeSpatial(""SGID93_DEMOGRAPHIC_CensusTracts2000.shp"",proj4string=CRS(""+proj=utm +zone=12 +ellps=GRS80 +datum=NAD83 +units=m +no_defs""))
</code></pre>

<p>and then convert to latlong using</p>

<pre><code>library(rgdal)

sftransformed=spTransform(sf,CRS(""+proj=longlat""))
</code></pre>

<p>and </p>

<p>plot(sftransformed,axes=T)</p>

<p>gives a plot with the right units on the axes. </p>

<p>Not sure if PBSmapping understands a proj4 string? Looks like it doesn't to be honest.</p>
"
1481913,163053,2009-09-26T19:03:01Z,1481032,5,TRUE,"<p>You may also want to look at these related questions, especially at Eduardo's responses:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/1441717/plotting-color-map-with-zip-codes-in-r-or-python/"">plotting-color-map-with-zip-codes-in-r-or-python</a></li>
<li><a href=""https://stackoverflow.com/questions/1260965/developing-geographic-thematic-maps-with-r"">developing-geographic-thematic-maps-with-r</a></li>
</ul>
"
1484400,163053,2009-09-27T20:29:34Z,1484307,6,FALSE,"<p>You need to remove the axis (by setting <code>yaxt = ""n""</code>) and then re-format it properly:</p>

<pre><code>plot((1:100)^3, log = ""y"", yaxt = ""n"")
axis(2, format(c(1,10,100)^3, scientific=FALSE))
</code></pre>

<p>This was <a href=""http://tolstoy.newcastle.edu.au/R/help/05/06/7640.html"" rel=""nofollow noreferrer"">asked before on R-help</a>.</p>
"
1484420,168137,2009-09-27T20:38:13Z,1484307,3,FALSE,"<p>Additionally, if you just don't like the look of 1e+03 scientific notation, the <a href=""http://cran.r-project.org/web/packages/sfsmisc/index.html"" rel=""nofollow noreferrer"">sfsmisc</a> package has the <code>axTexpr()</code> function to format axis labels in a * 10^k notation.</p>

<pre><code>library(sfsmisc)
example(axTexpr)
</code></pre>
"
1484570,135944,2009-09-27T21:48:27Z,1484472,3,FALSE,"<p>Hadley will doubtless correct me if I'm wrong here...</p>

<p>Here's the natural syntax: </p>

<pre><code>bplot + geom_point(aes(colour=trt), position=position_dodge(width=.5))
</code></pre>

<p>(position=""dodge"" will do the same thing, without the parameter.)</p>

<p>When I plot it, I get something that looks like a position_jitter(), which is presumably what you get too.</p>

<p>Curious, I went to look in the source, where I found the pos_dodge() function. (Type <em>pos_dodge</em> at an R prompt to see it...) Here's the end of it:</p>

<pre><code>within(df, {
  xmin &lt;- xmin + width / n * (seq_len(n) - 1) - diff * (n - 1) / (2 * n)
  xmax &lt;- xmin + d_width / n
  x &lt;- (xmin + xmax) / 2
})
</code></pre>

<p>n is the number of rows of the data frame. So it looks like it's dodging the individual points by a fraction indexed by the row! So the first point is dodged width/n, the second is dodged 2 * width/n, and the last is dodged n * width/n. </p>

<p>This is obviously not what you <em>meant</em>, although it is what you <em>said</em>. You may be stuck recreating the dodged boxplot manually, or using a different visualization, like faceting maybe?</p>

<pre><code>ggplot(to.analyze,aes(inj.site,relief)) + geom_boxplot() + facet_wrap(~ trt)
</code></pre>
"
1484926,135944,2009-09-28T00:37:04Z,1484904,4,TRUE,"<p>One way to do it is to use factors or lists of strings instead of indexes. So:</p>

<pre><code>cause1 &lt;- c(""Maltreat"", ""Non-malt"")[cause]

&gt; print(cause1)
 [1] ""Maltreat"" ""Maltreat"" ""Maltreat"" ""Maltreat"" ""Maltreat"" ""Non-malt""
 [7] ""Maltreat"" ""Non-malt"" ""Non-malt"" ""Non-malt"" ""Non-malt""

&gt; table(cause1, time)
          time
cause1     1 2 3
  Maltreat 2 2 2
  Non-malt 2 3 0
</code></pre>

<p>And, in case you're worried about memory or speed, R is pretty good at representing this sort of thing efficiently internally, with only a single instance of the whole string stored, and the rest done with indexes.</p>

<p>Incidentally, you'll be happier in the long run with data frames:</p>

<pre><code>&gt; df &lt;- data.frame(cause=as.factor(c(""Maltreat"", ""Non-malt"")[cause]), time=time)
&gt; summary(df)
      cause        time      
 Maltreat:6   Min.   :1.000  
 Non-malt:5   1st Qu.:1.000  
              Median :2.000  
              Mean   :1.818  
              3rd Qu.:2.000  
              Max.   :3.000  
&gt; table(df)
          time
cause      1 2 3
  Maltreat 2 2 2
  Non-malt 2 3 0
</code></pre>
"
1484968,163053,2009-09-28T01:00:23Z,1484904,5,FALSE,"<p>There are two easy ways to do this:</p>

<pre><code>z &lt;- table(cause, time)
</code></pre>

<p>Use the colnames/rownames functions:</p>

<pre><code>&gt; colnames(z)
[1] ""1"" ""2"" ""3""
&gt; rownames(z)
[1] ""1"" ""2""
</code></pre>

<p>Or use dimnames:</p>

<pre><code>&gt; dimnames(z)
$cause
[1] ""1"" ""2""
$time
[1] ""1"" ""2"" ""3""
&gt; dimnames(z)$cause
[1] ""1"" ""2""
</code></pre>

<p>In any case, choose your names as a vector and assign them:</p>

<pre><code>&gt; dimnames(z)$cause &lt;- c(""Maltreat"",""Non-malt"")
&gt; z
          time
cause      1 2 3
  Maltreat 2 2 2
  Non-malt 2 3 0
</code></pre>
"
1487352,143305,2009-09-28T14:29:40Z,1487320,3,FALSE,"<p>You may have to integrate R into your blogging engine, not unlike the MediaWiki extension.  </p>

<p>Or, going the other way, you take <a href=""http://cran.r-project.org/package=Rpad"" rel=""nofollow noreferrer""><strong>Rpad</strong></a> and wrap a blog engine around its web-based R interface.</p>
"
1489247,163053,2009-09-28T20:41:18Z,1489199,11,TRUE,"<p><a href=""http://www.nabble.com/Apply-as.factor-(or-as.numeric-etc)-to-multiple-columns-td24174823.html"" rel=""nofollow noreferrer"">This was also answered in R-Help.</a></p>

<p>I imagine that there's a better way to do it, but here are two options:</p>

<pre><code># use a sample data set
&gt; str(cars)
'data.frame':   50 obs. of  2 variables:
 $ speed: num  4 4 7 7 8 9 10 10 10 11 ...
 $ dist : num  2 10 4 22 16 10 18 26 34 17 ...
&gt; data.df &lt;- cars 
</code></pre>

<p>You can use <code>lapply</code>:</p>

<pre><code>&gt; data.df &lt;- data.frame(lapply(data.df, factor))
</code></pre>

<p>Or a <code>for</code> statement:</p>

<pre><code>&gt; for(i in 1:ncol(data.df)) data.df[,i] &lt;- as.factor(data.df[,i])
</code></pre>

<p>In either case, you end up with what you want:</p>

<pre><code>&gt; str(data.df)
'data.frame':   50 obs. of  2 variables:
 $ speed: Factor w/ 19 levels ""4"",""7"",""8"",""9"",..: 1 1 2 2 3 4 5 5 5 6 ...
 $ dist : Factor w/ 35 levels ""2"",""4"",""10"",""14"",..: 1 3 2 9 5 3 7 11 14 6 ...
</code></pre>
"
1489444,163053,2009-09-28T21:19:53Z,1489199,5,FALSE,"<p>I found an alternative solution in the <code>plyr</code> package:</p>

<pre><code># load the package and data
&gt; library(plyr)
&gt; data.df &lt;- cars
</code></pre>

<p>Use the colwise function:</p>

<pre><code>&gt; data.df &lt;- colwise(factor)(data.df)
&gt; str(data.df)
'data.frame':   50 obs. of  2 variables:
 $ speed: Factor w/ 19 levels ""4"",""7"",""8"",""9"",..: 1 1 2 2 3 4 5 5 5 6 ...
 $ dist : Factor w/ 35 levels ""2"",""4"",""10"",""14"",..: 1 3 2 9 5 3 7 11 14 6 ...
</code></pre>

<p>Incidentally, if you look inside the colwise function, it just uses <code>lapply</code>:</p>

<pre><code>df &lt;- as.data.frame(lapply(filtered, .fun, ...))
</code></pre>
"
1489722,168747,2009-09-28T22:41:25Z,1489526,6,TRUE,"<p>If <code>myStreets</code> is data.frame then <code>for</code> loop takes each column of it. So first step takes Addres and Addres$City doesn't make sense.</p>

<p>You could change <code>for</code> condition to loop over rows:</p>

<pre><code>for (i in 1:nrow(myStreets))  {
   myStreet &lt;- myStreets[i,]
   # rest is the same
}
</code></pre>

<p>To optimized your code you can also do something like: </p>

<pre><code>myGeoTable &lt;- data.frame( address=myStreet$address, lat=NA_real_, long=NA_real_, EID=NA_real_)
for (i in 1:nrow(myStreets))  {
  myStreet &lt;- myStreets[i,] 
  requestUrl &lt;- ...
  ...
  myGeoTable[i,2:4] &lt;- c(lat,long,NA)
}
</code></pre>
"
1489820,31945,2009-09-28T23:15:16Z,1489788,9,TRUE,"<p>Try</p>

<pre><code>substr(""cgtcgctgtttgtcaa[...]"", 5, 200)
</code></pre>

<p>See <a href=""http://wiki.r-project.org/rwiki/doku.php?id=rdoc:base:substr"" rel=""noreferrer"">substr()</a>.</p>
"
1489823,163053,2009-09-28T23:16:19Z,1489788,6,FALSE,"<p>Use the substring function:</p>

<pre><code>&gt; tmp.string &lt;- paste(LETTERS, collapse="""")
&gt; tmp.string &lt;- substr(tmp.string, 4, 10)
&gt; tmp.string
[1] ""DEFGHIJ""
</code></pre>
"
1490422,16632,2009-09-29T03:22:31Z,1489526,4,FALSE,"<p>If you're going to do this, I wouldn't talk about it in public.  It's against their terms of service.  I'd suggest using <a href=""https://webgis.usc.edu/"" rel=""nofollow noreferrer"">USC webgis</a> instead.  A couple of months ago I geocoded around half a million records without too many problems.</p>
"
1491337,144157,2009-09-29T08:47:40Z,1491124,4,FALSE,"<pre><code>by(glaciers[,1:3], glaciers$activity.level, function(x){round(mean(x),5)})
</code></pre>

<p><strong>UPDATE</strong></p>

<p>Here is a working example:</p>

<pre><code>glaciers &lt;- as.data.frame(matrix(rnorm(1000),ncol=4)) 
glaciers[,4] &lt;- sample(0:3,250,replace=TRUE) 
colnames(glaciers) &lt;- c(""A"",""B"",""C"",""activity.level"") 
by(glaciers[,1:3], glaciers$activity.level, function(x){round(mean(x),5)})
</code></pre>
"
1492110,143476,2009-09-29T12:00:24Z,1491124,10,FALSE,"<p>If you already have the output saved to a variable, say x:</p>

<pre><code>x &lt;- by(glaciers[,1:3],glaciers$activity.level,mean)
</code></pre>

<p>Then apply round() to each element (the output of by() in this case is a list).</p>

<pre><code>x[] &lt;- lapply(x,round,5)
x
</code></pre>

<p>reassigning to x[] rather than x allows x to retain attributes attached to it from by().  </p>

<p>Edit: round() actually changes the value of the variables but is decoupled from its printing. If you want to suppress the scientific notation output format, use format=""f"" argument to formatC()</p>

<pre><code>&gt; round(1.2345e10,5)
[1] 1.2345e+10
&gt; formatC(1.2345e10,digits=5,format=""f"")
[1] ""12345000000.00000""
</code></pre>

<p>So the correction to the expression originally posted would be </p>

<pre><code>x[] &lt;- lapply(x,formatC,digits=5,format=""f"")
</code></pre>
"
1492586,163053,2009-09-29T13:32:37Z,1487320,6,FALSE,"<p>I see that <a href=""http://learnr.wordpress.com/2009/09/29/wordpress-blogging-with-r-in-3-steps/"" rel=""noreferrer"">there is a posting on this very subject on the ""Learning R"" blog</a> today.  What a coincidence!</p>

<p>This uses Sweave to create the actual output, along with <a href=""http://srackham.wordpress.com/blogpost-readme/"" rel=""noreferrer"">a Python script called blogpost.py (from Stuart Rackham)</a> to upload the results onto the Wordpress blog.  I like that approach because using Sweave means that your blog output could easily be converted into a paper or a presentation (<a href=""http://en.wikipedia.org/wiki/Beamer_(LaTeX)"" rel=""noreferrer"">with Beamer</a>), so blogging becomes even more useful in the long run.</p>
"
1494004,135944,2009-09-29T17:45:02Z,1491124,1,FALSE,"<p>Do you want to round or to just truncate the number of digits shown? If the latter, use options(digits=3) or whatever.</p>

<pre><code>&gt; by(mpg[,8:9], mpg$cyl, mean)
mpg$cyl: 4
   cty    hwy 
21.012 28.802 
------------------------------------------------------------ 
mpg$cyl: 5
  cty   hwy 
20.50 28.75 
------------------------------------------------------------ 
mpg$cyl: 6
   cty    hwy 
16.215 22.823 
------------------------------------------------------------ 
mpg$cyl: 8
   cty    hwy 
12.571 17.629 
&gt; options(digits=3)
&gt; by(mpg[,8:9], mpg$cyl, mean)
mpg$cyl: 4
 cty  hwy 
21.0 28.8 
------------------------------------------------------------ 
mpg$cyl: 5
 cty  hwy 
20.5 28.8 
------------------------------------------------------------ 
mpg$cyl: 6
 cty  hwy 
16.2 22.8 
------------------------------------------------------------ 
mpg$cyl: 8
 cty  hwy 
12.6 17.6 
</code></pre>
"
1494040,143319,2009-09-29T17:54:54Z,1491124,1,FALSE,"<p>Only have a couple of minutes, but you might try looking at the <code>format()</code>, <code>formatC()</code>, and <code>prettyNum()</code> functions.  Their help files look like gibberish to me right now, but I haven't slept much.  However, I did use one of these functions - most likely <code>formatC()</code> - for a Sweave report some months ago in which I needed very specifically formatted numbers.</p>
"
1494115,135944,2009-09-29T18:14:17Z,1493969,2,FALSE,"<p>That is sorta tricky. Here's one way. It iterates over the list, inserting each time, so it's not too efficient. </p>

<pre><code>probes &lt;- rep(TRUE, 15)
probes.ind &lt;- ind + 0:(length(ind)-1)
for (i in probes.ind) {
  probes &lt;- c(probes[1:i], FALSE, probes[(i+1):length(probes)])
}

&gt; probes
 [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE
</code></pre>

<p>This should even work if ind has repeated elements, although ind does need to be sorted for the probes.ind construction to work.</p>
"
1494287,158065,2009-09-29T18:52:18Z,1493969,6,FALSE,"<p>How about this:</p>

<pre><code>&gt; probes &lt;- rep(TRUE, 15)
&gt; ind &lt;- c(5, 10)

&gt; probes.ind &lt;- rep(NA, length(probes))
&gt; probes.ind[ind] &lt;- FALSE
&gt; new.probes &lt;- as.vector(rbind(probes, probes.ind))
&gt; new.probes &lt;- new.probes[!is.na(new.probes)]
&gt; new.probes
 [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE
</code></pre>
"
1495204,168747,2009-09-29T22:04:24Z,1493969,27,TRUE,"<p>You can do some magic with indexes:</p>

<p>First create vector with output values:</p>

<pre><code>probs &lt;- rep(TRUE, 15)
ind &lt;- c(5, 10)
val &lt;- c( probs, rep(FALSE,length(ind)) )
# &gt; val
#  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
# [13]  TRUE  TRUE  TRUE FALSE FALSE
</code></pre>

<p>Now trick. Each old element gets rank, each new element gets half-rank </p>

<pre><code>id  &lt;- c( seq_along(probs), ind+0.5 )
# &gt; id
#  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0 11.0 12.0 13.0 14.0 15.0
# [16]  5.5 10.5
</code></pre>

<p>Then use <code>order</code> to sort in proper order:</p>

<pre><code>val[order(id)]
#  [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
# [13]  TRUE  TRUE  TRUE  TRUE  TRUE
</code></pre>
"
1495343,163053,2009-09-29T22:43:50Z,1493969,56,FALSE,"<p>These are all very creative approaches.  I think working with indexes is definitely the way to go (Marek's solution is very nice).</p>

<p>I would just mention that there is a function to do roughly that: <code>append()</code>.</p>

<pre><code>probes &lt;- rep(TRUE, 15)
probes &lt;- append(probes, FALSE, after=5)
probes &lt;- append(probes, FALSE, after=11)
</code></pre>

<p>Or you could do this recursively with your indexes (you need to grow the ""after"" value on each iteration):</p>

<pre><code>probes &lt;- rep(TRUE, 15)
ind &lt;- c(5, 10)
for(i in 0:(length(ind)-1)) 
    probes &lt;- append(probes, FALSE, after=(ind[i+1]+i))
</code></pre>

<p>Incidentally, <a href=""http://tolstoy.newcastle.edu.au/R/help/04/11/7695.html"" rel=""noreferrer"">this question was also previously asked on R-Help</a>.  As Barry says:</p>

<p>""Actually I'd say there were no ways of doing this, since I dont think you can actually insert into a vector - you have to create a new vector that produces the illusion of insertion!""</p>
"
1496663,144157,2009-09-30T07:53:44Z,1496582,2,FALSE,"<p>If the table is named x, then just use c(x).</p>
"
1496773,168168,2009-09-30T08:29:28Z,1496647,1,FALSE,"<p>Have a look at <a href=""http://had.co.nz/plyr/"" rel=""nofollow noreferrer"">plyr-package</a> - it allows to break a list/data frame into smaller pieces and then apply your function to this subset.</p>
"
1496818,172261,2009-09-30T08:42:46Z,1496647,7,TRUE,"<p>Replace the <code>min</code> (<code>max</code>) function  by <code>pmin</code> (<code>pmax</code>):</p>

<pre><code>&gt; invquad(1,2,3,3,""min"")
[1] -2
&gt; invquad(1,2,3,4,""min"")
[1] -2.414214
&gt; invquad(1,2,3,c(3,4),""min"")
[1] -2.000000 -2.414214
</code></pre>
"
1496833,172261,2009-09-30T08:46:28Z,1496582,5,TRUE,"<p>or you could use <code>as.vector(x)</code></p>
"
1496904,168168,2009-09-30T08:59:30Z,1487320,2,FALSE,"<p>There's a post on <a href=""http://blogisticreflections.wordpress.com/2009/09/20/welcome-to-blogistic-reflections/"" rel=""nofollow noreferrer"">Blogistic Reflections blog</a> how he is using Emacs/ESS org-mode to get the HTML export functionality.</p>
"
1497697,143305,2009-09-30T12:02:42Z,1497539,132,TRUE,"<p>If I understand your question correctly, then you probably want a density estimate along with the histogram:</p>

<pre><code>X &lt;- c(rep(65, times=5), rep(25, times=5), rep(35, times=10), rep(45, times=4))
hist(X, prob=TRUE)            # prob=TRUE for probabilities not counts
lines(density(X))             # add a density estimate with defaults
lines(density(X, adjust=2), lty=""dotted"")   # add another ""smoother"" density
</code></pre>

<p><em>Edit a long while later:</em></p>

<p>Here is a slightly more dressed-up version:</p>

<pre><code>X &lt;- c(rep(65, times=5), rep(25, times=5), rep(35, times=10), rep(45, times=4))
hist(X, prob=TRUE, col=""grey"")# prob=TRUE for probabilities not counts
lines(density(X), col=""blue"", lwd=2) # add a density estimate with defaults
lines(density(X, adjust=2), lty=""dotted"", col=""darkgreen"", lwd=2) 
</code></pre>

<p>along with the graph it produces:</p>

<p><img src=""https://i.stack.imgur.com/lHCqw.png"" alt=""enter image description here""></p>
"
1497796,143591,2009-09-30T12:25:20Z,1489788,3,FALSE,"<p>See also the Bioconductor package <a href=""http://www.bioconductor.org/packages/bioc/html/Biostrings.html"" rel=""nofollow noreferrer"">Biostrings</a> that is a good choice if you need to handle large biological sequences or set of sequences.</p>

<pre><code>#source(""http://bioconductor.org/biocLite.R"");biocLite(""Biostrings"") 
library(Biostrings)
s &lt;-paste(rep(""gtcgctgtttgtcaac"",20),collapse="""")
d &lt;- DNAString(s)
d[5:200]
as.character(d[5:200])
</code></pre>
"
1498116,163053,2009-09-30T13:27:34Z,1491124,6,FALSE,"<p><code>round()</code> doesn't make sense in this instance, since you're working with very large numebrs.  You want to use the format() command, and choose how many digits to display.  For instance, to show 3 significant digits:</p>

<pre><code>by(glaciers[,1:3], glaciers$activity.level, function(x) {
      as.numeric(format(mean(x), digits=3))
})
</code></pre>
"
1498153,179848,2009-09-30T13:32:39Z,1497539,22,FALSE,"<p>Here's the way I do it:</p>

<pre><code>foo &lt;- rnorm(100, mean=1, sd=2)
hist(foo, prob=TRUE)
curve(dnorm(x, mean=mean(foo), sd=sd(foo)), add=TRUE)
</code></pre>

<p>A bonus exercise is to do this with ggplot2 package ...</p>
"
1498294,180219,2009-09-30T13:58:11Z,1491124,2,FALSE,"<p>You asked ""How can I get my output to round to say 5 decimal places?"" but I think what you meant was ""How can I get my output to show say 6 significant figures?"" The following code combines two of the previous answers -- the idea of reassigning to res[], and the use of signif rather than round.</p>

<pre><code>glaciers &lt;- data.frame(aspect=runif(20)*100,
                       sun.duration=runif(20)*10000,
                       latitude=runif(20)*10^9,
                       activity.level=sample(c('Active','Inactive','Relict'),20,replace=TRUE))
res &lt;- by(glaciers[,1:3],glaciers$activity.level,mean)
res[] &lt;- lapply(res,signif,3)
res
</code></pre>

<p>This code produces the following output:</p>

<pre><code>glaciers$activity.level: Active
      aspect sun.duration     latitude 
    3.66e+01     4.72e+03     4.56e+08 
------------------------------------------------- 
glaciers$activity.level: Inactive
      aspect sun.duration     latitude 
    5.81e+01     5.28e+03     4.83e+08 
------------------------------------------------- 
glaciers$activity.level: Relict
      aspect sun.duration     latitude 
    6.08e+01     4.75e+03     3.98e+08 
</code></pre>
"
1499302,16632,2009-09-30T16:37:44Z,1496582,5,FALSE,"<p>It already is a vector.</p>

<pre><code>tbl &lt;- table(rpois(100, 10))
tbl[1]
tbl[2:5]
tbl[tbl &gt; 10]
</code></pre>
"
1499577,80741,2009-09-30T17:32:58Z,1402001,3,FALSE,"<p>You're probably looking for <a href=""http://www.inside-r.org/packages/cran/rodbc/docs/sqlUpdate"" rel=""nofollow noreferrer""><code>?sqlSave</code></a> which uses a parametrized <code>INSERT INTO</code> query (taking place in one operation) when you set <code>Fast=True</code>.</p>
"
1499883,142651,2009-09-30T18:30:09Z,1497539,24,FALSE,"<p>Such thing is easy with ggplot2</p>

<pre><code>library(ggplot2)
dataset &lt;- data.frame(X = c(rep(65, times=5), rep(25, times=5), rep(35, times=10), rep(45, times=4)))
ggplot(dataset, aes(x = X)) + geom_histogram(aes(y = ..density..)) + geom_density()
</code></pre>

<p>or to mimic the result from Dirk's solution</p>

<pre><code>ggplot(dataset, aes(x = X)) + geom_histogram(aes(y = ..density..), binwidth = 5) + geom_density()
</code></pre>
"
1499996,163053,2009-09-30T18:51:42Z,1336271,3,FALSE,"<p>R has many different relevant tools.  In particular, have a look <a href=""http://cran.r-project.org/web/views/Spatial.html"" rel=""nofollow noreferrer"">at the spatial view</a>.  A similar question <a href=""http://www.mail-archive.com/r-help@r-project.org/msg41821.html"" rel=""nofollow noreferrer"">was asked in R-Help before, so you may want to look at that</a>.  </p>

<p>Look at the <code>contour</code> functions.  Here's some data:</p>

<pre><code>x &lt;- seq(-3,3)
y &lt;- seq(-3,3)

z &lt;- outer(x,y, function(x,y,...) x^2 + y^2 )
</code></pre>

<p>An initial plot is somewhat rough:</p>

<pre><code>contour(x,y,z, lty=1)
</code></pre>

<p>Bill Dunlap suggested an improvement: ""It often works better to fit a smooth surface to the data, evaluate that surface on a finer grid, and pass the result to contour.  This ensures that contour lines don't cross one another and tends to avoid the spurious loops that you might get from smoothing the contour lines themselves.  Thin plate splines (Tps from library(""fields"")) and loess (among others) can fit the surface.""  </p>

<pre><code>library(""fields"")
contour(predict.surface(Tps(as.matrix(expand.grid(x=x,y=y)),as.vector(z))))
</code></pre>

<p>This results in a very smooth plot, because it uses <code>Tps()</code> to fit the data first, then calls <code>contour</code>.  It ends up looking like this (you can also use filled.contour if you want it to be shaded):</p>

<p><img src=""https://imgur.com/L7PIE.png""></p>

<p>For the plot, you can use either <code>lattice</code> (as in the above example) or the <code>ggplot2</code> package.  Use the <code>geom_contour()</code> function in that case.  An example <a href=""http://www.nabble.com/contour-plot-td24607771.html"" rel=""nofollow noreferrer"">can be found here (ht Thierry)</a>:</p>

<pre><code>ds &lt;- matrix(rnorm(100), nrow = 10) 
library(reshape) 
molten &lt;- melt(data = ds) 
library(ggplot2) 
ggplot(molten, aes(x = X1, y = X2, z = value)) + geom_contour()
</code></pre>
"
1503155,144157,2009-10-01T10:29:38Z,1502910,59,TRUE,"<p>Use rle():</p>

<pre><code>y &lt;- rle(c(1,0,0,0,1,0,0,0,0,0,2,0,0))
y$lengths[y$values==0]
</code></pre>
"
1503319,163053,2009-10-01T11:16:44Z,1502910,19,FALSE,"<p>This can be done in an efficient way by using indexes of where the values change:</p>

<pre><code>x &lt;- c(1,0,0,0,1,2,1,0,0,1,1)
</code></pre>

<p>Find where the values change:</p>

<pre><code>diffs &lt;- x[-1L] != x[-length(x)]
</code></pre>

<p>Get the indexes, and then get the difference in subsequent indexes:</p>

<pre><code>idx &lt;- c(which(diffs), length(x))
diff(c(0, idx))
</code></pre>
"
1504339,119963,2009-10-01T14:36:01Z,1504318,4,TRUE,"<p>Put a file in /usr/share/vim/vimfiles/ftdetect (for global) or .vim/ftdetect (for local) called Rnw.vim that looks something like this:</p>

<pre><code>"" Vim filetype detection plugin
"" Language:    sweavefile

autocmd BufRead,BufNewFile *.Rnw set filetype=tex
</code></pre>

<p>Edit: I'm pretty sure you could put this autocommand in your .vimrc too, since that's loaded on program start, before any files are loaded, but this is directory the natural place for filetype detection.</p>

<p>Edit: If you would like to give these files some treatment different from TeX, you could instead set the filetype to rnw, add scripts to the ftplugin, indent, and syntax directories as necessary, most likely sourcing the TeX scripts and then doing your own stuff.</p>
"
1504364,163053,2009-10-01T14:40:08Z,1504318,2,FALSE,"<p>Hope these pointers will help:</p>

<ul>
<li>Have a look at this: <a href=""http://feferraz.net/en/P/Sweave_Syntax_Highlighting_in_vim"" rel=""nofollow noreferrer""><a href=""http://feferraz.net/en/P/Sweave_Syntax_Highlighting_in_vim"" rel=""nofollow noreferrer"">http://feferraz.net/en/P/Sweave_Syntax_Highlighting_in_vim</a></a>.  </li>
<li>This was also previously <a href=""http://markmail.org/message/4xoblugk365b2pm4#query:vim%20Rnw%20as%20.tex+page:1+mid:nqrj7io4pwwzh6wl+state:results"" rel=""nofollow noreferrer"">on R-Help</a>.</li>
</ul>
"
1504885,163053,2009-10-01T16:06:48Z,1504832,5,FALSE,"<p>The <code>apply</code> functions are not always (or even generally) faster than a <code>for</code> loop.  That is a remnant of R's associate with S-Plus (in the latter, apply is faster than for).  One exception is <code>lapply</code>, which is frequently faster than <code>for</code> (because it uses C code).  <a href=""http://tolstoy.newcastle.edu.au/R/help/06/05/27255.html"" rel=""nofollow noreferrer"">See this related question</a>.</p>

<p>So you should use <code>apply</code> primarily to improve the clarity of code, not to improve performance.</p>

<p>You might <a href=""http://dirk.eddelbuettel.com/papers/useR2009hpcTutorial.pdf"" rel=""nofollow noreferrer"">find Dirk's presentation on high-performance computing useful</a>.  One other brute force approach is <a href=""http://www.milbo.users.sonic.net/ra/"" rel=""nofollow noreferrer"">""just-in-time compilation"" with Ra instead of the normal R version</a>, which is optimized to handle <code>for</code> loops.</p>

<p><i>[Edit:]</i> There are clearly many ways to achieve this, and this is by no means better even if it's more compact.  Just working with your code, here's another approach:</p>

<pre><code>dt &lt;- data.frame(table(dat))[,2:3]
dt.b &lt;- by(dt[,2], dt[,1], rle)
t(data.frame(lapply(dt.b, function(x) max(x$length))))
</code></pre>

<p>You would probably need to manipulate the output a little further.  </p>
"
1505144,143305,2009-10-01T16:50:21Z,1504709,1,TRUE,"<p>Why not just kill the underlying R process, start a new one and continue the session in the same ESS buffer?</p>
"
1505260,163053,2009-10-01T17:15:29Z,1504709,0,FALSE,"<p>This doesn't answer your specific question and my experience is on Windows, but assuming it's challenging in ESS, I just mention:</p>

<p>There are other IDE's which have no trouble doing this (e.g. Tinn-R, StatET).  In particular, for one supported on multiple operating systems, have a look at the StatET plugin for Eclipse.  One very nice feature of Eclipse is that not only can you run the commands on a console outside the IDE, but you can also run multiple console sessions at the same time.  This allows you to easily compare results side by side.</p>

<p>Needless to say, that's irrelevant if you're comfortable using ESS.</p>
"
1505765,160553,2009-10-01T18:49:10Z,1467807,0,FALSE,"<p><a href=""http://learnr.wordpress.com/2009/09/09/brew-creating-repetitive-reports/"" rel=""nofollow noreferrer"">Here is an interesting blog posting on this topic.</a> The author tries to create a report analogous to the United Nation's World Population Prospects: The 2008 Revision report.</p>

<p>Hope that helps,
Charlie</p>
"
1506013,143319,2009-10-01T19:40:00Z,1504832,3,FALSE,"<p>EDIT: Fixed.  I originally assumed that I would have to modify most of rle(), but it turns out only a few tweaks were needed.</p>

<p>This isn't an answer about an *apply method, but I wonder if this might not be a faster approach to the process overall.  As Shane says, loops aren't so bad.  And... I rarely get to show my code to anyone, so I'd be happy to hear some critique of this.</p>

<pre><code>#Shane, I told you this was awesome
dat &lt;- getSOTable(""http://stackoverflow.com/questions/1504832/help-me-replace-a-for-loop-with-an-apply-function"", 1)
colnames(dat) &lt;- c(""day"", ""user_id"")
#Convert to dates so that arithmetic works properly on them
dat$day &lt;- as.Date(dat$day)

#Custom rle for dates
rle.date &lt;- function (x)
{
    #Accept only dates
    if (class(x) != ""Date"")
        stop(""'x' must be an object of class \""Date\"""")
    n &lt;- length(x)
    if (n == 0L)
        return(list(lengths = integer(0L), values = x))
    #Dates need to be sorted
    x.sort &lt;- sort(x)
    #y is a vector indicating at which indices the date is not consecutive with its predecessor
    y &lt;- x.sort[-1L] != (x.sort + 1)[-n]
    #i returns the indices of y that are TRUE, and appends the index of the last value
    i &lt;- c(which(y | is.na(y)), n)
    #diff tells you the distances in between TRUE/non-consecutive dates. max gets the largest of these.
    max(diff(c(0L, i)))
}

#Loop
max.consec.use &lt;- matrix(nrow = length(unique(dat$user_id)), ncol = 1)
rownames(max.consec.use) &lt;- unique(dat$user_id)

for(i in 1:length(unique(dat$user_id))){
    user &lt;- unique(dat$user_id)[i]
    uses &lt;- subset(dat, user_id %in% user)
    max.consec.use[paste(user), 1] &lt;- rle.date(uses$day)
}

max.consec.use
</code></pre>
"
1506053,76235,2009-10-01T19:48:03Z,1504832,0,FALSE,"<p>If you've got a really long list of data than it sounds like maybe a clustering problem. Each cluster would be defined by a user and dates with a maximum separation distance of one. Then retrieve the largest cluster by user. I'll edit this if I think of a specific method.</p>
"
1506119,163053,2009-10-01T20:01:03Z,1504832,0,FALSE,"<p>This was <a href=""https://stackoverflow.com/questions/1434897/how-do-i-load-example-datasets-in-r/1434927#1434927"">Chris's suggestion for how to get the data</a>:</p>

<pre><code>dat &lt;- read.table(textConnection(
 ""day      user_id
 2008/11/01    2001
 2008/11/01    2002
 2008/11/01    2003
 2008/11/01    2004
 2008/11/01    2005
 2008/11/02    2001
 2008/11/02    2005
 2008/11/03    2001
 2008/11/03    2003
 2008/11/03    2004
 2008/11/03    2005
 2008/11/04    2001
 2008/11/04    2003
 2008/11/04    2004
 2008/11/04    2005
 ""), header=TRUE)
</code></pre>
"
1508864,163053,2009-10-02T10:52:22Z,1508513,3,FALSE,"<p>Without addressing the code in any detail, you're assigning values to <code>column</code>, which is a local variable within the loop (i.e. there is no relationship between <code>column</code> and <code>data</code> in that context).  You need to assign those values to the appropriate value in <code>data</code>.</p>

<p>Also, <code>data</code> will be local to your function, so you need to assign that back to <code>data</code> after running the function.</p>

<p>Incidentally, you can use <code>diff</code> to see if any value is incrementing rather than looping over every value:</p>

<pre><code>idx &lt;- apply(d, 2, function(x) !any(diff(x[!is.na(x)]) &lt; 0))
d[,idx] &lt;- blah
</code></pre>
"
1508888,134830,2009-10-02T10:56:45Z,1508513,2,FALSE,"<p><code>diff</code> calculates the difference between consecutive values in a vector.  You can apply it to each column in a dataframe using, e.g.</p>

<pre><code>dfr &lt;- data.frame(x = c(1,2,5,7,8), y = (1:5)^2)
as.data.frame(lapply(dfr, diff))

  x y
1 1 3
2 3 5
3 2 7
4 1 9
</code></pre>

<p><strong>EDIT:</strong> I just noticed a few more things.  You are using a matrix, not a data frame (as you stated in the question).  For your matrix 'd', you can use</p>

<pre><code>d_diff &lt;- apply(d, 2, diff)
#Find columns that are (strictly) increasing
incr &lt;- apply(d_diff, 2, function(x) all(x &gt; 0, na.rm=TRUE))
#Replace values in the approriate columns
d[2:nrow(d),incr] &lt;- d_diff[,incr]
</code></pre>
"
1508907,163053,2009-10-02T11:03:57Z,1508889,3,FALSE,"<p>Can you use something like this?</p>

<pre><code>length(unique(x))
</code></pre>
"
1508932,74658,2009-10-02T11:13:24Z,1508889,9,TRUE,"<p>These are a few add-on packages that might help (see <a href=""http://www.statmethods.net/stats/descriptives.html"" rel=""noreferrer"">Quick-R</a>)</p>

<p>Using the <a href=""http://cran.r-project.org/web/packages/Hmisc/index.html"" rel=""noreferrer"">Hmisc</a> package</p>

<pre><code>library(Hmisc)

describe(mydata) 
# n, nmiss, unique, mean, 5,10,25,50,75,90,95th percentiles 
# 5 lowest and 5 highest scores
</code></pre>

<p>Using the <a href=""http://cran.r-project.org/web/packages/pastecs/index.html"" rel=""noreferrer"">pastecs</a> package</p>

<pre><code>library(pastecs)

stat.desc(mydata) 
# nbr.val, nbr.null, nbr.na, min max, range, sum, 
# median, mean, SE.mean, CI.mean, var, std.dev, coef.var 
</code></pre>

<p>Using the <a href=""http://cran.r-project.org/web/packages/psych/index.html"" rel=""noreferrer"">psych</a> package</p>

<pre><code>library(psych)
describe(mydata)
# item name ,item number, nvalid, mean, sd, 
# median, mad, min, max, skew, kurtosis, se
</code></pre>

<p>I'd use describe.by from the psych package;</p>

<pre><code>&gt; describe.by(biastable, as.factor(Nominal))
group: 1
         var n mean   sd median trimmed  mad  min  max range  skew kurtosis   se
Nominal    1 9 1.00 0.00   1.00    1.00 0.00 1.00 1.00  0.00   NaN      NaN 0.00
Actual     2 8 0.12 0.01   0.12    0.12 0.01 0.11 0.13  0.03  0.09    -1.47 0.00
LinPred    3 8 0.99 0.08   0.98    0.99 0.10 0.89 1.09  0.20  0.04    -1.70 0.03
QuadPred   4 8 0.99 0.08   0.99    0.99 0.10 0.88 1.09  0.20 -0.04    -1.64 0.03
------------------------------------------------------------------------ 
group: 3
         var n mean   sd median trimmed  mad  min  max range skew kurtosis   se
Nominal    1 9 3.00 0.00   3.00    3.00 0.00 3.00 3.00  0.00  NaN      NaN 0.00
Actual     2 9 0.37 0.03   0.36    0.37 0.03 0.32 0.42  0.10 0.15    -1.50 0.01
LinPred    3 9 3.12 0.24   3.05    3.12 0.30 2.79 3.50  0.71 0.15    -1.52 0.08
QuadPred   4 9 3.10 0.23   3.06    3.10 0.34 2.79 3.46  0.67 0.12    -1.51 0.08
------------------------------------------------------------------------ 
group: 6
         var n mean   sd median trimmed  mad  min  max range skew kurtosis   se
Nominal    1 9 6.00 0.00   6.00    6.00 0.00 6.00 6.00  0.00  NaN      NaN 0.00
Actual     2 9 0.71 0.04   0.70    0.71 0.04 0.66 0.78  0.12 0.46    -1.30 0.01
LinPred    3 9 6.02 0.30   5.91    6.02 0.28 5.61 6.47  0.86 0.28    -1.43 0.10
QuadPred   4 9 5.99 0.31   5.93    5.99 0.25 5.55 6.49  0.94 0.26    -1.26 0.10
------------------------------------------------------------------------ 
group: 10
         var n  mean   sd median trimmed  mad   min   max range skew kurtosis   se
Nominal    1 9 10.00 0.00  10.00   10.00 0.00 10.00 10.00  0.00  NaN      NaN 0.00
Actual     2 9  1.16 0.07   1.14    1.16 0.09  1.06  1.25  0.19 0.09    -1.71 0.02
LinPred    3 9  9.85 0.60   9.76    9.85 0.74  9.16 10.72  1.56 0.24    -1.76 0.20
QuadPred   4 9  9.79 0.62   9.63    9.79 0.72  9.05 10.78  1.72 0.27    -1.65 0.21
------------------------------------------------------------------------ 
group: 30
         var n  mean   sd median trimmed  mad   min   max range skew kurtosis   se
Nominal    1 9 30.00 0.00  30.00   30.00 0.00 30.00 30.00  0.00  NaN      NaN 0.00
Actual     2 9  3.53 0.22   3.51    3.53 0.21  3.25  3.85  0.60 0.23    -1.58 0.07
LinPred    3 9 30.08 1.55  29.88   30.08 1.44 27.70 32.66  4.96 0.21    -1.27 0.52
QuadPred   4 9 29.92 1.51  30.00   29.92 1.44 27.44 32.38  4.94 0.04    -1.22 0.50
------------------------------------------------------------------------ 
group: 50
         var n  mean   sd median trimmed  mad   min   max range skew kurtosis   se
Nominal    1 9 50.00 0.00  50.00   50.00 0.00 50.00 50.00  0.00  NaN      NaN 0.00
Actual     2 9  5.91 0.51   5.82    5.91 0.43  5.43  6.94  1.51 0.90    -0.73 0.17
LinPred    3 9 50.40 3.98  48.77   50.40 3.21 44.89 57.37 12.48 0.49    -1.16 1.33
QuadPred   4 9 50.24 3.97  48.91   50.24 2.65 44.49 57.01 12.52 0.39    -1.21 1.32
------------------------------------------------------------------------ 
group: 150
         var n   mean   sd median trimmed   mad    min    max range  skew kurtosis   se
Nominal    1 9 150.00 0.00 150.00  150.00  0.00 150.00 150.00  0.00   NaN      NaN 0.00
Actual     2 6  17.23 0.97  17.20   17.23  0.67  15.90  18.80  2.90  0.25    -1.23 0.39
LinPred    3 6 147.19 8.11 147.01  147.19 11.13 138.04 155.39 17.36 -0.01    -2.22 3.31
QuadPred   4 6 147.77 7.95 147.48  147.77 10.95 139.60 157.78 18.17  0.07    -2.10 3.25
------------------------------------------------------------------------ 
group: 250
         var n   mean    sd median trimmed  mad    min    max range skew kurtosis   se
Nominal    1 9 250.00  0.00 250.00  250.00 0.00 250.00 250.00  0.00  NaN      NaN 0.00
Actual     2 9  28.83  1.18  28.70   28.83 0.89  27.10  31.20  4.10 0.59    -0.57 0.39
LinPred    3 9 246.29 10.57 245.98  246.29 9.31 231.46 264.81 33.35 0.33    -1.26 3.52
QuadPred   4 9 251.51  8.84 248.45  251.51 5.08 240.41 268.30 27.89 0.62    -1.04 2.95
&gt; 
</code></pre>
"
1509657,158065,2009-10-02T13:55:39Z,1508889,0,FALSE,"<p>Does <code>complete.cases</code> (or <code>sum(complete.cases)</code>) do what you want?</p>
"
1510194,163053,2009-10-02T15:18:21Z,1510039,3,FALSE,"<p>Trying to find cases where all the Supreme Court justices weren't involved?  Don't suppose that you have a small sample dataset that you could add?  </p>

<p>A thought: rbind the vectors on top of each other so that you have a dataset like data.frame(""justice"",""case"").  Then use hadley's <code>reshape</code> package (use the <code>cast</code> function) to sum the number of justices per case.  Any case with less than the total number of justices will be a ""bad case"".</p>
"
1510367,168747,2009-10-02T15:48:00Z,1508889,2,FALSE,"<p>What are ""blank values"" and ""text values""? If you have numeric vector then you could have NA's (<code>is.na()</code>), Inf's (<code>is.infinite()</code>), NaN's (<code>is.nan()</code>) and ""valid"" numeric values.</p>

<p>For ""valid"" numeric values (in the sense above) you could use <code>is.finite()</code>:</p>

<pre><code>is.finite(c(1,NA,Inf,NaN))
# [1]  TRUE FALSE FALSE FALSE
sum( is.finite(c(1,NA,Inf,NaN)) )
# [1] 1
</code></pre>

<p>So <code>colSums(is.numeric(x))</code> could be done like <code>colSums(is.finite(x))</code>.</p>
"
1511756,135944,2009-10-02T20:57:51Z,1511431,20,TRUE,"<p>You can certainly do this by counting external to ggplot, but one of the great things about ggplot is that you can do many of these statistics internally!</p>

<p>Using your mpg example above:</p>

<pre><code>ggplot(mpg) + 
  geom_point(aes(x=great_cty, y=great_hwy, 
                 size=..count..), stat=""bin"")
</code></pre>

<p><img src=""https://imgur.com/zUSzC.png"" alt=""alt text""></p>
"
1515176,147427,2009-10-03T23:45:57Z,1515143,2,TRUE,"<p>I think the issue is that ifelse returns a value, <em>not</em> do whatever is in the arguments. I've learned this the hard way before: <strong>ifelse != shorthand if</strong>, ifelse = vectorized if. From the help page:</p>

<blockquote>
  <p>ifelse(test, yes, no)</p>
  
  <p>'ifelse' returns a value with the same
  shape as 'test' which is
       filled with elements selected from either 'yes' or 'no' depending
       on whether the element of 'test' is 'TRUE' or 'FALSE'.</p>
</blockquote>

<p>So just use something like:</p>

<pre><code>if (.Platform$OS.type == ""unix"") {
  pdf(name, width=6, height=4) 
} else {
  wmf(name, width=6, height=4)
}
</code></pre>
"
1515230,142477,2009-10-04T00:13:01Z,1515193,1,FALSE,"<p>One possible solution is as follows (but am interested in alternatives):</p>

<pre><code>new.result &lt;- matrix(unlist(result), ncol=ncol(result), 
              dimnames=list(NULL, colnames(result)))
</code></pre>
"
1515237,147427,2009-10-04T00:17:06Z,1515193,9,TRUE,"<p>do.call on lists is very elegant, and fast. In fact do.call(rbind, my.list) once saved my ass when I needed to combine a huge list. It was by far the fastest solution.</p>

<p>To solve your problem, maybe something like:</p>

<pre><code>do.call(rbind, lapply(foo, unlist))


&gt; result.2 &lt;- do.call(rbind, lapply(foo, unlist))
&gt; result.2
       a   b
[1,]   1   2
[2,]  11  22
[3,] 111 222
&gt; result.2[, 'a']
[1]   1  11 111
&gt; 
</code></pre>
"
1515323,16632,2009-10-04T01:19:22Z,1515143,5,FALSE,"<p>Here's a somewhat more polished version of your function.  Improvements:</p>

<ul>
<li>doesn't mess with your working directory</li>
<li>avoids the duplicated if statement by looking up the device function from the extension</li>
</ul>

<p>-></p>

<pre><code>graph &lt;- function(filename) {
  ext &lt;- if(.Platform$OS.type == ""unix"") ""pdf"" else ""wmf""
  dev &lt;- match.fun(ext)
  path &lt;- paste(""graphics/"", filename, ""."", ext, sep = """")

  dev(path, width = 6, height = 4)
}
</code></pre>
"
1521435,143305,2009-10-05T17:46:21Z,1521390,14,TRUE,"<p>Use the lattice package:</p>

<pre><code>library(lattice)
histogram( ~ escore | inst, data=X)
</code></pre>

<p>if <code>X</code> is your data.frame object.</p>
"
1521613,158065,2009-10-05T18:19:59Z,1521390,12,FALSE,"<p>You can also do this in ggplot2:</p>

<pre><code>data.df &lt;- data.frame(inst = factor(sample(3, 426, replace=TRUE)), 
                      escore = sample(5, 426, replace=TRUE))
qplot(escore, fill=inst, data=data.df) + facet_wrap(~inst, ncol=3)
</code></pre>

<p><a href=""http://www.cs.princeton.edu/~jcone/hists.png"">alt text http://www.cs.princeton.edu/~jcone/hists.png</a></p>
"
1523177,163053,2009-10-06T01:18:26Z,1523126,105,FALSE,"<p>Not sure about how to have <code>read.csv</code> interpret it properly, but you can use <code>gsub</code> to replace <code>"",""</code> with <code>""""</code>, and then convert the string to <code>numeric</code> using <code>as.numeric</code>:</p>

<pre><code>y &lt;- c(""1,200"",""20,000"",""100"",""12,111"")
as.numeric(gsub("","", """", y))
# [1]  1200 20000 100 12111
</code></pre>

<p>This was <a href=""http://www.mail-archive.com/r-help@r-project.org/msg62846.html"" rel=""noreferrer"" title=""[R-help] thousands separator"">also answered previously on R-Help</a> (and in <a href=""http://www.mail-archive.com/r-help@r-project.org/msg69442.html"" rel=""noreferrer"" title=""[R-help] Q2. Is R capable of reading numbers that are represented with 1,000 separator commas?"">Q2 here</a>).</p>

<p>Alternatively, you can pre-process the file, for instance with <code>sed</code> in unix.</p>
"
1523235,71131,2009-10-06T01:47:26Z,1523126,3,FALSE,"<p>I think preprocessing is the way to go. You could use <a href=""http://sourceforge.net/apps/mediawiki/notepad-plus/index.php?title=Regular_Expressions"" rel=""nofollow noreferrer"">Notepad++</a> which has a regular expression replace option.</p>

<p>For example, if your file were like this:</p>

<pre><code>""1,234"",""123"",""1,234""
""234"",""123"",""1,234""
123,456,789
</code></pre>

<p>Then, you could use the regular expression <code>""([0-9]+),([0-9]+)""</code> and replace it with <code>\1\2</code></p>

<pre><code>1234,""123"",1234
""234"",""123"",1234
123,456,789
</code></pre>

<p>Then you could use  <code>x &lt;- read.csv(file=""x.csv"",header=FALSE)</code> to read the file.</p>
"
1523316,143377,2009-10-06T02:22:08Z,1523298,12,TRUE,"<p>The secret to happiness in ggplot2 is to put everything in the ""long"" (or what I guess matrix oriented people would call ""sparse"") format:</p>

<pre><code>df &lt;- rbind(data.frame(x=""n"",value=n),
            data.frame(x=""a"",value=a),
            data.frame(x=""p"",value=p))
qplot(value, colour=x, data=df, geom=""density"")
</code></pre>

<p>If you don't want colors:</p>

<pre><code>qplot(value, group=x, data=df, geom=""density"")
</code></pre>
"
1523415,144157,2009-10-06T03:15:28Z,1523126,11,TRUE,"<p>I want to use R rather than pre-processing the data as it makes it easier when the data are revised. Following Shane's suggestion of using <code>gsub</code>, I think this is about as neat as I can do:</p>

<pre><code>x &lt;- read.csv(""file.csv"",header=TRUE,colClasses=""character"")
col2cvt &lt;- 15:41
x[,col2cvt] &lt;- lapply(x[,col2cvt],function(x){as.numeric(gsub("","", """", x))})
</code></pre>
"
1525003,163053,2009-10-06T11:21:14Z,1524088,8,TRUE,"<p>If it's a source file, then use install.packages() and set the repos=NULL:</p>

<pre><code>install.packages(file_name_and_path, repos = NULL, type=""source"")
</code></pre>

<p>See this related question: <a href=""https://stackoverflow.com/questions/1474081/how-do-i-install-an-r-package-from-source"">How do I install an R package from source?</a></p>
"
1525482,144642,2009-10-06T13:09:28Z,1524088,3,FALSE,"<p>If it isn't a .tgz, is it in full directory form?  All you have to do is R CMD INSTALL dirname and it'll work.  The install.packages() function's only real advantage over a raw R CMD INSTALL is that it will do all the downloading, dependency matching, etc for you.</p>
"
1529587,57458,2009-10-07T05:08:17Z,1529559,0,FALSE,"<p>You have something that works. Are you worried about speed for some reason? Here's an alternative:</p>

<pre><code>y&lt;-c(0,3,2,1,0,0,2,5,0,1,0,0)

decider = function( x ) {
   if ( x == 0 ) {
      return(0)
   }

   return(1)
}

b = sapply( y, decider )
</code></pre>
"
1529598,163053,2009-10-07T05:11:38Z,1529559,6,FALSE,"<p>Try this:</p>

<pre><code>b &lt;- rep(0, length(y))
b[y != 0] &lt;- 1
</code></pre>

<p>This is efficient because y and b are the same size and rep() is very fast/vectorized.  </p>

<p><I>Edit:</I>Here's another approach:</p>

<pre><code>b &lt;- ifelse(y == 0, 0, 1) 
</code></pre>

<p>The ifelse() function is also vectorized. </p>
"
1529601,144157,2009-10-07T05:12:23Z,1529559,5,TRUE,"<pre><code>b &lt;- as.numeric(y!=0)
</code></pre>
"
1529686,147427,2009-10-07T05:37:39Z,1529559,2,FALSE,"<p>Use ifelse(). This is vectorized and (edit: somewhat) fast. </p>

<pre><code>&gt; y &lt;- c(0,3,2,1,0,0,2,5,0,1,0,0)
&gt; b &lt;- ifelse(y == 0, 0, 1)
 [1] 0 1 1 1 0 0 1 1 0 1 0 0
</code></pre>

<p>Edit 2: 
This approach is less fast than the as.numeric(y!=0) approach.</p>

<pre><code>&gt; t &lt;- Sys.time(); b &lt;- as.numeric(y!=0); Sys.time() - t # Rob's approach
Time difference of 0.0002379417 secs
&gt; t &lt;- Sys.time(); b &lt;- ifelse(y==0, 0, 1); Sys.time() - t # Shane's 2nd and my approach
Time difference of 0.000428915 secs
&gt; t &lt;- Sys.time(); b = sapply( y, decider ); Sys.time() - t # James's approach
Time difference of 0.0004429817 sec
</code></pre>

<p>But to some, ifelse may be trivially more readable than the as.numeric approach.</p>

<p>Note the OP's version took 0.0004558563 to run. </p>
"
1532659,135944,2009-10-07T16:16:29Z,1532535,8,TRUE,"<p>Short answer: You can't do that. It might make sense with 3 graphs, but what if you had a big lattice of 32 graphs? That would look noisy and bad. GGplot's philosophy is about doing the right thing with a minimum of customization, which means, naturally, that you can't customize things as much as other packages.</p>

<p>Long answer: You could fake it by constructing three separate ggplot objects and combining them. But it's not a very general solution. Here's some code from Hadley's book that assumes you've created ggplot objects a, b, and c. It puts a in the top row, with b and c in the bottom row.</p>

<pre><code>grid.newpage()
pushViewport(viewport(layout=grid.layout(2,2)))
vplayout&lt;-function(x,y)
    viewport(layout.pos.row=x,layout.pos.col=y)
print(a,vp=vplayout(1,1:2))
print(b,vp=vplayout(2,1))
print(c,vp=vplayout(2,2))
</code></pre>
"
1534985,1855677,2009-10-08T00:30:53Z,1529559,1,FALSE,"<pre><code> b&lt;-(y!=0)+0

&gt; b
 [1] 0 1 1 1 0 0 1 1 0 1 0 0
</code></pre>
"
1535082,1855677,2009-10-08T01:09:15Z,1523126,6,FALSE,"<p>""Preprocess"" in R:</p>

<pre><code>lines &lt;- ""www, rrr, 1,234, ttt \n rrr,zzz, 1,234,567,987, rrr""
</code></pre>

<p>Can use <code>readLines</code> on a <code>textConnection</code>. Then remove only the commas that are between digits:</p>

<pre><code>gsub(""([0-9]+)\\,([0-9])"", ""\\1\\2"", lines)

## [1] ""www, rrr, 1234, ttt \n rrr,zzz, 1234567987, rrr""
</code></pre>

<p>It's als useful to know but not directly relevant to this question that commas as decimal separators can be handled by read.csv2 (automagically) or read.table(with setting of the 'dec'-parameter). </p>

<p>Edit: Later I discovered how to use colClasses by designing a new class. See:</p>

<p><a href=""https://stackoverflow.com/questions/25088144/how-to-load-df-with-1000-separator-in-r-as-numeric-class/25090565#25090565"">How to load df with 1000 separator in R as numeric class?</a></p>
"
1535240,143305,2009-10-08T02:04:04Z,1535086,2,TRUE,"<p>Sounds like something is wrong with your system setup -- I don't expect this to be an R issue.</p>
"
1536621,144157,2009-10-08T09:14:47Z,1536590,12,TRUE,"<p>Use &amp; not &amp;&amp;. The latter only evaluates the first element of each vector.</p>

<p><strong>Update:</strong> to answer the second part, use the reshape package. Something like this will do it:</p>

<pre><code>tablex &lt;- recast(aggdata, Group.1 ~ variable * Group.2, id.var=1:2)
# Now add useful column and row names
colnames(tablex) &lt;- gsub(""x_"","""",colnames(tablex))
rownames(tablex) &lt;- tablex[,1]
# Finally remove the redundant first column
tablex &lt;- tablex[,-1]
</code></pre>

<p>Someone with more experience using reshape may have a simpler solution.</p>

<p>Note: Don't use table as a variable name as it conflicts with the table() function.</p>
"
1537077,60183,2009-10-08T10:58:47Z,1536486,3,TRUE,"<p>got to <code>customize-group: windows</code> and read the documentation for <code>scroll-step</code> and <code>scroll-conservatively</code>. If you set <code>scroll-conservatively</code> to a big value you should get the behavior you want. I tried 10000. You can also have a look at <a href=""http://www.gnu.org/software/emacs/manual/html_node/emacs/Scrolling.html#Scrolling"" rel=""nofollow noreferrer"">Info (emacs) Scrolling</a>.</p>

<p>Hope this helps!</p>
"
1538814,163053,2009-10-08T16:09:21Z,1538798,13,TRUE,"<p>Can't you just do this?</p>

<pre><code>ordered.colnames &lt;- ordered.colnames[ordered.colnames %in% colnames(dataset)]
</code></pre>
"
1539498,169947,2009-10-08T18:00:54Z,1536590,20,FALSE,"<p>The easiest solution is to change ""&amp;&amp;"" to ""&amp;"" in your code.</p>

<pre><code>&gt; aggdata[aggdata[,""Group.1""]==6 &amp; aggdata[,""Group.2""]==0.05,""x""]
[1] 0.9315789
</code></pre>

<p>My preferred solution would be to use subset():</p>

<pre><code>&gt; subset(aggdata, Group.1==6 &amp; Group.2==0.05)$x
[1] 0.9315789
</code></pre>
"
1541872,147427,2009-10-09T04:57:15Z,1541679,5,TRUE,"<p>Use <code>file.exists()</code> to test if the file is there, and if it is, append a string to the name.</p>

<p>Edit:</p>

<p>Thanks Marek, I'll expand on your idea a bit... he could add this to deal with both <code>save()</code> and <code>save.image()</code></p>

<pre><code>SafeSave &lt;- function( ..., file=stop(""'file' must be specified""), overwrite=FALSE, save.fun=save) {
  if ( file.exists(file) &amp; !overwrite ) stop(""'file' already exists"")
  save.fun(..., file=file)
}
</code></pre>

<p>I would not overwrite save... if <code>source()</code> was used in a REPL session, users may not be aware of the function overwrite. </p>
"
1542695,168747,2009-10-09T09:03:14Z,1541679,4,FALSE,"<p>As Vince wrote you could use <code>file.exists()</code> to check existence.</p>

<p>I suggest to replace original <code>save</code> function:</p>

<pre><code>save &lt;- function( ..., file=stop(""'file' must be specified""), overwrite=FALSE ) {
  if ( file.exists(file) &amp; !overwrite ) stop(""'file' already exists"")
  base::save(..., file=file)
}
</code></pre>

<p>You could write similar to replace <code>save.image()</code>.</p>
"
1544997,163053,2009-10-09T17:06:23Z,1544907,18,TRUE,"<p>This doesn't answer your question about cast, but you could certainly subset and do two melts, followed by a merge:</p>

<pre><code>dm1 &lt;- melt(d[,c(""Type"",""I.alt"",""idx06"",""idx07"",""idx08"")], id=c(""Type"",""I.alt""))
dm2 &lt;- melt(d[,c(""Type"",""I.alt"",""farve1"",""farve2"")], id=c(""Type"",""I.alt""))
colnames(dm2) &lt;- c(""Type"", ""I.alt"", ""variable2"", ""value2"")
dm &lt;- merge(dm1, dm2)
</code></pre>

<p>Or, equivalently, do one melt (like you're currently doing) then subset the melted dataframe twice (<code>idx &lt;- variable %in% c(""idx06"",""idx07"",""idx08""</code>) as one and <code>!idx</code> as the other) and merge that output.  </p>

<p>Either way you get what you want:</p>

<pre><code>&gt; head(dm)
              Type I.alt variable    value variable2 value2
1 Alvorligere vold  1154    idx08 1.108696    farve1    red
2 Alvorligere vold  1154    idx08 1.108696    farve2    red
3 Alvorligere vold  1154    idx07 1.027174    farve1    red
4 Alvorligere vold  1154    idx07 1.027174    farve2    red
5 Alvorligere vold  1154    idx06 1.000000    farve1    red
6 Alvorligere vold  1154    idx06 1.000000    farve2    red
</code></pre>
"
1545353,163053,2009-10-09T18:27:21Z,1545302,5,TRUE,"<p>I'm not an expert on this, but it's my understanding that this is possible with lattice, but not with ggplot2.  <a href=""http://learnr.wordpress.com/2009/07/15/ggplot2-version-of-figures-in-lattice-multivariate-data-visualization-with-r-part-5/"" rel=""noreferrer"">See this leanr blog post</a> for an example of a secondary axis plot.  <a href=""http://tolstoy.newcastle.edu.au/R/e2/help/07/07/20962.html"" rel=""noreferrer"">Also see Hadley's response to this question</a>.  </p>

<p>Here's an example of how to do it in lattice (from <a href=""http://www.mail-archive.com/r-help@stat.math.ethz.ch/msg90830.html"" rel=""noreferrer"">Gabor Grothendieck</a>):</p>

<pre><code>library(lattice)
library(grid)  # needed for grid.text

# data

Lines.raw &lt;- ""Date  Fo  Co
6/27/2007  57.1  13.9
6/28/2007  57.7  14.3
6/29/2007  57.8  14.3
6/30/2007  57  13.9
7/1/2007  57.1  13.9
7/2/2007  57.2  14.0
7/3/2007  57.3  14.1
7/4/2007  57.6  14.2
7/5/2007  58  14.4
7/6/2007  58.1  14.5
7/7/2007  58.2  14.6
7/8/2007  58.4  14.7
7/9/2007    58.7 14.8
""

# in reality next stmt would be DF &lt;- read.table(""myfile.dat"", header = TRUE)
DF &lt;- read.table(textConnection(Lines.raw), header = TRUE)
DF$Date &lt;- as.Date(DF$Date, ""%m/%d/%Y"")

par.settings &lt;- list(
        layout.widths = list(left.padding = 10, right.padding = 10),
        layout.heights = list(bottom.padding = 10, top.padding = 10)
)

xyplot(Co ~ Date, DF, default.scales = list(y = list(relation = ""free"")),
        ylab = ""C"", par.settings = par.settings)

trellis.focus(""panel"", 1, 1, clip.off = TRUE)
  pr &lt;- pretty(DF$Fo)
  at &lt;- 5/9 * (pr - 32)
  panel.axis(""right"", at = at, lab = pr, outside = TRUE)
  grid.text(""F"", x = 1.1, rot = 90) # right y axis label
trellis.unfocus()
</code></pre>
"
1545440,135944,2009-10-09T18:45:10Z,1545302,6,FALSE,"<p>Shane's answer, ""you can't in ggplot2,"" is correct, if incomplete. Arguably, it's not something you <em>want</em> to do. How do you decide how to scale the Y axis? Do you want the means of the lines to be the same? The range? There's no principled way of doing it, and it's too easy to make the results look like anything you want them to look like. Instead, what you might want to do, especially in a time-series like that, is to norm the two lines of data so that at a particular value of t, often min(t), Y1 = Y2 = 100. Here's an example I pulled off of the <a href=""http://bonddad.blogspot.com/2009/10/notes-on-job-losses-during-great.html"" rel=""nofollow noreferrer"">Bonddad Blog</a> (not using ggplot2, which is why it's ugly!) But you can cleanly tell the relative increase and decrease of the two lines, which have completely different underlying scales.</p>

<p><img src=""https://i242.photobucket.com/albums/ff90/AvaBrendan/1982mfg.png"" alt=""alt text""></p>
"
1546557,163053,2009-10-09T23:38:23Z,1545591,0,FALSE,"<p>I would just <a href=""http://groups.google.com/group/ggplot2/browse_thread/thread/5fa2756f15124b25"" rel=""nofollow noreferrer"">point out this related question.</a>  It's a real leap to go from this solution to solving your particular problem, but I'm posting this in case it might help.</p>

<p>With this sample data:</p>

<pre><code>b&lt;-cumsum(rnorm(100))
x&lt;-sample(c(1,2), size=100, replace=TRUE)
t&lt;-1:(length(b))
</code></pre>

<p>Here x would represent your values, b is the color (rising/falling), and t is the x-axis.  Reformat it with melt:</p>

<pre><code>library(ggplot2)
tmp &lt;- melt(data.frame(cbind(t,b,x)),meas=c(""x""))
head(tmp)
</code></pre>

<p>And plot it:</p>

<pre><code>ggplot(tmp, aes(x=t,groups=variable)) +
        facet_wrap(~variable) +
        geom_path(aes(y=b,colour=factor(value))) +
        labs(x=NULL) 
</code></pre>
"
1546967,160314,2009-10-10T03:40:03Z,1545591,5,TRUE,"<p>Your merge didn't work because you only had two colour variables, but three data values. Adding a third colour variable as padding seems to do the trick. geom_line takes it's colour from the previous datapoint's value, so the last value of ""value2"" is not used.</p>

<pre><code>d$farve3&lt;-NA
dm1 &lt;- melt(d[,c(""Type"",""I.alt"",""idx06"",""idx07"",""idx08"")], id=c(""Type"",""I.alt""))
dm2 &lt;- melt(d[,c(""Type"",""I.alt"",""farve1"",""farve2"",""farve3"")], id=c(""Type"",""I.alt""))
colnames(dm2) &lt;- c(""Type"", ""I.alt"", ""variable2"", ""value2"")
dm&lt;-cbind(dm1,dm2)
ggplot(dm, aes(x=variable,y=value,group=Type,label=Type,size=I.alt))+
  geom_line(aes(col=value2))+
  geom_text(data=subset(dm, variable==""idx08""),hjust=-0.2, size=2.5)+
  theme_bw()+
  scale_x_discrete(expand=c(0,1))+
  opts(legend.position=""none"")+
  scale_colour_manual(values=c(""green"",""red""))
</code></pre>
"
1549242,144157,2009-10-10T22:20:00Z,1548913,6,FALSE,"<p>Read the data into R using <code>x &lt;- read.csv(filename)</code>. Make sure the dates come in as character class and weight as numeric.<br>
Then use the following:</p>

<pre><code>require(zoo)
require(forecast) # Needed for the ses function
x$date &lt;- as.Date(x$date,""%m/%d/%Y"") # Guessing you are using the US date format
x$weight &lt;- zoo(x$weight,x$date) # Allows for irregular dates
plot(x$weight, xlab=""Date"", ylab=""Weight"") # Produce time plot
ewma &lt;- as.vector(fitted(ses(ts(x$weight)))) # Compute ewma with parameter selected using MLE
lines(zoo(ewma,x$date),col=""red"") # Add ewma line to plot
</code></pre>
"
1549687,163053,2009-10-11T02:35:30Z,1548913,2,FALSE,"<p>Looks like you need to handle an irregularly spaced time series, so ts is not an option. Use one of the other time series libraries. zoo is the most widely used, but some other options are timeSeries, xts, fts, and its.  Have a look at the CRAN view: <a href=""http://cran.r-project.org/web/views/TimeSeries.html"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/views/TimeSeries.html</a>. </p>

<p>One challange that I can see right now is your date format. I suggest either reformatting the date first in your data or else using the format() function in R, but you will need to convert those into a Date or POSIX object in R to use it with a time series package. </p>

<p>You can use the read.zoo() function to read in your file a a time series. Also have a look at the vignette. For the EWMA, I believe that there are several options there too. Rmetrics  and TTR both have versions.</p>

<p>I'll post an example when I get to a computer.  Incidentally, there are many resources available on this subject. Have a look at this ebook: <a href=""http://www.rmetrics.org/ebooks/TimeSeriesFAQ.pdf"" rel=""nofollow noreferrer"">http://www.rmetrics.org/ebooks/TimeSeriesFAQ.pdf</a>. </p>
"
1551899,144157,2009-10-11T21:58:45Z,1551554,7,FALSE,"<p>Modify your regression as follows:</p>

<pre><code>gas_b &lt;- lm(log(gasq_pop) - log(ps) ~ log(gasp) + log(pcincome) +
  I(log(pn)-log(ps)) + I(log(pd)-log(ps)) + log(years), data=gas) 
summary(gas_b)
</code></pre>

<p>If <code>b=coef(gas_b)</code>, then the relevant coefficients are </p>

<pre><code>log(pn): b[4]
log(pd): b[5]
log(ps): 1 - b[4] - b[5]
</code></pre>
"
1552655,144157,2009-10-12T04:08:14Z,1552438,4,FALSE,"<p>Assume your data set is a two-column dataframe called work.foo with variables a and b. Then the following code is one way to do it in R:</p>

<pre><code>work.bar &lt;- work.foo
work.bar$c &lt;- with( (a==0 &amp; b&gt;=1) + 2*(a==0 &amp; b==0) + 3*(a==1 &amp; b&gt;=1) + 
               4*(a==1 &amp; b==0), data=work.foo)
work.mean &lt;- by(work.bar[,1:2], work.bar$c, mean)
</code></pre>
"
1555358,16632,2009-10-12T15:53:42Z,1554942,1,FALSE,"<p>How about this?</p>

<pre><code>breaks &lt;- c(0, cumsum(chromosome_length))
scale_x_continuous(""Genome Position"", breaks = breaks + 0.5, labels = breaks)
</code></pre>
"
1555571,143319,2009-10-12T16:30:49Z,1552438,4,FALSE,"<p>An alternative is to use <code>ddply()</code> from the plyr package - you wouldn't even have to create a group variable, necessarily (although that's awfully convenient).</p>

<pre><code>ddply(work.foo, c(""a"", ""b""), function(x) c(mean(x$a, na.rm = TRUE), mean(x$b, na.rm = TRUE))
</code></pre>

<p>Of course, if you had the grouping variable, you'd just replace <code>c(""a"", ""b"")</code> with <code>""c""</code>.</p>

<p>The main advantage in my mind is that <code>plyr</code> functions will return whatever kind of object you like - ddply takes a data frame and gives you one back, dlply would return a list, etc.  <code>by()</code> and its *apply brethren usually just give you a list.  I think.</p>
"
1557169,143305,2009-10-12T21:58:35Z,1557137,9,FALSE,"<p>You can of course nest loops:</p>

<pre><code>R&gt; for (i in 1:3) for (j in 1:3) cat(i,j,i*j, ""\n"")
1 1 1 
1 2 2 
1 3 3 
2 1 2 
2 2 4 
2 3 6 
3 1 3 
3 2 6 
3 3 9 
R&gt; 
</code></pre>

<p>There is general sense that you <em>should not</em> as vectorised calls can be easier to read and write:</p>

<pre><code>R&gt; outer(1:3,1:3, ""*"")
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    2    4    6
[3,]    3    6    9
R&gt; 
</code></pre>

<p>but if it is easier to develop with nexted loops, then do so.</p>

<p>As for the 'second index dependent on first index' issue you can use lower.tri(), upper.tri() or indexing to achieve that too.</p>

<pre><code>R&gt; X &lt;- expand.grid(x=1:3, y=1:3)
R&gt; X &lt;- X[ X$x &gt;= X$y, ]
R&gt; outer(X$x, X$y, ""*"")
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    1    1    2    2    3
[2,]    2    2    2    4    4    6
[3,]    3    3    3    6    6    9
[4,]    2    2    2    4    4    6
[5,]    3    3    3    6    6    9
[6,]    3    3    3    6    6    9
R&gt; 
</code></pre>
"
1557535,163053,2009-10-12T23:41:26Z,1557137,10,TRUE,"<p>Sorry for asking what is potentially a dumb question, but in what sense can R not do this?  This works perfectly fine for me:</p>

<pre><code>N &lt;- 5
for (i in 0:(2*N)) {
   for (j in i:N) {
        print(paste(i,j,sep="",""))
    }
}
</code></pre>

<p>Could it be that you just didn't put parentheses around the end of your sequence?</p>

<p><i>Edit:</i>  I see...you want to enforce that the sequence i:3 is always &lt;= 3?  I don't think that's possible with either a sequence or within the clause of a for statement.  You can set a break within the loop, but that's no better than your current approach:</p>

<pre><code>for (i in 1:6) {
    for (j in i:3) {
        if(j &gt; 3) break()
        print(paste(i,j,sep="",""))
    }
}  
</code></pre>

<p>Here's another way to generate this sequence without a for loop:</p>

<pre><code>x &lt;- cbind(rep(c(1,2,3), 3),
    rep(c(1,2,3), each=3))
</code></pre>

<p>Or with expand.grid (as per Dirk's suggestion):</p>

<pre><code>x &lt;- expand.grid(x=1:3, y=1:3)
</code></pre>

<p>Then remove the unwanted cases:</p>

<pre><code>x[x[,1] &gt;= x[,2],]
</code></pre>

<p><i>Edit 2:</i> This may not suit your needs, but I believe that Quantlib has a Libor Market Model implementation.  I'm not sure if it's <a href=""http://dirk.eddelbuettel.com/code/rquantlib.html"" rel=""noreferrer"">exposed in RQuantlib</a>.</p>
"
1557812,144157,2009-10-13T01:26:47Z,1557137,5,FALSE,"<p>The issue here is that <code>i:3</code> is meaningful when <code>i&gt;3</code>. For example, <code>5:3</code> gives <code>(5,4,3)</code>. All you need is a simple if statement to prevent the second loop being run when <code>i&gt;3</code>.</p>

<pre><code>for (i in 1:6) {
    if(i &lt; 4) {
        for (j in i:3) {
            print(paste(i,j,sep="",""))
        }
    }
    # Do more operations for i &gt; 3...
}
</code></pre>

<p>However, if possible try to avoid the explicit looping. Dirk's and Shane's answers provide some ideas on how to do this.</p>
"
1560234,163053,2009-10-13T13:25:22Z,1559724,2,TRUE,"<p><i>Edit:</i> Following your clarification: no, I don't believe that you can use an apply function to do something recursively like that.  The whole point of an apply function is that it applies across the vector/matrix at the same time.</p>

<p>You may also want to <a href=""https://stackoverflow.com/questions/1478758/optimizing-the-computation-of-a-recursive-sequence"">look at this related question on stackoverflow</a>.</p>

<p><i>My old answer:</i></p>

<p>Try this:</p>

<pre><code>a1&lt;-runif(100)
a2&lt;-function(i, a1){
    a1[i]&lt;-a1[i-1]*a1[i]
    a1[i]
}
a3 &lt;- as.numeric(lapply(2:100, a2, a1=a1))
</code></pre>

<p>Unlike a <code>for</code> loop, you need to pass in a reference to anything that you need within an <code>lapply</code>.  The return is also a list, so you need to cast it back into whatever form you want.</p>

<p>You might also want to look at the plyr package for easy ways to do this kind of thing.</p>

<p>Beyond that, you can do your operation without a loop:</p>

<pre><code>a3 &lt;- a1[-length(a1)] * a1[-1]
</code></pre>

<p>In other words, these statements are completely equivalent:</p>

<pre><code>&gt; all((a1[-length(a1)] * a1[-1]) == as.numeric(lapply(2:100, a2, a1=a1)))
[1] TRUE
</code></pre>

<p>But the first version is preferable since it has no iterations.</p>
"
1560418,143305,2009-10-13T13:57:03Z,1560397,5,TRUE,"<p>As the saying goes:  <em>Use the source, Luke!</em> </p>

<p>R functions may be visible at the prompt, but are still parsed meaning that comments are stripped, code indentation is changed etc.   So I would always go to the source.</p>
"
1562384,168168,2009-10-13T19:23:24Z,1562124,15,TRUE,"<p>My understanding is that you need to extract the ID from the filename, and then merge the imported csv with the existing dataframe.</p>

<pre><code>df1 &lt;- read.csv(textConnection(""ID, var1, var2
A,  2,    2
B,  4,    5""))

# assuming the imported csv-files are in working directory
filenames &lt;- list.files(getwd(), pattern = ""ID_[A-Z].csv"")

# extract ID from filename
ids &lt;- gsub(""ID_([A-Z]).csv"", ""\\1"", filenames)

# import csv-files and append ID
library(plyr)
import &lt;- mdply(filenames, read.csv)
import$ID &lt;- ids[import$Var1]
import$Var1 &lt;- NULL

# merge imported csv-files and the existing dataframe
merge(df1, import)  
</code></pre>

<p>Result:</p>

<pre><code>ID var1 var2 time var4 var5
1  B    4    5    0    1    2
2  B    4    5    1    4    5
3  B    4    5    2    1    6
</code></pre>
"
1563978,144157,2009-10-14T02:34:53Z,1563961,22,TRUE,"<p>For the top 5%:</p>

<pre><code>n &lt;- 5
data[data$V2 &gt; quantile(data$V2,prob=1-n/100),]
</code></pre>
"
1567129,161808,2009-10-14T15:36:43Z,1560397,2,FALSE,"<p>I also like going to the source for the same reasons mentioned by Dirk, but for quick and dirty work I also rely on subsets of:</p>

<p><code>getAnywhere(), getFromNameSpace(), THEPACKAGE:::thefn, getGeneric()</code></p>

<p>to see functions which are not exported by the namespace of their package.</p>
"
1567734,143305,2009-10-14T17:17:54Z,1567718,7,FALSE,"<p>That may lead to <code>parse(eval(...))</code> at which point you are open for this critique:</p>

<pre><code>R&gt; library(fortunes)
R&gt; fortune(""parse"")

If the answer is parse() you should usually rethink the question.
   -- Thomas Lumley
      R-help (February 2005)

R&gt;
</code></pre>

<p>So do your functions have to be called <code>MyFunction.1</code> etc pp?</p>
"
1567800,135944,2009-10-14T17:30:49Z,1567718,2,FALSE,"<p>When a function is passed around as an object, it loses its name. See, for example, the results of the following lines:</p>

<pre><code>str(lm)
lm
</code></pre>

<p>You can get the arguments and the body of the function, but not the name.</p>

<p>My suggestion would be to construct a named list of functions, where the name could be printed:</p>

<pre><code>&gt; somefns &lt;- list(lm=lm, aggregate=aggregate)
&gt; str(somefns)
List of 2
 $ lm       :function (formula, data, subset, weights, na.action, method = ""qr"", 
    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, 
    contrasts = NULL, offset, ...)  
 $ aggregate:function (x, ...) 

&gt; somefns[[1]](dist ~ speed, data=cars)

Call:
somefns[[1]](formula = dist ~ speed, data = cars)

Coefficients:
(Intercept)        speed  
     -17.58         3.93  

&gt; names(somefns)[[1]]
[1] ""lm""
</code></pre>
"
1567846,163053,2009-10-14T17:39:18Z,1567718,14,FALSE,"<p>Another approach would be to pass the names of the functions into your report function, and then get the functions themselves with the <code>get()</code> command.  For instance:</p>

<pre><code>function.names &lt;- c(""which"",""all"")
fun1 &lt;- get(function.names[1])
fun2 &lt;- get(function.names[2])
</code></pre>

<p>Then you have the names in your original character vector, and the functions have new names as you defined them.  In this case, the <code>all</code> function is now being called as <code>fun2</code>:</p>

<pre><code>&gt; fun2(c(TRUE, FALSE))
[1] FALSE
</code></pre>

<p>Or, if you really want to keep the original function names, just assign them locally with the assign function:</p>

<pre><code>assign(function.names[2], get(function.names[2]))
</code></pre>

<p>If you run this command right now, you will end up with the <code>all</code> function in your <code>"".GlobalEnv""</code>.  You can see this with <code>ls()</code>.</p>
"
1568170,158065,2009-10-14T18:35:26Z,1567718,6,FALSE,"<p>You can get the unevaluated arguments of a function via <code>match.call</code>.  For example:</p>

<pre><code>&gt; x &lt;- function(y) print(match.call()[2])
&gt; x(lm)
lm()
</code></pre>
"
1568566,187864,2009-10-14T19:50:31Z,1568511,-3,FALSE,"<p>loop over y and move all matched value in X to correct location.</p>
"
1568670,28006,2009-10-14T20:07:51Z,1568511,1,FALSE,"<pre><code>x &lt;- c(2, 2, 3, 4, 1, 4, 4, 3, 3)
y &lt;- c(4, 2, 1, 3)
for(i in y) { z &lt;- c(z, rep(i, sum(x==i))) }
</code></pre>

<p>The result in z: 4 4 4 2 2 1 3 3 3</p>

<p>The important steps:</p>

<ol>
<li><p>for(i in y) -- Loops over the elements of interest.</p></li>
<li><p>z &lt;- c(z, ...) -- Concatenates each subexpression in turn</p></li>
<li><p>rep(i, sum(x==i)) -- Repeats i (the current element of interest) sum(x==i) times (the number of times we found i in x).</p></li>
</ol>
"
1568973,143319,2009-10-14T21:01:04Z,1568511,3,FALSE,"<p>You could convert <code>x</code> into an ordered factor:</p>

<pre><code>x.factor &lt;- factor(x, levels = y, ordered=TRUE)
sort(x)
sort(x.factor)
</code></pre>

<p>Obviously, changing your numbers into factors can radically change the way code downstream reacts to <code>x</code>.  But since you didn't give us any context about what happens next, I thought I would suggest this as an option.</p>
"
1568982,163053,2009-10-14T21:02:12Z,1568511,1,FALSE,"<p>[<i>Edit:</i> Clearly Ian has the right approach, but I will leave this in for posterity.]</p>

<p>You can do this without loops by indexing on your y vector.  Add an incrementing numeric value to y and merge them:</p>

<pre><code>y &lt;- data.frame(index=1:length(y), x=y)
x &lt;- data.frame(x=x)
x &lt;- merge(x,y)
x &lt;- x[order(x$index),""x""]
x
[1] 4 4 4 2 2 1 3 3 3
</code></pre>
"
1569203,160314,2009-10-14T21:47:28Z,1568511,58,TRUE,"<p>Here is a one liner...</p>

<pre><code>y[sort(order(y)[x])]
</code></pre>

<p>[edit:] This breaks down as follows:</p>

<pre><code>order(y)             #We want to sort by y, so order() gives us the sorting order
order(y)[x]          #looks up the sorting order for each x
sort(order(y)[x])    #sorts by that order
y[sort(order(y)[x])] #converts orders back to numbers from orders
</code></pre>
"
1569741,190277,2009-10-15T00:43:05Z,1568511,2,FALSE,"<p>How about?:</p>

<pre><code>rep(y,table(x)[as.character(y)])
</code></pre>

<p>(Ian's is probably still better)</p>
"
1570604,190352,2009-10-15T06:24:34Z,1570379,3,FALSE,"<p>Of course, I later answered my own question.  Although, is there a less hack-y way to do this?  I wonder if one could even fit different functions to different panels.</p>

<p>One technique is to use + scale_fill_manual and scale_colour_manual.  They allow one to specify what colors will be used.  So, in this case, let's say you have</p>

<pre><code>a&lt;-qplot(x, y, facets=~z)+stat_smooth(method=""lm"", aes(colour=z, fill=z))
</code></pre>

<p>You can specify colors for the fill and colour using the following.  Note, the second color is clear, as it is using a hex value with the final two numbers representing transparency.  So, 00=clear.</p>

<pre><code>a+stat_fill_manual(values=c(""grey"", ""#11111100""))+scale_colour_manual(values=c(""blue"", ""#11111100""))
</code></pre>
"
1571030,148801,2009-10-15T08:27:58Z,1570263,1,FALSE,"<p>If you're using base graphics, then <code>text()</code> is probably your best bet, and fiddling with coordinates etc is part of the game. If you want to learn a new framework, the <code>lattice</code> package is a reworking of the basic approach to plotting in R. It be installed by default so <code>help(package='lattice')</code> will get you started.</p>

<p>Here's a pretty good guide (pdf) to graphics in general in R, with a substantial section on <code>lattice</code>:
<a href=""http://wwwmaths.anu.edu.au/~johnm/r-book/2edn/xtras/rgraphics.pdf"" rel=""nofollow noreferrer"">download</a></p>
"
1571191,172261,2009-10-15T09:14:51Z,1570263,4,FALSE,"<p>Here are some alternative options to consider:<br>
 - the <em>gplots</em> package has a <code>textplot</code> function to add some text output in a base graphics plot.<br>
 - <em>plotrix</em> has a function <code>addtable2plot</code><br>
 - for <em>grid</em> graphics <code>grid.text()</code> is available and in <em>gridExtra</em> there is a function <code>grid.table()</code> (see, e.g., <a href=""http://wiki.r-project.org/rwiki/doku.php?id=tips:graphics-grid:table"" rel=""nofollow noreferrer"">R-Wiki</a>)</p>
"
1571985,121704,2009-10-15T12:11:41Z,1395323,40,TRUE,"<p>I just worked on this issue this morning. I found that you can get a list of fonts available to the <code>pdf()</code> command like this:</p>

<pre><code>&gt; names(pdfFonts())
 [1] ""serif""                ""sans""                 ""mono""                
 [4] ""AvantGarde""           ""Bookman""              ""Courier""             
 [7] ""Helvetica""            ""Helvetica-Narrow""     ""NewCenturySchoolbook""
[10] ""Palatino""             ""Times""                ""URWGothic""           
... etc ...
</code></pre>

<p>So I then went about my business with this:</p>

<pre><code>&gt; pdf(file=""plot.pdf"",family=""Palatino"", pointsize=16, width=16,height=10)
</code></pre>
"
1572196,143591,2009-10-15T12:55:48Z,1570050,1,FALSE,"<p>As usual the amazing <a href=""http://www.omegahat.org/"" rel=""nofollow noreferrer"">Omega Project for Statistical Computing</a> is a valuable resource! Take a look at the <a href=""http://www.omegahat.org/Rcompression/"" rel=""nofollow noreferrer"">Rcompression</a> package and try, for example, something like:</p>

<pre><code>?gzip    
txt &lt;- paste(rep(""This is a string"", 40), collapse = ""\n"")
v &lt;- gzip(txt))
writeBin(v, ""test.txt.zip"")
</code></pre>

<p>HTH</p>
"
1572924,16632,2009-10-15T14:55:42Z,1570379,25,TRUE,"<p>Don't think about picking a facet, think supplying a subset of your data to stat_smooth:</p>

<pre><code>ggplot(df, aes(x, y)) +
  geom_point() + 
  geom_smooth(data = subset(df, z ==""a"")) + 
  facet_wrap(~ z)
</code></pre>
"
1574067,170364,2009-10-15T18:02:52Z,1570050,1,FALSE,"<p>I think the command <code>gzfile()</code> may also do what you're looking for.  Also note that in the upcoming version 2.10.0 there are some enhancements to compression functions that may be relevant. (see <a href=""https://svn.r-project.org/R/trunk/NEWS"" rel=""nofollow noreferrer"">https://svn.r-project.org/R/trunk/NEWS</a> -- the svn server may ask you to accept a certificate)</p>
"
1576244,144157,2009-10-16T05:33:21Z,1576201,5,TRUE,"<p>If the dataset is a vector named <code>x</code>:</p>

<pre><code>(1:length(x))[x &gt; 0.5]
</code></pre>

<p>If the dataset is a data.frame or matrix named <code>x</code> and the variable of interest is in column <code>j</code>:</p>

<pre><code>(1:nrow(x))[x[,j] &gt; 0.5]
</code></pre>

<p>But if you just want to find the subset and don't really need the row numbers, use</p>

<pre><code>subset(x, x &gt; 0.5)
</code></pre>

<p>for a vector and </p>

<pre><code>subset(x, x[,j] &gt; 0.5)
</code></pre>

<p>for a matrix or data.frame.</p>
"
1576276,71131,2009-10-16T05:42:45Z,1576201,0,FALSE,"<p>Here's some dummy data:</p>

<pre><code>D&lt;-matrix(c(0.6,0.1,0.1,0.2,0.1,0.1,0.23,0.1,0.8,0.2,0.2,0.2),nrow=3)
</code></pre>

<p>Which looks like:</p>

<pre><code>&gt; D
     [,1] [,2] [,3] [,4]
[1,]  0.6  0.2 0.23  0.2
[2,]  0.1  0.1 0.10  0.2
[3,]  0.1  0.1 0.80  0.2
</code></pre>

<p>And here's the logical row index,</p>

<pre><code>index &lt;- (rowSums(D&gt;0.5))&gt;=1
</code></pre>

<p>You can use it to extract the rows you want:</p>

<pre><code>PeakRows &lt;- D[index,]
</code></pre>

<p>Which looks like this:</p>

<pre><code>&gt; PeakRows
     [,1] [,2] [,3] [,4]
[1,]  0.6  0.2 0.23  0.2
[2,]  0.1  0.1 0.80  0.2
</code></pre>
"
1577508,172261,2009-10-16T11:26:30Z,1577480,6,TRUE,"<p>I think this should do the same</p>

<pre><code>d &lt;- c(3,5,9,12,15);   
x &lt;- integer(max(d))    # initialize integer vector where all entries are zero;
                        # length(x) = max(d) (or last element of d)
x[d] &lt;- 1L              # set x to 1 at the position of each occurence           
</code></pre>
"
1578884,158065,2009-10-16T15:45:47Z,1576201,2,FALSE,"<p><code>which(x &gt; 0.5)</code></p>
"
1580433,113794,2009-10-16T21:07:06Z,1580308,0,FALSE,"<p>This link helps you? <a href=""http://rss.acs.unt.edu/Rdoc/library/R2HTML/html/HTML.data.frame.html"" rel=""nofollow noreferrer"">Write a data.frame (or matrix) to a HTML output</a></p>
"
1580556,163053,2009-10-16T21:34:00Z,1580308,3,TRUE,"<p>You can try using the format command:</p>

<pre><code>format(c(1,10,100)^3, scientific=FALSE)
</code></pre>

<p>Incidentally, I would recommend using <code>xtable</code> for the table itself.  See <a href=""https://stackoverflow.com/questions/1407680/how-can-i-produce-report-quality-tables-from-r"">this related question</a>.</p>
"
1581279,158065,2009-10-17T02:56:04Z,1581232,43,TRUE,"<p>You might want to consider transforming the column using <code>formatC</code></p>

<pre><code>&gt; formatC(1:10 * 100000, format=""d"", big.mark="","")
 [1] ""100,000""   ""200,000""   ""300,000""   ""400,000""   ""500,000""   ""600,000""  
 [7] ""700,000""   ""800,000""   ""900,000""   ""1,000,000""
</code></pre>
"
1588503,16632,2009-10-19T12:46:46Z,1586744,1,TRUE,"<p>Have you looked at <code>&lt;&lt;-</code> ? It makes assigning in the parent environment a little easier.</p>
"
1594225,131057,2009-10-20T12:22:22Z,1594121,1,FALSE,"<p>Use cumulative distribution function <a href=""http://en.wikipedia.org/wiki/Cumulative_distribution_function"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Cumulative_distribution_function</a></p>

<p>Then just use its inverse.
Check here for better picture <a href=""http://en.wikipedia.org/wiki/Normal_distribution"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Normal_distribution</a></p>

<p>That mean: pick random number from [0,1] and set as  CDF, then check Value</p>

<p>It is also called quantile function.</p>
"
1594913,158065,2009-10-20T14:15:24Z,1594121,-1,FALSE,"<p>You could use metropolis-hastings to get samples from the density.</p>
"
1595646,160314,2009-10-20T15:53:52Z,1594121,11,TRUE,"<p>Here is a (slow) implementation of the inverse cdf method when you are only given a density.</p>

<pre><code>den&lt;-dnorm #replace with your own density

#calculates the cdf by numerical integration
cdf&lt;-function(x) integrate(den,-Inf,x)[[1]]

#inverts the cdf
inverse.cdf&lt;-function(x,cdf,starting.value=0){
 lower.found&lt;-FALSE
 lower&lt;-starting.value
 while(!lower.found){
  if(cdf(lower)&gt;=(x-.000001))
   lower&lt;-lower-(lower-starting.value)^2-1
  else
   lower.found&lt;-TRUE
 }
 upper.found&lt;-FALSE
 upper&lt;-starting.value
 while(!upper.found){
  if(cdf(upper)&lt;=(x+.000001))
   upper&lt;-upper+(upper-starting.value)^2+1
  else
   upper.found&lt;-TRUE
 }
 uniroot(function(y) cdf(y)-x,c(lower,upper))$root
}

#generates 1000 random variables of distribution 'den'
vars&lt;-apply(matrix(runif(1000)),1,function(x) inverse.cdf(x,cdf))
hist(vars)
</code></pre>
"
1607454,143305,2009-10-22T14:03:38Z,1607413,2,FALSE,"<p>Did you consider contacting the <a href=""http://groups.google.com/group/rapache"" rel=""nofollow noreferrer""><strong>rapache Google Group</strong></a> as the <a href=""http://biostat.mc.vanderbilt.edu/rapache/index.html"" rel=""nofollow noreferrer""><strong>rapache home page</strong></a> suggests?  You may find more experienced reader there than here.</p>
"
1607727,172261,2009-10-22T14:41:23Z,1607413,1,FALSE,"<p>I have tested the following example and <code>cat(summary(fit)$adj.r.squared)</code>
 works in my (default) setup (latest rapache 1.1.8 and R 2.9.2 under Ubuntu 9.04)</p>

<pre><code>ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group &lt;- gl(2,10,20, labels=c(""Ctl"",""Trt""))
weight &lt;- c(ctl, trt)
fit &lt;- lm(weight ~ group - 1) # omitting intercept
setContentType(""text/html"")
cat('&lt;HTML&gt;&lt;BODY&gt;')
cat(summary(fit)$adj.r.squared)
cat('&lt;/BODY&gt;&lt;/HTML&gt;\n')
DONE
</code></pre>
"
1608170,143305,2009-10-22T15:48:21Z,1608130,72,TRUE,"<p>See <code>help(tryCatch)</code>:</p>

<blockquote>
  <p>Conditions are signaled by
  '<code>signalCondition</code>'.  In addition, the<br>
  '<code>stop</code>' and '<code>warning</code>' functions have
  been modified to also accept<br>
  condition arguments.</p>
</blockquote>

<p>and later under 'See Also':</p>

<blockquote>
  <p>'<code>stop</code>' and '<code>warning</code>' signal conditions, and '<code>try</code>' is essentially a
   simplified version of '<code>tryCatch</code>'.</p>
</blockquote>

<p>so you probably want <code>stop</code>.</p>
"
1608322,163053,2009-10-22T16:12:43Z,1608130,8,FALSE,"<p>Beyond the base functions that Dirk mentions:</p>

<p>The <a HREF=""http://cran.r-project.org/web/packages/R.oo/index.html"" rel=""noreferrer"">R.oo package</a> has additional exception handling functionality, including a throw() function which is very useful.  You can catch exceptions with the usual try or trycatch functions:</p>

<pre><code>&gt; try(throw(""Division by zero."")); print(""It's ok!"");
Error: [2009-10-22 10:24:07] Exception: Division by zero.
[1] ""It's ok!""
</code></pre>

<p>You can read more about it here: <a href=""http://www1.maths.lth.se/help/R/R.oo/"" rel=""noreferrer"">http://www1.maths.lth.se/help/R/R.oo/</a></p>
"
1611484,62389,2009-10-23T05:12:38Z,1607413,0,FALSE,"<p>Just found out that it is not with rapache. It is failing in R itself </p>

<pre><code> *** caught segfault ***
 address (nil), cause 'memory not mapped'

 Traceback:
  1: .Call(""La_chol2inv"", x, size, PACKAGE = ""base"")
  2: chol2inv(Qr$qr[p1, p1, drop = FALSE])
  3: summary.lm(fit)
  4: summary(fit)
  5: cat(summary(fit)$adj.r.squared)

 Possible actions:
 1: abort (with core dump, if enabled)
 2: normal R exit
 3: exit R without saving workspace
 4: exit R saving workspace
</code></pre>

<p>not sure what that means though</p>

<p>-Bharani</p>
"
1612342,62389,2009-10-23T09:32:27Z,1607413,0,TRUE,"<p>I finally figured out the problem. Reading the <a href=""https://bugs.launchpad.net/ubuntu/+source/rkward/+bug/264436"" rel=""nofollow noreferrer"">discussion</a> i 
wrongly libRlapck.so to lapack.so. Looks like that was causing 
problems. 
Did a clean install of R again and then modified apache to explicity 
load the libraries then it all worked 
Thanks 
 - Bharani </p>
"
1614566,165384,2009-10-23T16:32:45Z,1614331,1,FALSE,"<p>postscript takes a command argument, hence postscript(file="""",command=""|cat"")</p>
"
1614733,135944,2009-10-23T17:03:27Z,1614331,1,FALSE,"<p>Why on earth would you want to do that? R is not a very good system for manipulating Postscript files. If nothing else, you can use tempfile() to write the image to a file, which you can then read in using standard file functions. If you wanted to be fancy, you could perhaps use fifo() pipes, but I doubt it'll be much faster. But I suspect you'd be better off with a different approach.</p>
"
1614964,144642,2009-10-23T17:45:23Z,1614889,2,FALSE,"<p>Unless you wanted to get into something like writing a function to fire everything through isdebugged(), I don't think you can.</p>

<p>In debug.c, the function <code>do_debug</code> is what checks for the DEBUG flag being set on an object.  There are only three R functions which call the <code>do_debug</code> C call:  <code>debug</code>, <code>undebug</code> and <code>isdebugged</code>.</p>
"
1615805,16632,2009-10-23T20:49:41Z,1614331,0,FALSE,"<p>You should be able to use a textConnection as follows.</p>

<pre><code>tc &lt;- textConnection(""string"", ""w"")

postscript(tc)
plot(1:10)
dev.off()
</code></pre>

<p>But <code>string</code> remains blank - maybe a bug?</p>
"
1617033,29809,2009-10-24T05:14:12Z,1616983,0,FALSE,"<p>Check your path to see if /usr/local/bin comes before /usr/bin.  If it does, just make sure /usr/bin comes first:</p>

<pre><code>PATH=/usr/bin:${PATH}
</code></pre>

<p>(it's okay if /usr/bin is duplicated appears twice).</p>
"
1617150,172261,2009-10-24T06:17:28Z,1617061,33,TRUE,"<p>Convert your variable to a <code>factor</code>, and set the categories you wish to include in the result using <code>levels</code>. Values with a count of zero will then also appear in the result:</p>

<pre><code>y &lt;- c(0, 0, 1, 3, 4, 4)
table(factor(y, levels = 0:5))
# 0 1 2 3 4 5 
# 2 1 0 1 2 0 
</code></pre>
"
1618618,143305,2009-10-24T17:53:50Z,1616983,16,TRUE,"<p>This is not that well documented (e.g. I failed to locate it in either 'R Extension' or 'R Admin' right now) but Brian Ripley mentioned it a few times on the lists.  </p>

<p>Basically, at R compile time, settings are registered and the stored in $RHOME/etc/Makeconf.  One possibility is to edit that file directly, but you may not have root privileges or may not want to affect all other users.  So the better may be to create</p>

<pre><code>~/.R/Makevars
</code></pre>

<p>with entries</p>

<pre><code>CC=gcc-4.4
CXX=g++-4.4
</code></pre>

<p>plus whichever optmisation flags etc you want to set. That will the affect all subsequent uses of <code>R CMD INSTALL</code> or <code>R CMD check</code> or ... that you run.  </p>

<p>Other files in $RHOME/etc/ can similarly be overridden locally from <code>~/.R/</code>.</p>
"
1618760,190277,2009-10-24T18:56:37Z,1594121,6,FALSE,"<p>To clarify the ""use Metropolis-Hastings"" answer above:</p>

<p>suppose <code>ddist()</code> is your probability density function</p>

<p>something like:</p>

<pre><code>n &lt;- 10000
cand.sd &lt;- 0.1
init &lt;- 0
vals &lt;- numeric(n)
vals[1] &lt;- init 
oldprob &lt;- 0
for (i in 2:n) {
    newval &lt;- rnorm(1,mean=vals[i-1],sd=cand.sd)
    newprob &lt;- ddist(newval)
    if (runif(1)&lt;newprob/oldprob) {
        vals[i] &lt;- newval
    } else vals[i] &lt;- vals[i-1]
   oldprob &lt;- newprob
}
</code></pre>

<p>Notes:</p>

<ol>
<li>completely untested</li>
<li>efficiency depends on candidate distribution (i.e. value of <code>cand.sd</code>).
For maximum efficiency, tune <code>cand.sd</code> to an acceptance rate of 25-40%</li>
<li>results will be autocorrelated ... (although I guess you could always
<code>sample()</code> the results to scramble them, or thin)</li>
<li>may need to discard a ""burn-in"", if your starting value is weird</li>
</ol>

<p>The classical approach to this problem is rejection sampling (see e.g. Press et al <em>Numerical Recipes</em>)</p>
"
1622064,143319,2009-10-25T21:04:57Z,1621848,7,FALSE,"<p>Without any examples, it's hard to know how to respond.  The basic case of what you're describing, however, is this:</p>

<pre><code>#Just a very simple data frame
dat &lt;- data.frame(x = c(1, 2, 3))
#Compute the squared value of each value in x
dat$y &lt;- dat$x^2
#See the resultant data.frame, now with column y
dat
</code></pre>

<p>When you tell R to square a vector (or vector-like structure, like dat$x), it knows to square each value separately.  You don't need to explicitly loop over those values most of the time - although, as Dirk notes, you should only worry about optimizing your loops if they are causing you problems.  That said, I certainly prefer reading and writing</p>

<pre><code>dat$y &lt;- dat$x^2
</code></pre>

<p>to:</p>

<pre><code>for(i in 1:length(dat$x)){
  dat$y[i] &lt;- dat$x[i]^2
}
</code></pre>

<p>... where possible.</p>
"
1622481,189035,2009-10-25T23:31:03Z,1621848,1,FALSE,"<p>if parse.smiles() is a function you want to apply to all the entry of a vector ""vec"", then you can use:</p>

<pre><code>lapply(1:length(vec),parse.smiles(vec[i]))
</code></pre>
"
1622606,171187,2009-10-26T00:31:31Z,1622419,5,TRUE,"<p>A p-1 dimensional hyperplane is defined by a normal vector and a point that the plane passes through:</p>

<pre><code>n.(x-x0) = 0
</code></pre>

<p>where <code>n</code> is the normal vector of length p, <code>x0</code> is a point through which the hyperplane passes, <code>.</code> is a dot product, and the equation must be satisfied for any point <code>x</code> on the plane. We can also write this as</p>

<pre><code>n.x = p
</code></pre>

<p>where <code>p = n.x0</code> is just a number. This is a more compact representation of a hyperplane, which is parameterized by (n,p). To find your hyperplane, suppose your points are x1, ..., xp.
Form a matrix A with p-1 rows and p columns as follows. The rows of p are xi-x1, laid out as rows vectors, for all i>1 (there are only p-1 of them). If your p points are not ""collinear"" as you say (they need to be affinely independent), then matrix A will have rank p-1, and a nullspace dimension of 1. The one vector in the nullspace is the normal vector of the hyperplane. Once you find it (call it n), then <code>p = n.x1</code>. In order to find the nullspace of a matrix, you can use a QR decomposition (see <a href=""http://en.wikipedia.org/wiki/Kernel_%28matrix%29#Numerical_computation_of_null_space"" rel=""noreferrer"">here</a> for details).</p>
"
1623526,143305,2009-10-26T07:47:19Z,1622797,3,FALSE,"<p>I either use old-school print statements, or interactive analysis. For that, I first save state using <code>save()</code>, and then load that into an interactive session (for which I use Emacs/ESS). That allows for interactive work using the script code on a line-by-line basis.</p>

<p>But I often write/test/debug the code in interactive mode first before I deploy in a littler script.</p>
"
1625442,86643,2009-10-26T15:20:28Z,1309263,8,FALSE,"<p>Here's a simple algorithm for quantized data (months later):</p>

<pre><code>"""""" median1.py: moving median 1d for quantized, e.g. 8-bit data

Method: cache the median, so that wider windows are faster.
    The code is simple -- no heaps, no trees.

Keywords: median filter, moving median, running median, numpy, scipy

See Perreault + Hebert, Median Filtering in Constant Time, 2007,
    http://nomis80.org/ctmf.html: nice 6-page paper and C code,
    mainly for 2d images

Example:
    y = medians( x, window=window, nlevel=nlevel )
    uses:
    med = Median1( nlevel, window, counts=np.bincount( x[0:window] ))
    med.addsub( +, - )  -- see the picture in Perreault
    m = med.median()  -- using cached m, summ

How it works:
    picture nlevel=8, window=3 -- 3 1s in an array of 8 counters:
        counts: . 1 . . 1 . 1 .
        sums:   0 1 1 1 2 2 3 3
                        ^ sums[3] &lt; 2 &lt;= sums[4] &lt;=&gt; median 4
        addsub( 0, 1 )  m, summ stay the same
        addsub( 5, 1 )  slide right
        addsub( 5, 6 )  slide left

Updating `counts` in an `addsub` is trivial, updating `sums` is not.
But we can cache the previous median `m` and the sum to m `summ`.
The less often the median changes, the faster;
so fewer levels or *wider* windows are faster.
(Like any cache, run time varies a lot, depending on the input.)

See also:
    scipy.signal.medfilt -- runtime roughly ~ window size
    http://stackoverflow.com/questions/1309263/rolling-median-algorithm-in-c

""""""

from __future__ import division
import numpy as np  # bincount, pad0

__date__ = ""2009-10-27 oct""
__author_email__ = ""denis-bz-py at t-online dot de""


#...............................................................................
class Median1:
    """""" moving median 1d for quantized, e.g. 8-bit data """"""

    def __init__( s, nlevel, window, counts ):
        s.nlevel = nlevel  # &gt;= len(counts)
        s.window = window  # == sum(counts)
        s.half = (window // 2) + 1  # odd or even
        s.setcounts( counts )

    def median( s ):
        """""" step up or down until sum cnt to m-1 &lt; half &lt;= sum to m """"""
        if s.summ - s.cnt[s.m] &lt; s.half &lt;= s.summ:
            return s.m
        j, sumj = s.m, s.summ
        if sumj &lt;= s.half:
            while j &lt; s.nlevel - 1:
                j += 1
                sumj += s.cnt[j]
                # print ""j sumj:"", j, sumj
                if sumj - s.cnt[j] &lt; s.half &lt;= sumj:  break
        else:
            while j &gt; 0:
                sumj -= s.cnt[j]
                j -= 1
                # print ""j sumj:"", j, sumj
                if sumj - s.cnt[j] &lt; s.half &lt;= sumj:  break
        s.m, s.summ = j, sumj
        return s.m

    def addsub( s, add, sub ):
        s.cnt[add] += 1
        s.cnt[sub] -= 1
        assert s.cnt[sub] &gt;= 0, (add, sub)
        if add &lt;= s.m:
            s.summ += 1
        if sub &lt;= s.m:
            s.summ -= 1

    def setcounts( s, counts ):
        assert len(counts) &lt;= s.nlevel, (len(counts), s.nlevel)
        if len(counts) &lt; s.nlevel:
            counts = pad0__( counts, s.nlevel )  # numpy array / list
        sumcounts = sum(counts)
        assert sumcounts == s.window, (sumcounts, s.window)
        s.cnt = counts
        s.slowmedian()

    def slowmedian( s ):
        j, sumj = -1, 0
        while sumj &lt; s.half:
            j += 1
            sumj += s.cnt[j]
        s.m, s.summ = j, sumj

    def __str__( s ):
        return (""median %d: "" % s.m) + \
            """".join([ ("" ."" if c == 0 else ""%2d"" % c) for c in s.cnt ])

#...............................................................................
def medianfilter( x, window, nlevel=256 ):
    """""" moving medians, y[j] = median( x[j:j+window] )
        -&gt; a shorter list, len(y) = len(x) - window + 1
    """"""
    assert len(x) &gt;= window, (len(x), window)
    # np.clip( x, 0, nlevel-1, out=x )
        # cf http://scipy.org/Cookbook/Rebinning
    cnt = np.bincount( x[0:window] )
    med = Median1( nlevel=nlevel, window=window, counts=cnt )
    y = (len(x) - window + 1) * [0]
    y[0] = med.median()
    for j in xrange( len(x) - window ):
        med.addsub( x[j+window], x[j] )
        y[j+1] = med.median()
    return y  # list
    # return np.array( y )

def pad0__( x, tolen ):
    """""" pad x with 0 s, numpy array or list """"""
    n = tolen - len(x)
    if n &gt; 0:
        try:
            x = np.r_[ x, np.zeros( n, dtype=x[0].dtype )]
        except NameError:
            x += n * [0]
    return x

#...............................................................................
if __name__ == ""__main__"":
    Len = 10000
    window = 3
    nlevel = 256
    period = 100

    np.set_printoptions( 2, threshold=100, edgeitems=10 )
    # print medians( np.arange(3), 3 )

    sinwave = (np.sin( 2 * np.pi * np.arange(Len) / period )
        + 1) * (nlevel-1) / 2
    x = np.asarray( sinwave, int )
    print ""x:"", x
    for window in ( 3, 31, 63, 127, 255 ):
        if window &gt; Len:  continue
        print ""medianfilter: Len=%d window=%d nlevel=%d:"" % (Len, window, nlevel)
            y = medianfilter( x, window=window, nlevel=nlevel )
        print np.array( y )

# end median1.py
</code></pre>
"
1625980,16363,2009-10-26T17:04:26Z,1622797,6,TRUE,"<p>You could pass your command line arguments into an interactive shell with --args and then source('') the script.</p>

<pre><code>$ R --args -v

R version 2.8.1 (2008-12-22)
Copyright (C) 2008 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

&gt; require(getopt)
Loading required package: getopt
&gt; opt = getopt(c(
+ 'verbose', 'v', 2, ""integer""
+ ));
&gt; opt
$verbose
[1] 1
&gt; source('my_script.R')
</code></pre>

<p>You could now use the old browser() function to debug.</p>
"
1626229,163053,2009-10-26T17:44:39Z,1622797,3,FALSE,"<p>Another option is to work with the options(error) functionality.  Here's a simple example:</p>

<pre><code>options(error = quote({dump.frames(to.file=TRUE); q()}))
</code></pre>

<p>You can create as elaborate a script as you want on an error condition, so you should just decide what information you need for debugging.</p>

<p>Otherwise, if there are specific areas you're concerned about (e.g. connecting to a database), then wrap them in a tryCatch() function.</p>
"
1628413,192798,2009-10-27T02:13:15Z,1628383,2,FALSE,"<p>start by sorting obj1</p>

<p>then you can do a binary search in obj1 for each element of obj2.  knowing where the element would be, you can compare the distance to the two nearby elements of obj1, giving you the minimum distance.</p>

<p>runtime (where n1 = |obj1| and n2 = |obj2|):
(n1 + n2) log (n1)</p>
"
1628671,158065,2009-10-27T03:48:59Z,1628383,14,TRUE,"<p>I would use a step function sorted on the first vector.  This will avoid loops and is pretty fast in R.</p>

<pre><code>x &lt;- rnorm(1000)
y &lt;- rnorm(1000)
sorted.x &lt;- sort(x)
myfun &lt;- stepfun(sorted.x, 0:length(x))
</code></pre>

<p>Now <code>myfun(1)</code> will give you the index of the largest element of <code>sorted.x</code> whose value is less than <code>1</code>.  In my case,</p>

<pre><code>&gt; myfun(1)  
[1] 842
&gt; sorted.x[842]
[1] 0.997574
&gt; sorted.x[843]
[1] 1.014771
</code></pre>

<p>So you know that the closest element is either <code>sorted.x[myfun(1)]</code> or <code>sorted.x[myfun(1) + 1]</code>.  Consequently (and padding for 0), </p>

<pre><code>indices &lt;- pmin(pmax(1, myfun(y)), length(sorted.x) - 1)
mindist &lt;- pmin(abs(y - sorted.x[indices]), abs(y - sorted.x[indices + 1]))
</code></pre>
"
1631589,160314,2009-10-27T15:25:31Z,1630724,7,TRUE,"<p>you can override the %+% operator to have better string concatination syntax:</p>

<pre><code>'%+%' &lt;- function(x,y) paste(x,y,sep="""")

y&lt;-""y1""
x&lt;-""somethingorother""
query&lt;-
'SELECT DISTINCT x AS ' %+% x %+%',\n'    %+%
'                y AS ' %+% y %+% '\n'    %+%
' FROM tbl
 WHERE id=%s
 AND num=%d'

cat(query,""\n"")
</code></pre>

<p>yields:</p>

<pre><code>&gt; cat(query,""\n"")
SELECT DISTINCT x AS somethingorother,
                y AS y1
 FROM tbl
 WHERE id=%s
 AND num=%d 
</code></pre>
"
1632743,135944,2009-10-27T18:21:36Z,1630724,11,FALSE,"<p>If you're an old C programmer from way back, as I am, you might enjoy just using sprintf().</p>

<p>Borrowing Ian's example:</p>

<pre><code>y&lt;-""y1""
x&lt;-""somethingorother""
query &lt;- sprintf(
'SELECT DISTINCT x AS %s,
                 y AS %s,
 FROM tbl
 WHERE id=%%s
 AND num=%%d', x, y)
</code></pre>

<p>yields:</p>

<pre><code>&gt; cat(query,""\n"")
SELECT DISTINCT x AS somethingorother,
                 y AS y1,
 FROM tbl
 WHERE id=%s
 AND num=%d 
</code></pre>
"
1632941,83761,2009-10-27T18:57:04Z,1632772,25,FALSE,"<p>An ""easy"" way is to simply not have your strings set as factors when importing text data.</p>

<p>Note that the <code>read.{table,csv,...}</code> functions take a <code>stringsAsFactors</code> parameter, which is by default set to <code>TRUE</code>. You can set this to <code>FALSE</code> while you're importing and <code>rbind</code>-ing your data.</p>

<p>If you'd like to set the column to be a factor at the end, you can do that too.</p>

<p>For example:</p>

<pre><code>alltime &lt;- read.table(""alltime.txt"", stringsAsFactors=FALSE)
all2008 &lt;- read.table(""all2008.txt"", stringsAsFactors=FALSE)
alltime &lt;- rbind(alltime, all2008)
# If you want the doctor column to be a factor, make it so:
alltime$doctor &lt;- as.factor(alltime$doctor)
</code></pre>
"
1633025,172261,2009-10-27T19:11:15Z,1632772,4,FALSE,"<p>As suggested in the previous answer, read the columns as character and do the conversion to factors after <code>rbind</code>.
<code>SQLFetch</code> (I assume <em>RODBC</em>) has also the <code>stringsAsFactors</code> or the <code>as.is</code> argument to control the conversion of characters.
Allowed values are as for <code>read.table</code>, e.g., <code>as.is=TRUE</code> or some column number.</p>
"
1636762,163053,2009-10-28T11:46:55Z,1635278,19,TRUE,"<p>Your best bet is to use rda files.  You can use the <code>save()</code> and <code>load()</code> commands to write and read:</p>

<pre><code>set.seed(101)
a = data.frame(x1=runif(10), x2=runif(10), x3=runif(10))

save(a, file=""test.rda"")
load(""test.rda"")
</code></pre>

<p><i>Edit: </i> For completeness, just to cover what Harlan's suggestion might look like (i.e. wrapping the load command to return the data frame):</p>

<pre><code>loadx &lt;- function(x, file) {
  load(file)
  return(x)
}  

loadx(a, ""test.rda"")
</code></pre>

<hr>

<p>Alternatively, have a look at the hdf5, RNetCDF and ncdf packages.  I've experimented with the <a href=""http://cran.r-project.org/web/packages/hdf5/index.html"" rel=""noreferrer"">hdf5 package</a> in the past; this uses <a href=""http://www.hdfgroup.org/HDF5/"" rel=""noreferrer"">the NCSA HDF5 library</a>.  It's very simple:</p>

<pre><code>hdf5save(fileout, ...)
hdf5load(file, load = TRUE, verbosity = 0, tidy = FALSE)
</code></pre>

<p>A last option is to use binary file connections, but that won't work well in your case because readBin and writeBin only support vectors:</p>

<p>Here's a trivial example.  First write some data with ""w"" and append ""b"" to the connection:</p>

<pre><code>zz &lt;- file(""testbin"", ""wb"")
writeBin(1:10, zz)
close(zz)
</code></pre>

<p>Then read the data with ""r"" and append ""b"" to the connection:</p>

<pre><code>zz &lt;- file(""testbin"", ""rb"")
readBin(zz, integer(), 4)
close(zz)
</code></pre>
"
1638457,197321,2009-10-28T16:36:49Z,1630724,1,FALSE,"<p>I've ended up simply hitting the sql string with <code>sql &lt;- gsub(""\n"","""",sql)</code> and <code>sql &lt;- gsub(""\t"","""",sql)</code> before running it. The string itself can be as long as it needs to be, but stays free of any concatenation markup.</p>
"
1640729,168747,2009-10-28T23:12:31Z,1632772,30,TRUE,"<p>It could be caused by mismatch of types in two <code>data.frames</code>.</p>

<p>First of all check types (classes). To diagnostic purposes do this: </p>

<pre><code>new2old &lt;- rbind( alltime, all2008 ) # this gives you a warning
old2new &lt;- rbind( all2008, alltime ) # this should be without warning

cbind(
    alltime = sapply( alltime, class),
    all2008 = sapply( all2008, class),
    new2old = sapply( new2old, class),
    old2new = sapply( old2new, class)
)
</code></pre>

<p>I expect there be a row looks like:</p>

<pre><code>            alltime  all2008   new2old  old2new
...         ...      ...       ...      ...
some_column ""factor"" ""numeric"" ""factor"" ""character""
...         ...      ...       ...      ...
</code></pre>

<p>If so then explanation:
<code>rbind</code> don't check types match. If you analyse <code>rbind.data.frame</code> code then you could see that the first argument initialized output types. If in first data.frame type is a factor, then output data.frame column is factor with levels <code>unique(c(levels(x1),levels(x2)))</code>. But when in second data.frame column isn't factor then <code>levels(x2)</code> is <code>NULL</code>, so levels don't extend. </p>

<p><strong>It means that your output data are wrong! There are <code>NA</code>'s instead of true values</strong></p>

<p>I suppose that:</p>

<ol>
<li>you create you old data with another R/RODBC version so types were created with different methods (different settings - decimal separator maybe)</li>
<li>there are NULL's or some specific data in problematic column, eg. someone change column under database.</li>
</ol>

<p>Solution:</p>

<p>find wrong column and find reason why its's wrong and fixed. Eliminate cause not symptoms.</p>
"
1640872,168976,2009-10-28T23:54:05Z,1635278,11,FALSE,"<p>You may have a look at <code>saveRDS</code> and <code>readRDS</code>.  They are functions for serialization. </p>

<pre><code>x = data.frame(x1=runif(10), x2=runif(10), x3=runif(10))

saveRDS(x, file=""myDataFile.rds"")
x &lt;- readRDS(file=""myDataFile.rds"")
</code></pre>
"
1642228,172261,2009-10-29T08:11:46Z,1641488,1,FALSE,"<p>For instance, for the included example from <code>run.jags</code>, check the structure of the list  using</p>

<pre><code>sink(""results_str.txt"")
str(results$density)
sink()
</code></pre>

<p>Then you will see components named <em>layout</em>. The layout for the two plots of each variable can be set using</p>

<pre><code>results$density$m$layout &lt;- c(1,2)
print(results$density$m)
</code></pre>

<p>The plots for different parameters can be combined using the <code>c.trellis</code> method from the <code>latticeExtra</code> package.</p>

<pre><code>class(results$density$m) &lt;- ""trellis""   # overwrite class ""plotindpages""
class(results$density$c) &lt;- ""trellis""   # overwrite class ""plotindpages""
library(""latticeExtra"")
update(c(results$density$m, results$density$c), layout=c(2,2))
</code></pre>

<p><a href=""http://img88.imageshack.us/img88/6481/ctrellis.png"" rel=""nofollow noreferrer"">output of c.trellis http://img88.imageshack.us/img88/6481/ctrellis.png</a></p>

<p>Another approach is to use <code>grid</code> viewports:</p>

<pre><code>library(""grid"")
results$density$m$layout &lt;- c(2,1)
results$density$c$layout &lt;- c(2,1)
class(results$density$m) &lt;- ""trellis""
class(results$density$c) &lt;- ""trellis""
layout &lt;- grid.layout(2, 1, heights=unit(c(1, 1), c(""null"", ""null"")))
grid.newpage()
pushViewport(viewport(layout=layout))
pushViewport(viewport(layout.pos.row=1))
print(results$density$m, newpage=FALSE)
popViewport()
pushViewport(viewport(layout.pos.row=2))
print(results$density$c, newpage=FALSE)
popViewport()
popViewport()
</code></pre>

<p><a href=""http://img88.imageshack.us/img88/5967/grida.png"" rel=""nofollow noreferrer"">grid output http://img88.imageshack.us/img88/5967/grida.png</a></p>
"
1642376,80406,2009-10-29T08:53:25Z,1642201,6,FALSE,"<p>I've worked a bit with matlab and python/pylab for these purposes.  What these tools have 'built-in' is a programming environment, a shell, and gui tools designed for quickly looking at data from a variety of sources.</p>

<p>In a few commands, you can go from having a csv file to interactive plots on the screen, then to an image export in just about any format.  It takes a minute or two to go from data to visualization once you have the hang of it.  I would imagine this is uncommon in the C++ world (although I have seen some professors with pretty impressive work-flows).</p>

<p>I've tried R, but I can't say much useful about it.  It seems to offer about the same set of features, but it may be troublesome to Google for support.</p>

<p>If you are spending more than a couple minutes getting from data to plot using your current method, it's definitely worth learning one of these environments.  The best choice depends on your colleagues, your work environment, experience, and your budget.</p>
"
1642407,198752,2009-10-29T08:59:48Z,1642201,1,FALSE,"<p>I use R because on the one hand it has everything built in and on the other hand you can still manipulate almost everything or start from scratch.  Nevertheless, R is rather slow for heavy calculations (although I do all my Monte Carlo simulations in it).</p>

<p>I would say that Matlab is best for the availability of mathematical functionalities in general, R is best for data input/manipulation/visualisation/analysis/etc., and C++ for high-speed subroutines.  You can by the way easily integrate C++ (or C, fortran, ...) code in R.  Why not read and manipulate input data in R, apply the models in C++, and analyse/visualize output back in R?</p>
"
1643339,143305,2009-10-29T12:22:25Z,1642201,3,FALSE,"<p>This is a reasonable close double to the previous question on <a href=""https://stackoverflow.com/questions/1257021/suitable-functional-language-for-scientific-statistical-computing/""><strong>suitable functional language for scientific/statistical computing</strong></a> so you may want to peruse the long and detailed answers there.</p>

<p>Answers depends, as so often, on your experience and prior language training. I very much prefer <a href=""http://www.r-project.org"" rel=""nofollow noreferrer""><strong>R</strong></a> for data munging / modeling / visualization.</p>
"
1643482,163053,2009-10-29T12:49:13Z,1642201,16,TRUE,"<p>I have very little experience with F#, but regarding C++/Matlab/R: If the speed of your program's <b>execution</b> is the most important, use C++.  If speed of <b>implementation</b> is the most important, use Matlab or R.  This is true for a number of reasons, not the least of which is their massive libraries of math/stats packages.</p>

<p>Both Matlab and R can be sped up through parallelism: so generally, I think that speed and quality of implementation should be a bigger concern.  That's where the real ""value"" of programming is taking place, in the design of the application.  It's not a minor proposition if you can write 3 or 4 good R programs in the same time it takes you to write 1 good C++ program.</p>

<p>Regarding F#: so far as it is part of Microsoft's framework, it must have a lot to offer.  If you're developing in Visual Studio or working on a big .Net project (for instance), it might make sense to use F#.  On the other hand, you can call both Matlab and R from .Net applications, so I would probably argue that their libraries should be a bigger concern.  For instance, see <a href=""http://www.codeproject.com/KB/cs/RtoCSharp.aspx"" rel=""noreferrer"">this article as an example for R</a> and <a href=""http://www.mathworks.com/products/netbuilder/"" rel=""noreferrer"">the Matlab Builder</a>.</p>

<p>Long story short: <i>comparing F# and Matlab/R isn't a good comparison</i>.  F# is a general purpose programming language, while Matlab/R can be viewed as massive mathematical/data analysis toolkits.  Some people call Matlab or R from F# in order to take advantage of each language's benefits (e.g. see <a href=""http://cs.hubfs.net/forums/thread/10308.aspx"" rel=""noreferrer"">this discussion</a>, <a href=""http://www.strangelights.com/fsharp/wiki/default.aspx/FSharpWiki/FSharpAndMATLAB.html"" rel=""noreferrer"">this article on Matlab/F#</a>, or <a href=""http://cs.hubfs.net/blogs/thepopeofthehub/archive/2007/11/06/FSharpWithR.aspx"" rel=""noreferrer"">this article on R/F#</a>).  </p>

<p>So far as charting is concerned: R is extremely strong on this front.  Have a look at <a href=""http://cran.r-project.org/web/views/Graphics.html"" rel=""noreferrer"">the graphics view on CRAN</a> and <a href=""http://learnr.wordpress.com/2009/08/26/ggplot2-version-of-figures-in-lattice-multivariate-data-visualization-with-r-final-part/"" rel=""noreferrer"">this series of posts on the LearnR blog about Lattice and ggplot2</a>.</p>
"
1645086,169947,2009-10-29T16:52:01Z,1630724,3,FALSE,"<p>I'd recommend just using a plain string, and not embedding variable values into it.  Use placeholders instead.</p>

<pre><code>sql &lt;- ""SELECT foo FROM bar
    WHERE col1 = ?
    AND col2 = ?
    ORDER BY yomama""
</code></pre>

<p>I'm not sure if the double-quote is the best way to embed multi-line strings in R code (is there something like here-docs?), but it does work, unlike in Java.</p>

<p>Is there some reason you don't want to send <code>""\n""</code> or <code>""\t""</code> to your database?  They should be fine in the SQL.</p>
"
1645191,143377,2009-10-29T17:05:30Z,1644661,8,TRUE,"<p>One way is to construct the data.frame with the mean values before hand.</p>

<pre><code>library(reshape)
dfs &lt;- recast(data.frame(cat1, cat2, val), cat1+cat2~variable, fun.aggregate=mean)
qplot(val, data=df, geom=""histogram"", binwidth=0.2, facets=cat1~cat2) + geom_vline(data=dfs, aes(xintercept=val), colour=""red"") + geom_text(data=dfs, aes(x=val+1, y=1, label=round(val,1)), size=4, colour=""red"")
</code></pre>
"
1645228,143305,2009-10-29T17:10:49Z,1642119,0,FALSE,"<p>Hm, that sounds like a boosting approach applied to clustering, and a quick Google search reveals quite an existing literature on <a href=""http://www.google.com/search?q=boosting+clustering"" rel=""nofollow noreferrer"">boosting clustering</a>. Maybe that is a start?</p>

<p>As for R code, there are always the Task Views on <a href=""http://cran.r-project.org/web/views/Cluster.html"" rel=""nofollow noreferrer"">Clustering</a> and <a href=""http://cran.r-project.org/web/views/MachineLearning.html"" rel=""nofollow noreferrer"">Machine Learning</a>.</p>
"
1645957,72071,2009-10-29T19:20:49Z,1642201,0,FALSE,"<p>I always prototype my models in MATLAB.  If my prototype is fast enough, I refactor and it's done.  If not, I go back and implement certain functions in C to be called by MATLAB.  This requires knowledge of a low level language, which I think is always going to be the case if you are doing anything that is technically challenging.</p>

<p>I'm intrigued with <a href=""http://books.google.com/books?id=8Cf16JkKz30C&amp;pg=PA21&amp;lpg=PA21#v=onepage&amp;q=&amp;f=false"" rel=""nofollow noreferrer"">this Lisp flavor</a> if it ever gets off the ground.  </p>
"
1647274,168137,2009-10-30T00:04:36Z,1647236,2,FALSE,"<p>When I run your code, I get a warning:</p>

<pre><code>Warning message:
In grepl(df$letter, df$food) :
  argument 'pattern' has length &gt; 1 and only the first element will be used
</code></pre>

<p>This is confirmed by <code>?grepl</code> under <code>pattern</code>:</p>

<pre><code>If a character vector of length 2 or more is supplied, 
the first element is used with a warning.
</code></pre>

<p>So grepl is finding the a in both apple and pear. This doesn't solve your problem (apply or one of its variants?), but it does explain the output you are getting.</p>
"
1647409,178787,2009-10-30T00:39:03Z,1647236,5,TRUE,"<p>Thanks to Kevin's suggestion to use apply,</p>

<blockquote>
  <p><code>&gt;</code> mapply(grepl,df$letter,df$food)</p>
</blockquote>

<p>results in the desired output.</p>
"
1648684,172261,2009-10-30T08:55:47Z,1641488,1,FALSE,"<p>The easiest way to combine the plots is to use the results stored in results$mcmc:</p>

<pre><code># prepare data, see source code of ""run.jags""
thinned.mcmc &lt;- combine.mcmc(list(results$mcmc),
                             collapse.chains=FALSE,
                             return.samples=1000)
print(densityplot(thinned.mcmc[,c(1,2)], layout=c(1,2),
                  ylab=""Density"", xlab=""Value""))
</code></pre>
"
1650315,2979,2009-10-30T14:58:01Z,1649503,1,FALSE,"<p>In the Python interactive interpreter if an expression returns a value then that value is automatically printed. For example if you create a dictionary and extract a value from it the value is automatically printed, but if this was in an executing script this would not be the case. Look at the following simple example this is not an error but simply python printing the result of the expression:</p>

<pre><code>&gt;&gt;&gt; mymap = {""a"":23}
&gt;&gt;&gt; mymap[""a""]
23
</code></pre>

<p>The same code in a python script would produce no output at all.</p>

<p>In your code you are accessing a map like structure with the code: </p>

<p><code>&gt;&gt;&gt; robjects.r['pi']</code></p>

<p>This is returning some R2Py object for which the default string representation is: <code>&lt;RVector - Python:0x0121D8F0 / R:0x022A1760&gt;</code> </p>

<p>If you changed the code to something like:</p>

<p><code>pi = robjects.r['pi']</code></p>

<p>you would see no output but the result of the call (a vector) will be assigned to the variable <code>pi</code> and be available for you to use.</p>

<p>Looking at the <a href=""http://rpy.sourceforge.net/rpy_documentation.html"" rel=""nofollow noreferrer"">R2Py documentation</a> It seems many of the objects are by default printed as a type in &lt;> brackets and some memory address information.</p>
"
1650359,163053,2009-10-30T15:06:09Z,1649503,7,TRUE,"<p>Have you tried looking at the vector that's returned?</p>

<pre><code> &gt;&gt;&gt; pi = robjects.r['pi']
 &gt;&gt;&gt; pi[0]
 3.14159265358979
</code></pre>
"
1651151,16363,2009-10-30T17:15:40Z,1649503,5,FALSE,"<p>To expand on Shane's answer.  rpy2 uses the following Python objects to represent the basic R types:</p>

<ul>
<li>RVector: R scalars and vectors, R Lists are represented as RVectors with names, see below</li>
<li>RArray:  an R matrix, essentially the RVector with a dimension </li>
<li>RDataFrame: an R data.frame</li>
</ul>

<p>To coerce back to basic Python types <a href=""http://rpy.sourceforge.net/rpy2/doc/html/robjects_convert.html#a-simple-example"" rel=""nofollow noreferrer"">look here</a>.  </p>

<p>As an example, I use this to convert an R List to a python dict:</p>

<pre><code>rList = ro.r('''list(name1=1,name2=c(1,2,3))''')
pyDict = {}
for name,value in zip([i for i in rList.getnames()],[i for i in rList]):
    if len(value) == 1: pyDict[name] = value[0]
    else: pyDict[name] = [i for i in value]
</code></pre>
"
1652702,149046,2009-10-30T22:52:45Z,1652522,6,TRUE,"<p>You can do something along the following lines (I could not test as I have no such structure):</p>

<pre><code>extract.year &lt;- function(my.year) lapply(x, function(y) y[[my.year]])

x.by.year &lt;- sapply(my.list.of.years, function(my.year)
    do.call(rbind, extract.year(my.year)))   
</code></pre>

<p>The function extract year creates a list containing just the dataframes for the given year. Then you rbind them...</p>
"
1653089,16632,2009-10-31T01:24:16Z,1652522,31,FALSE,"<p>Collapse it into a list first:</p>

<pre><code>list &lt;- unlist(listoflists, recursive = FALSE)
df &lt;- do.call(""rbind"", list)
</code></pre>
"
1653331,144157,2009-10-31T03:42:16Z,1653271,6,FALSE,"<p>The median of two observations is simply the mean. So <code>rowMeans(s[,c(""A1"",""B1"")])</code>. Equivalently, <code>apply(s[,c(""A1"",""B1"")],1,median)</code></p>
"
1653383,16632,2009-10-31T04:13:43Z,1653271,6,FALSE,"<p>Another solution:</p>

<pre><code>library(plyr)
colwise(median)(s[c(""A1"", ""B1"")])
</code></pre>

<p>which has the advantage of returning a data frame.</p>
"
1653754,170792,2009-10-31T08:16:06Z,1653710,1,FALSE,"<p>Ok, it was easy
    merge(x,y)</p>
"
1654241,163053,2009-10-31T12:28:31Z,1653710,4,TRUE,"<p>This was <a HREF=""http://stackoverflow.com/questions/1299871/how-to-join-data-frames-in-r-inner-outer-left-right"">already answered on stackoverflow</a>.</p>

<p>Beyond merge, if you're more comfortable with SQL you should check out the sqldf package, which allows you to run SQL queries on data frames. </p>

<pre><code> library(sqldf)
 z &lt;- sqldf(""SELECT X1, X2, X3 FROM x JOIN y
      USING(X1)"")
</code></pre>

<p>That said, you will be better off learning the base R functions (merge, intersect, union, etc.) in the long run.</p>
"
1655072,163053,2009-10-31T17:47:16Z,1642119,1,TRUE,"<p>First, have a look at <a href=""http://people.brunel.ac.uk/~cssrajt/RConsensus/"" rel=""nofollow noreferrer"">Allan Tucker's code for consensus clustering</a>, related to his paper <a href=""http://people.brunel.ac.uk/~cssrajt/Papers/GBConsensus.pdf"" rel=""nofollow noreferrer"">""Consensus Clustering and Functional Interpretation of Gene Expression Data""</a>.</p>

<p>Here are a few other pointers:</p>

<ul>
<li>You mentioned that you're using <a href=""http://cran.r-project.org/web/packages/maanova/"" rel=""nofollow noreferrer"">the maanova package</a>; this can build a consensus tree out of bootstrap cluster result with the <code>consensus()</code> function.  Have you tried that?</li>
<li>The <a href=""http://cran.r-project.org/web/packages/ape/"" rel=""nofollow noreferrer"">ape package</a> is intended for phylogenetic tree analysis, so it's possibly not completely relevant, but you might look into it.  There is <a href=""http://www.mail-archive.com/r-help@stat.math.ethz.ch/msg55545.html"" rel=""nofollow noreferrer"">an example using hclust on R-Help</a>.</li>
<li>Similarly, the <a href=""http://www.bioconductor.org/packages/2.4/bioc/html/nem.html"" rel=""nofollow noreferrer"">nem package, which is part of bioconductor</a> has some examples.</li>
</ul>
"
1655169,163053,2009-10-31T18:23:59Z,1614889,7,TRUE,"<p>This is convoluted, but it works:</p>

<pre><code> find.debugged.functions &lt;- function(environments=search()) {
    r &lt;- do.call(""rbind"", lapply(environments, function(environment.name) {
    return(do.call(""rbind"", lapply(ls(environment.name), function(x) {
          if(is.function(get(x))) {
             is.d &lt;- try(isdebugged(get(x)))
             if(!(class(is.d)==""try-error"")) {
                return(data.frame(function.name=x, debugged=is.d))
             } else { return(NULL) }
          }
       })))
     }))
     return(r)
 }
</code></pre>

<p>You can run it across all your environments like so:</p>

<pre><code>find.debugged.functions()
</code></pre>

<p>Or just in your "".GlobalEnv"" with this:</p>

<pre><code> &gt; find.debugged.functions(1)
             function.name debugged
 1 find.debugged.functions    FALSE
 2                    test     TRUE
</code></pre>

<p>Here I created a test function which I am debugging.</p>
"
1655621,163053,2009-10-31T20:59:04Z,1655454,11,FALSE,"<p>What do you mean ""build a new dataset""?  Like this?</p>

<pre><code>s2 &lt;- data.frame(median1, median2, median3, median4)
</code></pre>

<p>Or else use cbind:</p>

<pre><code>s2 &lt;- cbind(median1, median2, median3, median4)
</code></pre>
"
1655954,163053,2009-10-31T23:51:44Z,1655792,1,FALSE,"<p>There are no functions to do that natively in R, but you can create an index of the original NA positions and then swap the values there after the lag.</p>

<pre><code>x &lt;- xts(c(100, 101, 97, 95, 99, NA, 104, 103, 103, 100), as.Date(""2009-11-01"") + 0:9)
lag.xts.na &lt;- function(x, ...) {
    na.idx &lt;- which(is.na(x))
    x2 &lt;- lag.xts(x, ...)
    x2[na.idx+1,] &lt;- x2[na.idx,]
    x2[na.idx,] &lt;- NA
    return(x2)
}

lag.xts.na(x)
           [,1]
2009-11-01   NA
2009-11-02  100
2009-11-03  101
2009-11-04   97
2009-11-05   95
2009-11-06   NA
2009-11-07   99
2009-11-08  104
2009-11-09  103
2009-11-10  103
</code></pre>

<p>Incidentally, are you just trying to deal with weekends/holidays or something along that line?  If so, you might consider dropping those positions from your series; that will dramatically simplify things for you.  Alternatively, the timeSeries package in Rmetrics has a number of functions to deal with business days.</p>
"
1656139,107437,2009-11-01T01:48:37Z,1656026,4,FALSE,"<p>There is no reason your code should not work as is.  It works for me just fine.  Are you attempting to do something more than the example code you gave?</p>

<p>Is there a reason not to just use <code>plot(median, pch = ""."", main = ""Foo"")</code>?</p>
"
1656204,143305,2009-11-01T02:19:07Z,1656026,4,FALSE,"<p>Did you look at the <code>mtext()</code> command?  Did you look at the <code>par(oma=c(...))</code> option?  Try something like</p>

<pre><code>oldpar &lt;- par(oma=c(0,0,2,0), mar=c(3,3,3,1), mfrow=c(1,2))
plot(cumsum(rnorm(1:100)), main=""First plot"", type='l')
plot(cumsum(rnorm(1:100)), main=""Second plot"", type='l')
mtext(""Overall title"", outer=TRUE, cex=1.5)
par(oldpar)
</code></pre>
"
1657982,200571,2009-11-01T18:55:32Z,1167448,1,FALSE,"<p>log(x) on a sparse matrix is a bad idea since log(0) isn't defined and most elements of a sparse matrix are zero.</p>

<p>If you would just like to get the log of the non-zero elements, try converting to a triplet sparse representation and taking a log of those values.</p>
"
1658055,172261,2009-11-01T19:20:43Z,1658032,6,TRUE,"<p>You probably want the packages <em><a href=""http://cran.r-project.org/web/packages/scatterplot3d/index.html"" rel=""nofollow noreferrer"">scatterplot3d</a></em> or <em><a href=""http://cran.r-project.org/web/packages/HH/index.html"" rel=""nofollow noreferrer"">HH</a></em> (function <code>regr2.plot</code>). See, e.g., this <a href=""http://rgraphgallery.blogspot.co.at/2013/04/rg-3d-scatter-plots-with-vertical-lines.html"" rel=""nofollow noreferrer"">example</a> for <code>scatterplot3d</code>.</p>
"
1658280,163053,2009-11-01T20:53:06Z,1658032,6,FALSE,"<p><a href=""http://cran.r-project.org/web/packages/lattice/index.html"" rel=""nofollow noreferrer"">Lattice supports 3D charts</a>.  See some of the nice examples from <a href=""http://lmdvr.r-forge.r-project.org/figures/figures.html"" rel=""nofollow noreferrer"">Lattice: Multivariate Data Visualization with R</a> on <a href=""http://learnr.wordpress.com/2009/07/20/ggplot2-version-of-figures-in-lattice-multivariate-data-visualization-with-r-part-6/"" rel=""nofollow noreferrer"">the LearnR blog</a>.  For instance, with <code>wireframe()</code>:</p>

<pre><code>pl &lt;- wireframe(fit.linear + fit.loess.1 + fit.loess.2 +
+     fit.locfit ~ wind * temperature | radiation, grid,
+     outer = TRUE, shade = TRUE, zlab = """")
</code></pre>

<p><img src=""https://learnr.files.wordpress.com/2009/07/chapter06-06_08_l_small.png""></p>
"
1660258,144157,2009-11-02T09:38:41Z,1660124,13,FALSE,"<p>If <code>x</code> is a dataframe with your data, then the following will do what you want:</p>

<pre><code>require(reshape)
recast(x, Category ~ ., fun.aggregate=sum)
</code></pre>
"
1660288,168168,2009-11-02T09:44:34Z,1660124,19,FALSE,"<pre><code>library(plyr)
ddply(tbl, .(Category), summarise, sum = sum(Frequency))
</code></pre>
"
1660958,95048,2009-11-02T12:15:52Z,1660124,14,FALSE,"<p>Just to add a third option:</p>

<pre><code>require(doBy)
summaryBy(Frequency~Category, data=yourdataframe, FUN=sum)
</code></pre>

<p>EDIT: this is a very old answer. Now I would recommend the use of group_by and summarise from dplyr, as in @docendo answer.</p>
"
1661144,172261,2009-11-02T12:52:46Z,1660124,194,TRUE,"<p>Using <code>aggregate</code>:</p>

<pre><code>aggregate(x$Frequency, by=list(Category=x$Category), FUN=sum)
  Category  x
1    First 30
2   Second  5
3    Third 34
</code></pre>

<hr>

<p>(embedding @thelatemail comment), <code>aggregate</code> has a formula interface too</p>

<pre><code>aggregate(Frequency ~ Category, x, sum)
</code></pre>

<p>Or if you want to aggregate multiple columns, you could use the <code>.</code> notation (works for one column too)</p>

<pre><code>aggregate(. ~ Category, x, sum)
</code></pre>

<hr>

<p>or <code>tapply</code>:</p>

<pre><code>tapply(x$Frequency, x$Category, FUN=sum)
 First Second  Third 
    30      5     34 
</code></pre>

<hr>

<p>Using this data:</p>

<pre><code>x &lt;- data.frame(Category=factor(c(""First"", ""First"", ""First"", ""Second"",
                                      ""Third"", ""Third"", ""Second"")), 
                    Frequency=c(10,15,5,2,14,20,3))
</code></pre>
"
1662207,16363,2009-11-02T16:14:29Z,1661479,11,TRUE,"<p>This is a tough one to answer.  </p>

<p>I recently switched some of my graphing workload from R to matplotlib.  In my humble opinion, I find matplotlib's graphs to be prettier (better default colors, they look crisper and more modern).  I also think matplotlib renders PNGs a whole lot better.</p>

<p>The real motivation for me though, was that I wanted to work with my underlying data in Python (and numpy) and not R. I think this is the big question to ask, in which language do you want to load, parse and manipulate your data?</p>

<p>On the other hand, a bonus for R is that the plotting defaults just work (there's a function for everything).  I find myself frequently digging through the matplotlib docs (they are thick) looking for some obscure way to adjust a border or increase a line thickness.  R's plotting routines have some maturity behind them.</p>
"
1662225,8206,2009-11-02T16:19:23Z,1661479,3,FALSE,"<p>I think that the largest advantage is that matplotlib is based on Python, which you say you already know. So, this is one language less to learn. Just spend the time mastering Python, and you'll benefit both directly for the plotting task at hand and indirectly for your other Python needs. </p>

<p>Besides, IMHO Python is an overall richer language than R, with far more libraries that can help for various tasks. You have to access data for plotting, and data comes in many forms. In whatever form it comes I'm sure Python has an efficient library for it.</p>

<p>And how about embedding those plots in more complete programs, say simple GUIs? matplotlib binds easily with Python's GUI libs (like PyQT) and you can make stuff that only your imagination limits.</p>
"
1662655,163053,2009-11-02T17:42:07Z,1660124,27,FALSE,"<p>This is somewhat <a href=""https://stackoverflow.com/questions/1407449/for-each-group-summarise-means-for-all-variables-in-dataframe-ddply-split"">related to this question</a>.  </p>

<p>You can also just use the <b>by()</b> function:</p>

<pre><code>x2 &lt;- by(x$Frequency, x$Category, sum)
do.call(rbind,as.list(x2))
</code></pre>

<p>Those other packages (plyr, reshape) have the benefit of returning a data.frame, but it's worth being familiar with by() since it's a base function.</p>
"
1663501,163053,2009-11-02T20:31:15Z,1663370,1,FALSE,"<p>I don't see what's especially wrong with your original solution, except that I don't know why you're using the eval() function.  That doesn't seem necessary to me.</p>

<p>You can also use an apply function, such as lapply.  Here's a working example.  I created dummy data as a <code>zoo()</code> time series (this isn't necessary, but since you're working with time series data anyway):</p>

<pre><code># x &lt;- some time series data
time &lt;- as.Date(""2003-02-01"") + c(1, 3, 7, 9, 14) - 1
x &lt;- zoo(data.frame(nominalprice=rnorm(5),realprice=rnorm(5)), time)
lapply(c(""nominalprice"", ""realprice""), function(c.name, x) { 
  png(paste(""c:/TimePlot-"", c.name, "".png"", sep=""""))
  plot(x[,c.name], main=c.name)
  dev.off()
}, x=x)
</code></pre>
"
1663540,136725,2009-11-02T20:37:48Z,1663370,2,FALSE,"<p>If your main issue is the need to type eval(parse(text=i)) instead of ``i'`, you could create a simpler-to-use functions for evaluating expressions from strings:</p>

<pre><code>e = function(expr) eval(parse(text=expr))
</code></pre>

<p>Then the R example could be simplified to:</p>

<pre><code>clist &lt;- c(""nominalprice"", ""realprice"")
for (i in clist) {
  png(paste(""c:/TimePlot-"", i, "".png"", sep=""""))
  plot(time, e(i))
  dev.off() 
}
</code></pre>
"
1663600,143377,2009-11-02T20:48:10Z,1663370,1,FALSE,"<p>Using ggplot2 and reshape: </p>

<pre><code>library(ggplot2)
library(reshape)
df &lt;- data.frame(nominalprice=rexp(10), time=1:10)
df &lt;- transform(df, realprice=nominalprice*runif(10,.9,1.1))
dfm &lt;- melt(df, id.var=c(""time""))
qplot(time, value, facets=~variable, data=dfm)
</code></pre>
"
1663602,158065,2009-11-02T20:48:40Z,1663370,14,TRUE,"<p>As other people have intimated, this would be easier if you had a dataframe with columns named <code>nominalprice</code> and <code>realprice</code>.  If you do not, you could always use <code>get</code>.  You shouldn't need <code>parse</code> at all here.</p>

<pre><code>clist &lt;- c(""nominalprice"", ""realprice"")
for (i in clist) {
   png(paste(""c:/TimePlot-"",i,"".png""), sep="""")
   plot(time, get(i))
   dev.off() 
}
</code></pre>
"
1663641,158065,2009-11-02T20:55:23Z,1663536,2,FALSE,"<p>Without knowing what format your data is in, I can only suggest you look at the <code>tapply</code> function.  From the help:</p>

<pre><code>&gt; n &lt;- 17; fac &lt;- factor(rep(1:3, length = n), levels = 1:5)
&gt; tapply(1:n, fac, sum)
 1  2  3  4  5 
51 57 45 NA NA 
</code></pre>
"
1665259,144157,2009-11-03T04:31:42Z,1664473,8,TRUE,"<p>There are a lot of questions and issues raised here. I'll try to respond to each of them.</p>

<p><code>Arima()</code> is just a wrapper for <code>arima()</code>, so it will give the same model.</p>

<p>arima() handles a model with differencing by using a diffuse prior. That is not the same as just differencing the data before fitting the model. Consequently, you will get slightly different results from <code>arima(x,order=c(p,1,q))</code> and <code>arima(diff(x),order=c(p,0,q))</code>.</p>

<p><code>auto.arima()</code> handles differencing directly and does not use a diffuse prior when fitting. So you will get the same results from <code>auto.arima(x,d=1,...)</code> and <code>auto.arima(diff(x),d=0,...)</code></p>

<p><code>auto.arima()</code> has an argument <code>max.order</code> which specifies the maximum of p+q. By default, <code>max.order=5</code>, so your <code>arima(5,1,4)</code> would not be considered. Increase <code>max.order</code> if you want to consider such large models (although I wouldn't recommend it).</p>

<p>You can't vectorize a loop involving nonlinear optimization at each iteration.</p>

<p>If you want to sort your output, you'll need to save it to a data.frame and then sort on the relevant column. The code currently just spits out the results as it goes and nothing is saved except for the most recent model fitted.</p>
"
1668218,155151,2009-11-03T15:54:21Z,1379549,2,FALSE,"<p>One thing that has saved me some time is the 'auto-insert' mode in emacs.  I have it set up so that each time I open a new .rnw file, emacs automatically sets up a basic document template and all I need to do is start writing my report.</p>

<p>Update: I've switched away from auto-insert.  Now I use the ""template.el"" approach.</p>
"
1670774,202185,2009-11-03T23:15:23Z,1468962,2,FALSE,"<p>It's easy to use the <code>agnes</code> function in the <strong>cluster</strong> package with a dissimilarity matrix. Just set the ""diss"" argument to TRUE. </p>

<p>If you can easily compute the dissimilarity matrix outside R, then that may be the way to go. Otherwise, you can just use the <code>cor</code> function in R to generate the similarity matrix (from which you can get the dissimilarity matrix by subtracting from 1).</p>
"
1671603,65148,2009-11-04T03:57:43Z,1621848,1,FALSE,"<p>The only reason looping is discouraged is that it is slow.  R is designed to work on vectors at a time and has lots of functions to accomplish this.  The whole apply family, as well as functions like Vectorize to help out.  So the idiom is that if your using for loops you're not thinking in R, but sometimes for loops really are just appropriate. </p>

<p>To do this in the R way of thinking, Vectorize your function, if it is not already vectorized (see the Vectorize function)  then call that function with the entire column as an argument and assign that to the new column.</p>

<pre><code>f&lt;-Vectorize(function(x,...),'x')
data$newcolumn&lt;-f(data[,1])
</code></pre>

<p>The apply family (apply, sapply, lapply, mapply, tapply) are also alternatives.  Most native R functions are already vectorized, but be careful when passing extra arguments that are supposed to be interpreted as vectors.</p>
"
1671716,65148,2009-11-04T04:52:18Z,1395233,6,TRUE,"<p>The simple answer is that you can't and shouldn't try to.  That breaks scope and could wreak havoc if it were allowed.  There are a few options that you can think about the problem differently.</p>

<p>first pass y as a function</p>

<pre><code>foo&lt;-function(x,y=min){
m&lt;-1:10
x+y(m)
}
</code></pre>

<p>if a simple function does not work you can move m to an argument with a default.</p>

<pre><code>foo&lt;-function(x,y=min(m),m=1:10){
x+y(m)
}
</code></pre>

<p>Since this is a toy example I would assume that this would be too trivial. If you insist on breaking scope then you can pass it as an expression that is evaluated explicitly.</p>

<pre><code>foo&lt;-function(x,y=expression(min(m))){
m&lt;-1:10
x+eval(y)
}
</code></pre>

<p>Then there is the option of returning a function from another function.  And that might work for you as well, depending on your purpose.</p>

<pre><code>bar&lt;-function(f)function(x,y=f(m)){
m&lt;-1:10
x+y
}
foo.min&lt;-bar(min)
foo.min(1)  #2
foo.max&lt;-bar(max)
foo.max(1)  #10
</code></pre>

<p>But now we are starting to get into the ridiculous.</p>
"
1674939,83761,2009-11-04T16:22:04Z,1671521,2,TRUE,"<p>This answer was recently asked (and answered) on the mailing list.</p>

<p>Check out this thread ""<a href=""http://lists.gnu.org/archive/html/igraph-help/2009-11/msg00042.html"" rel=""nofollow noreferrer"">Eigenvector Centrality</a>"" thread. It looks like a recompile of the R/igraph library might be in order, but should be rather straight forward.</p>
"
1676072,163053,2009-11-04T19:32:46Z,1670803,1,FALSE,"<p>I think that ""bootstrap output matrix missing"" is a fairly generic error, and (unless someone has explicit experience with your case) I would imagine that more information is needed to solve this.  </p>

<p>Without more information, I would suggest that you debug the Gest function.  You have two good options for that: </p>

<p>1) Use the <code>debug()</code> function:</p>

<pre><code> debug(Gest)
</code></pre>

<p>Now run your code.  Then you can walk through the Gest function and see where is breaks.  Before that point, look at all the environment variables by (for instance) using <code>ls()</code> and see if any assumptions are broken.  Presumably something isn't being set correctly.</p>

<p>2) Use <code>recover</code>:</p>

<pre><code> option(error=recover)
</code></pre>

<p>Then you will go into <code>browser</code> mode whenever the error occurs, and you can explore the workspace at that point.</p>
"
1677020,163053,2009-11-04T22:10:56Z,1676990,30,FALSE,"<p>There's probably a better way, but here are two approaches with <code>strsplit()</code>:</p>

<pre><code>as.numeric(data.frame(strsplit(tmp3, "" ""))[2,])
as.numeric(lapply(strsplit(tmp3,"" ""), function(x) x[2]))
</code></pre>

<p>The as.numeric() may not be necessary if you can use characters...</p>
"
1677048,28006,2009-11-04T22:14:14Z,1677021,2,FALSE,"<p>R doesn't compile. There are projects that try to get it compiled: <a href=""http://www.hipersoft.rice.edu/rcc/"" rel=""nofollow noreferrer"">http://www.hipersoft.rice.edu/rcc/</a> , <a href=""http://www.rforge.net/r2c/"" rel=""nofollow noreferrer"">http://www.rforge.net/r2c/</a> but I can't find any currently supported.</p>

<p>That said, the performance on modern hardware seems reasonable for even larger workloads I have thrown at it (millions of records).</p>
"
1677051,176180,2009-11-04T22:15:31Z,1677021,8,FALSE,"<p><a href=""http://en.wikipedia.org/wiki/Interpreted_language"" rel=""noreferrer"">Yes, the R language is interpreted</a>.</p>
"
1677054,125278,2009-11-04T22:16:00Z,1677021,5,FALSE,"<p>It could be either... you can build a compiler or interpreter for any programming language.  The language <strong>SHOULD</strong> be agnostic to the implementation :)</p>
"
1677058,5801,2009-11-04T22:16:59Z,1677021,14,TRUE,"<p>The <a href=""http://www.r-project.org/"" rel=""noreferrer"">R FAQ</a> says: ""The core of R is an interpreted computer language"".</p>
"
1677117,153349,2009-11-04T22:29:15Z,1677021,5,FALSE,"<p>Polaris878 is absolutely right. It's better to say that the default implementation of some language is interpreter or not. But not the language itself.</p>

<p>The question sounds very strange for me. ""interpreted or static or dynamic""... How can we compare translation strategy with typing?</p>

<p>And once again we should be very careful with terminology. It's better to say language with static/dynamic typing than static/dynamic language.</p>
"
1677140,143319,2009-11-04T22:32:58Z,1676990,5,FALSE,"<pre><code>substr(x = tmp3, start = 6, stop = 6)
</code></pre>

<p>So long as your strings are always the same length, this should do the trick.</p>

<p>(And, of course, you don't have to specify the argument names - <code>substr(tmp3, 6, 6)</code> works fine, too)</p>
"
1677502,18308,2009-11-04T23:55:13Z,1677489,1,FALSE,"<p>Certainly not on Palm OS.  Plus, most of the interaction is through the console, so typing with the virtual keyboard will be painful.</p>
"
1677611,143305,2009-11-05T00:28:24Z,1677489,3,TRUE,"<p>R can be embedded, and this is documented in the R Extensions manual (see the <a href=""http://cran.r-project.org/manuals.html"" rel=""nofollow noreferrer"">manuals</a> page for more). </p>

<p>The <a href=""http://dirk.eddelbuettel.com/code/rinside.html"" rel=""nofollow noreferrer"">RInside</a> classes make it easy to embed R inside C++ applications (at least on Linux / OS X). Windows has (D)COM.</p>

<p>But what probably meant to ask were 'ports of R to embedded devices' and that has come up a few times over the years on the mailing lists starting with the Sharp Zaurus etc.  Windows CE and Palm OS are non-starters due to the differences in the toolchain (as you will find out, even on Windows the MinGW tools need to be used).</p>
"
1677755,163053,2009-11-05T01:20:04Z,1677489,3,FALSE,"<p>R has also been used <a href=""http://tolstoy.newcastle.edu.au/R/e8/devel/09/10/0146.html"" rel=""nofollow noreferrer"">on the iPhone and Google Android</a>, if you're looking for a mobile OS.</p>
"
1679157,168747,2009-11-05T08:41:07Z,1676990,21,FALSE,"<p>One could use <code>read.table</code> on <code>textConnection</code>:</p>

<pre><code>X &lt;- read.table(textConnection(tmp3))
</code></pre>

<p>then</p>

<pre><code>&gt; str(X)
'data.frame':   10 obs. of  2 variables:
 $ V1: int  1500 1500 1510 1510 1520 1520 1530 1530 1540 1540
 $ V2: int  2 1 2 1 2 1 2 1 2 1
</code></pre>

<p>so <code>X$V2</code> is what you need.</p>
"
1685205,143305,2009-11-06T03:19:40Z,1685181,12,FALSE,"<p>Consider using the excellent <a href=""http://win-builder.r-project.org"" rel=""noreferrer"">CRAN Win-Builder</a> service to turn your R package sources into an installable zip file for Windows.  </p>

<p>You simply upload by ftp, and shortly thereafter get notice about your package. </p>
"
1685396,144157,2009-11-06T04:23:06Z,1685181,8,FALSE,"<p>You can't just zip up the directory from linux. You need to build specifically for Windows. I've put some instructions <a href=""http://robjhyndman.com/researchtips/building-r-packages-for-windows/"" rel=""noreferrer"">here</a>. However, if you are developing on some other platform first, then Dirk's solution is simpler.</p>
"
1686614,168168,2009-11-06T10:08:47Z,1686569,112,TRUE,"<pre><code>expr[expr$cell_type == ""hesc"", ]

expr[expr$cell_type %in% c(""hesc"", ""bj fibroblast""), ]
</code></pre>
"
1686618,172261,2009-11-06T10:09:06Z,1686569,58,FALSE,"<p>Use <code>subset</code> (for interactive use)</p>

<pre><code>subset(expr, cell_type == ""hesc"")
subset(expr, cell_type %in% c(""bj fibroblast"", ""hesc""))
</code></pre>

<p>or better <code>dplyr::filter()</code></p>

<pre><code>filter(expr, cell_type %in% c(""bj fibroblast"", ""hesc""))
</code></pre>
"
1686622,143591,2009-11-06T10:09:48Z,1676990,4,FALSE,"<p>This should do it:</p>

<pre><code>library(plyr)
ldply(strsplit(tmp3, split = "" ""))[[2]]
</code></pre>

<p>If you need a numeric vector, use</p>

<pre><code>as.numeric(ldply(strsplit(tmp3, split = "" ""))[[2]])
</code></pre>
"
1689117,167483,2009-11-06T03:36:09Z,1774559,1,FALSE,"<p>I may not be answering your question, because it's a bit vague, but some thoughts:</p>

<ol>
<li>You can store the location of 'my directory' in R's .GlobalEnv so that it starts there when you start R.  </li>
<li><a href=""http://www.stat.berkeley.edu/~epurdom/Saving/Saving.html"" rel=""nofollow noreferrer"">This article</a> discuses how to have different working directories with corresponding different "".RData"" files.</li>
<li><p>You could write a custom function that remembers the current directory before you set the new directory</p>

<pre><code>cd &lt;- function(x = """") {   
    logical (length = 0)
    if (!is.logical(x)) {    
        cwd &lt;- getwd()   
        Sys.setenv(""R_OLDWD""=cwd)
        setwd(x)    
      } else {
        setwd(print(paste(Sys.getenv(""R_OLDWD""))))
    }
}
</code></pre></li>
</ol>

<p>From the <a href=""http://cran.r-project.org/bin/windows/base/rw-FAQ.html#How-can-I-keep-workspaces-for-different-projects-in-different-directories_003f"" rel=""nofollow noreferrer"">R for Windows FAQ</a>:</p>

<blockquote>
  <p>The working directory is the directory from which Rgui or Rterm was launched, unless a shortcut was used when it is given by the `Start in' field of the shortcut's properties. You can find this from R code by the call getwd().</p>
  
  <p>The home directory is set as follows: If environment variable R_USER is set, its value is used. Otherwise if environment variable HOME is set, its value is used. After those two user-controllable settings, R tries to find system-defined home directories. It first tries to use the Windows ""personal"" directory (typically C:\Documents and Settings\username\My Documents on Windows XP and C:\Users\username\Documents on Vista). If that fails, if both environment variables HOMEDRIVE and HOMEPATH are set (and they normally are), the value is ${HOMEDRIVE}${HOMEPATH}. If all of these fail, the current working directory is used.</p>
  
  <p>You can find this from R code by Sys.getenv(""R_USER""). </p>
</blockquote>
"
1690753,168542,2009-11-06T21:54:46Z,1676990,9,FALSE,"<p>What I think is the most elegant way to do this</p>

<pre><code>&gt;     res &lt;- sapply(strsplit(tmp3, "" ""), ""[["", 2)
</code></pre>

<p>If you need it to be an integer</p>

<pre><code>&gt;     storage.mode(res) &lt;- ""integer""
</code></pre>
"
1691187,144157,2009-11-06T23:18:46Z,1774559,4,FALSE,"<p>I keep all the code associated with a particular project in a file (or more often a series of files). The first line is usually</p>

<pre><code>setwd(...)
</code></pre>

<p>which sets the directory.</p>

<p>Once a workspace has been saved in the desired directory, just start R by opening that workspace (rather than from the desktop or start menu). Then the directory is already set to where you want it.</p>
"
1692254,205459,2009-11-07T06:45:53Z,1548913,0,FALSE,"<p>There is a really good book on Time Series in R just came out this summer</p>

<p><a href=""http://rads.stackoverflow.com/amzn/click/0387886974"" rel=""nofollow noreferrer"">http://www.amazon.com/Introductory-Time-R-Use/dp/0387886974</a></p>

<p>if you'd like to dig deeper into the subject. </p>

<p>-k</p>
"
1692342,143377,2009-11-07T07:41:24Z,1692336,2,TRUE,"<p>My solution is to get the indexes of the distance vector,  given a row and the size of the matrix. I got this from <a href=""http://www.codeguru.com/cpp/cpp/algorithms/general/article.php/c11211/"" rel=""nofollow noreferrer"">codeguru</a></p>

<pre><code>int Trag_noeq(int row, int col, int N)
{
   //assert(row != col);    //You can add this in if you like
   if (row&lt;col)
      return row*(N-1) - (row-1)*((row-1) + 1)/2 + col - row - 1;
   else if (col&lt;row)
      return col*(N-1) - (col-1)*((col-1) + 1)/2 + row - col - 1;
   else
      return -1;
}
</code></pre>

<p>After translating to R, assuming indexes start at 1, and assuming a lower tri instead of upper tri matrix I got.<br>
EDIT: Using the vectorized version contributed by rcs</p>

<pre><code>noeq.1 &lt;- function(i, j, N) {
    i &lt;- i-1
    j &lt;- j-1
    ix &lt;- ifelse(i &lt; j,
                 i*(N-1) - (i-1)*((i-1) + 1)/2 + j - i,
                 j*(N-1) - (j-1)*((j-1) + 1)/2 + i - j) * ifelse(i == j, 0, 1)
    ix
}

## To get the indexes of the row, the following one liner works:

getrow &lt;- function(z, N) noeq.1(z, 1:N, N)

## to get the row sums

getsum &lt;- function(d, f=sum) {
    N &lt;- attr(d, ""Size"")
    sapply(1:N, function(i) {
        if (i%%100==0) print(i)
        f(d[getrow(i,N)])
    })
}
</code></pre>

<p>So, with the example:</p>

<pre><code>sumd2 &lt;- getsum(d)
</code></pre>

<p>This was <em>much</em> slower than as.matrix for small matrices before vectorizing. But just about 3x as slow after vectorizing. In a Intel Core2Duo 2ghz applying the sum by row of the size 10000 matrix took just over 100s. The as.matrix method fails. Thanks rcs!</p>
"
1692796,172261,2009-11-07T11:48:54Z,1692336,4,FALSE,"<p>This is a vectorized version of the function <code>noeq</code> (either argument <code>i</code> or <code>j</code>):</p>

<pre><code>noeq.1 &lt;- function(i, j, N) {
    i &lt;- i-1
    j &lt;- j-1
    ifelse(i &lt; j,
           i*(N-1) - ((i-1)*i)/2 + j - i,
           j*(N-1) - ((j-1)*j)/2 + i - j) * ifelse(i == j, 0, 1)
}   

&gt; N &lt;- 4
&gt; sapply(1:N, function(i) sapply(1:N, function(j) noeq(i, j, N)))
     [,1] [,2] [,3] [,4]
[1,]    0    1    2    3
[2,]    1    0    4    5
[3,]    2    4    0    6
[4,]    3    5    6    0
&gt; sapply(1:N, function(i) noeq.1(i, 1:N, N))
     [,1] [,2] [,3] [,4]
[1,]    0    1    2    3
[2,]    1    0    4    5
[3,]    2    4    0    6
[4,]    3    5    6    0
</code></pre>

<p>Timings are done on a 2.4 GHz Intel Core 2 Duo (Mac OS 10.6.1):</p>

<pre><code>&gt; N &lt;- 1000
&gt; system.time(sapply(1:N, function(j) noeq.1(1:N, j, N)))
   user  system elapsed 
  0.676   0.061   0.738 
&gt; system.time(sapply(1:N, function(i) sapply(1:N, function(j) noeq(i, j, N))))
   user  system elapsed 
 14.359   0.032  14.410
</code></pre>
"
1693035,16632,2009-11-07T13:34:04Z,1688228,4,TRUE,"<p>Try this</p>

<pre><code>library(ggplot2)
a &lt;- data.frame( x=1:100, y=sin(seq(0.1,10,0.1) )) 
b &lt;- data.frame( x=1:2, y=sin(seq(0.1,0.2, length = 2) )) 
l &lt;- melt(list(a=a,b=b),id.vars=""x"") 

more_than &lt;- function(n) {
  function(df)  {
    if (nrow(df) &gt; n) {
      df
    }
  }
}

lbig &lt;- ddply(l, ""L1"", more_than(5))

qplot( x, value, data=l ) + geom_smooth() + facet_wrap( ~ L1 )
qplot( x, value, data=l ) + geom_smooth(data = lbig) + facet_wrap( ~ L1 )
</code></pre>
"
1694272,163053,2009-11-07T20:19:21Z,1692336,5,FALSE,"<p>First of all, for anyone who hasn't seen this yet, I strongly recommend <b><a href=""http://rwiki.sciviews.org/doku.php?id=tips:programming:code_optim2"" rel=""nofollow noreferrer"">reading this article on the r-wiki</a></b> about code optimization.</p>

<p>Here's another version without using <code>ifelse</code> (that's a relatively slow function):</p>

<pre><code>noeq.2 &lt;- function(i, j, N) {
    i &lt;- i-1
    j &lt;- j-1
    x &lt;- i*(N-1) - (i-1)*((i-1) + 1)/2 + j - i
    x2 &lt;- j*(N-1) - (j-1)*((j-1) + 1)/2 + i - j
    idx &lt;- i &lt; j
    x[!idx] &lt;- x2[!idx]
    x[i==j] &lt;- 0
    x
}
</code></pre>

<p>And timings on my laptop:</p>

<pre><code>&gt; N &lt;- 1000
&gt; system.time(sapply(1:N, function(i) sapply(1:N, function(j) noeq(i, j, N))))
   user  system elapsed 
  51.31    0.10   52.06 
&gt; system.time(sapply(1:N, function(j) noeq.1(1:N, j, N)))
   user  system elapsed 
   2.47    0.02    2.67 
&gt; system.time(sapply(1:N, function(j) noeq.2(1:N, j, N)))
   user  system elapsed 
   0.88    0.01    1.12 
</code></pre>

<p>And lapply is faster than sapply:</p>

<pre><code>&gt; system.time(do.call(""rbind"",lapply(1:N, function(j) noeq.2(1:N, j, N))))
   user  system elapsed 
   0.67    0.00    0.67 
</code></pre>
"
1699296,158065,2009-11-09T05:54:32Z,1699046,84,FALSE,"<p>You can use the <a href=""http://stat.ethz.ch/R-manual/R-devel/library/base/html/by.html"" rel=""noreferrer""><code>by()</code></a> function:</p>

<pre><code>by(dataFrame, 1:nrow(dataFrame), function(row) dostuff)
</code></pre>

<p>But iterating over the rows directly like this is rarely what you want to; you should try to vectorize instead.  Can I ask what the actual work in the loop is doing?</p>
"
1701141,205459,2009-11-09T14:02:05Z,1699046,85,TRUE,"<p>You can try this, using <a href=""http://stat.ethz.ch/R-manual/R-devel/library/base/html/apply.html"" rel=""noreferrer""><code>apply()</code></a> function</p>

<pre><code>&gt; d
  name plate value1 value2
1    A    P1      1    100
2    B    P2      2    200
3    C    P3      3    300

&gt; f &lt;- function(x, output) {
 wellName &lt;- x[1]
 plateName &lt;- x[2]
 wellID &lt;- 1
 print(paste(wellID, x[3], x[4], sep="",""))
 cat(paste(wellID, x[3], x[4], sep="",""), file= output, append = T, fill = T)
}

&gt; apply(d, 1, f, output = 'outputfile')
</code></pre>
"
1701155,163053,2009-11-09T14:04:18Z,1699046,61,FALSE,"<p>First, Jonathan's point about vectorizing is correct.  If your getWellID() function is vectorized, then you can skip the loop and just use cat or write.csv:</p>

<pre><code>write.csv(data.frame(wellid=getWellID(well$name, well$plate), 
         value1=well$value1, value2=well$value2), file=outputFile)
</code></pre>

<p>If getWellID() isn't vectorized, then Jonathan's recommendation of using <code>by</code> or knguyen's suggestion of <code>apply</code> should work.  </p>

<p>Otherwise, if you really want to use <code>for</code>, you can do something like this:</p>

<pre><code>for(i in 1:nrow(dataFrame)) {
    row &lt;- dataFrame[i,]
    # do stuff with row
}
</code></pre>

<p>You can also try to use the <code>foreach</code> package, although it requires you to become familiar with that syntax.  Here's a simple example:</p>

<pre><code>library(foreach)
d &lt;- data.frame(x=1:10, y=rnorm(10))
s &lt;- foreach(d=iter(d, by='row'), .combine=rbind) %dopar% d
</code></pre>

<p>A final option is to use a function out of the <code>plyr</code> package, in which case the convention will be very similar to the apply function.  </p>

<pre><code>library(plyr)
ddply(dataFrame, .(x), function(x) { # do stuff })
</code></pre>
"
1702548,169947,2009-11-09T17:35:10Z,1686569,26,FALSE,"<p>The reason <code>expr[expr[2] == 'hesc']</code> doesn't work is that for a data frame, <code>x[y]</code> selects columns, not rows.  If you want to select rows, change to the syntax <code>x[y,]</code> instead:</p>

<pre><code>&gt; expr[expr[2] == 'hesc',]
  expr_value cell_type
4   5.929771      hesc
5   5.873096      hesc
6   5.665857      hesc
</code></pre>
"
1703673,204903,2009-11-09T20:44:44Z,1528428,3,FALSE,"<p>I think your problem is the model formula, since <a href=""http://old.nabble.com/Error-term-in-logistic-regression-td9155597.html"" rel=""nofollow noreferrer"">logistic regression models have no error term</a>. Thus you model <code>CASE ~ 1</code> should be replaced by something like <code>CASE ~ x</code> (the predictor variable <code>x</code> is mandatory). Here is your example, modified:</p>

<pre><code>CASE &lt;- rbinom(100,1,0.5)
x &lt;- 1:100
posterior_m0 &lt;- MCMClogit (CASE ~ x, b0 = 0, B0 = 1)
classic_m0 &lt;- glm (CASE ~ x,  family=binomial(link=""logit""), na.action=na.pass)
</code></pre>

<p>So I think your problem is not related to the MCMCpack library (disclaimer: I have never used this package).</p>
"
1704413,7714,2009-11-09T22:35:11Z,1704324,3,TRUE,"<p>There's a large but somewhat out of date list <a href=""http://www.efg2.com/Lab/Library/Delphi/MathFunctions/StatisticsAndProbability.htm#Sources"" rel=""nofollow noreferrer"">here</a>. I've never seen anything as comprehensive as SAS or R, however. Then again, SAS and R are more comprehensive than most stats packages for <em>any</em> language.</p>
"
1704787,9217,2009-11-10T00:02:55Z,1704324,4,FALSE,"<p>The library <a href=""http://www.unilim.fr/pages_perso/jean.debord/tpmath/tpmath.htm"" rel=""nofollow noreferrer"">here</a> might be of some help, although I admit I never used the statistic functions so will not be able to vouch for them. You are probably not going to run into many compatibility problems with most of these math libraries, since most of the Unicode changes revolved around strings and characters, not the numerical types.</p>
"
1706701,172261,2009-11-10T09:41:50Z,1641488,2,TRUE,"<p>Here's a way to ditch the ""V1"" panels by manipulation of the Trellis structure:</p>

<pre><code>p1 &lt;- results$density$c
p2 &lt;- results$density$m

p1$layout &lt;- c(1,1)
p1$index.cond[[1]] &lt;- 1   # remove second index
p1$condlevels[[1]] &lt;- ""c""   # remove ""V1""
class(p1) &lt;- ""trellis""   # overwrite class ""plotindpages""

p2$layout &lt;- c(1,1)
p2$index.cond[[1]] &lt;- 1   # remove second index
p2$condlevels[[1]] &lt;- ""m""   # remove ""V1""
class(p2) &lt;- ""trellis""   # overwrite class ""plotindpages""

library(grid)
layout &lt;- grid.layout(2, 1, heights=unit(c(1, 1), c(""null"", ""null"")))
grid.newpage()
pushViewport(viewport(layout=layout))
pushViewport(viewport(layout.pos.row=1))
print(p1, newpage=FALSE)
popViewport()
pushViewport(viewport(layout.pos.row=2))
print(p2, newpage=FALSE)
popViewport()
popViewport()
</code></pre>

<p><a href=""http://img142.imageshack.us/img142/3272/ctrellisa.png"" rel=""nofollow noreferrer"">pot of c.trellis() result http://img142.imageshack.us/img142/3272/ctrellisa.png</a></p>
"
1708193,163053,2009-11-10T14:03:22Z,1708074,13,TRUE,"<p>My first two suggestions are pretty basic: (1) wrap your function call in a <code>try()</code> (that frequently provides more information with S4 classes) and (2) call <code>traceback()</code> after the error is thrown (that can sometimes give hints to where the problem is really occuring).</p>

<p>Calling <code>debug()</code> won't help in this scenario, so you need to use <code>trace</code> or <code>browser</code>.  From the debug help page:</p>

<pre><code>""In order to debug S4 methods (see Methods), you need to use trace, typically 
calling browser, e.g., as ""
  trace(""plot"", browser, exit=browser, signature = c(""track"", ""missing"")) 
</code></pre>

<p>S4 classes can be hard to work with; one example of this is the comment in <a href=""http://cran.r-project.org/web/packages/debug/index.html"" rel=""noreferrer"">the <code>debug</code> package</a> documentation (regarding the usage of <code>mtrace()</code> with S4 classes):</p>

<pre><code>""I have no plans to write S4 methods, and hope not to have to
debug other people’s!""
</code></pre>

<p>A <a href=""http://old.nabble.com/debugging-S4-methods-td25697205.html"" rel=""noreferrer"">similar question was asked recently on R-Help</a>.  The recommendation from Duncan Murdoch:</p>

<pre><code>""You can insert a call to browser() if you want to modify the source.  If
you'd rather not do that, you can use trace() to set a breakpoint in it.
The new setBreakpoint() function in R 2.10.0 will also work, if you
install the package from source with the R_KEEP_PKG_SOURCE=yes
environment variable set.  It allows you to set a breakpoint at a
particular line number in the source code.""
</code></pre>

<p>I've never done this before myself (and it requires R 2.10.0), but you might try installing from source with <code>R_KEEP_PKG_SOURCE=yes</code>.</p>

<p>Incidentally, the NADA package is on R-Forge, which means that <a href=""http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/pkg/R/reg.R?rev=83&amp;root=nada&amp;view=markup"" rel=""noreferrer"">it's very easy to browse the source</a>.</p>
"
1710387,16363,2009-11-10T19:04:32Z,1710340,11,TRUE,"<pre><code>&gt; x&lt;-c('1','2','3','4')
&gt; combn(x,2)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] ""1""  ""1""  ""1""  ""2""  ""2""  ""3"" 
[2,] ""2""  ""3""  ""4""  ""3""  ""4""  ""4""
</code></pre>
"
1710952,143305,2009-11-10T20:27:04Z,1710853,30,TRUE,"<p>Use the virtual framebuffer X11 server -- we do the same to build packages requiring X11 for R builds in headless chroots.  Taking e.g. pars of the Build-Depends from <code>rggobi</code>:</p>

<blockquote>
  <p>xvfb xauth xfonts-base</p>
</blockquote>

<p>After installing these you can use the <code>xvfb-run</code> command.  If you start R via e.g.</p>

<pre><code>xvfb-run R --no-save
</code></pre>

<p>you should now be able to use routines and commands requiring X11 as e.g. some of the plotting devices, or the tcl/tk initialization which also insists on having X11.  </p>

<p>The same trick is useful for web servers.</p>
"
1712666,143305,2009-11-11T02:41:51Z,1712316,2,FALSE,"<p>Without knowing the particulars of this packages, John Chambers ""Software for Data Analsys"" (2008, Springer) has a good discussion on debugging, for example via </p>

<pre><code>&gt; options(error=recover) 
</code></pre>

<p>which may be of help here.</p>
"
1714901,172261,2009-11-11T12:31:52Z,1714818,3,TRUE,"<p>Check the class structure of the resulting <em>S4</em> objects with <code>str</code>, extract the relevant variables to build a dataframe and use <code>write.table</code>/<code>write.csv</code> to export the results. For instance, for the prediction <code>pred</code>:</p>

<pre><code>R&gt; library(""ROCR"")
R&gt; data(ROCR.simple)
R&gt; pred &lt;- prediction(ROCR.simple$predictions, ROCR.simple$labels)
R&gt; perf &lt;- performance(pred, ""fnr"", ""fpr"")
R&gt; str(pred)
Formal class 'prediction' [package ""ROCR""] with 11 slots
  ..@ predictions:List of 1
  .. ..$ : num [1:200] 0.613 0.364 0.432 0.14 0.385 ...
  ..@ labels     :List of 1
  .. ..$ : Ord.factor w/ 2 levels ""0""&lt;""1"": 2 2 1 1 1 2 2 2 2 1 ...
  ..@ cutoffs    :List of 1
  .. ..$ : num [1:201] Inf 0.991 0.985 0.985 0.983 ...
  ..@ fp         :List of 1
  .. ..$ : num [1:201] 0 0 0 0 1 1 2 3 3 3 ...
  ..@ tp         :List of 1
  .. ..$ : num [1:201] 0 1 2 3 3 4 4 4 5 6 ...
  ..@ tn         :List of 1
  .. ..$ : num [1:201] 107 107 107 107 106 106 105 104 104 104 ...
  ..@ fn         :List of 1
  .. ..$ : num [1:201] 93 92 91 90 90 89 89 89 88 87 ...
  ..@ n.pos      :List of 1
  .. ..$ : int 93
  ..@ n.neg      :List of 1
  .. ..$ : int 107
  ..@ n.pos.pred :List of 1
  .. ..$ : num [1:201] 0 1 2 3 4 5 6 7 8 9 ...
  ..@ n.neg.pred :List of 1
  .. ..$ : num [1:201] 200 199 198 197 196 195 194 193 192 191 ...

R&gt; write.csv(data.frame(fp=pred@fp, fn=pred@fn), file=""result_pred.csv"")
</code></pre>

<p>and for performance <code>perf</code>:</p>

<pre><code>R&gt; str(perf)
Formal class 'performance' [package ""ROCR""] with 6 slots
  ..@ x.name      : chr ""False positive rate""
  ..@ y.name      : chr ""False negative rate""
  ..@ alpha.name  : chr ""Cutoff""
  ..@ x.values    :List of 1
  .. ..$ : num [1:201] 0 0 0 0 0.00935 ...
  ..@ y.values    :List of 1
  .. ..$ : num [1:201] 1 0.989 0.978 0.968 0.968 ...
  ..@ alpha.values:List of 1
  .. ..$ : num [1:201] Inf 0.991 0.985 0.985 0.983 ...

R&gt; write.csv(data.frame(fpr=perf@x.values,
                        fnr=perf@y.values, 
                        alpha.values=perf@alpha.values), 
             file=""result_perf.csv"")
</code></pre>
"
1715488,163053,2009-11-11T14:25:19Z,1714280,85,TRUE,"<p>If you haven't done so already, have a look at <a href=""http://cran.r-project.org/web/views/TimeSeries.html"" rel=""noreferrer"">the time series view on CRAN</a>, especially the section on multivariate time series.</p>

<p>In finance, one traditional way of doing this is with a factor model, frequently with either a BARRA or Fama-French type model.  Eric Zivot's <a href=""http://books.google.com/books?id=sxODP2l1mX8C"" rel=""noreferrer"">""Modeling financial time series with S-PLUS""</a> gives a good overview of these topics, but it isn't immediately transferable into R.  Ruey Tsay's ""<a href=""http://rads.stackoverflow.com/amzn/click/0471690740"" rel=""noreferrer"">Analysis of Financial Time Series</a>"" (available in the TSA package on CRAN) also has a nice discussion of factor models and principal component analysis in chapter 9.  </p>

<p>R also has a number of packages that cover <A href=""http://en.wikipedia.org/wiki/Vector_autoregression"" rel=""noreferrer"">vector autoregression (VAR)</a> models.  In particular, I would recommend looking at Bernhard Pfaff's <a href=""http://cran.r-project.org/web/packages/vars/index.html"" rel=""noreferrer"">VAR Modelling (vars)</a> package and <a href=""http://cran.r-project.org/web/packages/vars/vignettes/vars.pdf"" rel=""noreferrer"">the related vignette</a>.</p>

<p>I strongly recommend looking at <a href=""http://faculty.chicagobooth.edu/ruey.tsay/teaching/"" rel=""noreferrer""><b>Ruey Tsay's homepage</b></a> because it covers all these topics, and provides the necessary R code.  In particular, look at the <a href=""http://faculty.chicagobooth.edu/ruey.tsay/teaching/ama/"" rel=""noreferrer"">""Applied Multivariate Analysis""</a>, <a href=""http://faculty.chicagobooth.edu/ruey.tsay/teaching/bs41202/sp2009/"" rel=""noreferrer"">""Analysis of Financial Time Series""</a>, and <a href=""http://faculty.chicagobooth.edu/ruey.tsay/teaching/mts/sp2009/"" rel=""noreferrer"">""Multivariate Time Series Analysis""</a> courses.</p>

<p>This is a very large subject and there are many good books that cover it, including both multivariate time series forcasting and seasonality.  Here are a few more:</p>

<ol>
<li>Kleiber and Zeileis. ""<a href=""http://rads.stackoverflow.com/amzn/click/0387773169"" rel=""noreferrer"">Applied Econometrics with R</a>"" doesn't address this specifically, but it covers the overall subject very well (see also the AER package on CRAN).</li>
<li>Shumway and Stoffer. ""<a href=""http://rads.stackoverflow.com/amzn/click/0387293175"" rel=""noreferrer"">Time Series Analysis and Its Applications: With R Examples</a>"" has examples of multivariate ARIMA models.</li>
<li>Cryer. ""<a href=""http://rads.stackoverflow.com/amzn/click/0387759581"" rel=""noreferrer"">Time Series Analysis: With Applications in R</a>"" is a classic on the subject, updated to include R code.</li>
</ol>
"
1716064,143305,2009-11-11T15:47:19Z,1716012,8,FALSE,"<p>Direct equivalents of tic and toc do not exist. </p>

<p>Please see <code>help(system.time)</code> as well as the R Extensions manual about profiling.  Discussions of profiling and profiling tools is also in the 'Intro to HPC with R' slides referenced on the <a href=""http://cran.r-project.org/web/views/HighPerformanceComputing.html"" rel=""noreferrer"">High Performance Computing with R taskview</a></p>
"
1716344,134830,2009-11-11T16:28:58Z,1716012,36,FALSE,"<p>There are plenty of profiling tools in R, as Dirk mentioned.  If you want the simplicity of tic/toc, then you can do it in R too.</p>

<p>EDIT: I've cannibalised the garbage collection functionality from the MATLAB package, and <code>tic</code> now lets you choose whether you are interested in total elapsed time or just the user time.</p>

<pre><code>tic &lt;- function(gcFirst = TRUE, type=c(""elapsed"", ""user.self"", ""sys.self""))
{
   type &lt;- match.arg(type)
   assign("".type"", type, envir=baseenv())
   if(gcFirst) gc(FALSE)
   tic &lt;- proc.time()[type]         
   assign("".tic"", tic, envir=baseenv())
   invisible(tic)
}

toc &lt;- function()
{
   type &lt;- get("".type"", envir=baseenv())
   toc &lt;- proc.time()[type]
   tic &lt;- get("".tic"", envir=baseenv())
   print(toc - tic)
   invisible(toc)
}
</code></pre>

<p>Usage is, e.g., <code>tic(); invisible(qr(matrix(runif(1e6), nrow=1e3))); toc()</code></p>
"
1717011,197321,2009-11-11T18:01:41Z,1716600,0,FALSE,"<p>The problem was in some theme parameters I'd set, so it went away once I started building a runnable example to reproduce here. Thanks for the help.</p>
"
1717437,172261,2009-11-11T19:18:37Z,1716012,13,FALSE,"<p>There is a MATLAB emulation package <em><a href=""http://cran.r-project.org/web/packages/matlab/index.html"" rel=""noreferrer"">matlab</a></em> on CRAN. It has implementations of <code>tic</code> and <code>toc</code> (but they look very similar to the functions in Richie Cottons answer; ""elapsed"" is used instead of ""user.self"" in <code>proc.time()</code>)</p>

<pre><code>&gt; tic
function (gcFirst = FALSE) 
{
    if (gcFirst == TRUE) {
        gc(verbose = FALSE)
    }
    assign(""savedTime"", proc.time()[3], envir = .MatlabNamespaceEnv)
    invisible()
}
&lt;environment: namespace:matlab&gt;
&gt; toc
function (echo = TRUE) 
{
    prevTime &lt;- get(""savedTime"", envir = .MatlabNamespaceEnv)
    diffTimeSecs &lt;- proc.time()[3] - prevTime
    if (echo) {
        cat(sprintf(""elapsed time is %f seconds"", diffTimeSecs), 
            ""\n"")
        return(invisible())
    }
    else {
        return(diffTimeSecs)
    }
}
&lt;environment: namespace:matlab&gt;
</code></pre>
"
1721990,209467,2009-11-12T12:53:02Z,1721961,2,TRUE,"<p>I think it's your R version. GOFrame wrapping is described to be supported in the bioconductor AnnotationDBI only since the latest release.</p>

<p>I just tried it and it works with R 2.10.0</p>

<p>Enjoy!</p>
"
1721995,143305,2009-11-12T12:53:26Z,1721961,1,FALSE,"<p>As per the <a href=""http://www.bioconductor.org/packages/release/bioc/html/AnnotationDbi.html"" rel=""nofollow noreferrer"">package description</a>, the <code>Go.db</code> package is only suggested rather than depended upon. Hence, a simple</p>

<pre><code> library(GO.db)
</code></pre>

<p>seems to be what you need to do.</p>
"
1722565,16632,2009-11-12T14:32:12Z,1719447,10,TRUE,"<p>Just use the for loop.  Any built-in functions will degenerate to that anyway, and you'll lose clarity of expression, unless you carefully build a function that generalises outer to work with lists.  </p>

<p>The biggest improvement you could make would be to preallocate the matrix:</p>

<pre><code>M &lt;- list()
length(M) &lt;- numElements ^ 2
dim(M) &lt;- c(numElements, numElements)
</code></pre>

<p>PS.  A list is a vector.</p>
"
1724155,143305,2009-11-12T17:55:30Z,1724024,14,TRUE,"<p>First off, a disclaimer: I use <a href=""http://dirk.eddelbuettel.com/code/rcpp.html"" rel=""nofollow noreferrer"">Rcpp</a> all the time.  In fact, when (having been renamed by the time from Rcpp) RcppTemplate had already been orphaned and without updates for two years, I started to maintain it under its initial name of Rcpp (under which it had been contributed to <a href=""http://dirk.eddelbuettel.com/code/rquantlib.html"" rel=""nofollow noreferrer"">RQuantLib</a>). That was about a year ago, and I have made a couple of incremental changes that you can find documented in the ChangeLog. </p>

<p>Now RcppTemplate has very recently come back after a full thirty-five months without any update or fix. It contains interesting new code, but it appears that it is not backwards compatible so I won't use it where I already used Rcpp. </p>

<p><a href=""http://r-forge.r-project.org/projects/rcppbind/"" rel=""nofollow noreferrer"">Rcppbind</a> was not very actively maintained whenever I checked.  Whit Armstrong also has a templated interface package called <a href=""http://github.com/armstrtw/rabstraction"" rel=""nofollow noreferrer"">rabstraction</a>.</p>

<p><a href=""http://cran.r-project.org/web/packages/inline/index.html"" rel=""nofollow noreferrer"">Inline</a> is something completely different: it eases the compile / link cycle by 'embedding' your program as an R character string that then gets compiled, linked, and loaded.  I have talked to Oleg about having inline support Rcpp which would be nice.</p>

<p><a href=""http://www.swig.org"" rel=""nofollow noreferrer"">Swig</a> is interesting too. Joe Wang did great work there and wrapped all of QuantLib for R. But when I last tried it,  it no longer worked due to some changes in R internals. According to someone from the Swig team, Joe may still work on it.  The goal of Swig is larger libraries anyway.  This project could probably do with a revival but it is not without technical challenges.</p>

<p>Another mention should go to <a href=""http://dirk.eddelbuettel.com/code/rinside.html"" rel=""nofollow noreferrer"">RInside</a> which works with Rcpp and lets you embed R inside of C++ applications.</p>

<p>So to sum it up: <a href=""http://dirk.eddelbuettel.com/code/rcpp.html"" rel=""nofollow noreferrer"">Rcpp</a>  works well for me, especially for small exploratory projects where you just want to add a function or two. It's focus is ease of use, and it allows you to 'hide' some of the R internals that are not always fun to work with.  I know of a number of other users whom I have helped on and and off via email.  So I would say go for this one. </p>

<p>My 'Intro to HPC with R' tutorials have some examples of Rcpp, RInside and inline.</p>

<p><strong>Edit:</strong>  So let's look at a concrete example (taken from the 'HPC with R Intro' slides and borrowed from Stephen Milborrow who took it from Venables and Ripley).  The task is to enumerate all possible combinations of the determinant of a 2x2 matrix containing only single digits in each position.  This can be done in clever vectorised ways (as we discuss in the tutorial slides) or by brute force as follows:</p>

<pre><code>#include &lt;Rcpp.h&gt;

RcppExport SEXP dd_rcpp(SEXP v) {
  SEXP  rl = R_NilValue;        // Use this when there is nothing to be returned.
  char* exceptionMesg = NULL;   // msg var in case of error

  try {
    RcppVector&lt;int&gt; vec(v);     // vec parameter viewed as vector of ints
    int n = vec.size(), i = 0;
    if (n != 10000) 
       throw std::length_error(""Wrong vector size"");
    for (int a = 0; a &lt; 9; a++)
      for (int b = 0; b &lt; 9; b++)
        for (int c = 0; c &lt; 9; c++)
          for (int d = 0; d &lt; 9; d++)
            vec(i++) = a*b - c*d;

    RcppResultSet rs;           // Build result set to be returned as list to R
    rs.add(""vec"", vec);         // vec as named element with name 'vec'
    rl = rs.getReturnList();    // Get the list to be returned to R.
  } catch(std::exception&amp; ex) {
    exceptionMesg = copyMessageToR(ex.what());
  } catch(...) {
    exceptionMesg = copyMessageToR(""unknown reason"");
  }

  if (exceptionMesg != NULL) 
     Rf_error(exceptionMesg);

  return rl;
}
</code></pre>

<p>If you save this as, say, <code>dd.rcpp.cpp</code> and have <a href=""http://dirk.eddelbuettel.com/code/rcpp.html"" rel=""nofollow noreferrer"">Rcpp</a> installed, then simply use</p>

<pre><code>PKG_CPPFLAGS=`Rscript -e 'Rcpp:::CxxFlags()'`  \
    PKG_LIBS=`Rscript -e 'Rcpp:::LdFlags()'`  \
    R CMD SHLIB dd.rcpp.cpp
</code></pre>

<p>to build a shared library. We use <code>Rscript</code> (or <code>r</code>) to ask <a href=""http://dirk.eddelbuettel.com/code/rcpp.html"" rel=""nofollow noreferrer"">Rcpp</a> about its header and library locations. Once built, we can load and use this from R as follows:</p>

<pre><code>dyn.load(""dd.rcpp.so"")

dd.rcpp &lt;- function() {
    x &lt;- integer(10000)
    res &lt;- .Call(""dd_rcpp"", x)
    tabulate(res$vec)
}
</code></pre>

<p>In the same way, you can send vectors, matrics, ... of various R and C++ data types back end forth with ease.  Hope this helps somewhat.</p>

<p><strong>Edit 2 (some five+ years later):</strong></p>

<p>So this answer just got an upvote and hence bubbled up in my queue.  A <em>lot</em> of time has passed since I wrote it, and Rcpp has gotten <em>a lot</em> richer in features.  So I very quickly wrote this</p>

<pre><code>#include &lt;Rcpp.h&gt;

// [[Rcpp::export]]
Rcpp::IntegerVector dd2(Rcpp::IntegerVector vec) {
    int n = vec.size(), i = 0;
    if (n != 10000) 
        throw std::length_error(""Wrong vector size"");
    for (int a = 0; a &lt; 9; a++)
        for (int b = 0; b &lt; 9; b++)
            for (int c = 0; c &lt; 9; c++)
                for (int d = 0; d &lt; 9; d++)
                    vec(i++) = a*b - c*d;
    return vec;
}

/*** R
x &lt;- integer(10000)
tabulate( dd2(x) )
*/
</code></pre>

<p>which can be used as follows with the code in a file <code>/tmp/dd.cpp</code> </p>

<pre><code>R&gt; Rcpp::sourceCpp(""/tmp/dd.cpp"")    # on from any other file and path

R&gt; x &lt;- integer(10000)

R&gt; tabulate( dd2(x) )
 [1]  87 132 105 155  93 158  91 161  72 104  45 147  41  96
[15]  72 120  36  90  32  87  67  42  26 120  41  36  27  75
[29]  20  62  16  69  19  28  49  45  12  18  11  57  14  48
[43]  10  18   7  12   6  46  23  10   4  10   4   6   3  38
[57]   2   4   2   3   2   2   1  17
R&gt; 
</code></pre>

<p>Some of the key differences are:</p>

<ul>
<li>simpler build: just <code>sourceCpp()</code> it; even executes R test code at the end</li>
<li>full-fledged <code>IntegerVector</code> type</li>
<li>exception-handling wrapper automatically added by <code>sourceCpp()</code> code generator</li>
</ul>
"
1728422,134830,2009-11-13T10:35:05Z,1727772,312,TRUE,"<p><strong>An update, several years later</strong></p>

<p>This answer is old, and R has moved on.  Tweaking <a href=""https://www.rdocumentation.org/packages/utils/topics/read.table"" rel=""noreferrer""><code>read.table</code></a> to run a bit faster has precious little benefit.  Your options are:</p>

<ol>
<li><p>Using <a href=""https://www.rdocumentation.org/packages/utils/topics/fread"" rel=""noreferrer""><code>fread</code></a> in <a href=""https://cran.r-project.org/web/packages/data.table/index.html"" rel=""noreferrer""><code>data.table</code></a> for importing data from csv/tab-delimited files directly into R. See <a href=""https://stackoverflow.com/a/15058684/134830"">mnel's answer</a>.</p></li>
<li><p>Using <a href=""https://www.rdocumentation.org/packages/readr/topics/read_table"" rel=""noreferrer""><code>read_table</code></a> in <a href=""https://cran.r-project.org/web/packages/readr/index.html"" rel=""noreferrer""><code>readr</code></a> (on CRAN from April 2015).  This works much like <code>fread</code> above.  The <em>readme</em> in the link explains the difference between the two functions (<code>readr</code> currently claims to be ""1.5-2x slower"" than <code>data.table::fread</code>).</p></li>
<li><p><a href=""https://www.rdocumentation.org/packages/iotools/topics/read.csv.raw"" rel=""noreferrer""><code>read.csv.raw</code></a> from <a href=""https://cran.r-project.org/web/packages/iotools/index.html"" rel=""noreferrer""><code>iotools</code></a> provides a third option for quickly reading CSV files.</p></li>
<li><p>Trying to store as much data as you can in databases rather than flat files.  (As well as being a better permanent storage medium, data is passed to and from R in a binary format, which is faster.) <a href=""https://www.rdocumentation.org/packages/sqldf/topics/read.csv.sql"" rel=""noreferrer""><code>read.csv.sql</code></a> in the <a href=""https://cran.r-project.org/web/packages/sqldf/index.html"" rel=""noreferrer""><code>sqldf</code></a> package, as described in <a href=""https://stackoverflow.com/a/1820610/134830"">JD Long's answer</a>, imports data into a temporary SQLite database and then reads it into R.  See also: the <a href=""https://cran.r-project.org/web/packages/RODBC/index.html"" rel=""noreferrer""><code>RODBC</code></a> package, and the reverse depends section of the <a href=""https://cran.r-project.org/web/packages/DBI/index.html"" rel=""noreferrer""><code>DBI</code> package</a> page. <a href=""https://cran.r-project.org/web/packages/MonetDB.R/index.html"" rel=""noreferrer""><code>MonetDB.R</code></a> gives you a data type that pretends to be a data frame but is really a MonetDB underneath, increasing performance.  Import data with its <a href=""https://www.rdocumentation.org/packages/MonetDB.R/topics/monetdb.read.csv"" rel=""noreferrer""><code>monetdb.read.csv</code></a> function.  <a href=""https://cran.r-project.org/web/packages/dplyr/index.html"" rel=""noreferrer""><code>dplyr</code></a> allows you to work directly with data stored in several types of database.</p></li>
<li><p>Storing data in binary formats can also be useful for improving performance.  Use <code>saveRDS</code>/<code>readRDS</code> (see below), or the <a href=""https://cran.rstudio.com/web/packages/h5/index.html"" rel=""noreferrer""><code>h5</code></a> or <a href=""https://www.bioconductor.org/packages/release/bioc/html/rhdf5.html"" rel=""noreferrer""><code>rhdf5</code></a> packages for HDF5 format.</p></li>
</ol>

<hr>

<p><strong>The original answer</strong></p>

<p>There are a couple of simple things to try, whether you use read.table or scan.</p>

<ol>
<li><p>Set <code>nrows</code>=<em>the number of records in your data</em> (<code>nmax</code> in <code>scan</code>).</p></li>
<li><p>Make sure that <code>comment.char=""""</code> to turn off interpretation of comments.</p></li>
<li><p>Explicitly define the classes of each column using <code>colClasses</code> in <code>read.table</code>.</p></li>
<li><p>Setting <code>multi.line=FALSE</code> may also improve performance in scan.</p></li>
</ol>

<p>If none of these thing work, then use one of the <a href=""https://cran.r-project.org/web/views/HighPerformanceComputing.html"" rel=""noreferrer"">profiling packages</a> to determine which lines are slowing things down.  Perhaps you can write a cut down version of <code>read.table</code> based on the results.</p>

<p>The other alternative is filtering your data before you read it into R.</p>

<p>Or, if the problem is that you have to read it in regularly, then use these methods to read the data in once, then save the data frame as a binary blob with <del><a href=""https://www.rdocumentation.org/packages/base/topics/save"" rel=""noreferrer""><code>save</code></a></del> <a href=""https://www.rdocumentation.org/packages/base/topics/saveRDS"" rel=""noreferrer""><code>saveRDS</code></a>, then next time you can retrieve it faster with <del><a href=""https://www.rdocumentation.org/packages/base/topics/load"" rel=""noreferrer""><code>load</code></a></del> <code>readRDS</code>.</p>
"
1729861,163053,2009-11-13T15:18:57Z,1727772,29,FALSE,"<p>This was previously <a href=""http://markmail.org/message/zeew2gr2x77cglcv#query:r-project%20quickly%20read%20large%20file+page:1+mid:bovcytjhdkzmefru+state:results"" rel=""noreferrer"">asked on <b>R-Help</b></a>, so that's worth reviewing.</p>

<p>One suggestion there was to use <code>readChar()</code> and then do string manipulation on the result with <code>strsplit()</code> and <code>substr()</code>.  You can see the logic involved in readChar is much less than read.table.</p>

<p>I don't know if memory is an issue here, but you might also <a href=""http://cran.r-project.org/web/packages/HadoopStreaming/index.html"" rel=""noreferrer"">want to take a look at the <b>HadoopStreaming</b> package</a>.  This <a href=""http://en.wikipedia.org/wiki/Hadoop"" rel=""noreferrer"">uses Hadoop</a>, which is a MapReduce framework designed for dealing with large data sets.  For this, you would use the hsTableReader function.  This is an example (but it has a learning curve to learn Hadoop):</p>

<pre><code>str &lt;- ""key1\t3.9\nkey1\t8.9\nkey1\t1.2\nkey1\t3.9\nkey1\t8.9\nkey1\t1.2\nkey2\t9.9\nkey2\""
cat(str)
cols = list(key='',val=0)
con &lt;- textConnection(str, open = ""r"")
hsTableReader(con,cols,chunkSize=6,FUN=print,ignoreKey=TRUE)
close(con)
</code></pre>

<p>The basic idea here is to break the data import into chunks.  You could even go so far as to use one of the parallel frameworks (e.g. snow) and run the data import in parallel by segmenting the file, but most likely for large data sets that won't help since you will run into memory constraints, which is why map-reduce is a better approach.</p>
"
1730178,179848,2009-11-13T16:07:37Z,1721536,3,FALSE,"<p>I think <code>df.all$number</code> needs to be an ordered factor. Try <code>df.all$number &lt;- ordered(df.all$number)</code></p>
"
1732903,172261,2009-11-14T01:21:24Z,1725609,2,TRUE,"<p>Use the <code>verbosity</code> argument in <code>hdfload()</code> and check your environment using <code>ls()</code>. <code>hdfload()</code> has a side effect, it loads the tables into the current environment, <code>NULL</code> is the default return value. If you use the argument <code>load=TRUE</code> the objects are returned as the components of a named list:</p>

<ul>
<li><code>load</code>: A logical value. If 'FALSE', 
the objects are returned as the
         components of a named list. If 'TRUE' (the default), the
        objects are loaded as individual variables with their own
        names - the function returns nothing in this case.  </li>
</ul>

<p>On my Mac everything works well with the same setup. </p>

<pre><code>R&gt; rm(list=ls())
R&gt; ls()
character(0)
R&gt; hdf5load(""TestHDF5.h5"", verbosity=3)
hdf5_global_verbosity=3 load=1
Processing object: Dataset0 ...... its a dataset...Dataset has ID335544326
Dataset has tid 201326902
Dataset has space id 268435467
Dataset has rank 1
Dataset has dims/maxdims: 1 / 1 
Allocating vector with rank=1 dim=1
calling vector_io. Hangs here with big datsets
Setting buffer size in plist
About to read with bufsize = 50
in string_ref: count=1, size=25 srcbf=25
leaving string_ref
 Done read
in vector_io: permuting
in vector_io: tidying
Phew. Done it. calling iinfo-&gt;add
Rank &gt; 1 or not VECSXP
Calling  hdf5_load_attributes 
back from  hdf5_load_attributes 
...Finished dataset 
Processing object: Table0 ...... its a dataset...Dataset has ID335544327
Dataset has tid 201326906
Dataset has space id 268435468
Dataset has rank 1
Dataset has dims/maxdims: 1 / 1 
Dataset has type = VECSXP and rank 1
Reading...
....done
in string_ref: count=1, size=25 srcbf=25
leaving string_ref
...Finished dataset 
NULL
R&gt; ls()
[1] ""Dataset0"" ""Table0""
</code></pre>
"
1734941,143305,2009-11-14T17:28:39Z,1734896,10,FALSE,"<p>There are many ways:</p>

<ul>
<li>use <code>sink()</code></li>
<li>open a file via <code>file()</code> and write results to it</li>
<li>place your code in a file and run it via <code>R CMD BATCH file.R</code> which creates output</li>
<li>explicitly write results data via <code>write.table()</code> or its variants like <code>write.csv()</code></li>
</ul>

<p>This is fairly elementary so you will probably benefit from reading the 'Introduction to R' manual, or one of the numerous books on R.</p>

<p>The simplest solution may be</p>

<pre><code>R&gt; X &lt;- rnorm(100)
R&gt; summary(X)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 -2.480  -0.618  -0.223  -0.064   0.609   2.440 
R&gt; write.table(matrix(summary(X)[c(1,3,6)], nrow=1), \
               file=""/tmp/foo.txt"", col.names=FALSE, row.names=FALSE)
R&gt; system(""cat /tmp/foo.txt"")
-2.48 -0.223 2.44
R&gt; 
</code></pre>

<p>where I force the subset of <code>summary()</code> to be a matrix of one row.</p>
"
1734953,172261,2009-11-14T17:33:12Z,1734896,4,FALSE,"<p>You might also have a look at the <a href=""http://cran.r-project.org/doc/manuals/R-data.pdf"" rel=""nofollow noreferrer"">R Data Import/Export manual</a> (Section 1.2 <a href=""http://cran.r-project.org/doc/manuals/R-data.html#Export-to-text-files"" rel=""nofollow noreferrer"">Export to text files</a>).</p>
"
1735008,160314,2009-11-14T17:49:25Z,1734896,38,FALSE,"<p>A simple way is to convert the output that you want to print to file, and convert it to a text string via capture.output. then you can simply cat the output to the file.</p>

<pre><code>dat&lt;-data.frame(a=rnorm(100),b=rnorm(100),c=rnorm(100))
mod&lt;-lm(a~b+c,data=dat)
out&lt;-capture.output(summary(mod))
cat(out,file=""out.txt"",sep=""\n"",append=TRUE)
out&lt;-capture.output(vcov(mod))
cat(out,file=""out.txt"",sep=""\n"",append=TRUE)
</code></pre>

<p>this creates a file out.txt containing</p>

<pre><code>Call:
lm(formula = a ~ b + c, data = dat)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.67116 -0.81736 -0.07006  0.76551  2.91055 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  0.01196    0.11724   0.102    0.919
b            0.11931    0.12601   0.947    0.346
c           -0.09085    0.13267  -0.685    0.495

Residual standard error: 1.171 on 97 degrees of freedom
Multiple R-squared: 0.0183, Adjusted R-squared: -0.001944 
F-statistic: 0.9039 on 2 and 97 DF,  p-value: 0.4084 

              (Intercept)             b             c
(Intercept)  0.0137444761 -0.0006929722 -0.0005721338
b           -0.0006929722  0.0158784141  0.0042188705
c           -0.0005721338  0.0042188705  0.0176018744
</code></pre>
"
1735192,211116,2009-11-14T18:52:52Z,1734896,7,FALSE,"<p>The important thing here is to learn that the summary function, as in:</p>

<pre><code>summary(Variable1)
</code></pre>

<p>doesn't print the summary. It works out the summary and then returns it. The command line processor does the printing, just before popping up the next '>' prompt.</p>

<p>Lots of R functions work like that. Hence you can nearly always get return values by assignment. So if you do:</p>

<pre><code>x = summary(Variable1)
</code></pre>

<p>then it won't get printed. But then type 'x' and it will. The command line prints the last thing evaluated.</p>

<p>Once you've got 'x', you can use the Import/Export methods to save for later.</p>
"
1735260,117357,2009-11-14T19:13:19Z,1721536,3,TRUE,"<p>Hadley has provided a solution. Here's a replication of the problem and the solution.</p>

<p>The goal is to get the bars labeled ""S"" to come before the bars labeled ""P"". This doesn't happen by default because R orders levels alphabetically.</p>

<pre><code>df &lt;- read.csv(""http://pealco.net/code/ggplot_dodge/df.txt"")
ggplot(df, aes(gram, V1, fill=number))
    + geom_bar(stat=""identity"", position=""dodge"")
</code></pre>

<p><a href=""http://pealco.net/code/ggplot_dodge/wrongorder.png"" rel=""nofollow noreferrer"">alt text http://pealco.net/code/ggplot_dodge/wrongorder.png</a></p>

<p>As Hadley commented in another answer, ""you need to reorder based on the x variables, not the y variable"". Though I'm not sure why this works.</p>

<p>To flip the order of the factors in this example, you can convert the factor to numeric and multiply by -1.</p>

<pre><code>df &lt;- with(df, df[order(gram, -as.numeric(number)), ])
</code></pre>

<p>Plotting again shows that his works.</p>

<p><a href=""http://pealco.net/code/ggplot_dodge/rightorder.png"" rel=""nofollow noreferrer"">alt text http://pealco.net/code/ggplot_dodge/rightorder.png</a></p>

<p>I'd still like so more explanation about why <code>df &lt;- with(df, df[order(gram, -as.numeric(number)), ])</code> works.</p>
"
1735646,143305,2009-11-14T21:20:44Z,1735540,22,FALSE,"<p>Subsetting and sorting your data;</p>

<pre><code>valact &lt;- subset(val, variable=='actual')
valsort &lt;- valact[ order(-valact[,""Value""]),]
</code></pre>

<p>From there it's just a standard <code>boxplot()</code> with a very manual cumulative function on top:</p>

<pre><code>op &lt;- par(mar=c(3,3,3,3)) 
bp &lt;- barplot(valsort [ , ""Value""], ylab="""", xlab="""", ylim=c(0,1),    
              names.arg=as.character(valsort[,""State""]), main=""How's that?"") 
lines(bp, cumsum(valsort[,""Value""])/sum(valsort[,""Value""]), 
      ylim=c(0,1.05), col='red') 
axis(4)
box() 
par(op)
</code></pre>

<p>which should look like this</p>

<p><a href=""http://dirk.eddelbuettel.com/misc/jdlong_pareto.png"">alt text http://dirk.eddelbuettel.com/misc/jdlong_pareto.png</a></p>

<p>and it doesn't even need the overplotting trick as <code>lines()</code> happily annotates the initial plot.</p>
"
1735757,143305,2009-11-14T22:03:28Z,1735540,3,FALSE,"<p>Also, see the package <a href=""http://cran.r-project.org/package=qcc"" rel=""nofollow noreferrer"">qcc</a> which has a function <code>pareto.chart()</code>.  Looks like it uses base graphics too, so start your bounty for a ggplot2-solution :-)</p>
"
1735807,203420,2009-11-14T22:24:16Z,1734896,4,TRUE,"<p>You can also access individual attributes of the <code>summary</code> command. For example</p>

<pre><code>&gt; x=summary(seq(1:10))
&gt; attributes(x)
&gt; attributes(x)
$names
[1] ""Min.""    ""1st Qu."" ""Median""  ""Mean""    ""3rd Qu."" ""Max.""   

$class
[1] ""table""

&gt; x[""1st Qu.""]
1st Qu. 
3.25
</code></pre>
"
1736141,158065,2009-11-15T00:37:09Z,1735540,13,TRUE,"<p>The bars in ggplot2 are ordered by the ordering of the levels in the factor.</p>

<pre><code>val$State &lt;- with(val, factor(val$State, levels=val[order(-Value), ]$State))
</code></pre>
"
1740818,135870,2009-11-16T08:26:41Z,1740774,11,TRUE,"<p>For vectors, use the <code>paste()</code> function and specify the <code>collapse</code> argument:</p>

<pre><code>x &lt;- c(1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0)
paste( x, collapse = '' )

[1] ""10000011010110""
</code></pre>
"
1741848,74314,2009-11-16T12:21:15Z,1741820,18,FALSE,"<p>The operators &lt;- and = assign into the environment in which they are evaluated. The operator &lt;- can be used anywhere, <strong><em>whereas the operator = is only allowed at the top level</em></strong> (e.g., in the complete expression typed at the command prompt) or as one of the subexpressions in a braced list of expressions. </p>
"
1742550,134830,2009-11-16T14:36:35Z,1741820,420,TRUE,"<p>The difference in <a href=""http://www.rdocumentation.org/packages/base/topics/assignOps"" rel=""noreferrer"">assignment operators</a> is clearer when you use them to set an argument value in a function call.  For example:</p>

<pre><code>median(x = 1:10)
x   
## Error: object 'x' not found
</code></pre>

<p>In this case, <code>x</code> is declared within the scope of the function, so it does not exist in the user workspace.</p>

<pre><code>median(x &lt;- 1:10)
x    
## [1]  1  2  3  4  5  6  7  8  9 10
</code></pre>

<p>In this case, <code>x</code> is declared in the user workspace, so you can use it after the function call has been completed.</p>

<hr>

<p>There is a general preference among the R community for using <code>' &lt;- '</code> for assignment (other than in function signatures)  for compatibility with (very) old versions of S-Plus.  Note that the spaces help to clarify situations like</p>

<pre><code>x&lt;-3
# Does this mean assignment?
x &lt;- 3
# Or less than?
x &lt; -3
</code></pre>

<hr>

<p>Most R IDEs have keyboard shortcuts to make <code>' &lt;- '</code> easier to type.  <kbd>Ctrl</kbd> + <kbd>=</kbd> in Architect, <kbd>Alt</kbd> + <kbd>-</kbd> in RStudio, <kbd>Shift</kbd> + <kbd>-</kbd> (underscore) in emacs+ESS.</p>

<hr>

<p>If you prefer writing <code>=</code> to <code>&lt;-</code> but want to use the more common assignment symbol for publicly released code (on CRAN, for example), then you can use one of the <a href=""http://www.rdocumentation.org/packages/formatR/topics/tidy_source"" rel=""noreferrer""><code>tidy_*</code></a> functions in the <code>formatR</code> package to automatically replace <code>=</code> with <code>&lt;-</code>.</p>

<pre><code>library(formatR)
tidy_source(text = ""x = 1:5"", arrow = TRUE)
## x &lt;- 1:5
</code></pre>

<hr>

<p>The answer to the question ""Why does <code>x &lt;- y = 5</code> throw an error but not <code>x &lt;- y &lt;- 5</code>?"" is ""It's down to the magic contained in the parser"".  R's syntax contains <a href=""http://shape-of-code.coding-guidelines.com/2012/02/29/parsing-r-code-freedom-of-expression-is-not-always-a-good-idea/"" rel=""noreferrer"">many ambiguous cases</a> that have to be resolved one way or another.  The parser chooses to resolve the bits of the expression in different orders depending on whether <code>=</code> or <code>&lt;-</code> was used.</p>

<p>To understand what is happening, you need to know that assignment silently returns the value that was assigned.  You can see that more clearly by explicitly printing, for example <code>print(x &lt;- 2 + 3)</code>.</p>

<p>Secondly, it's clearer if we use prefix notation for assignment.  So </p>

<pre><code>x &lt;- 5
`&lt;-`(x, 5)  #same thing

y = 5
`=`(y, 5)   #also the same thing
</code></pre>

<p>The parser interprets <code>x &lt;- y &lt;- 5</code> as </p>

<pre><code>`&lt;-`(x, `&lt;-`(y, 5))
</code></pre>

<p>We might expect that <code>x &lt;- y = 5</code> would then be</p>

<pre><code>`&lt;-`(x, `=`(y, 5))
</code></pre>

<p>but actually it gets interpreted as</p>

<pre><code>`=`(`&lt;-`(x, y), 5)
</code></pre>

<p><del>I'm not sure why this happens, since <a href=""http://stat.ethz.ch/R-manual/R-patched/library/base/html/Syntax.html"" rel=""noreferrer""><code>=</code> is higher precedence than <code>&lt;-</code></a>.  Maybe a bug in the parser, but good luck trying to get that changed now.</del></p>

<p><del>The <a href=""https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=14955"" rel=""noreferrer"">documentation is wrong</a>. <code>=</code> is actually lower precedence than <code>&lt;-</code>, which is why the order is unexpectedly different.</del></p>

<p>The <a href=""http://www.rdocumentation.org/packages/base/topics/Syntax"" rel=""noreferrer""><code>?Syntax</code></a> help page now correctly shows that <code>=</code> is lower precedence than <code>&lt;-</code>.</p>
"
1742591,61027,2009-11-16T14:44:30Z,1741820,74,FALSE,"<p>Google's R style guide simplifies the issue by prohibiting the ""="" for assignment. Not a bad choice.</p>

<p><a href=""https://google.github.io/styleguide/Rguide.xml"" rel=""noreferrer"">https://google.github.io/styleguide/Rguide.xml</a></p>

<p>The R manual goes into nice detail on all 5 assignment operators.</p>

<p><a href=""http://stat.ethz.ch/R-manual/R-patched/library/base/html/assignOps.html"" rel=""noreferrer"">http://stat.ethz.ch/R-manual/R-patched/library/base/html/assignOps.html</a></p>
"
1743746,163053,2009-11-16T17:47:13Z,1743698,68,FALSE,"<p>You can use the <code>parse()</code> function to convert the characters into an expression.  You need to specify that the input is text, because parse expects a file by default:</p>

<pre><code>eval(parse(text=""5+5""))
</code></pre>
"
1743796,135944,2009-11-16T17:55:29Z,1743698,281,TRUE,"<p>The <code>eval()</code> function evaluates an expression, but <code>""5+5""</code> is a string, not an expression. Use <code>parse()</code> with <code>text=&lt;string&gt;</code> to change the string into an expression:</p>

<pre><code>&gt; eval(parse(text=""5+5""))
[1] 10
&gt; class(""5+5"")
[1] ""character""
&gt; class(parse(text=""5+5""))
[1] ""expression""
</code></pre>

<p>Calling <code>eval()</code> invokes many behaviours, some are not immediately obvious:</p>

<pre><code>&gt; class(eval(parse(text=""5+5"")))
[1] ""numeric""
&gt; class(eval(parse(text=""gray"")))
[1] ""function""
&gt; class(eval(parse(text=""blue"")))
Error in eval(expr, envir, enclos) : object 'blue' not found
</code></pre>

<p>See also <a href=""https://stackoverflow.com/a/12193902/59087"">tryCatch</a>.</p>
"
1744368,183988,2009-11-16T19:38:23Z,1428750,11,FALSE,"<p>If R/ESS is hogging up so much compute time that your emacs/ESS is unresponsive to C-c C-c, you can also save it by sending an INTERRUPT signal from the terminal.</p>

<p>First: figure out R's processID  using <code>top</code> or <code>ps</code>. (mine was 98490
Then:
<code>kill -2 98490</code>
That sends an interrupt signal and you get your ESS/Emacs and R session back</p>
"
1745592,163053,2009-11-16T23:35:58Z,1745514,12,TRUE,"<p>You can either check that it's a matrix with is.matrix or else convert it with as.matrix after the parameter is passed:</p>

<pre><code>foo &lt;- function(x)
{
   if(!is.matrix(x)) stop(""x must be a matrix"")
   # I want to make sure x is of type ""matrix""
   solve(x)
}
</code></pre>
"
1745687,163053,2009-11-16T23:58:39Z,1745622,43,TRUE,"<p>When in doubt, test yourself.  The first approach is both easier and faster.</p>

<pre><code>&gt; create.matrix &lt;- function(size) {
+ x &lt;- matrix()
+ length(x) &lt;- size^2
+ dim(x) &lt;- c(size,size)
+ x
+ }
&gt; 
&gt; system.time(x &lt;- matrix(data=NA,nrow=10000,ncol=10000))
   user  system elapsed 
   4.59    0.23    4.84 
&gt; system.time(y &lt;- create.matrix(size=10000))
   user  system elapsed 
   0.59    0.97   15.81 
&gt; identical(x,y)
[1] TRUE
</code></pre>

<p>Regarding the difference between NA and NULL:</p>

<p>There are actually four special constants.  </p>

<blockquote>
  <p>In addition, there are four special constants, NULL, NA, Inf, and NaN.</p>
  
  <p>NULL is used to indicate the empty object. NA is used for absent (“Not Available”) data values. Inf denotes infinity and NaN is not-a-number in the IEEE floating point calculus (results of the operations respectively 1/0 and 0/0, for instance). </p>
</blockquote>

<p>You can read more in <a href=""http://cran.r-project.org/doc/manuals/R-lang.html#Literal-constants"" rel=""noreferrer"">the R manual on language definition</a>.</p>
"
1745737,143305,2009-11-17T00:10:10Z,1745514,3,FALSE,"<p>Just for completeness: besides </p>

<pre><code>is.matrix(foo) 
</code></pre>

<p>you can also test for </p>

<pre><code>class(foo) == ""matrix"" 
</code></pre>

<p>which also works for non-standard that do not have <code>is.foo()</code> functions.</p>
"
1746498,16632,2009-11-17T04:02:40Z,1745514,11,FALSE,"<p><code>stopifnot(is.matrix(x))</code></p>
"
1748467,209467,2009-11-17T12:17:14Z,1740774,1,FALSE,"<p>Have you tried</p>

<pre><code>write.table(x,row.names=FALSE,col.names=FALSE,eol=""\t"")
1   0   0   0   0   0   1   1   0   1   0   1   1   0   
</code></pre>

<p>By changing the eol (end of line) character, you can decide if and what separator to use.</p>
"
1748901,172261,2009-11-17T13:33:59Z,1748590,4,FALSE,"<p>See this <a href=""http://blog.revolution-computing.com/2009/10/revolution-r-coming-to-ubuntu.html"" rel=""nofollow noreferrer"">blog post</a> from REvolution for more information.
REvolution R 3.0 should be 100% compatible to R-2.9.2 from <a href=""http://cran.r-project.org/"" rel=""nofollow noreferrer"">CRAN</a>.
Basically, they use multi-threaded, high performance linear algebra libraries and optimizing compilers.
REvolution enhancements include:</p>

<ul>
<li>high performance math libraries optimized to take advantage of processor cache, vector instructions and multithreading (<a href=""http://software.intel.com/en-us/intel-mkl/"" rel=""nofollow noreferrer"">Intel Math Kernel Library - MKL</a>) and </li>
<li>optimizing compilers and runtime libraries.</li>
</ul>

<p>There are some benchmarks on the REvolution webpage: <a href=""http://revolution-computing.com/products/r-performance.php"" rel=""nofollow noreferrer"">REvolution R Performance</a> and <a href=""http://revolution-computing.com/products/benchmarks.php"" rel=""nofollow noreferrer"">Simple Benchmarks</a>.</p>

<p>Although they have contributed several interesting extensions to the R community under an OSS license (<a href=""http://cran.r-project.org/web/packages/foreach/index.html"" rel=""nofollow noreferrer"">foreach</a>, <a href=""http://cran.r-project.org/web/packages/iterators/index.html"" rel=""nofollow noreferrer"">iterators</a>, <a href=""http://cran.r-project.org/web/packages/doSNOW/index.html"" rel=""nofollow noreferrer"">doSNOW</a>, and <a href=""http://cran.r-project.org/web/packages/doMC/index.html"" rel=""nofollow noreferrer"">doMC</a>), the MKL extension is proprietary.</p>

<p>Personally, I've switched to (CRAN) R 2.10.0 to have the latest R features.</p>
"
1748932,143305,2009-11-17T13:37:41Z,1748590,10,TRUE,"<p>Yes, on a multicore machine, the Intel MKL libraries implementing the BLAS -- and provided by the package <code>revolution-mkl</code> and turned-on by package <code>r-revolution-revobase</code> will work in parallel for linear algebra problems, and you should see a difference to the base case of using just the libblas* packages.  </p>

<p>However, your example above is not that significant, I often do something like</p>

<pre><code> mean(replicate(N, system.time( someStuffHere() )[""elapsed""]), trim=0.05)
</code></pre>

<p>to compute a trimmed mean over a number of replications.  </p>

<p>More importantly, note that your example includes the RNG draws in the timings which are 
i) expensive, and ii) invariant to the method used so you should generated that outside of <code>system.time()</code>.</p>

<p>Besides the MKL, <code>revolution-r</code> also brings in some of the REvolution-authored packages from CRAN that can be used for parallel execution.</p>

<p>(Disclaimer: I helped REvo in putting this together for Ubuntu 9.10)</p>
"
1751651,163053,2009-11-17T20:46:32Z,1751540,7,FALSE,"<p>Using <code>colnames</code> is the only way that I'm aware of for a data.frame, although colnames() is itself a vector so there's no need to do any iterating on it.  This version handles two columns:</p>

<pre><code>foo &lt;- function(cname) {
   answer &lt;- data.frame(1:5, 1:5)
   colnames(answer) &lt;- cname
   return(answer)
}
&gt; foo(c(""a"",""b""))
  a b
1 1 1
2 2 2
3 3 3
4 4 4
5 5 5
</code></pre>
"
1752989,16632,2009-11-18T01:13:21Z,1751540,4,FALSE,"<p>Here's an alternative using <code>substitute</code> and <code>eval</code>.</p>

<pre><code>foo &lt;- function(var) {
  eval(substitute(data.frame(var = 1:5)), list(var = as.name(var)))
}
</code></pre>

<p>I hope you'll agree that the <code>colnames</code> solution is simpler.</p>
"
1753418,143305,2009-11-18T03:17:18Z,1753299,2,FALSE,"<p>First off, I have not used <a href=""http://cran.r-project.org/package=kernlab"" rel=""nofollow noreferrer"">kernlab</a> much.  But simply looking at the docs, I do see working examples for the <code>predict.ksvm()</code> method.  Copying and pasting, and omitting the prints to screen:</p>

<pre><code> ## example using the promotergene data set
 data(promotergene)

 ## create test and training set
 ind &lt;- sample(1:dim(promotergene)[1],20)
 genetrain &lt;- promotergene[-ind, ]
 genetest &lt;- promotergene[ind, ]

 ## train a support vector machine
 gene &lt;-  ksvm(Class~.,data=genetrain,kernel=""rbfdot"",\
               kpar=list(sigma=0.015),C=70,cross=4,prob.model=TRUE)

 ## predict gene type probabilities on the test set
 genetype &lt;- predict(gene,genetest,type=""probabilities"")
</code></pre>

<p>That seems pretty straight-laced: use random sampling to generate a training set <code>genetrain</code>  and its complement <code>genetest</code>, then fitting via <code>ksvm</code> and a call to a <code>predict()</code> method using the fit, and new data in a matching format.  This is very standard.</p>

<p>You may find the <a href=""http://cran.r-project.org/package=caret"" rel=""nofollow noreferrer"">caret</a> package by Max Kuhn useful. It provides a general evaluation and testing framework for a variety of regression, classification and machine learning methods and packages, including <a href=""http://cran.r-project.org/package=kernlab"" rel=""nofollow noreferrer"">kernlab</a>, and contains several vignettes plus a <a href=""http://www.jstatsoft.org/v28/i05/"" rel=""nofollow noreferrer"">JSS paper</a>.</p>
"
1755688,143305,2009-11-18T12:40:20Z,1755642,8,TRUE,"<p>I think you want <code>abline(model)</code> here as in this example from the help page:</p>

<pre><code> z &lt;- lm(dist ~ speed, data = cars)
 plot(cars)
 abline(z) # equivalent to abline(reg = z) or
 abline(coef = coef(z))
</code></pre>
"
1755932,177390,2009-11-18T13:22:15Z,1755642,4,FALSE,"<pre><code>x &lt;- rnorm(100)
y &lt;- rnorm(100)
z &lt;- rnorm(100)

model &lt;- lm(x~y+z)
plot(x,type=""l"",col=""green"")
lines(fitted(model),col=""blue"")
</code></pre>

<p>I tried this and it seems to work</p>
"
1756273,163053,2009-11-18T14:13:40Z,1756209,2,FALSE,"<p>Neither examples nor demos are required to build a package.</p>

<p>The <code>example()</code> function can be run on any other function, and it just runs the commands in the ""Examples:"" section of the help file (excluding ones that have ## Not run:).  Also see <a href=""https://stackoverflow.com/questions/1454211/what-does-not-run-mean-in-r-help-pages"">this related stackoverflow question</a>.  </p>

<p>For instance, look at <code>?example</code> and then call <code>example(example)</code>.</p>

<p>Regarding the idea of finding all functions that have examples: that's possible, although I'm not aware of any particularly <em>easy</em> way of doing it.  I would probably use the existing examples function but alter it so that it doesn't execute the example functions and then run it across all functions in my installed packages.  But that's probably not worth the effort because it will return a <strong>huge</strong> list of functions (most R documentation has an example).  You're better of just trying example(function.name) whenever you're curious about a specific task.</p>

<p>You've already answered your own question about <code>demo()</code>: </p>

<pre><code>demo(package=.packages(all.available = TRUE)).  
</code></pre>
"
1756389,143305,2009-11-18T14:30:17Z,1756209,1,FALSE,"<p>You can always look at the sources of package 'myPkg', or even run <code>R CMD check myPkg</code> over the sources as that will extract a file <code>myPkg-Ex.R</code> containing the examples.</p>
"
1757131,134830,2009-11-18T16:13:21Z,1751540,1,FALSE,"<p>A minor adjustment to Shane's code, in case you really want to use <code>substitute</code>, or you really can't be bothered to type the extra quotes.</p>

<pre><code>foo &lt;- function(a) {
   answer &lt;- data.frame(1:5)
   colnames(answer) &lt;- as.character(substitute(a))
   answer
}
foo(mycolname)

  mycolname
1         1
2         2
3         3
4         4
5         5
</code></pre>
"
1757352,83761,2009-11-18T16:42:50Z,1753299,21,TRUE,"<p>If you think about how the support vector machine might ""use"" the kernel matrix, you'll see that you can't really do this in the way you're trying (as you've seen :-)</p>

<p>I actually struggled a bit with this when I first was using kernlab + a kernel matrix ... coincidentally, it was also for graph kernels!</p>

<p>Anyway, let's first realize that since the SVM doesn't know how to calculate your kernel function, it needs to have these values already calculated between your new (testing) examples, and the examples it picks out as the support vectors during the training step.</p>

<p>So, you'll need to calculate the kernel matrix for <em>all</em> of your examples together. You'll later train on some and test on the others by removing rows + columns from the kernel matrix when appropriate.  Let me show you with code.</p>

<p>We can use the example code in the <code>ksvm</code> documentation to load our workspace with some data:</p>

<pre><code>library(kernlab)
example(ksvm)
</code></pre>

<p>You'll need to hit return a few (2) times in order to let the plots draw, and let the example finish, but you should now have a kernel matrix in your workspace called <code>K</code>. We'll need to recover the <code>y</code> vector that it should use for its labels (as it has been trampled over by other code in the example):</p>

<pre><code>y &lt;- matrix(c(rep(1,60),rep(-1,60)))
</code></pre>

<p>Now, pick a subset of examples to use for testing</p>

<pre><code>holdout &lt;- sample(1:ncol(K), 10)
</code></pre>

<p>From this point on, I'm going to:</p>

<ol>
<li>Create a training kernel matrix named <code>trainK</code> from the original <code>K</code> kernel matrix.</li>
<li>Create an SVM model from my training set <code>trainK</code></li>
<li>Use the support vectors found from the model to create a testing kernel matrix <code>testK</code> ... this is the weird part. If you look at the code in <code>kernlab</code> to see how it uses the support vector indices, you'll see why it's being done this way. It might be possible to do this another way, but I didn't see any documentation/examples on <em>predicting</em> with a kernel matrix, so I'm doing it ""the hard way"" here.</li>
<li>Use the SVM to predict on these features and report accuracy</li>
</ol>

<p>Here's the code:</p>

<pre><code>trainK &lt;- as.kernelMatrix(K[-holdout,-holdout])  # 1
m &lt;- ksvm(trainK, y[-holdout], kernel='matrix')  # 2
testK &lt;- as.kernelMatrix(K[holdout, -holdout][,SVindex(m), drop=F]) # 3
preds &lt;- predict(m, testK)  # 4
sum(sign(preds) == sign(y[holdout])) / length(holdout) # == 1 (perfect!)
</code></pre>

<p>That should just about do it. Good luck!</p>

<p><strong>Responses to comment below</strong></p>

<blockquote>
  <p>what does K[-holdout,-holdout] mean? (what does the ""-"" mean?)</p>
</blockquote>

<p>Imagine you have a vector <code>x</code>, and you want to retrieve elements 1, 3, and 5 from it, you'd do:</p>

<pre><code>x.sub &lt;- x[c(1,3,5)]
</code></pre>

<p>If you want to retrieve everything from <code>x</code> <em>except</em> elements 1, 3, and 5, you'd do:</p>

<pre><code>x.sub &lt;- x[-c(1,3,5)]
</code></pre>

<p>So <code>K[-holdout,-holdout]</code> returns all of the rows and columns of <code>K</code> <em>except</em> for the rows we want to holdout.</p>

<blockquote>
  <p>What are the arguments of your as.kernelMatrix - especially the [,SVindex(m),drop=F] argument (which is particulary strange because it looks like that entire bracket is a matrix index of K?)</p>
</blockquote>

<p>Yeah, I inlined two commands into one:</p>

<pre><code>testK &lt;- as.kernelMatrix(K[holdout, -holdout][,SVindex(m), drop=F])
</code></pre>

<p>Now that you've trained the model, you want to give it a new kernel matrix with your testing examples. <code>K[holdout,]</code> would give you only the rows which correspond to the training examples in <code>K</code>, and all of the columns of <code>K</code>.</p>

<p><code>SVindex(m)</code> gives you the indexes of your support vectors from your <strong>original</strong> training matrix -- remember, those rows/cols have <code>holdout</code> removed. So for those column indices to be correct (ie. reference the correct sv column), I must first remove the <code>holdout</code> columns.</p>

<p>Anyway, perhaps this is more clear:</p>

<pre><code>testK &lt;- K[holdout, -holdout]
testK &lt;- testK[,SVindex(m), drop=FALSE]
</code></pre>

<p>Now <code>testK</code> only has the rows of our <em>testing examples</em> and the columns that correspond to the support vectors. <code>testK[1,1]</code> will have the value of the kernel function computed between your first testing example, and the first support vector. <code>testK[1,2]</code> will have the kernel function value between your 1st testing example and the second support vector, etc.</p>

<p><strong>Update (2014-01-30) to answer comment from @wrahool</strong></p>

<p>It's been a while since I've played with this, so the particulars of <code>kernlab::ksvm</code> are a bit rusty, but in principle this should be correct :-) ... here goes:</p>

<blockquote>
  <p>what is the point of <code>testK &lt;- K[holdout, -holdout]</code> - aren't you removing the columns that correspond to the test set?</p>
</blockquote>

<p>Yes. The short answer is that if you want to <code>predict</code> using a kernel matrix, you have to supply the a matrix that is of the dimension <code>rows</code> by <code>support vectors</code>. For each row of the matrix (the new example you want to predict on) the values in the columns are simply the value of the kernel matrix evaluated between that example and the support vector.</p>

<p>The call to <code>SVindex(m)</code> returns the index of the support vectors given in the dimension of the <em>original</em> training data.</p>

<p>So, first doing <code>testK &lt;- K[holdout, -holdout]</code> gives me a <code>testK</code> matrix with the rows of the examples I want to predict on, and the columns are from the same examples (dimension) the model was trained on.</p>

<p>I further subset the columns of <code>testK</code> by <code>SVindex(m)</code> to only give me the columns which (now) correspond to my support vectors. Had I not done the first <code>[, -holdout]</code> selection, the indices returned by <code>SVindex(m)</code> may not correspond to the right examples (unless all <code>N</code> of your testing examples are the last <code>N</code> columns of your matrix).</p>

<blockquote>
  <p>Also, what exactly does the drop = FALSE condition do?</p>
</blockquote>

<p>It's a bit of defensive coding to ensure that after the indexing operation is performed, the object that is returned is of the same type as the object that was indexed.</p>

<p>In R, if you index only one dimension of a 2D (or higher(?)) object, you are returned an object of the lower dimension. I don't want to pass a <code>numeric</code> vector into <code>predict</code> because it wants to have a <code>matrix</code></p>

<p>For instance</p>

<pre><code>x &lt;- matrix(rnorm(50), nrow=10)

class(x)
[1] ""matrix""

dim(x)
[1] 10  5

y &lt;- x[, 1]

class(y)
[1] ""numeric""

dim(y)
NULL
</code></pre>

<p>The same will happen with <code>data.frame</code>s, etc.</p>
"
1757699,169947,2009-11-18T17:27:59Z,1335830,38,FALSE,"<p>I bet you want a simple <code>if</code> statement instead of <code>ifelse</code> - in R, <code>if</code> isn't just a control-flow structure, it can return a value:</p>

<pre><code>&gt; if(TRUE) c(1,2) else c(3,4)
[1] 1 2
&gt; if(FALSE) c(1,2) else c(3,4)
[1] 3 4
</code></pre>
"
1758926,169947,2009-11-18T20:42:11Z,1748590,4,FALSE,"<p>Just to reiterate what Dirk mentioned about timing - in your case, constructing the matrix is taking almost all the time.  Look what happens (on my system, where I don't have REvolution) when I yank it outside the timing function:</p>

<pre><code>&gt; system.time(t(matrix(rnorm(10000000),ncol=1000)))
   user  system elapsed 
  2.256   0.317   2.576 

&gt; mt &lt;- matrix(rnorm(10000000),ncol=1000)
&gt; system.time(t(mt))
   user  system elapsed 
  0.137   0.070   0.204 
</code></pre>

<p>In other words, over 90% of the time is spent constructing the matrix, under 10% transposing it.</p>
"
1760440,16632,2009-11-19T01:58:55Z,1760068,3,FALSE,"<p>You'll need to manually specify group = 1 because by default ggplot2 groups by the combination of all categorical variables on the plot. </p>
"
1762585,203420,2009-11-19T11:12:27Z,1760068,0,FALSE,"<p>I'm not sure if I've missed what you are trying to do, but are basically wanting a step function. For example:</p>

<pre><code>rates = c(0.00000000 ,0.00000000 ,0.00000000 ,0.02017059 ,0.32707402, 0.54013169 ,0.71698958 ,0.81120944 ,0.87283637 ,0.91411649 ,0.91273334 ,0.95627322 ,0.92879819 ,0.98088779 ,0.90406674 ,1.00000000 ,1.00000000, 1.00000000 )
age = seq(0, 85, 5)

#ReJig the variables
r2 = sort(rep(rates,2));r2 = r2[1:(length(r2)-1)]
a = sort(rep(age,2));a = a[2:(length(a))]

library(ggplot2)
ggplot() + geom_line(aes(x=a, y=r2))
</code></pre>

<p>HTH</p>
"
1769581,172261,2009-11-20T10:24:49Z,1769365,19,TRUE,"<p>I would use <code>subset</code> combined with <code>duplicated</code> to filter non-unique timestamps in the second data frame:</p>

<pre><code>R&gt; df_ &lt;- read.table(textConnection('
                     ts         v
1 ""2009-09-30 10:00:00"" -2.081609
2 ""2009-09-30 10:15:00"" -2.079778
3 ""2009-09-30 10:15:00"" -2.113531
4 ""2009-09-30 10:15:00"" -2.124716
5 ""2009-09-30 10:15:00"" -2.102117
6 ""2009-09-30 10:30:00"" -2.093542
7 ""2009-09-30 10:30:00"" -2.092626
8 ""2009-09-30 10:45:00"" -2.086339
9 ""2009-09-30 11:00:00"" -2.080144
'), as.is=TRUE, header=TRUE)

R&gt; subset(df_, !duplicated(ts))
                   ts      v
1 2009-09-30 10:00:00 -2.082
2 2009-09-30 10:15:00 -2.080
6 2009-09-30 10:30:00 -2.094
8 2009-09-30 10:45:00 -2.086
9 2009-09-30 11:00:00 -2.080
</code></pre>

<p><strong>Update:</strong> To select a specific value you can use <code>aggregate</code></p>

<pre><code>aggregate(df_$v, by=list(df_$ts), function(x) x[1])  # first value
aggregate(df_$v, by=list(df_$ts), function(x) tail(x, n=1))  # last value
aggregate(df_$v, by=list(df_$ts), function(x) max(x))  # max value
</code></pre>
"
1770343,143305,2009-11-20T13:10:00Z,1769365,6,FALSE,"<p>I think you are looking at data structures for time-indexed objects, and not for a dictionary.  For the former, look at the <a href=""http://cran.r-project.org/package=zoo"" rel=""noreferrer"">zoo</a> and <a href=""http://cran.r-project.org/package=xts"" rel=""noreferrer"">xts</a> packages which offer much better time-pased subsetting:</p>

<pre><code>R&gt; library(xts)
R&gt; X &lt;- xts(data.frame(val=rnorm(10)), \
            order.by=Sys.time() + sort(runif(10,10,300)))
R&gt; X
                        val
2009-11-20 07:06:17 -1.5564
2009-11-20 07:06:40 -0.2960
2009-11-20 07:07:50 -0.4123
2009-11-20 07:08:18 -1.5574
2009-11-20 07:08:45 -1.8846
2009-11-20 07:09:47  0.4550
2009-11-20 07:09:57  0.9598
2009-11-20 07:10:11  1.0018
2009-11-20 07:10:12  1.0747
2009-11-20 07:10:58  0.7062
R&gt; X[""2009-11-20 07:08::2009-11-20 07:09""]
                        val
2009-11-20 07:08:18 -1.5574
2009-11-20 07:08:45 -1.8846
2009-11-20 07:09:47  0.4550
2009-11-20 07:09:57  0.9598
R&gt; 
</code></pre>

<p>The <code>X</code> object is ordered by a time sequence -- make sure it is of type POSIXct so you may need to parse your dates first.  Then we can just index for '7:08 to 7:09 on the give day'. </p>
"
1770829,163053,2009-11-20T14:36:25Z,1770787,4,FALSE,"<p>One thing that you can do is use <code>unclass()</code>.</p>

<pre><code> ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
 trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
 group &lt;- gl(2,10,20, labels=c(""Ctl"",""Trt""))
 weight &lt;- c(ctl, trt)
 anova(lm.D9 &lt;- lm(weight ~ group))
 s &lt;- summary(lm.D90 &lt;- lm(weight ~ group - 1))
</code></pre>

<p>Use names to investigate it:</p>

<pre><code> &gt; names(unclass(s))
  [1] ""call""          ""terms""         ""residuals""     ""coefficients""  ""aliased""            ""sigma""         ""df""            ""r.squared""     ""adj.r.squared""
 [10] ""fstatistic""    ""cov.unscaled"" 
</code></pre>

<p>And then reference a specific value:</p>

<pre><code> &gt; s$r.squared
 [1] 0.9817833
</code></pre>
"
1770842,143305,2009-11-20T14:38:51Z,1770787,6,FALSE,"<p>Besides <code>unclass()</code>, try <code>str()</code>. Or read the source code to see how the other accessors do it.</p>

<p><em>Edit:</em> Here is for example the source code of the S4 class method <code>summary</code> for the <code>ur.df</code> object you were looking at:</p>

<pre><code>setMethod(""summary"", ""ur.df"", function(object){
  return(new(""sumurca"", classname=""ur.df"", test.name=object@test.name,\
    testreg=object@testreg, teststat=object@teststat, cval=object@cval, \
    bpoint=NULL, signif=NULL, model=object@model, type=NULL, auxstat=NULL, \
    lag=NULL, H=NULL, A=NULL, lambda=NULL, pval=NULL, V=NULL, W=NULL, P=NULL))
})
</code></pre>

<p>and it uses a standard <code>@</code> accessor for S4 object elements.  </p>
"
1771083,134830,2009-11-20T15:12:05Z,1770787,12,TRUE,"<p>As another alternative, take a look at <code>attributes</code>. e.g.</p>

<pre><code>example(ur.df)
attributes(lc.df) #lc.df is an ur.df object created during by example.

$y
[1] 10.4831 10.4893 10.5022 10.5240 10.5329 10.5586 10.5190 10.5381
[9] 10.5422 10.5361 10.5462 10.5459 10.5552 10.5548 10.5710 10.5861
[17] 10.5864 10.5802 10.6006 10.6168 10.6275 10.6414 10.6629 10.6758
[25] 10.6881 10.7240 10.7143 10.7222 10.7156 10.6964 10.6990 10.7081
[33] 10.7142 10.7078 10.7073 10.6954 10.6910 10.6967 10.7015 10.7083
[41] 10.7127 10.6922 10.6874 10.6989 10.7224 10.7452 10.7462 10.7663
[49] 10.7633 10.7737 10.8282 10.7872 10.8015 10.8139 10.7909 10.8029
[57] 10.7868 10.7979 10.8007 10.8008 10.7991 10.7956 10.8005 10.8160
[65] 10.8260 10.8405 10.8482 10.8633 10.8633 10.8615 10.8732 10.8649
[73] 10.8793 10.8909 10.8938 10.9116 10.9202 10.9409 10.9663 10.9700
[81] 10.9808 10.9878 11.0048 11.0272 11.0420 11.0701 11.0751 11.0964
[89] 11.1069 11.1123 11.1231 11.1223 11.1303 11.1307 11.1389 11.1325
[97] 11.1261 11.1232 11.1220

$model
[1] ""trend""

$lags
[1] 3

# etc.
</code></pre>

<p>If you don't want the full output, then <code>names(attributes(lc.df))</code> returns only the, um, names.</p>

<pre><code>[1] ""y""         ""model""     ""lags""      ""cval""      ""res""       ""teststat""  ""testreg""   ""test.name"" ""class"" 
</code></pre>
"
1772995,163053,2009-11-20T20:17:52Z,1772964,2,TRUE,"<p>It would be helpful if you provided more detail about what you're trying to do.</p>

<p>One ""problem"" (if there is one?) is that every time you ""grow"" the matrix, you will actually be recreating the entire matrix from scratch, which is a very memory inefficient.  There is no such thing as inserting a value into a matrix in R.</p>

<p>An alternative approach would be to store each object in your local environment (with the <code>assign()</code> function) and then assemble your matrix at the end once you know how many objects there are (with <code>get()</code>).</p>
"
1773517,163053,2009-11-20T21:59:43Z,1773366,30,FALSE,"<p>I definitely agree that this isn't intuitive (<a href=""https://stackoverflow.com/questions/1535021/whats-the-biggest-r-gotcha-youve-run-across"">I made that point before on SO</a>).  In defense of R, I think that knowing when you have a missing value is useful (i.e. this is not a bug).  The <code>==</code> operator is explicitly designed to notify the user of NA or NaN values.  See ?""=="" for more information.  It states:</p>

<blockquote>
  <p>Missing values ('NA') and 'NaN' values are regarded as
       non-comparable even to themselves, so comparisons involving them
       will always result in 'NA'.</p>
</blockquote>

<p>In other words, a missing value isn't comparable using a binary operator (because it's unknown).</p>

<p>Beyond is.na(), you could also do:</p>

<pre><code>which(a$col2==2) # tests explicitly for TRUE
</code></pre>

<p>Or</p>

<pre><code>a$col2 %in% 2 # only checks for 2
</code></pre>

<p>%in% is defined as using the <code>match()</code> function: </p>

<pre><code>'""%in%"" &lt;- function(x, table) match(x, table, nomatch = 0) &gt; 0'
</code></pre>

<p>This is also covered in <a href=""http://www.burns-stat.com/pages/Tutor/R_inferno.pdf"" rel=""nofollow noreferrer"">""The R Inferno""</a>.</p>

<p>Checking for NA values in your data is <em>crucial</em> in R, because many important operators don't handle it the way you expect.  Beyond ==, this is also true for things like &amp;, |, &lt;, sum(), and so on.  I am always thinking ""what would happen if there was an NA here"" when I'm writing R code.  Requiring an R user to be careful with missing values is ""by design"".</p>

<h3>Update: How is NA handled when there are multiple logical conditions?</h3>

<p><code>NA</code> is a logical constant and you might get unexpected subsetting if you don't think about what might be returned (e.g. <code>NA | TRUE == TRUE</code>). These truth tables from  <code>?Logic</code> may provide a useful illustration:</p>

<pre><code>outer(x, x, ""&amp;"") ## AND table
#       &lt;NA&gt; FALSE  TRUE
#&lt;NA&gt;     NA FALSE    NA
#FALSE FALSE FALSE FALSE
#TRUE     NA FALSE  TRUE

outer(x, x, ""|"") ## OR  table
#      &lt;NA&gt; FALSE TRUE
#&lt;NA&gt;    NA    NA TRUE
#FALSE   NA FALSE TRUE
#TRUE  TRUE  TRUE TRUE
</code></pre>
"
1774282,200303,2009-11-21T02:12:57Z,1774112,1,FALSE,"<p>I think I fixed it.</p>

<p>I found the Eclipse option which writes stderr to a console, and basically got the trace this guy got, talking about</p>

<p>SEVERE: Unsupported JRI version (API found: 108, required: 109)</p>

<p><a href=""http://lists.r-forge.r-project.org/pipermail/statet-user/2009-August/000200.html"" rel=""nofollow noreferrer"">http://lists.r-forge.r-project.org/pipermail/statet-user/2009-August/000200.html</a></p>

<p>Basically my rJava installation was out of date. The ubuntu repo gives 0.6.3, but installing the latest 0.8.1 from cran fixed it.</p>
"
1774723,180892,2009-11-21T06:29:52Z,1774559,0,FALSE,"<p>I use StatET and Eclipse as my R user interface. It automatically sets the working directory to the location of my project folder. workspace =  ${project_loc}.
It also automatically loads any saved workspace, when starting R from a particular project.</p>
"
1775333,216064,2009-11-21T12:20:08Z,1774559,0,FALSE,"<p>On Windows, I put a file Rgui.bat from code.google.com/p/batchfiles in my project directory and use this to start R.</p>
"
1776097,163053,2009-11-21T17:20:13Z,1776014,37,TRUE,"<p>Use the <code>source()</code> command.  In your case:</p>

<pre><code>source(""/path/to/file/my_fn_lib1.r"")
</code></pre>

<p>Incidentally, creating a package is fairly easy with the <code>package.skeleton()</code> function (if you're planning to reuse this frequently).  </p>
"
1777809,16632,2009-11-22T04:26:25Z,1777351,3,FALSE,"<p>Are your variables measured on a common scale?  If yes, then don't scale.  If no, then it's probably a good idea to scale.</p>

<p>If you are trying to predict the value of another variable, PCA is probably not the correct tool.  Maybe you should look at a regression model instead.</p>
"
1779955,216611,2009-11-22T20:48:15Z,1761852,1,FALSE,"<p>Here you go - ??forecast gave vars::predict, Predict method for objects of class varest and vec2var as an answer, which looks precisely as you want it. Increasing u looks like impulse response analysis, so look it up!</p>
"
1779979,172261,2009-11-22T20:58:51Z,1779916,5,TRUE,"<p>Use a string as first argument in <code>as.Date()</code> and select a specific weekday (format <code>%w</code>, value 0-6). There are seven possible dates in each week, therefore <code>strptime</code> needs more information to select a unique date. Otherwise the current day and month are returned.</p>

<pre><code>&gt; as.Date(paste(""200947"", ""0"", sep=""-""), format=""%Y%W-%w"")
[1] ""2009-11-22""
</code></pre>
"
1782785,172261,2009-11-23T12:12:31Z,1782704,14,TRUE,"<p>Package <code>zoo</code> has a function <code>na.locf()</code>:</p>

<pre><code>R&gt; library(""zoo"")
R&gt; na.locf(c(1, 2, 3, 4))
[1] 1 2 3 4
R&gt; na.locf(c(1, NA, NA, 2, 3, NA, 4))
[1] 1 1 1 2 3 3 4
</code></pre>

<p><code>na.locf</code>: Last Observation Carried Forward;
Generic function for replacing each ‘NA’ with the most recent non-‘NA’ prior to it.</p>

<p>See the source code of the function <code>na.locf.default</code>, it doesn't need a <code>for</code>-loop.</p>
"
1783275,78912,2009-11-23T13:59:15Z,1782704,12,FALSE,"<p>I'm doing some minimal copy&amp;paste from the zoo library (thanks again rcs for pointing me at it) and this is what I really needed:</p>

<pre><code>fillInTheBlanks &lt;- function(S) {
  ## NA in S are replaced with observed values

  ## accepts a vector possibly holding NA values and returns a vector
  ## where all observed values are carried forward and the first is
  ## also carried backward.  cfr na.locf from zoo library.
  L &lt;- !is.na(S)
  c(S[L][1], S[L])[cumsum(L)+1]
}
</code></pre>
"
1784037,163053,2009-11-23T15:54:29Z,1761852,2,FALSE,"<p>This subject is covered very nicely in Chapters 4 and 8 of Bernhard Pfaff's book, ""<a href=""http://rads.stackoverflow.com/amzn/click/0387759662"" rel=""nofollow noreferrer"">Analysis of Integrated and Cointegrated Time Series with R</a>"", for which the <strong>vars</strong> and <strong>urca</strong> packages were written.</p>

<p>The vec2var step is necessary if you want to use the predict functionality that's available.  </p>

<p>A more complete answer <a href=""http://old.nabble.com/R:-Use-VAR-model-to-predict-response-to-change-in-values-of-certain-variables-td26477723.html"" rel=""nofollow noreferrer"">was provided on the R-Sig-Finance list</a>.  See also <a href=""http://www.mail-archive.com/r-help@r-project.org/msg04938.html"" rel=""nofollow noreferrer"">this related thread</a>.</p>
"
1785342,163053,2009-11-23T19:17:32Z,1785118,1,FALSE,"<p>You want to recover the formula from a loess object?  You might be able to do something like this:</p>

<pre><code>&gt; cars.lo &lt;- loess(dist ~ speed, cars)
&gt; formula(unclass(cars.lo)$terms)
dist ~ speed
</code></pre>

<p><em>Edit:</em> Sorry...I think that I misinterpreted what you wanted.  There is no simple way to express the loess model in the form of an equation.</p>
"
1785419,188595,2009-11-23T19:30:27Z,1785118,7,FALSE,"<p>Loess doesn't give you an equation [1]. If you just want to get the values returned by the loess function you use <code>predict(loess.object, new.data)</code></p>

<p>[1] From wikipedia: </p>

<blockquote>
  <p>Another disadvantage of LOESS is the
  fact that it does not produce a
  regression function that is easily
  represented by a mathematical formula.
  This can make it difficult to transfer
  the results of an analysis to other
  people. In order to transfer the
  regression function to another person,
  they would need the data set and
  software for LOESS calculations.</p>
</blockquote>
"
1785450,217242,2009-11-23T19:39:01Z,1181025,2,FALSE,"<p>Best to keep simple, and see if linear methods work ""well enuff"". You can judge your goodness of fit GENERALLY by looking at the R squared AND F statistic, together, never separate. Adding variables to your model that have no bearing on your dependant variable can increase R2, so you must also consider F statistic. </p>

<p>You should also compare your model to other nested, or more simpler, models. Do this using log liklihood ratio test, so long as dependant variables are the same.</p>

<p>Jarque–Bera test is good for testing the normality of the residual distribution.</p>
"
1785887,162436,2009-11-23T20:54:47Z,1785737,3,TRUE,"<p>You can find all of the corresponding generic functions for an S3 class using methods(). So in your case:</p>

<pre><code>methods(class=fv)
</code></pre>
"
1786576,168747,2009-11-23T22:41:02Z,1785737,1,FALSE,"<p>It is clear described in help to <code>UseMethod</code>.</p>

<blockquote>
  <p>When a function calling <code>UseMethod(""fun"")</code> is applied to an object with class attribute <code>c(""first"", ""second"")</code>, the system searches for a function called <code>fun.first</code> and, if it finds it, applies it to the object. If no such function is found a function called <code>fun.second</code> is tried. If no class name produces a suitable function, the function <code>fun.default</code> is used, if it exists, or an error results.</p>
</blockquote>

<p>So if you looking for proper function you need to check all posibilities, eg.:</p>

<pre><code>fun_seeker &lt;- function(obj,fun) {
  funs_to_check &lt;- paste(fun,c(class(obj),""default""),sep=""."")
  funs_exists &lt;- funs_to_check %in% methods(fun)
  if (any(funs_exists)) funs_to_check[funs_exists][1] else stop(""No applicable method"")
}
fun_seeker(matrix(rnorm(100),10),""plot"")
fun_seeker(matrix(rnorm(100),10),""summary"")
</code></pre>
"
1786622,144157,2009-11-23T22:49:47Z,1785118,6,FALSE,"<p>There is no formula. Loess is a nonparametric method. It can't be expressed as a simple equation.</p>
"
1786901,158065,2009-11-23T23:52:04Z,1786667,4,FALSE,"<p>Ok, I'm going to guess that <code>x</code> is a data frame.  In which case <code>length</code> returns the number of columns, not the number of elements.  You want <code>nrow</code> instead.  Note that if <code>foo</code> is a data frame, getting a single column by <code>foo$bar</code> will return a data frame with one column.</p>

<pre><code>&gt; by(1:10, rep(1:5, 2), length)
rep(1:5, 2): 1
[1] 2
------------------------------------------------------------ 
rep(1:5, 2): 2
[1] 2
------------------------------------------------------------ 
rep(1:5, 2): 3
[1] 2
------------------------------------------------------------ 
rep(1:5, 2): 4
[1] 2
------------------------------------------------------------ 
rep(1:5, 2): 5
[1] 2
&gt; by(data.frame(1:10), rep(1:5, 2), length)
rep(1:5, 2): 1
[1] 1
------------------------------------------------------------ 
rep(1:5, 2): 2
[1] 1
------------------------------------------------------------ 
rep(1:5, 2): 3
[1] 1
------------------------------------------------------------ 
rep(1:5, 2): 4
[1] 1
------------------------------------------------------------ 
rep(1:5, 2): 5
[1] 1
&gt; by(data.frame(1:10), rep(1:5, 2), nrow)
rep(1:5, 2): 1
[1] 2
------------------------------------------------------------ 
rep(1:5, 2): 2
[1] 2
------------------------------------------------------------ 
rep(1:5, 2): 3
[1] 2
------------------------------------------------------------ 
rep(1:5, 2): 4
[1] 2
------------------------------------------------------------ 
rep(1:5, 2): 5
[1] 2
</code></pre>
"
1787913,65148,2009-11-24T05:02:41Z,1774559,1,FALSE,"<p>This depends on the system that your using.  There are a few tricks to use but if your looking to run R from a system menu and have it remember the directory the quick answer is no that won't happen.  Linux is pretty easy just navigate to the directory in the terminal first and that will solve the problem.  I have no idea about macs, But I can talk about windows extensively.  First if you navigate to the directory and save your workspace once then you can use the saved .RData file to double click and restore your workspace including current directory.  My personal, and biased opinion is to use an editor like <a href=""http://notepad-plus.sf.net"" rel=""nofollow noreferrer"">Notepad++</a> with <a href=""http://sf.net/projects/npptor"" rel=""nofollow noreferrer"">NppToR</a> that way when you spawn a Rgui window you inherit the active directory from the current script. It also provides a menu command to adjust the working directory to the current script's directory.</p>

<p>Another point is that you can always set the working directory with the <code>setwd(""path/to/dir/"")</code> command inside any R session on any platform.</p>
"
1788024,168168,2009-11-24T05:36:33Z,1785320,4,TRUE,"<p>Assuming your source table looks something like this:</p>

<pre><code>dfm &lt;- structure(list(ID1 = structure(c(1L, 2L, 3L, 1L, 2L, 3L, 1L, 
2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L), .Label = c(""ID1a"", ""ID1b"", ""ID1c""
), class = ""factor""), ID2 = structure(c(1L, 1L, 1L, 2L, 
2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L), .Label = c(""ID2a"", 
""ID2b"", ""ID2c"", ""ID2d"", ""ID2e""), class = ""factor""), value = c(6459695L, 
7263529L, 7740364L, 885473L, 1411355L, 1253524L, 648019L, 587785L, 
682977L, 453613L, 612730L, 886897L, 1777308L, 2458672L, 3559283L
)), .Names = c(""ID1"", ""ID2"", ""value""), row.names = c(NA, 
-15L), class = ""data.frame"")

&gt; head(dfm)
   ID1  ID2   value
1 ID1a ID2a 6459695
2 ID1b ID2a 7263529
3 ID1c ID2a 7740364
4 ID1a ID2b  885473
5 ID1b ID2b 1411355
6 ID1c ID2b 1253524
</code></pre>

<p>Using <code>ddply</code> first to calculate the percentages, and <code>cast</code> to present the data in the required format</p>

<pre><code>library(reshape)
library(plyr)

df1 &lt;- ddply(dfm, .(ID1), summarise, ID2 = ID2, pct = value / sum(value))
dfc &lt;- cast(df1, ID1 ~ ID2)

dfc
   ID1      ID2a       ID2b       ID2c       ID2d      ID2e
1 ID1a 0.6318101 0.08660638 0.06338147 0.04436700 0.1738350
2 ID1b 0.5888996 0.11442735 0.04765539 0.04967784 0.1993399
3 ID1c 0.5480662 0.08875735 0.04835905 0.06279786 0.2520195
</code></pre>

<p>Compared to your example, this is missing the row totals, these need to be added separately.</p>

<p>Not sure though, whether this solution is more elegant than the one you currently have.</p>
"
1788834,172261,2009-11-24T09:07:44Z,1788779,18,TRUE,"<p>Just enter the name of a function/method without parentheses:</p>

<pre><code>R&gt; base::rev.default 
function (x) 
if (length(x)) x[length(x):1L] else x
&lt;environment: namespace:base&gt;
</code></pre>

<p>See also <em>R-Help Desk - Accessing the Sources</em> in <a href=""http://www.r-project.org/doc/Rnews/Rnews_2006-4.pdf"" rel=""noreferrer"">R News Volume 6/4, October 2006</a>.</p>
"
1790459,16632,2009-11-24T14:35:04Z,1787578,4,TRUE,"<p>Try this to get you started</p>

<pre><code>ggplot() + 
  geom_point(aes(x, y, colour = ""actual""), data = df.actual) + 
  geom_line(aes(x, y, colour = ""approximate""), data = df.approx) + 
  scale_colour_discrete(""Type"")
</code></pre>
"
1790910,216064,2009-11-24T15:45:19Z,1788779,9,FALSE,"<p>To find out which methods you want to see, write <code>methods(funcOfInterest)</code></p>

<p>Sometimes it does not suffice to <code>print(funcOfInterest.class)</code>. Try <code>print(getAnywhere(funcOfInterest.class))</code> then.</p>
"
1791274,163053,2009-11-24T16:38:02Z,1791164,4,TRUE,"<p>One simple suggestion:</p>

<p>Can you just use a tryCatch block around your read.table (or whichever read command you're using)?  Then just skip a file if you get that error message.  You can also compile a list of corrupted files within the catch block (I recommend doing that so that you are tracking corrupted files for future reference when running a big batch process like this).  Here's the pseudo code:</p>

<pre><code>corrupted.files &lt;- data.frame()
for(i in 1:nrow(files)) {
    x &lt;- tryCatch(read.table(file=files[i]), error = function(e) 
         if(e==""something"") { corrupted.files &lt;- rbind(corrupted.files, files[i]) } 
         else { stop(e) }, 
       finally=print(paste(""finished with"", files[i], ""at"", Sys.time())))
    if(nrow(x)) # do something with the uncorrupted data            
}
</code></pre>
"
1791321,134830,2009-11-24T16:44:11Z,1788779,15,FALSE,"<p>How you find the source code depends on the type of function. See <a href=""https://stackoverflow.com/questions/1439348/how-to-examine-the-code-of-a-function-in-r-thats-object-class-sensitive/1444512#1444512"">my answer</a> to this related question.</p>

<p>As rcs pointed out, if you want to specify a package, you can use <code>::</code>.  </p>

<pre><code>&gt; lattice::xyplot
function (x, data, ...) 
UseMethod(""xyplot"")
&lt;environment: namespace:lattice&gt;
</code></pre>

<p>Not all functions from a package will be exported (i.e. made publically available); for these you need to use <code>:::</code>.</p>

<pre><code>&gt; lattice::xyplot.formula
Error: 'xyplot.formula' is not an exported object from 'namespace:lattice'

&gt; lattice:::xyplot.formula
function (x, data = NULL, allow.multiple = is.null(groups) || 
    outer, outer = !is.null(groups), auto.key = FALSE, aspect = ""fill"", 
    panel = lattice.getOption(""panel.xyplot""), prepanel = NULL, 
    scales = list(), strip = TRUE, groups = NULL, xlab, xlim, 
    ylab, ylim, drop.unused.levels = lattice.getOption(""drop.unused.levels""), 
    ..., lattice.options = NULL, default.scales = list(), subscripts = !is.null(groups), 
    subset = TRUE) 
{
    formula &lt;- x
    dots &lt;- list(...)
# etc.
</code></pre>
"
1791394,134830,2009-11-24T16:54:57Z,1786667,2,TRUE,"<p>If you are trying to get the number of records for different groups of your data, then the easiest way to do it is usually with <code>table</code>. It isn't clear from your post which data frame you want to use &ndash; is it <code>x</code> or <code>nn$info</code>?  with this in mind, your code should look something like</p>

<pre><code>with(nn$info, table(party, state=st))
</code></pre>

<p>Here's an example anyone can replicate, using the <code>Cars93</code> dataset in the <code>MASS</code> package.</p>

<pre><code>&gt; with(Cars93, table(Type, AirBags))
         AirBags
Type      Driver &amp; Passenger Driver only None
  Compact                  2           9    5
  Large                    4           7    0
  Midsize                  7          11    4
  Small                    0           5   16
  Sporty                   3           8    3
  Van                      0           3    6
</code></pre>
"
1794436,144157,2009-11-25T03:27:20Z,1794318,7,TRUE,"<p>RGoogleDocs is on omegahat: <a href=""http://www.omegahat.org/RGoogleDocs/"" rel=""noreferrer"">http://www.omegahat.org/RGoogleDocs/</a><br>
RGoogleData is on RForge: <a href=""http://r-forge.r-project.org/projects/rgoogledata/"" rel=""noreferrer"">http://r-forge.r-project.org/projects/rgoogledata/</a></p>

<p>RForge is a sourceforge-like place for the development of R packages.<br>
Omegahat is more general, providing open-source software for statistical applications. That includes some R packages, but also software for other environments.</p>

<p>R packages on both sites can be installed directly using <code>install.packages()</code> with the appropriate repos setting.</p>
"
1795906,134830,2009-11-25T10:07:17Z,1785737,0,FALSE,"<p>In this case we are dealing with an S3 class.  (This is the older, simpler object oriented style.)  This means that when you type <code>plot(K)</code> for an object with class <code>""fv"" ""data.frame""</code>, R does the following:</p>

<p>It looks through the search path to find a function named <code>plot</code>.  Type <code>search()</code> to see where R looks.  Assuming you haven't done something silly like defining your own plot function, it should find the version in the <code>graphics</code> package.</p>

<p>This function has some special logic for dealing with an input where <code>x</code> is a function, then calls <code>UseMethod</code>.</p>

<p><code>UseMethod</code> looks on the serach path for a function named <code>plot.fv</code>, and calls it if it finds it.</p>

<p>Failing that, it looks for <code>plot.data.frame</code>.</p>

<p>Failing that, it looks for <code>plot.default</code>.</p>

<p>If that couldn't be found an error would have been thrown.  (Though <code>plot.default</code> exists in the graphics package, so you'd have to try quite hard to get an error here.)</p>
"
1797458,172261,2009-11-25T14:54:36Z,1787578,4,FALSE,"<p>This is some kind of hack to modify the legend by manipulation of the grid object:</p>

<pre><code>library(""ggplot2"")
df.actual &lt;- data.frame(x=1:100, y=(1:100)*2)
df.approx &lt;- data.frame(x=1:150, y=(1:150)*2 + 5 + rnorm(150, mean=3))
p &lt;- ggplot() +
     geom_point(aes(x, y, colour=""Actual""), data=df.actual) +
     geom_line(aes(x, y, colour=""Approximate""), data=df.approx) +
     scale_colour_manual(name=""Type"",
                         values=c(""Actual""=""black"", ""Approximate""=""black""))
library(""grid"")
grob &lt;- ggplotGrob(p)
tmp &lt;- grid.ls(getGrob(grob, ""key.segments"", grep=TRUE, global=TRUE))$name
grob &lt;- removeGrob(grob, tmp[1])  # remove first line segment in legend key
tmp &lt;- grid.ls(getGrob(grob, ""key.points"", grep=TRUE, global=TRUE))$name
grob &lt;- removeGrob(grob, tmp[2])  # remove second point in legend key
grid.draw(grob)
</code></pre>

<p><a href=""http://img134.imageshack.us/img134/8427/ggplotlegend.png"" rel=""nofollow noreferrer"">ggplot2 output http://img134.imageshack.us/img134/8427/ggplotlegend.png</a></p>
"
1798206,143377,2009-11-25T16:35:18Z,1797712,10,TRUE,"<p>The line should be </p>

<pre><code>print(p + geom_abline(intercept = inter, slope = slop, colour = ""red"", size = 1))
</code></pre>

<p>In pdf devices, ggplot (and lattice) only writes to file when explicitly printed.</p>
"
1801349,163053,2009-11-26T03:13:48Z,1801064,14,FALSE,"<p>Try using <code>par</code> before you plot.  </p>

<pre><code> par(mfrow = c(2, 1))
</code></pre>
"
1802198,4907,2009-11-26T08:04:23Z,1801064,0,FALSE,"<p>If you want the 2 plots in separate windows or files you can select new devices before calling each plot command.  See: </p>

<blockquote>
  <p>?Devices</p>
</blockquote>

<p>And,</p>

<blockquote>
  <p>?dev.cur</p>
</blockquote>
"
1803317,134830,2009-11-26T12:06:33Z,1801064,37,FALSE,"<p>If you just want to see two different plotting windows open at the same time, use <code>dev.new</code>, e.g.</p>

<pre><code>plot(1:10)
dev.new()
plot(10:1)
</code></pre>

<p>If you want to draw two plots in the same window then, as Shane mentioned, set the <code>mfrow</code> parameter.</p>

<pre><code>par(mfrow = c(2,1))
plot(1:10)
plot(10:1)
</code></pre>

<p>If you want to try something a little more advanced, then you can take a look at lattice graphics or ggplot, both of which are excellent for creating conditioned plots (plots where different subsets of data appear in different frames).</p>

<p>A lattice example:</p>

<pre><code>library(lattice)
dfr &lt;- data.frame(
  x   = rep(1:10, 2), 
  y   = c(1:10, 10:1), 
  grp = rep(letters[1:2], each = 10)
)
xyplot(y ~ x | grp, data = dfr)
</code></pre>

<p>A ggplot example.  (You'll need to download ggplot from CRAN first.)</p>

<pre><code>library(ggplot2)
qplot(x, y, data = dfr, facets = grp ~ .)
#or equivalently
ggplot(dfr, aes(x, y)) + geom_point() + facet_grid(grp ~ .)
</code></pre>
"
1803643,172261,2009-11-26T13:23:23Z,1803627,12,TRUE,"<p><code>strptime</code> returns class <code>POSIXlt</code>, you need <code>POSIXct</code> in the data frame:</p>

<pre><code>R&gt; class(strptime(""2009-09-30 10:00:00"", ""%Y-%m-%d %H:%M:%S"", tz=""UTC""))
[1] ""POSIXt""  ""POSIXlt""
R&gt; class(as.POSIXct(""2009-09-30 10:00:00"", ""%Y-%m-%d %H:%M:%S"", tz=""UTC""))
[1] ""POSIXt""  ""POSIXct""
</code></pre>

<p>Class <code>POSIXct</code> represents the (signed) number of seconds since the beginning of
1970 as a numeric vector. Class <code>POSIXlt</code> is a named list of vectors representing sec, min, hour, mday, mon, year, etc.</p>

<pre><code>R&gt; unclass(strptime(""2009-09-30 10:00:00"", ""%Y-%m-%d %H:%M:%S"", tz=""UTC""))
$sec
[1] 0
$min
[1] 0
$hour
[1] 10
$mday
[1] 30
$mon
[1] 8
$year
[1] 109
$wday
[1] 3
$yday
[1] 272
$isdst
[1] 0
attr(,""tzone"")
[1] ""UTC""

R&gt; unclass(as.POSIXct(""2009-09-30 10:00:00"", ""%Y-%m-%d %H:%M:%S"", tz=""UTC""))
[1] 1.254e+09
attr(,""tzone"")
[1] ""UTC""
</code></pre>
"
1806053,190277,2009-11-26T23:03:55Z,1805149,13,FALSE,"<p>Answer: you don't want pc$rotation, it's the rotation matrix and not the matrix of rotated values (scores).</p>

<p>Make up some data:</p>

<pre><code>x1 = runif(100)
x2 = runif(100)
y = rnorm(2+3*x1+4*x2)
d = cbind(x1,x2)

pc = prcomp(d)
dim(pc$rotation)
## [1] 2 2
</code></pre>

<p>Oops.  The ""x"" component is what we want.  From ?prcomp: </p>

<blockquote>
  <p>x: if ‘retx’ is true the value of the rotated data (the centred (and scaled if requested) data multiplied by the ‘rotation' matrix) is returned.</p>
</blockquote>

<pre><code>dim(pc$x)
## [1] 100   2
lm(y~pc$x[,1]+pc$x[,2])
## 
## Call:
## lm(formula = y ~ pc$x[, 1] + pc$x[, 2])

## Coefficients:
## (Intercept)    pc$x[, 1]    pc$x[, 2]  
##     0.04942      0.14272     -0.13557  
</code></pre>
"
1807941,134830,2009-11-27T10:27:44Z,1801487,4,FALSE,"<p>The code <code>r = lm(y ~ x1+x2)</code> means we model y as a linear function of x1 and x2.  Since the model will not be perfect, there will be a residual term (i.e. the left-over that the model failed to fit).</p>

<p>In maths, as Rob Hyndman noted in the comments, <code>y = a + b1*x1 + b2*x2 + e</code>, where <code>a</code>, <code>b1</code> and <code>b2</code> are constants and <code>e</code> is your residual (which is assumed to be normally distributed).</p>

<p>To look at a concrete example, consider the iris data that comes with R.</p>

<pre><code>model1 &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data=iris)
</code></pre>

<p>Now we can extract the constants from the model (equivalent to <code>a</code>, <code>b1</code>, <code>b2</code> and in this case <code>b3</code> too).</p>

<pre><code>&gt; coefficients(model1)
(Intercept)  Sepal.Width Petal.Length  Petal.Width 
1.8559975    0.6508372    0.7091320   -0.5564827
</code></pre>

<p>The residuals have been calculated for each row of data that was used in the model.</p>

<pre><code>&gt; residuals(model1)
           1             2             3             4             5       
0.0845842387  0.2100028184 -0.0492514176 -0.2259940935 -0.0804994772
# etc. There are 150 residuals and 150 rows in the iris dataset.
</code></pre>

<hr>

<p>(EDIT: Cut summary info as not relevent.)</p>

<hr>

<p>EDIT:</p>

<p>The <code>Error</code> value you mention in your comments in explained on the help page to aov.</p>

<pre><code>If the formula contains a single ‘Error’ term, this is used to
specify error strata, and appropriate models are fitted within
each error stratum.
</code></pre>

<p>Compare the following (adapted from the <code>?aov</code> page.)</p>

<pre><code>&gt; utils::data(npk, package=""MASS"")
&gt; aov(yield ~  N*P*K, npk)
Call:
   aov(formula = yield ~ N * P * K, data = npk)

Terms:
                       N        P        K      N:P      N:K      P:K    N:P:K Residuals
Sum of Squares  189.2817   8.4017  95.2017  21.2817  33.1350   0.4817  37.0017  491.5800
Deg. of Freedom        1        1        1        1        1        1        1        16

Residual standard error: 5.542901 
Estimated effects may be unbalanced

&gt; aov(yield ~  N*P*K + Error(block), npk)
Call:
aov(formula = yield ~ N * P * K + Error(block), data = npk)

Grand Mean: 54.875 

Stratum 1: block

Terms:
                    N:P:K Residuals
Sum of Squares   37.00167 306.29333
Deg. of Freedom         1         4

Residual standard error: 8.750619 
Estimated effects are balanced

Stratum 2: Within

Terms:
                        N         P         K       N:P       N:K       P:K Residuals
Sum of Squares  189.28167   8.40167  95.20167  21.28167  33.13500   0.48167 185.28667
Deg. of Freedom         1         1         1         1         1         1        12

Residual standard error: 3.929447 
1 out of 7 effects not estimable
Estimated effects may be unbalanced
</code></pre>
"
1809671,171659,2009-11-27T16:35:17Z,1809525,2,FALSE,"<p>You could simply sum each one separatly. Let's call your data frame df :</p>

<pre><code>USA &lt;- sum(df[df$countrydes==""USA"",]$Insured)
CANADA &lt;- sum(df[df$countrydes==""CANADA"",]$Insured)
</code></pre>

<p>But with aggregate() you can handle all the countries in one line. </p>

<pre><code> aggsumcount &lt;- aggregate(x=df$Insured,by=list(df$countrydes),FUN=sum)
</code></pre>
"
1809707,134830,2009-11-27T16:41:14Z,1809525,3,FALSE,"<p>This will do the trick:</p>

<pre><code># Define your data
dfr &lt;- data.frame(
   countrydes=rep(c(""USA"", ""CANADA""), 2), 
   Insured=c(4500, 4500, 7500, 7600))
# Sum by country
totals &lt;- with(dfr, by(Insured, countrydes, sum))
# Plot the answer
barplot(totals)
</code></pre>

<p>(As Etiennebr mentioned, you could use <code>aggregate</code> instead of <code>by</code>, in which case you need to coerce <code>countrydes</code> to be a list.)</p>
"
1809723,163053,2009-11-27T16:46:36Z,1809518,7,FALSE,"<p>Yes, use <code>ls()</code>.   </p>

<p>You can use search() to see what's in the search path: </p>

<pre><code>&gt; search() 
[1] "".GlobalEnv""        ""package:stats""     ""package:graphics""
[4] ""package:grDevices"" ""package:utils""     ""package:datasets""
[7] ""package:methods""   ""Autoloads""         ""package:base""
</code></pre>

<p>You can search a particular package with the full name:</p>

<pre><code> &gt; ls(""package:graphics"")
 [1] ""abline""          ""arrows""          ""assocplot""       ""axis""
 ....
</code></pre>

<p>I also suggest <a href=""https://stackoverflow.com/questions/1386767/are-there-any-good-r-object-browsers"">that you look <b>at this related question</b> on stackoverflow</a> which includes some more creative approaching to browsing the environment.  If you're using ESS, then you can use Ess-rdired.</p>

<p>To get the help pages on a particular topic, you can either use <code>help(function.name)</code> or <code>?function.name</code>.  You will also find the <code>help.search()</code> function useful if you don't know the exact function name or package.  And lastly, <a href=""http://cran.r-project.org/web/packages/sos/"" rel=""nofollow noreferrer"">have a look at the <strong>sos</strong> package</a>.</p>
"
1809726,30911,2009-11-27T16:47:17Z,1809518,0,FALSE,"<pre><code>help(topic) #for documentation on a topic
?topic

summary(mydata) #an overview of data objects try

ls() # lists all objects in the local namespace

str(object) # structure of an object
ls.str() # structure of each object returned by ls()

apropos(""mytopic"") # string search of the documentation
</code></pre>

<hr>

<p>All from the <a href=""http://cran.r-project.org/doc/contrib/Short-refcard.pdf"" rel=""nofollow noreferrer"">R reference card</a></p>
"
1809990,74658,2009-11-27T17:58:19Z,1801064,0,FALSE,"<p>An alternative answer is to assign the plot as an object, then you can display it when you want
i.e</p>

<pre><code>abcplot&lt;-plot(pc) title(main='abc',xlab='xx',ylab='yy')

sdfplot&lt;-plot(pcs) title(main='sdf',xlab='sdf',ylab='xcv')

abcplot # Displays the abc plot
sdfplot # Displays the sdf plot
abcplot # Displays the abc plot again
</code></pre>
"
1810493,16632,2009-11-27T20:14:15Z,1809518,6,TRUE,"<p><code>help(package = packagename)</code> will list all non-internal functions in a package.</p>
"
1811771,136328,2009-11-28T06:23:26Z,1811651,0,FALSE,"<p>In general, with FA you cannot directly interpret the factor loadings because they are not unique (rotation problem). Other than that, I hate to sound like psychologist (statistician joke...), but you have a low p-value!</p>
"
1813609,172261,2009-11-28T19:57:34Z,1813550,35,TRUE,"<p>You could use <code>table</code>:</p>

<pre><code>R&gt; x &lt;- read.table(textConnection('
   Believe Age Gender Presents Behaviour
1    FALSE   9   male       25   naughty
2     TRUE   5   male       20      nice
3     TRUE   4 female       30      nice
4     TRUE   4   male       34   naughty'
), header=TRUE)

R&gt; table(x$Believe)

FALSE  TRUE 
    1     3 
</code></pre>
"
1813628,167973,2009-11-28T20:04:11Z,1813550,13,FALSE,"<pre><code>sum(Santa$Believe)
</code></pre>
"
1813788,172261,2009-11-28T21:07:32Z,1813700,2,FALSE,"<p>All combinations in a single plot:</p>

<pre><code>pairs(pc$x)
</code></pre>

<p>To select a specific combination just use:</p>

<pre><code>plot(pc$x[, c(1,3)])  # e.g. pc1 and pc3
</code></pre>
"
1814592,65148,2009-11-29T03:51:21Z,1812702,1,FALSE,"<p>Your question is not easy to answer.  There is not one definitive function.</p>

<p><code>formals</code> is the function that gives the <strong>named</strong> arguments to a function and their defaults in a named list, but you can always have variable arguments through the <code>...</code> parameter and hidden named arguments with embedded <code>hadArg</code> function.  To get a list of those you would have to use a getAnywhere and then scan the expression for the hasArg.  I can't think of a automatic way of doing it yourself.  That is if the functions hidden arguments are not documented.</p>
"
1815629,86837,2009-11-29T14:12:46Z,1815606,1,FALSE,"<p>You can wrap the r script in a bash script and retrieve  the script's path as a bash variable like so:</p>

<pre><code>#!/bin/bash
     # [environment variables can be set here]
     path_to_script=$(dirname $0)

     R --slave&lt;&lt;EOF
        source(""$path_to_script/other.R"")

     EOF
</code></pre>
"
1815743,49922,2009-11-29T15:04:28Z,1815606,43,FALSE,"<p>You can use the <code>commandArgs</code> function to get all the options that were passed by Rscript to the actual R interpreter and search them for <code>--file=</code>. If your script was launched from the path or if it was launched with a full path, the <code>script.name</code> below will start with a <code>'/'</code>. Otherwise, it must be relative to the <code>cwd</code> and you can concat the two paths to get the full path.</p>

<p><b>Edit:</b> it sounds like you'd only need the <code>script.name</code> above and to strip off the final component of the path. I've removed the unneeded <code>cwd()</code> sample and cleaned up the main script and posted my <code>other.R</code>. Just save off this script and the <code>other.R</code> script into the same directory, <code>chmod +x</code> them, and run the main script.</p>

<p><b>main.R</b>:</p>

<pre><code>#!/usr/bin/env Rscript
initial.options &lt;- commandArgs(trailingOnly = FALSE)
file.arg.name &lt;- ""--file=""
script.name &lt;- sub(file.arg.name, """", initial.options[grep(file.arg.name, initial.options)])
script.basename &lt;- dirname(script.name)
other.name &lt;- paste(sep=""/"", script.basename, ""other.R"")
print(paste(""Sourcing"",other.name,""from"",script.name))
source(other.name)
</code></pre>

<p><b>other.R</b>:</p>

<pre><code>print(""hello"")
</code></pre>

<p><b>output</b>:</p>

<pre><code>burner@firefighter:~$ main.R
[1] ""Sourcing /home/burner/bin/other.R from /home/burner/bin/main.R""
[1] ""hello""
burner@firefighter:~$ bin/main.R
[1] ""Sourcing bin/other.R from bin/main.R""
[1] ""hello""
burner@firefighter:~$ cd bin
burner@firefighter:~/bin$ main.R
[1] ""Sourcing ./other.R from ./main.R""
[1] ""hello""
</code></pre>

<p>This is what I believe dehmann is looking for.</p>
"
1816032,66549,2009-11-29T16:51:40Z,1813550,20,FALSE,"<p>I think of this as a two-step process: </p>

<ol>
<li><p>subset the original data frame according to the filter supplied
(Believe==FALSE); then</p></li>
<li><p>get the row count of this subset</p></li>
</ol>

<p>For the first step, the <strong><em>subset</em></strong> function is a good way to do this (just an alternative to ordinary index or <em>bracket</em> notation).</p>

<p>For the second step, i would use <strong><em>dim</em></strong> or <strong><em>nrow</em></strong></p>

<p>One advantage of using <em>subset</em>: you don't have to parse the result it returns to get the result you need--just call <em>nrow</em> on it directly.</p>

<p>so in your case:</p>

<pre><code>v = nrow(subset(Santa, Believe==FALSE))     # 'subset' returns a data.frame
</code></pre>

<p>or wrapped in an <em>anonymous function</em>:</p>

<pre><code>&gt;&gt; fnx = function(fac, lev){nrow(subset(Santa, fac==lev))}

&gt;&gt; fnx(Believe, TRUE)
      3
</code></pre>

<p>Aside from <em>nrow</em>, <em>dim</em> will also do the job. This function returns the <em>dimensions</em> of a data frame (rows, cols) so you just need to supply the appropriate index to access the number of rows: </p>

<pre><code>v = dim(subset(Santa, Believe==FALSE))[1] 
</code></pre>

<p>An answer to the OP posted before this one shows the use of a contingency table. I don't like that approach for the general problem as recited in the OP. Here's the reason. Granted, the general problem of <em>how many rows in this data frame have value x in column C?</em> can be answered using a contingency table as well as using a ""filtering"" scheme (as in my answer here). If you want row counts for all values for a given factor variable (column) then a contingency table (via calling <em>table</em> and passing in the column(s) of interest) is the most sensible solution; however, the OP asks for the count of a <em>particular</em> value in a factor variable, not counts across all values. Aside from the performance hit (might be big, might be trivial, just depends on the size of the data frame and the processing pipeline context in which this function resides). And of course once the result from the call to table is returned, you still have to <em>parse</em> from that result just the count that you want. </p>

<p>So that's why, to me, this is a filtering rather than a cross-tab problem.</p>
"
1816254,172261,2009-11-29T18:12:22Z,1816238,13,TRUE,"<p>To stop this smart behaviour, add <code>(ess-toggle-underscore nil)</code> to your .emacs after <code>ess-site</code> has been loaded.</p>
"
1816391,143377,2009-11-29T18:50:26Z,1816238,2,FALSE,"<p>What rcs said. But pressing _ again will turn the ""&lt;-"" into ""_"" when you need it. So, when you need an underscore, just press it twice.</p>
"
1816487,16632,2009-11-29T19:21:05Z,1815606,32,FALSE,"<pre><code>frame_files &lt;- lapply(sys.frames(), function(x) x$ofile)
frame_files &lt;- Filter(Negate(is.null), frame_files)
PATH &lt;- dirname(frame_files[[length(frame_files)]])
</code></pre>

<p>Don't ask me how it works though, because I've forgotten :/</p>
"
1816543,172261,2009-11-29T19:47:08Z,1816480,4,FALSE,"<p>Have a look at the packages <code>lattice</code> or <code>ggplot2</code>, the plot functions in these packages create objects which can be assigned to variables and can be printed or plotted at a later stage.</p>

<p>For instance with <code>lattice</code>:</p>

<pre><code>library(""lattice"")
i &lt;- 1
assign(sprintf(""a%d"", i), xyplot(1:10 ~ 1:10))
print(a1) # you have to ""print"" or ""plot"" the objects explicitly
</code></pre>

<p>Or append the objects to a list:</p>

<pre><code>p &lt;- list()
p[[1]] &lt;- xyplot(...)
p[[2]] &lt;- xyplot(...)
</code></pre>
"
1816551,135870,2009-11-29T19:49:15Z,1816480,5,TRUE,"<p>If you want a ""vector"" of plot objects, the easiest way is probably to store them in a <code>list</code>.  Use <code>paste()</code> to create a name for your plot and then add it to the list:</p>

<pre><code># Create a list to hold the plot objects.
pltList &lt;- list()

for( i in 2:15 ){

  # Get data, perform analysis, ect.

  # Create plot name.
  pltName &lt;- paste( 'a', i, sep = '' )

  # Store a plot in the list using the name as an index.
  # Note that the plotting function used must return an *object*.
  # Functions from the `graphics` package, such as `plot`, do not return objects.
  pltList[[ pltName ]] &lt;- some_plotting_function()

}
</code></pre>

<p>If you didn't want to store the plots in a list and literally wanted to create a new object that had the name contained in <code>pltName</code>, then you could use <code>assign()</code>:</p>

<pre><code># Use assign to create a new object in the Global Environment
# that gets it's name from the value of pltName and it's contents
# from the results of plot()
assign( pltName, plot(), envir = .GlobalEnv )
</code></pre>
"
1816987,172261,2009-11-29T22:22:35Z,1816719,4,FALSE,"<p>The labeling can be done in the following way:</p>

<pre><code>library(""ggplot2"")
x &lt;- data.frame(a=1:10, b=rnorm(10))
x$lab &lt;- rep("""", 10)   # create empty labels
x$lab[c(1,3,4,5)] &lt;- LETTERS[1:4]   # some labels
ggplot(data=x, aes(x=a, y=b, label=lab)) + geom_point() + geom_text(vjust=0)
</code></pre>
"
1817045,16632,2009-11-29T22:45:55Z,1816719,15,TRUE,"<p>Use subsetting:</p>

<pre><code>library(ggplot2)
x &lt;- data.frame(a=1:10, b=rnorm(10))
x$lab &lt;- letters[1:10]
ggplot(data=x, aes(a, b, label=lab)) + 
  geom_point() + 
  geom_text(data = subset(x, abs(b) &gt; 0.2), vjust=0)
</code></pre>
"
1818313,158065,2009-11-30T07:28:40Z,1817305,2,TRUE,"<p>This should do the trick I think:</p>

<pre><code>by(d, d$level1, function(x) cbind(x[,1:2], t(t(x[,-1:-2]) / colSums(x[,-1:-2], na.rm=TRUE))))
</code></pre>

<p>You can run a <code>do.call(rbind,...)</code> on that if you want everything in one data frame.</p>
"
1819210,134830,2009-11-30T11:16:21Z,1817305,1,FALSE,"<p>As I understand the question, you have the totals, using: </p>

<pre><code>totals &lt;- cast(level1 ~ variable, data=melt(d, na.rm=T),sum)
</code></pre>

<p>... and you want to convert them to percentages.  (Note that you called the first column ""L1"" in your question text, but the data structure calls the column ""level1"".)</p>

<p>Going from totals to percentages is more straightforward than you think.</p>

<pre><code>prc &lt;- 100 * totals[,-1] / colSums(totals[,-1])
rownames(prc) &lt;- totals[,1]
</code></pre>
"
1819459,180892,2009-11-30T12:14:10Z,1811651,3,FALSE,"<p>I posted an <a href=""http://jeromyanglim.blogspot.com/2009/10/factor-analysis-in-r.html"" rel=""nofollow noreferrer"">example factor analysis in R</a> looking at the factor structure of a personality test. It shows how to extract some of the common information that you might want (e.g., communalities; tests of number of factors; variance explained by factors; rotations; etc.).</p>
"
1820610,37751,2009-11-30T15:48:11Z,1727772,233,FALSE,"<p>I didn't see this question initially and asked a similar question a few days later. I am going to take my previous question down, but I thought I'd add an answer here to explain how I used <code>sqldf()</code> to do this.</p>

<p>There's been <a href=""https://stat.ethz.ch/pipermail/r-help/2007-August/138315.html"" rel=""noreferrer"">little bit of discussion</a> as to the best way to import 2GB or more of text data into an R data frame. Yesterday I wrote a <a href=""http://www.cerebralmastication.com/?p=416"" rel=""noreferrer"">blog post</a> about using <code>sqldf()</code> to import the data into SQLite as a staging area, and then sucking it from SQLite into R. This works really well for me. I was able to pull in 2GB (3 columns, 40mm rows) of data in &lt; 5 minutes. By contrast, the <code>read.csv</code> command ran all night and never completed. </p>

<p>Here's my test code:</p>

<p>Set up the test data:</p>

<pre><code>bigdf &lt;- data.frame(dim=sample(letters, replace=T, 4e7), fact1=rnorm(4e7), fact2=rnorm(4e7, 20, 50))
write.csv(bigdf, 'bigdf.csv', quote = F)
</code></pre>

<p>I restarted R before running the following import routine:</p>

<pre><code>library(sqldf)
f &lt;- file(""bigdf.csv"")
system.time(bigdf &lt;- sqldf(""select * from f"", dbname = tempfile(), file.format = list(header = T, row.names = F)))
</code></pre>

<p>I let the following line run all night but it never completed:</p>

<pre><code>system.time(big.df &lt;- read.csv('bigdf.csv'))
</code></pre>
"
1820729,143305,2009-11-30T16:07:10Z,1820590,1,FALSE,"<p>For your second question: Multi-page pdfs are easy -- see <code>help(pdf)</code>:</p>

<pre><code> onefile: logical: if true (the default) allow multiple figures in one
          file.  If false, generate a file with name containing the
          page number for each page.  Defaults to ‘TRUE’.
</code></pre>

<p>For your main question, I don't understand if you want to store the <em>plot inputs</em> in a list for later processing, or the <em>plot outputs</em>.  If it is the latter, I am not sure that <code>plot()</code> returns an object you can store and retrieve.  </p>
"
1820931,143377,2009-11-30T16:42:42Z,1820590,2,FALSE,"<p>There is a bug in your code concerning list subscripting. It should be</p>

<pre><code>pltList[[pltName]]
</code></pre>

<p>not</p>

<pre><code>pltList[pltName]
</code></pre>

<p>Note:</p>

<pre><code>class(pltList[1])
[1] ""list""
</code></pre>

<p>pltList[1] is a <em>list</em> containing the first element of pltList.</p>

<pre><code>class(pltList[[1]])
[1] ""ggplot""
</code></pre>

<p>pltList[[1]] is the first <em>element</em> of pltList.</p>
"
1820945,163053,2009-11-30T16:44:49Z,1820590,1,FALSE,"<p>Another suggestion regarding your second question would be to use either Sweave or Brew as they will give you complete control over how you display your multi-page pdf.</p>

<p>Have a look <a href=""https://stackoverflow.com/questions/1429907/workflow-for-statistical-analysis-and-report-writing"">at this related question</a>.</p>
"
1821743,83761,2009-11-30T19:06:16Z,1819418,1,FALSE,"<p>If you're trying to work on huge affymetrix datasets, you might have better luck using packages from <a href=""http://groups.google.com/group/aroma-affymetrix/web/overview"" rel=""nofollow noreferrer"">aroma.affymetrix</a>.</p>

<p>Also, bioconductor is a (particularly) fast moving project and you'll typically be asked to upgrade to the latest version of R in order to get any continued ""support"" (help on the BioC mailing list). I see that Thrawn also mentions having a similar problem with R 2.10, but you still might think about upgrading anyway.</p>
"
1821936,172261,2009-11-30T19:38:20Z,1820590,4,FALSE,"<p>I think you should use the <code>data</code> argument in <code>qplot</code>, i.e., store your vectors in a data frame.</p>

<p>See Hadley's book, Section 4.4:</p>

<p><em>The restriction on the data is simple: it must be a data frame. This is restrictive, and unlike other graphics packages in R. Lattice functions can take an optional data frame or use vectors directly from the global environment. ...</em></p>

<p><em>The data is stored in the plot object as a copy, not a reference. This has two
important consequences: if your data changes, the plot will not; and ggplot2 objects are entirely self-contained so that they can be save()d to disk and later load()ed and plotted without needing anything else from that session.</em> </p>
"
1822128,158065,2009-11-30T20:14:08Z,1820590,8,TRUE,"<p>Ok, so if your plot command is changed to </p>

<pre><code>p &lt;- qplot(data = data.frame(x = x, y = y),
           x, y,   
           xlab = ""Radius [km]"", 
           ylab = ""Services [log]"",
           xlim = x_range,
           ylim = c(0,10),
           main = paste(""Sample"",i)
           ) + geom_abline(intercept = inter, slope = slop, colour = ""red"", size = 1)           
</code></pre>

<p>then everything works as expected.  Here's what I suspect is happening (although Hadley could probably clarify things).  When ggplot2 ""saves"" the data, what it actually does is save a data frame, and the names of the parameters.  So for the command as I have given it, you get </p>

<pre><code>&gt; summary(pltList[[""a1""]])
data: x, y [50x2]
mapping:  x = x, y = y
scales:   x, y 
faceting: facet_grid(. ~ ., FALSE)
-----------------------------------
geom_point:  
stat_identity:  
position_identity: (width = NULL, height = NULL)

mapping: group = 1 
geom_abline: colour = red, size = 1 
stat_abline: intercept = 2.55595281266726, slope = 0.05543539319091 
position_identity: (width = NULL, height = NULL)
</code></pre>

<p>However, if you don't specify a <code>data</code> parameter in qplot, all the variables get evaluated in the current scope, because there is no attached (read: saved) data frame.  </p>

<pre><code>data: [0x0]
mapping:  x = x, y = y
scales:   x, y 
faceting: facet_grid(. ~ ., FALSE)
-----------------------------------
geom_point:  
stat_identity:  
position_identity: (width = NULL, height = NULL)

mapping: group = 1 
geom_abline: colour = red, size = 1 
stat_abline: intercept = 2.55595281266726, slope = 0.05543539319091 
position_identity: (width = NULL, height = NULL)
</code></pre>

<p>So when the plot is generated the second time around, rather than using the original values, it uses the <em>current</em> values of <code>x</code> and <code>y</code>.</p>
"
1822324,163053,2009-11-30T20:54:38Z,1810650,4,TRUE,"<p>This really boils down to what your comfortable level.  That said, igraph is a primarily a C library (you can <a href=""http://sourceforge.net/projects/igraph/"" rel=""nofollow noreferrer"">browse all the source code on sourceforge</a>), so the most logical way to extend it is probably in C.  For instance, the closeness function in R just call the related C function:</p>

<pre><code>&gt; closeness
function (graph, v = V(graph), mode = c(""all"", ""out"", ""in"")) 
{
    if (!is.igraph(graph)) {
        stop(""Not a graph object"")
    }
    mode &lt;- igraph.match.arg(mode)
    mode &lt;- switch(mode, out = 1, `in` = 2, all = 3)
    on.exit(.Call(""R_igraph_finalizer"", PACKAGE = ""igraph""))
    .Call(""R_igraph_closeness"", graph, as.igraph.vs(v), as.numeric(mode), 
        PACKAGE = ""igraph"")
}
</code></pre>

<p>Here is <a href=""http://igraph.bzr.sourceforge.net/bzr/igraph/0.6-main/annotate/head%3A/src/centrality.c"" rel=""nofollow noreferrer"">the existing centrality sourcecode</a>.</p>
"
1825906,209563,2009-12-01T12:29:44Z,1819418,5,TRUE,"<p>The problem is that all the core functions use INTs instead of LONGs for generating R objects. For example, your error message comes from array.c in /src/main</p>

<pre><code>if ((double)nr * (double)nc &gt; INT_MAX)
    error(_(""too many elements specified""));
</code></pre>

<p>where nr and nc are integers generated before, standing for the number of rows and columns of your matrix:</p>

<pre><code>nr = asInteger(snr);
nc = asInteger(snc);
</code></pre>

<p>So, to cut it short, everything in the source code should be changed to LONG, possibly not only in array.c but in most core functions, and that would require some rewriting. Sorry for not being more helpful, but i guess this is the only solution. Alternatively, you may wait for R 3.x next year, and hopefully they will implement this...</p>
"
1825969,143305,2009-12-01T12:43:21Z,1823681,2,FALSE,"<p>As Jonathan said in the comments, you need proper dimensions:</p>

<pre><code>R&gt; X &lt;- matrix(1:4, ncol=2)
R&gt; t(X) %*% X 
     [,1] [,2]
[1,]    5   11
[2,]   11   25
R&gt; 
</code></pre>

<p>But you also should use the proper tools so maybe look at the <code>loglin</code> function in the <code>stats</code> package, and/or the <code>loglm</code> function in the <code>MASS</code> package. Both will be installed by default with your R installation. </p>
"
1826617,143305,2009-12-01T14:41:39Z,1826519,3,FALSE,"<p>Yes to your second and third questions -- that's what you need to do as you cannot have multiple 'lvalues' on the left of an assignment.</p>
"
1826621,163053,2009-12-01T14:42:15Z,1826519,10,FALSE,"<p>There's no right answer to this question.  I really depends on what you're doing with the data.  In the simple example above, I would strongly suggest:</p>

<ol>
<li>Keep things as simple as possible.</li>
<li>Wherever possible, it's a best practice to keep your functions vectorized.  That provides the greatest amount of flexibility and speed in the long run.</li>
</ol>

<p>Is it important that the values 1 and 2 above have names?  In other words, why is it important in this example that 1 and 2 be named a and b, rather than just r[1] and r[2]?  One important thing to understand in this context is that a and b are <em>also</em> both vectors of length 1.  So you're not really changing anything in the process of making that assignment, other than having 2 new vectors that don't need subscripts to be referenced:</p>

<pre><code>&gt; r &lt;- c(1,2)
&gt; a &lt;- r[1]
&gt; b &lt;- r[2]
&gt; class(r)
[1] ""numeric""
&gt; class(a)
[1] ""numeric""
&gt; a
[1] 1
&gt; a[1]
[1] 1
</code></pre>

<p>You can also assign the names to the original vector if you would rather reference the letter than the index:</p>

<pre><code>&gt; names(r) &lt;- c(""a"",""b"")
&gt; names(r)
[1] ""a"" ""b""
&gt; r[""a""]
a 
1 
</code></pre>

<p><em>[Edit]</em> Given that you will be applying min and max to each vector separately, I would suggest either using a matrix (if a and b will be the same length and the same data type) or data frame (if a and b will be the same length but can be different data types) or else use a list like in your last example (if they can be of differing lengths and data types).  </p>

<pre><code>&gt; r &lt;- data.frame(a=1:4, b=5:8)
&gt; r
  a b
1 1 5
2 2 6
3 3 7
4 4 8
&gt; min(r$a)
[1] 1
&gt; max(r$b)
[1] 8
</code></pre>
"
1826755,209467,2009-12-01T15:01:58Z,1826519,30,FALSE,"<p>Usually I wrap the output into a list, which is very flexible (you can have any combination of numbers, strings, vectors, matrices, arrays, lists, objects int he output)</p>

<p>so like:</p>

<pre><code>func2&lt;-function(input) {
   a&lt;-input+1
   b&lt;-input+2
   output&lt;-list(a,b)
   return(output)
}

output&lt;-func2(5)

for (i in output) {
   print(i)
}

[1] 6
[1] 7
</code></pre>
"
1828862,163053,2009-12-01T20:56:12Z,1828742,127,TRUE,"<p>Not sure if this is what you mean, but try setting <code>las=1</code>.  Here's an example:</p>

<pre><code>require(grDevices)
tN &lt;- table(Ni &lt;- stats::rpois(100, lambda=5))
r &lt;- barplot(tN, col=rainbow(20), las=1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/szTlv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/szTlv.png"" alt=""output""></a></p>

<p>That represents the style of axis labels. (0=parallel, 1=all horizontal, 2=all perpendicular to axis, 3=all vertical)</p>
"
1828867,172261,2009-12-01T20:56:27Z,1828742,75,FALSE,"<p>Use <code>par(las=1)</code>.</p>

<p>See <code>?par</code>:</p>

<pre><code>las
numeric in {0,1,2,3}; the style of axis labels.
0: always parallel to the axis [default],
1: always horizontal,
2: always perpendicular to the axis,
3: always vertical.
</code></pre>
"
1829651,83761,2009-12-01T23:21:19Z,1826519,60,FALSE,"<p>I somehow stumbled on this clever hack on the internet ... I'm not sure if it's nasty or beautiful, but it lets you create a ""magical"" operator that allows you to unpack multiple return values into their own variable. The <code>:=</code> function <a href=""http://code.google.com/p/miscell/source/browse/rvalues/rvalues.r"" rel=""noreferrer"">is defined here</a>, and included below for posterity:</p>

<pre><code>':=' &lt;- function(lhs, rhs) {
  frame &lt;- parent.frame()
  lhs &lt;- as.list(substitute(lhs))
  if (length(lhs) &gt; 1)
    lhs &lt;- lhs[-1]
  if (length(lhs) == 1) {
    do.call(`=`, list(lhs[[1]], rhs), envir=frame)
    return(invisible(NULL)) 
  }
  if (is.function(rhs) || is(rhs, 'formula'))
    rhs &lt;- list(rhs)
  if (length(lhs) &gt; length(rhs))
    rhs &lt;- c(rhs, rep(list(NULL), length(lhs) - length(rhs)))
  for (i in 1:length(lhs))
    do.call(`=`, list(lhs[[i]], rhs[[i]]), envir=frame)
  return(invisible(NULL)) 
}
</code></pre>

<p>With that in hand, you can do what you're after:</p>

<pre><code>functionReturningTwoValues &lt;- function() {
  return(list(1, matrix(0, 2, 2)))
}
c(a, b) := functionReturningTwoValues()
a
#[1] 1
b
#     [,1] [,2]
# [1,]    0    0
# [2,]    0    0
</code></pre>

<p>I don't know how I feel about that. Perhaps you might find it helpful in your interactive workspace. Using it to build (re-)usable libraries (for mass consumption) might not be the best idea, but I guess that's up to you.</p>

<p>... you know what they say about responsibility and power ...</p>
"
1830738,65148,2009-12-02T04:57:33Z,1810662,5,FALSE,"<p>A few ways.  The most convenient is layout.  <code>?layout</code> for more details</p>

<pre><code>r&lt;-lm(y~x1+x2)
layout(matrix(1:4,2,2))
plot(r)
</code></pre>

<p>will produce a window with four plots.</p>
"
1830888,222656,2009-12-02T05:45:59Z,1191689,2,FALSE,"<p>in python, try PyMC.  There is an example of multilevel modeling with it here:  <a href=""http://groups.google.com/group/pymc/browse_thread/thread/c6ce37a80edf7f85/1bfd9138c8db891d"" rel=""nofollow noreferrer"">http://groups.google.com/group/pymc/browse_thread/thread/c6ce37a80edf7f85/1bfd9138c8db891d</a></p>
"
1831937,172261,2009-12-02T10:13:19Z,1831245,6,TRUE,"<p>I'm not sure if this is what you mean:</p>

<pre><code>p &lt;- ggplot(emp, aes(x=s, y=t)) +
     geom_tile(aes(fill=log(V1))) +
     theme_bw() +
     scale_fill_gradient(low=""white"", high=""black"") +
     theme(axis.text.x=element_text(angle=-90, hjust=0, size=5),
           axis.text.y=element_text(vjust=0, size=5),
           panel.grid.minor=element_blank(),
           panel.grid.major=element_blank()) +
     geom_vline(xintercept=which(levels(emp$s) %in% c(""142"", ""30"")) + c(0.25, -0.25))
print(p)
</code></pre>

<p><img src=""https://i.stack.imgur.com/qE3Wc.png"" alt=""ggplot2 output""></p>
"
1832314,222853,2009-12-02T11:22:36Z,1191689,2,FALSE,"<p>I apply hierarchical Bayes models in R in combination with JAGS (Linux) or sometimes WinBUGS (Windows, or Wine).  Check out the book of Andrew Gelman, as referred to above.</p>
"
1833211,163053,2009-12-02T14:24:50Z,1829429,36,TRUE,"<p>Just looked at the help for <code>which()</code> after posting this and found the answer: the arr.ind parameter.</p>

<pre><code>which(a==23, arr.ind=TRUE)
     row col
[1,]   3   5
</code></pre>
"
1833996,134830,2009-12-02T16:18:27Z,1832761,3,FALSE,"<p>Since <code>seriesNodes</code> is a list of nodes, there is no easy way to avoid the implicit looping. Simple operations like getting the length are not computationally intensive, so I wouldn't lose any sleep over not being able to vectorise.</p>

<p>Note that you can use <code>sapply(seriesNodes, length)</code>, instead of <code>mapply</code>, since there is only one argument to the <code>length</code> function.</p>

<p>The ""proper R way"" to do things is to use <code>(s|m)apply</code> calls to extract vectors of useful bits of data, then analyse those in the usual manner.</p>

<p>Finally, if you really are desperate to vectorise counting events, use <code>names(unlist(seriesNodes))</code> and then count the occurances of <code>""series.children.event.name""</code> in between each occurance of <code>""series.name""</code>.  This is undoubtedly uglier, and possibly slower than the <code>sapply</code> call.</p>
"
1834212,143305,2009-12-02T16:45:25Z,1833622,8,TRUE,"<p>Let me focus on</p>

<blockquote>
  <p>What is bugging me is that I can't
  work out why this code works—I was
  amazed that it doesn't just throw an
  error.</p>
</blockquote>

<p>Why do you think so?   What happens is that we have</p>

<ol>
<li><code>y</code> being assigned an expression</li>
<li>that expression being an if <code>if() ...</code></li>
<li>leading to either a <code>TRUE</code> or <code>FALSE</code> in the test</li>
<li>leading to either one of the two branches being entered</li>
<li>leading to the respective code being evaluated </li>
<li>leading to its final value being the value of the right-hand-side</li>
<li>leading to this value being assigned to <code>y</code></li>
</ol>

<p>Seems logical to me.  </p>
"
1834634,169947,2009-12-02T17:49:53Z,1833622,2,FALSE,"<p>It's not really very deep - many languages use the ""val = x ? y : z"" construct for this.  In R, that's just folded into an if/else construct, so you write ""val = if(x) y else z"" instead.</p>
"
1834915,163053,2009-12-02T18:36:25Z,1834833,5,FALSE,"<p>I could be wrong, but having used rJava fairly extensively, I'm quite sure that nothing like that exists.  </p>

<p>While implementing my own packages with rJava, I went through the source code for many of the packages that reverse depend on it (you can see these <a href=""http://cran.r-project.org/web/packages/rJava/index.html"" rel=""nofollow noreferrer"">at the bottom of the CRAN page</a>).  None of these had anything that looked like automated code.</p>

<p>Incidentally, for anyone unfamiliar with it, I strongly recommend reading the vignette and looking at the source code for <a href=""http://cran.r-project.org/web/packages/helloJavaWorld/index.html"" rel=""nofollow noreferrer"">the helloJavaWorld package</a>; it was specifically created to help with this process.</p>

<p>What you're suggesting would be a very welcome improvement.  I'm happy to collaborate on creating it if anyone has interest.</p>

<p><em>Edit</em></p>

<p><a href=""https://stackoverflow.com/users/499163/romain"">Romain Francois</a> just pointed out that this feature now exists in rJava (actually, as of <a href=""http://www.rforge.net/rJava/news.html"" rel=""nofollow noreferrer"">October 2009</a>).  Here is an example:</p>

<pre><code>&gt; require( rJava ) ; .jinit() 
&gt; attach( javaImport( ""java.lang"" ) ) 
&gt; Math$PI 
[1] 3.141593 
&gt; Math$abs( -3 ) 
[1] 3
</code></pre>

<p>Once you import a class, you also get auto-completion by, for instance, Math$[tab].  </p>
"
1835099,172261,2009-12-02T19:04:51Z,1833622,4,FALSE,"<p>Control structures such as <code>if(....)  ... else ...</code> are equivalent to function calls with the relevant (lazy-evaluated) subexpressions as the argument:</p>

<pre><code>`if`(TRUE, ""yes"", ""no"")
</code></pre>

<p>Basically, <code>if</code> is a <em>special primitive</em> function:</p>

<pre><code>R&gt; sapply(ls(""package:base"", all=TRUE), function(x) is.primitive(get(x)))[""if""]
  if 
TRUE 
R&gt; sapply(ls(""package:base"", all=TRUE), function(x) typeof(get(x)))[""if""]
       if 
""special"" 
</code></pre>

<p>The relevant section (Section 3.2.1) in the <a href=""http://cran.r-project.org/doc/manuals/R-lang.html"" rel=""nofollow noreferrer"">R Language Definition manual</a> says:</p>

<p>Because if/else statements are the same as other statements you can assign the value of them. The two examples below are equivalent.</p>

<pre><code>R&gt; if( any(x &lt;= 0) ) y &lt;- log(1+x) else y &lt;- log(x)
R&gt; y &lt;- if( any(x &lt;= 0) ) log(1+x) else log(x)
</code></pre>
"
1837105,135870,2009-12-03T01:31:44Z,1832761,3,TRUE,"<p>You could also use <code>xpathApply</code> or <code>xpathSApply</code>-- these functions extract node sets using an XPath specification and then execute a function each set.  Both of these functions are provided by the <code>XML</code> package.  In order to use these functions, the XML document must be parsed using <code>xmlInternalTreeParse</code> or with the <code>useInternalNodes</code> option of <code>xmlTreeParse</code> set to be true:</p>

<pre><code>require( XML )

countEvents &lt;- function( series ){

  events &lt;- xmlElementsByTagName( series, 'event' )
  return( length( events ) ) 

}

doc &lt;- xmlTreeParse( ""sample.xml"", useInternalNodes = T )

xpathSApply( doc, '/TimeSeries/series', countEvents )
[1] 6 9 5
</code></pre>

<p>I don't know if it is any ""faster"", but the code is definitely cleaner and very explicit to anyone who knows the XPath syntax and how an <code>apply</code> function operates.</p>
"
1838006,163053,2009-12-03T06:16:58Z,1837968,39,FALSE,"<p>Yes.  For vectors you can simply use the <code>%in%</code> operator or <code>is.element()</code> function.</p>

<pre><code>&gt; x[!(x %in% y)]
1
</code></pre>

<p>For a matrix, there are many difference approaches.  <code>merge()</code> is probably the most straight forward.  I suggest <a href=""https://stackoverflow.com/questions/1299871/how-to-join-data-frames-in-r-inner-outer-left-right"">looking at this question for that scenario</a>.</p>
"
1838152,203786,2009-12-03T06:53:26Z,1837968,78,TRUE,"<p>you can use the setdiff() (set difference) function:</p>

<pre><code>&gt; setdiff(x, y)
[1] 1
</code></pre>
"
1838603,168747,2009-12-03T08:55:14Z,1833622,3,FALSE,"<p>About second part of your question - <code>{}</code> is used to group expressions, it works diffrent than in a function definition. As you can read in <code>help(""Paren"")</code>:</p>

<blockquote>
  <p>For <code>‘{’</code>, the result of the last expression evaluated.</p>
</blockquote>

<p>But all expressions are evaluated in the current environment:</p>

<pre><code>y &lt;- if (TRUE) {
    print(paste(""Current frame:"",sys.nframe()))
    y &lt;- 3
    z &lt;- 5
    4
}
# [1] ""Current frame: 0""
# result:
y # [1] 4
z # [1] 5

# compare to use function:
v &lt;- if(TRUE) (function(){      
    print(paste(""Current frame:"",sys.nframe()))
    v &lt;- 3
    w &lt;- 5
    4
})()
# [1] ""Current frame: 1""
v # [1] 4
w # Error: object 'w' not found
</code></pre>

<p>Conclusion is <strong>blocks don't behave like functions</strong>.</p>

<p>edit:
If you want use blocks as function you could use <code>local</code>:</p>

<pre><code>a &lt;- if (TRUE) local({
    print(paste(""Current frame:"",sys.nframe()))
    a &lt;- 3
    b &lt;- 5
    4
})
# [1] ""Current frame: 6""
a # [1] 4
b # Error: object 'b' not found
</code></pre>

<p>Then you could use <code>return</code> too:</p>

<pre><code>a &lt;- if (TRUE) local({
    print(paste(""Current frame:"",sys.nframe()))
    a &lt;- 3
    b &lt;- 5
    return(7)
    4
})
# [1] ""Current frame: 6""
a # [1] 7
b # Error: object 'b' not found
</code></pre>
"
1840043,163053,2009-12-03T14:05:32Z,1840024,1,TRUE,"<p>Here's one efficient solution without loops:</p>

<pre><code>library(""zoo"")
S &lt;- c(50, 100, 150, 180, 210, 200, 190, 182, 175, 185, 195, 205)

thresholdOnOff &lt;- function(x, low, high, initial.value=FALSE) {
    require(""zoo"")
    s &lt;- rep(NA, length(x))
    s[1] &lt;- initial.value
    s[x &gt; high] &lt;- TRUE
    s[x &lt; low] &lt;- FALSE
    return(na.locf(s))
}

thresholdOnOff(S, 180, 200)
</code></pre>

<p>Alternatively, you can use ifelse in one line to solve this, but that will be much slower if you have big data:</p>

<pre><code>na.locf(ifelse(S &gt; 200, TRUE, ifelse(S &lt; 180, FALSE, NA)))
</code></pre>
"
1841448,16632,2009-12-03T17:15:47Z,1840024,1,FALSE,"<p>Stick with a for loop.  If the values are not independent then there's no advantage in using apply.  </p>

<p>Unless you have a very good reason you should avoid side-effects in functions.</p>
"
1843328,163053,2009-12-03T22:12:13Z,1842736,2,TRUE,"<p>I found <a href=""http://n4.nabble.com/converting-a-table-to-a-dataframe-or-a-matrix-td856133.html#a856133"" rel=""nofollow noreferrer"">2 solutions on R-Help</a>:</p>

<pre><code>head(as.table(ftable.result), Inf)
</code></pre>

<p>Or </p>

<pre><code>t &lt;- as.table(ftable.result)
class(t) &lt;- ""matrix""
</code></pre>
"
1844702,180892,2009-12-04T03:43:36Z,1837968,12,FALSE,"<p>The help file in R for <a href=""http://stat.ethz.ch/R-manual/R-patched/library/base/html/sets.html"" rel=""noreferrer"">setdiff, union, intersect, setequal, and is.element</a> provides information on the standard set functions in R. </p>
"
1844868,143305,2009-12-04T04:29:35Z,1844829,3,FALSE,"<p>Your best bet may be the XML package -- see for example this <a href=""https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package"">previous question</a>.</p>
"
1847346,163053,2009-12-04T14:38:33Z,1844829,21,TRUE,"<p>Not really sure how you want to process that page, because it's really messy.  As we <a href=""https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags"">re-learned in this famous stackoverflow question</a>, it's not a good idea to do regex on html, so you will definitely want to parse this with the XML package.  </p>

<p>Here's an example to get you started:</p>

<pre><code>require(RCurl)
require(XML)
webpage &lt;- getURL(""http://www.haaretz.com/"")
webpage &lt;- readLines(tc &lt;- textConnection(webpage)); close(tc)
pagetree &lt;- htmlTreeParse(webpage, error=function(...){}, useInternalNodes = TRUE)
# parse the tree by tables
x &lt;- xpathSApply(pagetree, ""//*/table"", xmlValue)  
# do some clean up with regular expressions
x &lt;- unlist(strsplit(x, ""\n""))
x &lt;- gsub(""\t"","""",x)
x &lt;- sub(""^[[:space:]]*(.*?)[[:space:]]*$"", ""\\1"", x, perl=TRUE)
x &lt;- x[!(x %in% c("""", ""|""))]
</code></pre>

<p>This results in a character vector of mostly just webpage text (along with some javascript):</p>

<pre><code>&gt; head(x)
[1] ""Subscribe to Print Edition""              ""Fri., December 04, 2009 Kislev 17, 5770"" ""Israel Time:Â 16:48Â (EST+7)""           
[4] ""Â Â Make Haaretz your homepage""          ""/*check the search form*/""               ""function chkSearch()"" 
</code></pre>
"
1847446,170792,2009-12-04T14:54:26Z,1837968,9,FALSE,"<pre><code>x[is.na(match(x,y))]
</code></pre>
"
1848004,162832,2009-12-04T16:14:19Z,1844829,2,FALSE,"<p>I know you asked for R. But maybe python+beautifullsoup is the way forward here? Then do your analysis with R you have scraped the screen with beautifullsoup?</p>
"
1848373,3201,2009-12-04T17:10:35Z,1848331,6,FALSE,"<p>I've used the technique described here: <a href=""http://learnr.wordpress.com/2009/10/06/export-data-frames-to-multi-worksheet-excel-file/"" rel=""nofollow noreferrer"">Export Data Frames To Multi-worksheet Excel File</a></p>
"
1848531,143305,2009-12-04T17:40:58Z,1848331,3,FALSE,"<p>The <a href=""http://cran.r-project.org/doc/manuals/R-data.html"" rel=""nofollow noreferrer"">R Data Import/Export manual</a> should be considered the best source of advice for these questions.</p>

<p>For <em>reading</em> you can indeed use the <a href=""http://cran.r-project.org/web/packages/RODBC/index.html"" rel=""nofollow noreferrer"">RODBC</a> package.  An easier solutoion may be <code>read.xls()</code> from the <a href=""http://cran.r-project.org/web/packages/gdata/index.html"" rel=""nofollow noreferrer"">gdata</a></p>

<p>For <em>writing</em> you can use one of the wrapper packages such as <a href=""http://cran.r-project.org/web/packages/WriteXLS/index.html"" rel=""nofollow noreferrer"">WriteXLS</a> which wraps around Perl libraries that know how to write in the proprietary and not formally documented xls format.</p>

<p>In general, xlsx will <em>not</em> be a solution as this format is newer, once again proprietary and not documented.  For that reason there are even fewer tools coping with this.</p>
"
1849388,NA,2009-12-04T20:14:56Z,1395528,119,TRUE,"<p>…or a shorter try:</p>

<pre><code>library(XML)
library(RCurl)
library(rlist)
theurl &lt;- getURL(""https://en.wikipedia.org/wiki/Brazil_national_football_team"",.opts = list(ssl.verifypeer = FALSE) )
tables &lt;- readHTMLTable(theurl)
tables &lt;- list.clean(tables, fun = is.null, recursive = FALSE)
n.rows &lt;- unlist(lapply(tables, function(t) dim(t)[1]))
</code></pre>

<p>the picked table is the longest one on the page</p>

<pre><code>tables[[which.max(n.rows)]]
</code></pre>
"
1852776,225468,2009-12-05T17:13:56Z,1848331,0,FALSE,"<p>There is a package that may help with dealing with excel 2007, but I haven't tried it. </p>

<p><a href=""http://www.omegahat.org/RExcelXML/"" rel=""nofollow noreferrer"">http://www.omegahat.org/RExcelXML/</a> </p>
"
1853866,172261,2009-12-05T23:35:09Z,1853703,16,TRUE,"<p>You probably want <a href=""http://had.co.nz/ggplot2/stat_function.html"" rel=""noreferrer""><code>stat_function</code></a>:</p>

<pre><code>library(""ggplot2"")
eq &lt;- function(x) {x*x}
tmp &lt;- data.frame(x=1:50, y=eq(1:50))

# Make plot object
p &lt;- qplot(x, y, data=tmp, xlab=""X-axis"", ylab=""Y-axis"")
c &lt;- stat_function(fun=eq)
print(p + c)
</code></pre>

<p>and if you really want to use <code>curve()</code>, i.e., the computed x and y coordinates:</p>

<pre><code>qplot(x, y, data=as.data.frame(curve(eq)), geom=""line"")
</code></pre>
"
1857885,225468,2009-12-07T04:54:11Z,1487320,1,FALSE,"<p>Most blogging software has an XML-RPC interface. So the easiest way to blog from R, could be by using this package: <a href=""http://www.omegahat.org/XMLRPC/"" rel=""nofollow noreferrer"">http://www.omegahat.org/XMLRPC/</a> and RCurl which can also be found on that site. </p>

<p>This would be by far the easiest way to go. If you google XML-RPC and Wordpress you can find code written for php, but it could help for writing the R code as well.</p>
"
1858365,175422,2009-12-07T07:19:58Z,1858280,0,FALSE,"<p>Update Tablename set column='1' where column='NA'</p>
"
1858562,158065,2009-12-07T08:13:04Z,1858280,4,FALSE,"<pre><code>x[is.na(x)] &lt;- 1
</code></pre>
"
1858776,160314,2009-12-07T09:11:19Z,1858280,5,TRUE,"<p>Jonathan has the right answer for a vector, which you can apply to column a in data frame dat using:</p>

<pre><code>&gt; dat&lt;-data.frame(a=c(11,2,11,NA),b=c(1,1,1,1))
&gt; dat$a[is.na(dat$a)] &lt;- 1
</code></pre>

<p>For completeness using Deducer's 'Recode Variables' dialog, which can do much more complicated recodings, produces the following code.</p>

<pre><code>&gt; library(Deducer)
&gt; dat[c(""a"")] &lt;- recode.variables(dat[c(""a"")] , ""NA -&gt; 1;"")
</code></pre>
"
1861173,143377,2009-12-07T16:38:14Z,1860913,1,FALSE,"<p>A useful trick is to combine the id variables into a character vector and then do the reshape.</p>

<pre><code>tbl$NEWID &lt;- with(tbl, paste(ID, DATE1, DATE2, sep="";""))
tbl2 &lt;- recast(tbl2, NEWID ~ VALTYPE, measure.var=""VALUE"")
</code></pre>

<p>It's about 40% faster in a problem of similar size in my intel core2 duo 2.2ghz macbook.</p>
"
1861226,143377,2009-12-07T16:45:19Z,1861160,2,FALSE,"<p>data frames are lists. Suppose the distance between time stamps is in the vector ""x"" in list/data.frame y. you could do <code>sort(-table(y[[""x""]]))[1]</code> to get the mode.</p>
"
1863596,135944,2009-12-07T23:19:03Z,1860913,1,FALSE,"<p>What about doing this in a non-R-like manner? I assume you have a TYPE1 and a TYPE2 row for each value of ID,DATE1,DATE2? Then sort the dataframe by those variables, and write a big for loop. You can repeatedly do rbind() operations to build the table, or you could try to pre-allocate the table (maybe) and just assign the VALUE.TYPE1 and VALUE.TYPE2 slots with [&lt;-, which should do the assignment in-place. </p>

<p>(Note that if you're using rbind(), I believe that it's inefficient if you have any factor variables, so make sure everything is a character instead!)</p>
"
1863655,216064,2009-12-07T23:35:03Z,1860913,1,FALSE,"<p>Maybe you could use the cat() function?</p>
"
1863799,163053,2009-12-08T00:23:18Z,1861160,2,FALSE,"<p>The best way to approach this is probably to use an irregular time series object (see <a href=""http://cran.r-project.org/web/views/TimeSeries.html"" rel=""nofollow noreferrer"">the time series view on CRAN</a>).  You have several good options (e.g. timeSeries, its, fts, xts), but the most popular of these is <a href=""http://cran.r-project.org/web/packages/zoo/index.html"" rel=""nofollow noreferrer"">the <b>zoo</b> package</a>.  You can create a time series like so:</p>

<pre><code>library(zoo)
x.Date &lt;- as.Date(""2003-02-01"") + c(1, 3, 7, 9, 14) - 1
x &lt;- zoo(rnorm(5), x.Date)
</code></pre>

<p>Then, to see the difference in time between each event, you can just use the diff function to create a <code>difftime</code> object:</p>

<pre><code>&gt; diff(index(x))
Time differences in days
[1] 2 4 2 5
</code></pre>

<p>You can analyze these time difference just like you would any other variable, for instance:</p>

<pre><code>&gt; summary(diff(index(x)))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   2.00    2.00    3.00    3.25    4.25    5.00 
</code></pre>

<p>Similarly, to find the most common time difference, you can use any other standard approach such as <code>table()</code>:</p>

<pre><code>&gt; table(diff(index(x)))
2 4 5 
2 1 1 
</code></pre>
"
1867538,83761,2009-12-08T15:02:18Z,1867139,1,FALSE,"<p>I never would have expected that to happen, but trying a small test case produces the same results you're giving.</p>

<p>Since the result of <code>df[1,]</code> is itself a <code>data.frame</code>, one fix I thought to try was to use <code>unlist</code> -- seems to work:</p>

<pre><code>&gt; df &lt;- data.frame(a=LETTERS[1:10], b=LETTERS[11:20], c=LETTERS[5:14])
&gt; df[1,]
  a b c
1 A K E
&gt; as.character(df[1,])
[1] ""1"" ""1"" ""1""
&gt; as.character(unlist(df[2,]))
[1] ""B"" ""L"" ""F""
</code></pre>

<p>I think turning the <code>data.frame</code> into a <code>matrix</code> first would also get around this:</p>

<pre><code>m &lt;- as.matrix(df)
&gt; as.character(m[2,])
[1] ""B"" ""L"" ""F""
</code></pre>

<p>To avoid issues with factors in your <code>data.frame</code> you might want to set <code>stringsAsFactors=TRUE</code> when reading in your data from the text file, eg:</p>

<pre><code>clusters &lt;- read.delim(""test"", sep=""\t"", fill=TRUE, header=FALSE,
                       stringsAsFactors=FALSE)
</code></pre>

<p>And, after all that, the unexpected behavior seems to come from the fact that the original affy probes in your data.frame are treated as factors. So, doing the <code>stringsAsFactors=FALSE</code> thing will side-step the fanfare:</p>

<pre><code>df &lt;- data.frame(a=LETTERS[1:10], b=LETTERS[11:20],
                 c=LETTERS[5:14], stringsAsFactors=FALSE)
&gt; as.character(df[1,])
[1] ""A"" ""K"" ""E""
</code></pre>
"
1867665,168747,2009-12-08T15:22:54Z,1867139,6,TRUE,"<p>By default character columns are converted to factors. You can avoid this by setting <code>as.is=TRUE</code> argument:</p>

<pre><code>clusters &lt;- read.delim(""test"", sep=""\t"", fill=TRUE, header=FALSE, as.is=TRUE)
</code></pre>

<p>If you only pass arguments from text file to character vector you could do something like:</p>

<pre><code>x &lt;- readLines(""test"")
xx &lt;- strsplit(x,split=""\t"")
xx[[1]] # xx is a list
# [1] ""248585_at""      ""250887_at""      ""245638_s_at""    ""AFFX-BioC-5_at""
</code></pre>
"
1868737,190277,2009-12-08T17:59:48Z,1857747,2,FALSE,"<p>Did you try it?</p>

<pre><code>&gt; d = data.frame(t=1:4,V=c(0.15,0.23,0.67,0.85))
&gt; Vt = function(t,k,q) {((1-exp(k*t))/(k*t))*q^2/(2*k)}
&gt; nls(V~Vt(t,k,q),data=d,start=list(k=0.1,q=0.1),trace=TRUE)
1.483693 :  0.1 0.1 
1.099885 :  -7.358501 -3.593847 
Error in numericDeriv(form[[3L]], names(ind), env) : 
  Missing value or an infinity produced when evaluating the model
</code></pre>

<p>Doesn't actually work, of course, but that's because I haven't sat down
and figured out sensible starting values etc..  (If you're really going
to try to fit a 2-parameter nonlinear model to 4 data points, you're
going to have lots of hassles. I hope this is just a subsample of 
your data ...)</p>

<p>In this case of course you could also just write out the formula within
nls()</p>
"
1869492,158065,2009-12-08T20:06:18Z,1867647,4,FALSE,"<p>Yes, just separate your assignments by commas:</p>

<pre><code>&gt; head(swiss)
             Fertility Agriculture Examination Education Catholic
Courtelary        80.2        17.0          15        12     9.96
Delemont          83.1        45.1           6         9    84.84
Franches-Mnt      92.5        39.7           5         5    93.40
Moutier           85.8        36.5          12         7    33.77
Neuveville        76.9        43.5          17        15     5.16
Porrentruy        76.1        35.3           9         7    90.57
             Infant.Mortality
Courtelary               22.2
Delemont                 22.2
Franches-Mnt             20.2
Moutier                  20.3
Neuveville               20.6
Porrentruy               26.6

&gt; head(transform(swiss, Agriculture = Agriculture ^ 2, Catholic = -Catholic))
             Fertility Agriculture Examination Education Catholic
Courtelary        80.2      289.00          15        12    -9.96
Delemont          83.1     2034.01           6         9   -84.84
Franches-Mnt      92.5     1576.09           5         5   -93.40
Moutier           85.8     1332.25          12         7   -33.77
Neuveville        76.9     1892.25          17        15    -5.16
Porrentruy        76.1     1246.09           9         7   -90.57
             Infant.Mortality
Courtelary               22.2
Delemont                 22.2
Franches-Mnt             20.2
Moutier                  20.3
Neuveville               20.6
Porrentruy               26.6
</code></pre>
"
1871605,125507,2009-12-09T04:38:07Z,1441717,2,FALSE,"<p>In Python, you can use shapefiles from the US census along with the <code>basemap</code> package.  Here is an example of <a href=""http://matplotlib.svn.sourceforge.net/viewvc/matplotlib/trunk/toolkits/basemap/examples/fillstates.py?view=markup"" rel=""nofollow noreferrer"">filling in states</a> according to population.</p>
"
1872303,78912,2009-12-09T07:57:22Z,1861160,0,TRUE,"<p>I think I would settle with this one (works if the most common step really occurs more often than in 50% of the cases).</p>

<pre><code>mostCommonStep &lt;- function(L) {
  ## returns the value of the most common difference between
  ## subsequent elements.

  ## takes into account only forward steps, all negative steps are
  ## discarded.  works with list, data.frame, matrix.
  L &lt;- diff(unlist(sapply(as.list(L), as.numeric)))
  as.numeric(quantile(L[L&gt;0], 0.5))
}
</code></pre>
"
1873164,227898,2009-12-09T11:07:01Z,1872982,0,FALSE,"<p>I've never heard of such a function.  Perhaps you should quickly write one yourself?</p>

<pre><code>toString &lt;- function(a){
    output &lt;- ""c("";
    for(i in 1:(length(a)-1)){
        output &lt;- paste(output, a[i], "","", sep="""")
    }
    output &lt;- paste(output, a[length(a)], "")\n"", sep="""")
    cat(output)
}
</code></pre>
"
1873351,172261,2009-12-09T11:40:26Z,1872982,15,TRUE,"<p>You can use <code>deparse</code>:</p>

<pre><code>R&gt; x &lt;- c(1, 2, 3)
R&gt; deparse(x)
[1] ""c(1, 2, 3)""
R&gt; class(deparse(x))
[1] ""character""
</code></pre>
"
1873381,227930,2009-12-09T11:45:56Z,1872982,12,FALSE,"<p>using <code>dput</code>:</p>

<pre><code>a &lt;- c(1, 2, 3);
dput(a)
</code></pre>
"
1874563,168747,2009-12-09T15:17:21Z,1874443,53,TRUE,"<p>There is nice function <code>count.fields</code> (see help) which counts number of column per row:</p>

<pre><code>count.fields(""test"", sep = ""\t"")
#[1] 1 2 3 4 5 6 7 8
</code></pre>

<p>So, using your second solution:</p>

<pre><code>no_col &lt;- max(count.fields(""test"", sep = ""\t""))
data &lt;- read.table(""test"",sep=""\t"",fill=TRUE,col.names=1:no_col)
data
#   X1 X2 X3 X4 X5 X6 X7 X8
# 1  1 NA NA NA NA NA NA NA
# 2  1  2 NA NA NA NA NA NA
# 3  1  2  3 NA NA NA NA NA
# 4  1  2  3  4 NA NA NA NA
# 5  1  2  3  4  5 NA NA NA
# 6  1  2  3  4  5  6 NA NA
# 7  1  2  3  4  5  6  7 NA
# 8  1  2  3  4  5  6  7  8
</code></pre>
"
1874833,163053,2009-12-09T15:54:46Z,1874443,5,FALSE,"<p>Using <code>count.fields</code> is definitely the right approach for this, but just for completeness:</p>

<p>Another option is to bring in all the raw text and parse it within R:</p>

<pre><code>x &lt;- readLines(textConnection(
""1\t
1\t2
1\t2\t3
1\t2\t3\t4
1\t2\t3\t4\t5
1\t2\t3\t4\t5\t6""))
x &lt;- strsplit(x,""\t"")
</code></pre>

<p>To combine a list of unequal length vectors, the easiest approach is to use the <code>rbind.fill</code> function from <code>plyr</code>:</p>

<pre><code>library(plyr)
# requires data.frames with column names
x &lt;- lapply(x,function(x) {x &lt;- as.data.frame(t(x)); colnames(x)=1:length(x); return(x)})
do.call(rbind.fill,x)
1    2    3    4    5    6
1 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;
2 1    2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;
3 1    2    3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;
4 1    2    3    4 &lt;NA&gt; &lt;NA&gt;
5 1    2    3    4    5 &lt;NA&gt;
6 1    2    3    4    5    6
</code></pre>
"
1875327,163053,2009-12-09T17:04:05Z,1875192,9,TRUE,"<p>To do this with the normal plot command, I would usually create one plot and then add more lines using the <code>lines()</code> function.</p>

<p>Otherwise you can use lattice or ggplot2.  Here's some data:</p>

<pre><code>df &lt;- data.frame(a = runif(10), b = runif(10), c = runif(10), x = 1:10)
</code></pre>

<p>You can use <code>xyplot()</code> from lattice:</p>

<pre><code>library(lattice)
xyplot(a + b + c ~ x, data = df, type = ""l"", auto.key=TRUE)
</code></pre>

<p>Or <code>geom_line()</code> in ggplot2:</p>

<pre><code>library(ggplot2)
ggplot(melt(df, id.vars=""x""), aes(x, value, colour = variable,
        group = variable)) + geom_line() + theme_bw()
</code></pre>

<p>Here's another example including points at each pair (from <a href=""http://learnr.wordpress.com/2009/07/02/ggplot2-version-of-figures-in-lattice-multivariate-data-visualization-with-r-part-4/"" rel=""noreferrer"">this post on the learnr blog</a>):</p>

<pre><code>library(lattice)
dotplot(VADeaths, type = ""o"", auto.key = list(lines = TRUE,
     space = ""right""), main = ""Death Rates in Virginia - 1940"",
     xlab = ""Rate (per 1000)"")
</code></pre>

<p>And the same plot using ggplot2:</p>

<pre><code>library(ggplot2)
p &lt;- ggplot(melt(VADeaths), aes(value, X1, colour = X2,
             group = X2))
p + geom_point() + geom_line() + xlab(""Rate (per 1000)"") +
         ylab("""") + opts(title = ""Death Rates in Virginia - 1940"")
</code></pre>
"
1875842,143305,2009-12-09T18:28:44Z,1875795,24,TRUE,"<p>You probably want to look at these packages:</p>

<ul>
<li><a href=""http://cran.r-project.org/web/packages/ff/index.html"" rel=""nofollow noreferrer"">ff</a> for 'flat-file' storage and very efficient retrieval (can do data.frames; different data types)</li>
<li><a href=""http://cran.r-project.org/web/packages/bigmemory/index.html"" rel=""nofollow noreferrer"">bigmemory</a> for out-of-R-memory but still in RAM (or file-backed) use (can only do matrices; same data type)</li>
<li><a href=""http://cran.r-project.org/web/packages/biglm/index.html"" rel=""nofollow noreferrer"">biglm</a> for out-of-memory model fitting with <code>lm()</code> and <code>glm()</code>-style models.</li>
</ul>

<p>and also see the <a href=""http://cran.r-project.org/web/views/HighPerformanceComputing.html"" rel=""nofollow noreferrer"">High-Performance Computing</a> task view.</p>
"
1876585,163053,2009-12-09T20:17:51Z,1876085,4,TRUE,"<p>A column <em>must be all of one data type</em>; you can't mix logical and numeric.  </p>

<p>Not sure how you would even do ""long"" analysis on multiple different data types because usually those are the same variables with different groupings.  If you need to, try converting your logical values to numeric first (with <code>as.numeric</code>).</p>

<p>While you're not using the <code>reshape</code> package, Hadley made this point in his discussion of the <code>melt()</code> function, which is performing the same task (see <a href=""http://www.jstatsoft.org/v21/i12/paper"" rel=""nofollow noreferrer"">this paper, for instance</a>):</p>

<blockquote>
  <p>In the current implementation [of melt], there is only one assumption that melt makes: all measured values must be of the same type, e.g., numeric, factor, date. We need this assumption because the molten data is stored in an R data frame, and <strong>the value column can be only one type</strong>. Most of the time this is not a problem as there are few cases where it makes sense to combine different types of variables in the cast output.</p>
</blockquote>

<p><em>Edit:</em></p>

<p>I think you may be trying to do two things at once.  Is this what you want?</p>

<pre><code>a &lt;- reshape(example.data[,-c(36:39)], direction = 'long', varying = c(29:32), v.names = c(""Math34""), times = 2006:2009, idvar = ""schoolnum"")
b &lt;- reshape(example.data[,-c(29:32)], direction = 'long', varying = c(36:39)-4, v.names = c(""TFCIn""), times = 2006:2009, idvar = ""schoolnum"")
c &lt;- merge(a,b)
</code></pre>
"
1880635,143305,2009-12-10T12:23:10Z,1879843,1,FALSE,"<p>Did you load the <code>RUnit</code> package?</p>

<p>Your best bet is probably to look at a package containing existing code using <code>RUnit</code>. </p>
"
1880659,143305,2009-12-10T12:26:44Z,1879933,1,TRUE,"<p>Just add a file <code>cleanup</code> which removes them in your top-level directory. Also, you could build a tarball or zip archive first via <code>R CMD build</code> and the check this archive via <code>R CMD check</code> -- that should skip these filese as well. </p>

<p>Also, exactly how are you calling <code>R CMD check</code>, and what is your directory layout?  With R 2.10.0 on Linux, I just ran <code>touch pkg/man/foo.Rd~</code> for one of my packages, and <code>R CMD check pkg</code> (where <code>pkg</code> is the top-level directory as common for source projects stored on R-Forge)
did not issue this warning you are seeing.  The file was not removed by <code>cleanup</code> as that currently purges only in <code>src</code>.</p>
"
1880691,163053,2009-12-10T12:32:39Z,1879843,4,TRUE,"<p>Where are you putting your unit tests?  You may not want to put them into the <code>R</code> directory.  A more standard approach is to put them under <code>inst\unitTests</code>.  Have a look at <a href=""http://wiki.r-project.org/doku.php?id=developers:runit"" rel=""nofollow noreferrer"">this R-wiki page regarding the configuration.</a></p>

<p>Alternatively, you can specify what files will be exported in your NAMESPACE, and by extension, what functions should and should not be documented.</p>

<p>Beyond that, ideally you should have your tests run when R CMD CHECK is called; that's part of the design.  In which case, you should create a test script to call your tests in a separate <code>tests</code> directory.  And you will need to load the RUnit package in that script (but you don't need to make it a dependency of your package).</p>

<p><em>Edit 1:</em></p>

<p>Regarding your failure because it can't find the checkEquals function: I would change you function to be like this:</p>

<pre><code>test.fillInTheBlanks &lt;- function() {
  require(RUnit)
  checkEquals(fillInTheBlanks(c(1, NA, NA, 2, 3, NA, 4)), c(1, 1, 1, 2, 3, 3, 4))
  checkEquals(fillInTheBlanks(c(1, 2, 3, 4)), c(1, 2, 3, 4))
  checkEquals(fillInTheBlanks(c(NA, NA, 2, 3, NA, 4)), c(2, 2, 2, 3, 3, 4))
}
</code></pre>

<p>That way the package is loaded when the function is called or it will inform the user that the package is required.</p>

<p><em>Edit 2:</em></p>

<p>From <a href=""http://cran.r-project.org/doc/manuals/R-exts.html#Package-subdirectories"" rel=""nofollow noreferrer"">""Writing R Extensions""</a>:</p>

<blockquote>
  <p>Note that all user-level objects in a package should be documented; if a package pkg contains user-level objects which are for “internal” use only, it should provide a file pkg-internal.Rd which documents all such objects, and clearly states that these are not meant to be called by the user. See e.g. the sources for package grid in the R distribution for an example. Note that packages which use internal objects extensively should hide those objects in a name space, when they do not need to be documented (see Package name spaces).</p>
</blockquote>

<p>You can use the pkg-internal.Rd file as one option, but if you intend on having many hidden objects, this is usually handled in the declarations in the NAMESPACE.</p>
"
1885330,153545,2009-12-11T01:43:27Z,1885264,0,FALSE,"<p>It looks like the 2D plot is a layout of a microelectronic circuit, albeit with some detail skipped, and the 3D plot is perhaps a thermal plot of the same circuit.</p>

<p>I don't know enough about R's capabilities, but I imagine it would be easier to generate the two plots separately with R from the same dataset which represents the layout information (but with and without the thermal data) and then combine them with a graphics manipulation program.</p>
"
1885338,70702,2009-12-11T01:46:14Z,1885264,2,FALSE,"<p>I will give a theoretical Idea,</p>

<p>In the same 3D plot, select a plane perpendicular to the 3D surface (just below the 3D-surface) and project all the values to it. Instead of 2D &amp; 3D plot, you will use only a 3D plot, which also plots your surface.</p>

<p>HTH</p>
"
1885577,2509,2009-12-11T03:11:21Z,1885264,0,FALSE,"<p>No help in R, but you can do something similar in <a href=""http://root.cern.ch/"" rel=""nofollow noreferrer"">ROOT</a> as seen in this image:</p>

<p><img src=""https://root.cern.ch/root/html524/MACRO_THistPainter_51_c2.gif"" alt=""histogram plotted with SURF3 option""></p>

<p>taken from the <a href=""http://root.cern.ch/root/html524/THistPainter.html"" rel=""nofollow noreferrer""><code>THistPainter</code> class documentation</a>.</p>

<p>The code is open source and could be examined if wanted for reimplementation.</p>
"
1886297,229255,2009-12-11T06:57:05Z,1886264,3,FALSE,"<p>Something like <code>b = filter(a, rep(1.0/5, 5), sides=1)</code> will do the job, although you will probably get zeros in the first few slots, instead of NaN.  R has a large library of built-in functions, and ""R style"" is to use those wherever possible.  Take a look at the documentation for the <code>filter</code> function.</p>
"
1886768,188595,2009-12-11T09:09:02Z,1886644,4,FALSE,"<p>Look at the R help for <code>package.skeleton</code>: this function</p>

<blockquote>
  <p>automates some of the setup for a new source package. It creates directories, saves functions, data, and R code files to appropriate places, and creates skeleton help files and a ‘Read-and-delete-me’ file describing further steps in packaging.</p>
</blockquote>

<p>The directory structure created by <code>package.skeleton</code> includes a <code>data</code> directory. If you put your data here it will be distributed with the package.</p>
"
1886891,172261,2009-12-11T09:35:21Z,1886644,12,TRUE,"<p>You can put your csv files in the <code>data</code> directory or in <code>inst/extdata</code>.
See the <em><a href=""http://cran.r-project.org/doc/manuals/R-exts.html#Data-in-packages"" rel=""noreferrer"">Writing R Extensions manual - Section 1.1.5 Data in packages</a></em>.</p>

<p>To import the data you can use, e.g.,</p>

<pre><code>R&gt; data(""achieve"", package=""flexclust"")
</code></pre>

<p>or</p>

<pre><code>R&gt; read.table(system.file(""data/achieve.txt"", package = ""flexclust""))
</code></pre>
"
1886982,229513,2009-12-11T09:53:22Z,1885264,0,FALSE,"<p>Maybe you should try to make an opengl texture out of your 2d picture and map it on a 3d polygon to be included in your scenegraph?</p>

<p>Don't really understand if you wish to do it with R specifically, so maybe diving in opengl is a too low level for you. In case you'd be ready for that, you may reuse a simple java library that simplify plotting 3d surface: <a href=""http://code.google.com/p/jzy3d"" rel=""nofollow noreferrer"">http://code.google.com/p/jzy3d</a></p>

<p>Hope that helps,
Martin</p>
"
1887107,172261,2009-12-11T10:17:06Z,1886808,4,TRUE,"<p>This should get you started:</p>

<pre><code># create some data
tmp &lt;- seq(as.POSIXct(""1890-03-01"", tz=""GMT""),
           as.POSIXct(""1920-03-01"", tz=""GMT""),
           by=""month"")
df &lt;- data.frame(date=tmp,
                 val=rnorm(length(tmp)))

# plot data
plot(df$date, df$val, xaxt=""n"")
tickpos &lt;- seq(as.POSIXct(""1890-01-01"", tz=""GMT""),
               as.POSIXct(""1920-01-01"", tz=""GMT""),
               by=""5 years"")
axis.POSIXct(side=1, at=tickpos)
</code></pre>

<p><a href=""http://img704.imageshack.us/img704/9341/axis.png"" rel=""nofollow noreferrer"">alt text http://img704.imageshack.us/img704/9341/axis.png</a></p>
"
1887380,172261,2009-12-11T11:15:18Z,1887195,1,TRUE,"<p>The object <code>b</code> has class <code>POSIXlt</code>. Arrays of <code>POSIXlt</code> dates always return a length of 9,
since the represent a named list of nine vectors:</p>

<pre><code>R&gt; class(b)
[1] ""POSIXt""  ""POSIXlt""

R&gt; unclass(b)
$sec
[1] 33 38 43 49 49 16 13 13
$min
[1]  0  0  0  0 58 53  4 16
$hour
[1] 18 18 18 18  1  1  8 16
$mday
[1] 31 31 31 31  1  1  1  1
$mon
[1] 6 6 6 6 7 7 7 7
$year
[1] 109 109 109 109 109 109 109 109
$wday
[1] 5 5 5 5 6 6 6 6
$yday
[1] 211 211 211 211 212 212 212 212
$isdst
[1] 1 1 1 1 1 1 1 1
</code></pre>

<p>Class <code>POSIXct</code>, which represents the (signed) number of seconds since the beginning of
1970 as a numeric vector, gives you the expected length:</p>

<pre><code>R&gt; length(as.POSIXct(a))
[1] 8
R&gt; unclass(as.POSIXct(a))
[1] 1.249e+09 1.249e+09 1.249e+09 1.249e+09 1.249e+09 1.249e+09 1.249e+09
[8] 1.249e+09
attr(,""tzone"")
[1] """"
</code></pre>
"
1887411,168747,2009-12-11T11:21:18Z,1887195,0,FALSE,"<p>As you can see in <code>?strptime</code> it converts character strings to class <code>POSIXlt</code>. In R there are two types of times: <code>POSIXlt</code> and <code>POSIXct</code>.
Description is in <code>?DateTimeClasses</code>, but to shortcut:</p>

<blockquote>
  <p>Class ""POSIXct"" represents the (signed) number of seconds since the
  beginning of 1970 as a numeric
  vector.</p>
  
  <p>Class ""POSIXlt"" is a named
  list of vectors representing 
  sec    0–61: seconds
  min    0–59: minutes hour    0–23: hours mday    1–31: day of the month
  mon    0–11: months after the first of
  the year. year    Years since 1900.
  wday    0–6 day of the week, starting
  on Sunday. yday    0–365: day of the
  year. isdst    Daylight savings time
  flag. Positive if in force, zero if
  not, negative if unknown.</p>
</blockquote>

<p>So your <code>b</code> is list of 9 vectors, 8-length each.</p>

<p>You can see:</p>

<pre><code>sapply(b,length)
</code></pre>

<p>You could use exact conversion:</p>

<pre><code>b_1 = as.POSIXlt(a, ""%Y-%m-%d %H:%M:%S"",tz="""")
b_2 = as.POSIXct(a, ""%Y-%m-%d %H:%M:%S"",tz="""")

length(b_1) # 9
length(b_2) # 8
</code></pre>
"
1887931,143305,2009-12-11T13:00:15Z,1886264,5,FALSE,"<p>Because these rolling functions often apply with time-series data, some of the newer and richer time-series data-handling packages already do that for you:</p>

<pre><code>R&gt; library(zoo)   ## load zoo
R&gt; speed &lt;- c(1,1,1,1,1,4,6,3,6,8,9)
R&gt; zsp &lt;- zoo( speed, order.by=1:length(speed) )  ## creates a zoo object
R&gt; rollmean(zsp, 5)                               ## default use
  3   4   5   6   7   8   9 
1.0 1.6 2.6 3.0 4.0 5.4 6.4 
R&gt; rollmean(zsp, 5, na.pad=TRUE, align=""right"")   ## with padding and aligned
  1   2   3   4   5   6   7   8   9  10  11 
 NA  NA  NA  NA 1.0 1.6 2.6 3.0 4.0 5.4 6.4 
R&gt; 
</code></pre>

<p>The <a href=""http://cran.r-project.org/web/packages/zoo/"" rel=""noreferrer"">zoo</a> has excellent documentation that will show you many, many more examples, in particular how to do this with real (and possibly irregular) dates;  <a href=""http://cran.r-project.org/web/packages/xts/"" rel=""noreferrer"">xts</a> extends this further but <a href=""http://cran.r-project.org/web/packages/zoo/"" rel=""noreferrer"">zoo</a> is a better starting point.</p>
"
1887976,143305,2009-12-11T13:10:22Z,1886571,2,TRUE,"<p>That is a pretty good solution. A regular expression may be more robust in case you fewer or more digits: swap the beginnning <code>$Rev :</code> and the trailing <code>$</code> for empty strings and you should have the revision left.</p>

<p>The only problem with the per-file properties is that they only update when this file itself is updated by subversion.  </p>

<p>For that reason (and many others), consider making a local package.  Your DESCRIPTION file will a) change often enough for the new version number and b) can simply be extended by new fields you simply add e.g.</p>

<pre><code>Revision: $Rev$
</code></pre>

<p>You can read the content from R via <code>read.dcf()</code> after which you can then do your trick of stripping the dollar signs and colon, or use a regular expression.</p>
"
1888047,143305,2009-12-11T13:24:48Z,1886808,2,FALSE,"<p>You get what rcs (correctly !) suggested by default using <a href=""http://cran.r-project.org/web/packages/zoo"" rel=""nofollow noreferrer"">zoo</a> as plot with lines and the same axis:</p>

<pre><code>R&gt; library(zoo)
R&gt; zdf &lt;- zoo(df$val, order.by=df$date)
R&gt; plot(zdf)
R&gt; 
</code></pre>

<p>The <code>help(plot.zoo)</code> examples show to do fancier date indexing, essentially what rcs showed you but with an additional formatting via, say, </p>

<pre><code>R&gt; fmt &lt;- ""%Y-%m""  ## year-mon
R&gt; txt &lt;- format(index(zdf), fmt)
R&gt; plot(zdf, xaxt='n')
R&gt; axis(side=1, at=index(zdf), lab=txt)
R&gt; 
</code></pre>

<p>If you subset <code>at</code> and <code>lab</code> you get fewer ticks too.</p>
"
1888068,143305,2009-12-11T13:28:53Z,1887195,0,FALSE,"<p>Just for the record, this FAQ issue is about to change in R 2.11.0:</p>

<blockquote>
  <p>2.11.0 NEW FEATURES</p>
  
  <p>length(POSIXlt) now returns the length
  of the corresponding abstract
  timedate-vector rather than always 9
  (the length of the underlying list
  structure). (Wish of PR#14073 and
  PR#10507.)</p>
</blockquote>

<p>That's from the December 2 entry of the RSS feed summarising daily changes in the Subversion archive, the <a href=""http://developer.r-project.org"" rel=""nofollow noreferrer"">developer page</a> for details about the feed.</p>
"
1888266,163053,2009-12-11T14:03:04Z,1888151,0,FALSE,"<p>I use a build script which checks out a certain revision from svn, updates the DESCRIPTION with the version number, and builds it into a tar.gz.  </p>

<p>I manage the process a little more by using the roadmap feature in Jira to bundle issues/svn commits as versions (the Apache foundation uses the same basic approach <a href=""https://issues.apache.org/jira/browse/HARMONY?report=com.atlassian.jira.plugin.system.project:roadmap-panel"" rel=""nofollow noreferrer"">as in the this example</a>).  </p>
"
1888328,54321,2009-12-11T14:14:15Z,1888151,0,FALSE,"<p>Would it be easier to use the timestamp  Dec 11, 2009 12:01:03  turns into 20091211120103.  Then you could put a tag in subversion for the build as well and know which source corresponds to which build.</p>
"
1888380,83761,2009-12-11T14:21:15Z,1886264,1,FALSE,"<p>You can also use a combination of <code>cumsum</code> and <code>diff</code> to get the sum over sliding windows. You'll need to pad with your own <code>NaN</code>, though:</p>

<pre><code>&gt; speed &lt;- c(1,1,1,1,1,4,6,3,6,8,9)
&gt; diff(cumsum(c(0,speed)), 5)/5
[1] 1.0 1.6 2.6 3.0 4.0 5.4 6.4
</code></pre>
"
1888473,2094,2009-12-11T14:37:57Z,1888151,1,FALSE,"<p>Calling 'svnversion &lt;working-copy&gt;' should give you the information you need.</p>

<p>If your working copy is modified, or has files from more than one version you get more than just a version number. Either a range (multiple revisions) or suffixed letters (Modified, Switched or Sparse working copies).</p>

<pre><code>$ svnversion --help
usage: svnversion [OPTIONS] [WC_PATH [TRAIL_URL]]

  Produce a compact 'version number' for the working copy path
  WC_PATH.  TRAIL_URL is the trailing portion of the URL used to
  determine if WC_PATH itself is switched (detection of switches
  within WC_PATH does not rely on TRAIL_URL).  The version number
  is written to standard output.  For example:

    $ svnversion . /repos/svn/trunk
    4168

  The version number will be a single number if the working
  copy is single revision, unmodified, not switched and with
  an URL that matches the TRAIL_URL argument.  If the working
  copy is unusual the version number will be more complex:

   4123:4168     mixed revision working copy
   4168M         modified working copy
   4123S         switched working copy
   4123P         partial working copy, from a sparse checkout
   4123:4168MS   mixed revision, modified, switched working copy

  If invoked on a directory that is not a working copy, an
  exported directory say, the program will output 'exported'.

  If invoked without arguments WC_PATH will be the current directory.

Valid options:
  -n [--no-newline]        : do not output the trailing newline
  -c [--committed]         : last changed rather than current revisions
  -h [--help]              : display this help
  --version                : show program version information
</code></pre>
"
1888867,143305,2009-12-11T15:35:15Z,1888151,2,FALSE,"<p>I also use svnversion to automate this. E.g. for littler, which is of course compiled, I do this:</p>

<pre><code>#!/bin/sh -e

svnversion() {
    svnrevision=`LC_ALL=C svn info | awk '/^Revision:/ {print $2}'`
    svndate=`LC_ALL=C svn info | awk '/^Last Changed Date:/ {print $4,$5}'`

    now=`date`

    cat &lt;&lt;EOF &gt; svnversion.h

// Do not edit!  This file was autogenerated
//      by $0
//      on $now
//
// svnrevision and svndate are as reported by svn at that point in time,
// compiledate and compiletime are being filled gcc at compilation

#include &lt;stdlib.h&gt;

static const char* svnrevision = ""$svnrevision"";
static const char* svndate = ""$svndate"";
static const char* compiletime = __TIME__;
static const char* compiledate = __DATE__;

EOF
}

if [ ""$#"" -ge 0 ]; then
    if [ ""$1"" = ""--svnversion"" ]; then
        svnversion
        exit
    fi
fi

test -f svnversion.h || svnversion
</code></pre>

<p>from the Makefile and then use that as in</p>

<pre><code>void showVersionAndExit() {
    printf(""%s ('%s') version %s\n\tsvn revision %s as of %s\n\t""
           ""built at %s on %s\n"",
           binaryName, programName, VERSION,
           svnrevision, svndate, compiletime, compiledate);
    /* more code below ... */
</code></pre>

<p>The same could be done for R, easiest by accessing the DESCRIPTION file as I suggested to Mario in answering <a href=""https://stackoverflow.com/questions/1886571/making-the-subversion-revision-number-visible-in-my-r-scripts"">his earlier question</a>.</p>

<p>Then, and to finally answer your question :), you could massage that number you get from svnversion for the repository itself (or its top-level entry) in any may you like to mod the DESCRIPTION file.  But then you modify the file and are out of sync, so you resubmit, get a new revision, ...   so you need to agree with yourself on some way to break this loop.</p>
"
1890253,163053,2009-12-11T19:12:31Z,1890215,21,FALSE,"<p>You might want to consider using <a href=""http://www.stat.uni-muenchen.de/~leisch/Sweave/"" rel=""noreferrer"">Sweave</a>.  There is a lot of great documentation available for this on the Sweave website (and elsewhere).  It has very simple syntax: just put your R code between <code>&lt;&lt;&gt;&gt;=</code> and <code>@</code>.</p>

<p>Here's a simple example that ends up <a href=""http://www.stat.uni-muenchen.de/~leisch/Sweave/example-1.pdf"" rel=""noreferrer"">looking like this</a>:</p>

<pre><code>\documentclass[a4paper]{article}

\title{Sweave Example 1}
\author{Friedrich Leisch}

\begin{document}

\maketitle

In this example we embed parts of the examples from the
\texttt{kruskal.test} help page into a \LaTeX{} document:

&lt;&lt;&gt;&gt;=
data(airquality)
library(ctest)
kruskal.test(Ozone ~ Month, data = airquality)
@
which shows that the location parameter of the Ozone 
distribution varies significantly from month to month. Finally we
include a boxplot of the data:

\begin{center}
&lt;&lt;fig=TRUE,echo=FALSE&gt;&gt;=
boxplot(Ozone ~ Month, data = airquality)
@
\end{center}

\end{document}
</code></pre>

<p>To build the document, you can just call <code>R CMD Sweave file.Rnw</code> or run <code>Sweave(file)</code> from within R.  </p>
"
1890504,143305,2009-12-11T19:59:55Z,1890215,51,TRUE,"<p>Shane is spot-on, you do want Sweave. Eventually.</p>

<p>As a newbie, you may better off separating task though. For that, do this:</p>

<ol>
<li>open a device:   <code>pdf(""figures/myfile.pdf"", height=6, width=6)</code>.</li>
<li>plot your R object:  <code>plot(1:10, type='l', main='boring')</code>  -- and remember that lattice and ggplot need an explicit <code>print</code> around <code>plot</code>.</li>
<li>important: close your device: <code>dev.off()</code>  to finalize the file.</li>
<li>optional: inspect the pdf file.</li>
<li>in LaTeX, use <code>usepackage{graphicx}</code> in the document header, use<br>
<code>\includegraphics[width=0.98\textwidth]{figures/myfile}</code>  to include the figure created earlier and note that file extension is optional.</li>
<li>run this through <code>pdflatex</code> and enjoy.</li>
</ol>
"
1891086,136407,2009-12-11T21:48:33Z,1890215,3,FALSE,"<p>This is a dupe of a question on SO that I can't find.</p>

<p>But: 
<a href=""http://r-forge.r-project.org/projects/tikzdevice/"" rel=""nofollow noreferrer"">http://r-forge.r-project.org/projects/tikzdevice/</a> -- tikz output from r
and
<a href=""http://www.rforge.net/pgfSweave/"" rel=""nofollow noreferrer"">http://www.rforge.net/pgfSweave/</a> tikz code via sweave.</p>

<p>Using tikz will give you a look consistent with the rest of your document, plus it will use latex to typeset all the text in your graphs.</p>

<p><strong>EDIT</strong>
<a href=""https://stackoverflow.com/questions/1395105/getting-latex-into-r-plots/1395553#1395553"">Getting LaTeX into R Plots</a></p>
"
1892613,135870,2009-12-12T07:02:21Z,1890215,52,FALSE,"<p>I would recommend the <code>tikzDevice</code> package for producing output for inclusion in LaTeX documents:</p>

<p><a href=""http://cran.r-project.org/web/packages/tikzDevice/index.html"" rel=""noreferrer"">http://cran.r-project.org/web/packages/tikzDevice/index.html</a></p>

<p>The tikzDevice converts graphics produced in R to code that can be interpreted by the LaTeX package <code>tikz</code>. TikZ provides a very nice vector drawing system for LaTeX.  Some good examples of TikZ output are located at:</p>

<p><a href=""http://www.texample.net/"" rel=""noreferrer"">http://www.texample.net/</a></p>

<p>The <code>tikzDevice</code> may be used like any other R graphics device:</p>

<pre><code>require( tikzDevice )

tikz( 'myPlot.tex' )

plot( 1, 1, main = '\\LaTex\\ is $\\int e^{xy}$' )

dev.off()
</code></pre>

<p>Note that the backslashes in LaTeX macros must be doubled as R interprets a single backslash as an escape character. To use the plot in a LaTeX document, simply include it:</p>

<pre><code>\include{path/to/myPlot.tex}
</code></pre>

<p>The <code>pgfSweave</code> package contains <code>Sweave</code> functionality that can handle the above step for you.  Make sure that your document contains <code>\usepackage{tikz}</code> somewhere in the LaTeX preamble.</p>

<p><a href=""http://cran.r-project.org/"" rel=""noreferrer"">http://cran.r-project.org/</a></p>

<p>The advantages of  <code>tikz()</code> function as compared to <code>pdf()</code> are:</p>

<ul>
<li><p>The font of labels and captions in your figures always matches the font used in your LaTeX document.  This provides a unified look to your document.</p></li>
<li><p>You have all the power of the LaTeX typesetter available for creating mathematical annotation and can use arbitrary LaTeX code in your figure text.</p></li>
</ul>

<p>Disadvantages of the <code>tikz()</code> function are:</p>

<ul>
<li><p>It does not scale well to handle plots with lots of components.  These are things such as <code>persp()</code> plots of large matricies.  The shear number of graphic elements can cause LaTeX to slow to a crawl or run out of memory.</p></li>
<li><p>The package is currently flagged as beta. This means that the interface or functionality of the package is subject to change if the authors find a compelling reason to do so.</p></li>
</ul>

<p>I should end this post by disclaiming that I am an author of both the <code>tikzDevice</code> and <code>pgfSweave</code> packages so my opinion may be biased.  However, I have used both packages to produce several academic reports in the last year and have been very satisfied with the results.</p>
"
1893203,210211,2009-12-12T12:03:18Z,1885264,0,FALSE,"<p>What you're looking for is called a texture map -- and if it's not provided in the R graphics package, you may be able to do it ""by hand"".  The suggestion below may not be fast or convenient (or even helpful, as I'm not really familiar with R), but it may actually work...</p>

<p>Since you know you can draw a 3D surface plot with specified colors, you can try drawing a flat 3D surface using the colors of your image.</p>

<p>If R also lacks methods for extracting its data from image formats, there is an image format called PPM (standing for Portable PixMap), one variant of which is basically space-separated decimal numbers.  After converting your image to this format (using Photoshop, say, or some dedicated image conversion program), it should be relatively easy to input into R.</p>
"
1893536,172261,2009-12-12T14:05:58Z,1893257,4,TRUE,"<p>The gray levels are hardcoded in the <code>spectrogram</code> function (<code>gray()</code>), to override this setting you could use, for instance, the following:</p>

<pre><code># define a color palette
colors &lt;- colorRampPalette(c(""#007FFF"", ""blue"", ""#000077""))  
gray &lt;- function(x) colors(255*x)  # redefine gray palette
spectrogram(test1$amplitude[,1], test1$instantfreq[,1])
gray &lt;- grDevices::gray  # reset gray palette function
</code></pre>

<p>Another option is to use the source of the <code>spectrogram</code> function to define your own plot function which has an argument for the color palette.</p>
"
1894997,163053,2009-12-12T23:11:42Z,1894190,2,FALSE,"<p>This doesn't directly answer your question, but I strongly recommend watching <a HREF=""http://www.drewconway.com/zia/?p=1221"" rel=""nofollow noreferrer"">Drew Conway's presentation on SNA in R</a> if you haven't already seen it.   </p>
"
1896784,163053,2009-12-13T15:12:06Z,1896419,83,TRUE,"<p><em>Edit:</em></p>

<p>I just saw that you pointed out one of your dimensions is a date.  In that case, <a href=""http://www.quantmod.com/examples/chartSeries3d/"" rel=""noreferrer"">have a look at Jeff Ryan's chartSeries3d</a> which is designed to chart 3-dimensional time series.  Here he shows the yield curve over time:</p>

<p><a href=""http://www.quantmod.com/examples/chartSeries3d/chartSeries3d-thumb.png"" rel=""noreferrer"">chartSeries example http://www.quantmod.com/examples/chartSeries3d/chartSeries3d-thumb.png</a></p>

<p><em>Original Answer:</em></p>

<p>As I understand it, you want a countour map to be the projection on the plane beneath the 3D surface plot.  I don't believe that there's an easy way to do this other than creating the two plots and then combining them.  You may <a href=""http://cran.r-project.org/web/views/Spatial.html"" rel=""noreferrer"">find the spatial view helpful for this</a>.</p>

<p>There are two primary R packages for 3D plotting: <strong><a href=""http://cran.r-project.org/web/packages/rgl/"" rel=""noreferrer"">rgl</a></strong> (or you can use the related <a href=""http://cran.r-project.org/web/packages/rgl/"" rel=""noreferrer"">misc3d</a> package) and <strong><a href=""http://cran.r-project.org/web/packages/scatterplot3d/"" rel=""noreferrer"">scatterplot3d</a></strong>.</p>

<p><strong>rgl</strong></p>

<p>The rgl package uses OpenGL to create interactive 3D plots (<a href=""http://rgl.neoscientists.org/"" rel=""noreferrer"">read more on the rgl website</a>).  Here's an example using the <code>surface3d</code> function:</p>

<pre><code>library(rgl)
data(volcano)
z &lt;- 2 * volcano # Exaggerate the relief
x &lt;- 10 * (1:nrow(z)) # 10 meter spacing (S to N)
y &lt;- 10 * (1:ncol(z)) # 10 meter spacing (E to W)
zlim &lt;- range(z)
zlen &lt;- zlim[2] - zlim[1] + 1
colorlut &lt;- terrain.colors(zlen,alpha=0) # height color lookup table
col &lt;- colorlut[ z-zlim[1]+1 ] # assign colors to heights for each point
open3d()
rgl.surface(x, y, z, color=col, alpha=0.75, back=""lines"")
</code></pre>

<p>The alpha parameter makes this surface partly transparent.  Now you have an interactive 3D plot of a surface and you want to create a countour map underneath.  rgl allows you add more plots to an existing image:</p>

<pre><code>colorlut &lt;- heat.colors(zlen,alpha=1) # use different colors for the contour map
col &lt;- colorlut[ z-zlim[1]+1 ] 
rgl.surface(x, y, matrix(1, nrow(z), ncol(z)),color=col, back=""fill"")
</code></pre>

<p>In this surface I set the heights=1 so that we have a plane underneath the other surface.  This ends up looking like this, and can be rotated with a mouse:</p>

<p><a href=""http://i45.tinypic.com/12637gy.jpg"" rel=""noreferrer"">3D surface plot http://i45.tinypic.com/12637gy.jpg</a></p>

<p><strong>scatterplot3d</strong></p>

<p>scatterplot3d is a little more like other plotting functions in R (<a href=""http://cran.r-project.org/web/packages/scatterplot3d/vignettes/s3d.pdf"" rel=""noreferrer"">read the vignette</a>).  Here's a simple example:</p>

<pre><code>temp &lt;- seq(-pi, 0, length = 50)
x &lt;- c(rep(1, 50) %*% t(cos(temp)))
y &lt;- c(cos(temp) %*% t(sin(temp)))
z &lt;- c(sin(temp) %*% t(sin(temp)))
scatterplot3d(x, y, z, highlight.3d=TRUE,
 col.axis=""blue"", col.grid=""lightblue"",
 main=""scatterplot3d - 2"", pch=20)
</code></pre>

<p>In this case, you will need to overlay the images.  The R-Wiki <a href=""http://wiki.r-project.org/rwiki/doku.php?id=tips:graphics-misc:translucency"" rel=""noreferrer"">has a nice post on creating a tanslucent background image</a>.</p>
"
1897837,144537,2009-12-13T21:30:41Z,1894190,7,FALSE,"<p>This is a good question, and provides some opportunity for further exploration of SNA in R.  I am more familiar with the <a href=""http://igraph.sourceforge.net/index.html"" rel=""nofollow noreferrer"">igraph package</a>, so I will answer your question using the the functions in that library.</p>

<p>The first part of your question has a fairly straightforward solution:</p>

<pre><code># Convert data frame to graph using incidence matrix
G&lt;-graph.incidence(as.matrix(data),weighted=TRUE,directed=FALSE)
summary(G)
# Vertices: 12 
# Edges: 30 
# Directed: TRUE 
# No graph attributes.
# Vertex attributes: type, name.
# Edge attributes: weight.
</code></pre>

<p>This returns a graph object with undirected and weighted edges from the incidence matrix.  To generate the affiliations graphs from the bipartite graph you have two options.  The quick and easy one is this:</p>

<pre><code>proj&lt;-bipartite.projection(G)
</code></pre>

<p>This will return a list with each projection indexed as $proj1 and proj2, the unfortunate thing is that these projects do not contain the edge weights that you would normally want when performing this manipulation.  To do this the best solution is to simply perform the matrix multiplication yourself.</p>

<pre><code># Create the matrices, and set diagonals to zero
M&lt;-as.matrix(data)
affil.matrix&lt;-M%*%t(M)
diag(affil.matrix)&lt;-0
cases.matrix&lt;-t(M)%*%M
diag(cases.matrix)&lt;-0
# Create graph objects from matrices
affil.graph&lt;-graph.incidence(affil.matrix,weighted=TRUE)
cases.graph&lt;-graph.incidence(cases.matrix,weighted=TRUE)
</code></pre>

<p>Generating the plots with the attribute data is a bit trickier and requires more coding, but I recommend looking through some of the <a href=""http://igraph.sourceforge.net/screenshots.html"" rel=""nofollow noreferrer"">igraph examples</a>, or even <a href=""http://www.drewconway.com/zia/wp-content/uploads/redirects/sna_in_r.html"" rel=""nofollow noreferrer"">some of my own</a> as there is plenty there to get you started.  Good luck!</p>
"
1897862,170713,2009-12-13T21:35:57Z,1897704,0,FALSE,"<p>I think what you need is an inner product. For two vectors <code>v,u</code> (in <code>R^n</code> or any other inner-product spaces) <code>&lt;v,u&gt;/|v||u|= cos(alpha)</code>. (were <code>alpha</code> is the angle between the vectors)</p>

<p>for more details see:</p>

<p><a href=""http://en.wikipedia.org/wiki/Inner_product_space"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Inner_product_space</a></p>
"
1897874,211275,2009-12-13T21:40:24Z,1897704,6,FALSE,"<p>You should use the dot product.  Say you have <em>V</em>₁ = (<em>x</em>₁, <em>y</em>₁, <em>z</em>₁) and <em>V</em>₂ = (<em>x</em>₂, <em>y</em>₂, <em>z</em>₂), then the dot product, which I'll denote by <em>V</em>₁·<em>V</em>₂, is calculated as </p>

<blockquote>
  <p><em>V</em>₁·<em>V</em>₂ = <em>x</em>₁·<em>x</em>₂ + <em>y</em>₁·<em>y</em>₂ + <em>z</em>₁·<em>z</em>₂ = |<em>V</em>₁| · |<em>V</em>₂| · cos(<em>θ</em>);</p>
</blockquote>

<p>What this means is that that sum shown on the left is equal to the product of the absolute values of the vectors times the cosine of the angle between the vectors. the absolute value of the vectors <em>V</em>₁ and <em>V</em>₂ are calculated as </p>

<blockquote>
  <p>|<em>V</em>₁| = √(<em>x</em>₁² + <em>y</em>₁² + <em>z</em>₁²), and<br>
  |<em>V</em>₂| = √(<em>x</em>₂² + <em>y</em>₂² + <em>z</em>₂²),</p>
</blockquote>

<p>So, if you rearrange the  first equation above, you get</p>

<blockquote>
  <p>cos(<em>θ</em>) = (<em>x</em>₁·<em>x</em>₂ + <em>y</em>₁·<em>y</em>₂ + <em>z</em>₁·<em>z</em>₂) ÷ (|<em>V</em>₁|·|<em>V</em>₂|),</p>
</blockquote>

<p>and you just need the arccos function (or inverse cosine) applied to cos(<em>θ</em>) to get the angle.</p>

<p>Depending on your arccos function, the angle may be in degrees or radians. </p>

<p>(For two dimensional vectors, just forget the <em>z</em>-coordinates and do the same calculations.) </p>

<p>Good luck,</p>

<p>John Doner</p>
"
1898026,94732,2009-12-13T22:26:55Z,1897704,38,TRUE,"<p>According to page 5 of <a href=""http://www.math.uh.edu/~jmorgan/Math6397/day13/LinearAlgebraR-Handout.pdf"" rel=""noreferrer"">this PDF</a>, <code>sum(a*b)</code> is the R command to find the dot product of vectors <code>a</code> and <code>b</code>, and <code>sqrt(sum(a * a))</code> is the R command to find the norm of vector <code>a</code>, and <code>acos(x)</code> is the R command for the arc-cosine.  It follows that the R code to calculate the angle between the two vectors is</p>

<pre><code>theta &lt;- acos( sum(a*b) / ( sqrt(sum(a * a)) * sqrt(sum(b * b)) ) )
</code></pre>
"
1898503,143305,2009-12-14T01:36:01Z,1898101,3,FALSE,"<p>Ok, I just checked. I was wrong in my earlier comment. From <code>help(x11)</code> where a lot of detail is available -- the new Cairo-based devices do have anti-aliasing available:</p>

<blockquote>
  <p>x11                 package:grDevices 
  R Documentation</p>
  
  <p>X Window System Graphics</p>
  
  <p>Description:</p>

<pre><code> ‘X11’ starts a graphics device driver for the X Window System
 (version 11).  This can only be done on machines/accounts that
 have access to an X server.

 ‘x11’ is recognized as a synonym for ‘X11’.
</code></pre>
  
  <p>Usage:</p>

<pre><code> X11(display = """", width, height, pointsize, gamma, bg, canvas,
     fonts, xpos, ypos, title, type, antialias)

 X11.options(..., reset = FALSE)
</code></pre>
  
  <p>Arguments: </p>
  
  <p>[...]    </p>

<pre><code> fonts: X11 font description strings into which weight, slant and
      size will be substituted.  There are two, the first for fonts
      1 to 4 and the second for font 5, the symbol font.  See
      section ‘Fonts’. 
</code></pre>
  
  <p>[...] </p>

<pre><code> antialias: for cairo types, the typeof anti-aliasing (if any) to be
      used.  One of ‘c(""default"", ""none"", ""gray"", ""subpixel"")’. 
</code></pre>
  
  <p>[...]</p>
  
  <p>Details:</p>

<pre><code> The defaults for all of the arguments of ‘X11’ are set by
 ‘X11.options’: the ‘Arguments’ section gives the ‘factory-fresh’
 defaults.

 The initial size and position are only hints, and may not be acted
 on by the window manager.  Also, some systems (especially laptops)
 are set up to appear to have a screen of a different size to the
 physical screen.

 Option ‘type’ selects between two separate devices: R can be built
 with support for neither, ‘type = ""Xlib""’ or both.  Where both are
 available, types ‘""cairo""’ and ‘""nbcairo""’ offer

    * antialiasing of text and lines.

    * translucent colours.

    * scalable text, including to sizes like 4.5 pt.

    * full support for UTF-8, so on systems with suitable fonts you
      can plot in many languages on a single figure (and this will
      work even in non-UTF-8 locales).  The output should be
      locale-independent.

 ‘type = ""nbcairo""’ is the same device as ‘type=""cairo""’ without
 buffering: which is faster will depend on the X11 connection.
 Both will be slower than ‘type = ""Xlib""’, especially on a slow X11
 connection as all the rendering is done on the machine running R
 rather than in the X server.

 All devices which use an X11 server (including the ‘type = ""Xlib""’
 versions of bitmap devices such as ‘png’) share internal
 structures, which means that they must use the same ‘display’ and
 visual.  If you want to change display, first close all such
 devices. 
</code></pre>
  
  <p>[...and more...]</p>
</blockquote>
"
1898873,143305,2009-12-14T04:12:14Z,1898815,1,TRUE,"<p>Yes, you may have found a bug there in terms of the handling of attributes. On the other hand, who cares?  <code>c</code> and <code>d</code> are effectively equal:</p>

<pre><code>R&gt; c - d

2003-01-07 0 0
2003-01-15 0 0
2003-01-17 0 0
2003-01-18 0 0
2003-02-17 0 0
2003-02-22 0 0
R&gt; 
</code></pre>

<p>Inspecting the objects gives nothing away:</p>

<pre><code>R&gt; str(c)
‘zoo’ series from 2003-01-07 to 2003-02-22
  Data: num [1:6, 1:2] 0.79 -0.731 1.574 -0.694 0.358 ...
  Index:  POSIXct[1:6], format: ""2003-01-07"" ""2003-01-15"" 
      ""2003-01-17"" ""2003-01-18"" ""2003-02-17"" ""2003-02-22""
R&gt; str(d)
‘zoo’ series from 2003-01-07 to 2003-02-22
  Data: num [1:6, 1:2] 0.79 -0.731 1.574 -0.694 0.358 ...
  Index:  POSIXct[1:6], format: ""2003-01-07"" ""2003-01-15"" 
      ""2003-01-17"" ""2003-01-18"" ""2003-02-17"" ""2003-02-22""
</code></pre>

<p>I suggest you send a polite mail to the maintainers of <code>zoo</code> illustrating the case.</p>
"
1899053,202244,2009-12-14T05:20:13Z,1898101,13,TRUE,"<p>On Windows, there is no built-in anti-aliasing. I don't know whether it is planned for future releases or not. You can get a Cairo-based graphics device from either the <code>cairoDevice</code> or <code>Cairo</code> packages; however, you will need to install <code>GTK+</code> first:</p>

<p>Download and install <code>Gtk+ 2.12.9 Runtime Environment Revision 2</code> from <a href=""http://gladewin32.sourceforge.net/"" rel=""noreferrer"">http://gladewin32.sourceforge.net/</a></p>

<p>Another option would be to use Java-based graphics through <code>JGR</code> (<code>http://jgr.markushelbig.org/</code>). Also a <code>Qt</code>-based device is under development, I think.</p>
"
1899057,7193,2009-12-14T05:20:51Z,1899008,7,TRUE,"<p>To get the bias, just evaluate the model with a feature vector of all zeros.  To get the coefficient of the first feature, evaluate the model with a feature vector with a ""1"" in the first position, and zeros everywhere else - and then subtract the bias, which you already know.  I'm afraid I don't know R syntax, but conceptually you want something like this:</p>

<pre><code>bias = my.model.eval([0, 0, 0])
f1 = my.model.eval([1, 0, 0]) - bias
f2 = my.model.eval([0, 1, 0]) - bias
f3 = my.model.eval([0, 0, 1]) - bias
</code></pre>

<p>To test that you did it correctly, you can try something like this:</p>

<pre><code>assert(bias + f1 + f2 + f3 == my.model.eval([1, 1, 1]))
</code></pre>
"
1899313,118402,2009-12-14T06:49:16Z,1899243,2,FALSE,"<p>What is your link function?  </p>

<p>The way you describe it sounds like a basic linear regression with autocorrelated errors.  In that case, one option is to use <code>lm</code> to get a consistent estimate of your coefficients and use <a href=""http://rss.acs.unt.edu/Rdoc/library/sandwich/html/NeweyWest.html"" rel=""nofollow noreferrer"">Newey-West HAC standard errors</a>.</p>

<p>I'm not sure the best answer for GLM more generally.</p>
"
1899627,144157,2009-12-14T08:21:48Z,1899243,7,TRUE,"<p>The <a href=""http://cran.r-project.org/web/packages/GLMMarp/index.html"" rel=""noreferrer"">GLMMarp</a> package will fit these models. If you just want a linear model with Gaussian errors, you can do it with the <code>arima()</code> function where the covariates are specified via the <code>xreg</code> argument.</p>
"
1900233,78912,2009-12-14T11:01:45Z,1888151,0,FALSE,"<p>I see that ./cleanup is called during the <code>R CMD build ...</code> command.</p>

<p>I think I am going to...</p>

<ul>
<li>keep <code>DESCRIPTION.template</code> in the repository.</li>
<li>instruct subversion to ignore <code>DESCRIPTION</code>.</li>
<li>use the <code>cleanup</code> script.</li>
<li>accept I must run <code>R CMD build</code> twice (meta-information in <code>DESCRIPTION</code> is checked by <code>R CMD build</code> before it is computed by <code>cleanup</code>).</li>
</ul>

<p><code>cleanup</code>:</p>

<pre><code>#!/bin/bash
FLAGS=$(svnversion . | tr -cd A-Z)

VERSIONS=$(svnversion . | tr -d A-Z)
VERSIONS=$(echo $VERSIONS:$VERSIONS | cut -d: -f1,2)
LOW=${VERSIONS%:*}
HIGH=${VERSIONS#*:}

[ $LOW -ne $HIGH ] &amp;&amp; echo ""- mixed revisions in local copy ($LOW:$HIGH)""
[ ""$FLAGS"" != """" ] &amp;&amp; echo ""- local copy has flags: $FLAGS""
echo ""- highest revision in local copy: $HIGH""

sed -re ""s/(^Version:[^-]*).+$/\1-$HIGH/"" DESCRIPTION.template &gt; DESCRIPTION

DATE=`LC_ALL=C svn info | awk '/^Last Changed Date:/ {print $4,$5}'`

now=$(date)
cat &lt;&lt;EOF &gt; R/version.R
# Do not edit!  This file was autogenerated
#      by $0
#      on $now
#
# DO NOT put this file under version control!
#
# SVNVERSION as the highest revision reported by svnversion.
# DATE as the Last Changed Date reported by svn info.

SVNVERSION &lt;- ""$HIGH""
SVNDATE &lt;- ""$DATE""
EOF
</code></pre>

<p>obviously I have to copy DESCRIPTION.template > DESCRIPTION in order to bootstrap the process.</p>

<hr>

<p>thanks everybody for the useful comments and I'm interested in reading if you think I could achieve the same result in a cleaner way.</p>

<pre><code>mario@lt41:~/Local/.../Trunk/Rnens$ R CMD build src
* checking for file 'src/DESCRIPTION' ... OK
* preparing 'src':
* checking DESCRIPTION meta-information ... OK
* running cleanup
- highest revision in local copy: 8762
* removing junk files
* checking for LF line-endings in source and make files
* checking for empty or unneeded directories
* building 'NenS_0.1-8762.tar.gz'
</code></pre>

<p>but I must also say, I am satisfied with the current solution and I hope it will be useful to others.</p>
"
1901200,83761,2009-12-14T14:21:33Z,1899008,4,FALSE,"<p>If I'm not mistaken, I think you're asking how to extract the W vector of the SVM, where W is defined as:</p>

<pre><code>W = \sum_i y_i * \alpha_i * example_i
</code></pre>

<p>Ugh: don't know best way to write equations here, but this just is the sum of the weight * support vectors. After you calculate the W, you can extract the ""weight"" for the feature you want.</p>

<p>Assuming this is correct, you'd:</p>

<ol>
<li>Get the indices of your data that are the support vectors</li>
<li>Get their weights (alphas)</li>
<li>Calculate <code>W</code></li>
</ol>

<p>kernlab stores the support vector indices and their values in a list (so it works on multiclass problems, too), anyway any use of list manipulation is just to get at the real data (you'll see that the length of the lists returned by <code>alpha</code> and <code>alphaindex</code> are just 1 if you just have a 2-class problem, which I'm assuming you do).</p>

<pre><code>my.model &lt;- ksvm(result ~ f1+f2+f3, data=gold, kernel=""vanilladot"", type=""C-svc"")
alpha.idxs &lt;- alphaindex(my.model)[[1]]  # Indices of SVs in original data
alphas &lt;- alpha(my.model)[[1]]
y.sv &lt;- gold$result[alpha.idxs]
# for unscaled data
sv.matrix &lt;- as.matrix(gold[alpha.idxs, c('f1', 'f2', 'f3')])
weight.vector &lt;- (y.sv * alphas) %*% sv.matrix
bias &lt;- b(my.model)
</code></pre>

<p><code>kernlab</code> actually scales your data first before doing its thing. You can get the (scaled) weights like so (where, I guess, the bias should be 0(?))</p>

<pre><code>weight.vector &lt;- (y.sv * alphas) %*% xmatrix(my.model)[[1]]
</code></pre>

<p>If I understood your question, this should get you what you're after.</p>
"
1907150,143305,2009-12-15T12:36:15Z,1905258,5,FALSE,"<p>That's half a dozen or more questions bundled into one, which makes it difficult to answer.</p>

<p>So let's try from the inside out:  First try to solve your RODBC wrapper problem. A code representation will suggest itself.  I would start with simple functions, and then maybe build a package around it. That already gives you some encapsulation.</p>

<p>Much of the rest is style.  Some prominent R codes swear by S4, while other swear about it.  You can always read the packages of others as well as code in R itself.   And you can always re-implement your RODBC wrapper in different ways and the compare your own approaches.</p>

<p><em>Edit:</em> Reflecting you updated and much shortened question:  Pick some packages from CRAN, in particular among those you use. I think you will quickly find some more or less interesting according to your style.</p>
"
1907188,143305,2009-12-15T12:44:05Z,1907129,3,TRUE,"<p>You may have to manually loop over the files in the <code>R</code> directory and <code>source()</code> them, maybe with something like <code>source(dir(""/some/Path"", pattern=""*.R"", full.names=TRUE)</code>. </p>

<p>But I have the feeling that <code>R CMD INSTALL</code> does a little more. You may be better off working from the installed code.  And just running your unit tests directly, as you do and as the wiki page suggests, is already pretty good.  So no better scheme from me.  But keep us posted.</p>

<p><em>Edit:</em>  Also note that R 2.10.1 gives us new options to accelerate <code>R CMD INSTALL</code>:</p>

<blockquote>
  <p>2.10.1 NEW FEATURES</p>
  
  <p>R CMD INSTALL has new options --no-R,
  --no-libs, --no-data, --no-help, --no-demo, --no-exec, and --no-inst to suppress installation of the specified
  part of the package. These are
  intended for special purposes (e.g.
  building a database of help pages
  without fully installing all
  packages).</p>
</blockquote>

<p>That should help too.</p>
"
1907328,78912,2009-12-15T13:07:36Z,1907129,0,FALSE,"<p>further additions/corrections to the script.</p>

<p>I can now invoke it as <code>doRUnit.R --standalone</code>
or have it invoked by <code>R CMD check</code></p>

<pre><code>      if(!is.null(script.name)) {
        setwd(dirname(script.name))
        path &lt;- '../inst/RUnit/'
      }
.
.
.

  if (is.null(opt$standalone)) {
    cat(""\nRunning unit tests of installed library\n"")
    library(package=pkg, character.only=TRUE)
  } else {
    cat(""\nRunning unit tests of uninstalled library\n"")
    source(dir(""../R/"", pattern="".*\\.R"", full.names=TRUE))
  }
</code></pre>
"
1907788,16632,2009-12-15T14:23:10Z,1905258,5,FALSE,"<p>For 3.  Use roxygen - it works like javadoc to take comments in your source files and build Rd files.</p>
"
1907855,163053,2009-12-15T14:33:39Z,1905258,6,TRUE,"<p>Whether to use S3, S4, or a package at all is mostly a style issue (as Dirk says), but I would suggest using one of those if you want to have a very well structured object (just as you would in any OOP language).  For instance, all the time series classes have time series objects (I believe that they're all S3 with the exception of <em>its</em>) because it allows them to enforce certain behavior around the construction and usage of those objects.  Similarly with the question about creating a package: it's a good idea to do this if you will be re-using your code frequently or if the code will be useful to someone else.  It requires a little more effort, but the added organizational structure can easily make up for the cost.</p>

<p>Regarding S3 vs. S4 (discussed on R-Help <a href=""http://tolstoy.newcastle.edu.au/R/e2/devel/07/02/2159.html"" rel=""noreferrer"">here</a> and <a href=""http://www.opensubscriber.com/message/r-help@r-project.org/12767510.html"" rel=""noreferrer"">here</a>), the basic guideline is that S3 classes are more <em>""quick and dirty""</em> while S4 classes place more <em>rigid control over objects and types</em>.  If you're working on Bioconductor, you typically will use S4 (see, for instance, <a href=""http://www.bioconductor.org/workshops/2008/advanced_R/S4_Lecture.pdf"" rel=""noreferrer"">""S4 classes and methods""</a>).  </p>

<p>I would recommend reading some of the following:</p>

<ol>
<li><a href=""http://christophe.genolini.free.fr/webTutorial/S4tutorialV0-5en.pdf"" rel=""noreferrer"">""A (Not So) Short Introduction to S4"" by Christophe Genolini</a></li>
<li><a href=""http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf"" rel=""noreferrer"">""Programmers' niche: A simple class, in S3 and S4"" by Thomas Lumley</a></li>
<li><a href=""http://cran.r-project.org/doc/vignettes/Brobdingnag/brob.pdf"" rel=""noreferrer"">""Brobdingnag: a ''hello world'' package using S4 methods"" by Robin K. S. Hankin</a></li>
<li><a href=""http://cran.r-project.org/doc/Rnews/Rnews_2003-1.pdf"" rel=""noreferrer"">""Converting packages to S4"" by Douglas Bates</a></li>
<li><a href=""http://developer.r-project.org/howMethodsWork.pdf"" rel=""noreferrer"">""How S4 Methods Work"" by John Chambers</a></li>
</ol>

<p>For documentation, Hadley's suggestion is spot on: Roxygen will make life easier and puts the documentation right next to the code.  That aside, you may still want to provide other comments in your code <em>beyond</em> what Roxygen or the man files require, in which case it's a good practice to comment your code for other developers.  Those comments will not end up in your package; they will only be visible in the source code.</p>
"
1908028,143305,2009-12-15T15:01:06Z,1908010,5,FALSE,"<p>You will want to read up on S4 classes which use the @ symbol.</p>
"
1908095,172261,2009-12-15T15:11:25Z,1908010,25,TRUE,"<p>See <code>?'@'</code>:</p>

<ul>
<li><p>Description:</p>

<p>Extract the contents of a slot in a object with a formal (S4)
class structure.</p></li>
<li><p>Usage:</p>

<p><code>object@name</code></p>

<p>...</p></li>
</ul>

<hr>

<p>The S language has two object systems, known informally as S3 and S4.</p>

<ul>
<li>S3 objects, classes and methods have been available in R
from the beginning, they are informal, yet <em>very interactive</em>.
S3 was first described in the <em>White Book</em> (Statistical Models in S).</li>
<li>S3 is not a real class system, it mostly is a set of naming
conventions.</li>
<li>S4 objects, classes and methods are much more formal and
rigorous, hence <em>less interactive</em>. S4 was first described
in the <em>Green Book</em> (Programming with Data). In R it is
available through the <code>methods</code> package, attached by default
since version 1.7.0.</li>
</ul>

<p>See also this document: <a href=""http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Leisch.pdf"" rel=""noreferrer"">S4 Classes and Methods</a>.</p>
"
1908582,37751,2009-12-15T16:19:54Z,1905258,4,FALSE,"<p>somewhat more style related than substance, but the <a href=""http://google-styleguide.googlecode.com/svn/trunk/google-r-style.html"" rel=""nofollow noreferrer"">Google R style guide</a> is worth reading:</p>
"
1909396,135944,2009-12-15T18:25:11Z,1909357,2,TRUE,"<p>Found it, with extensive use of <a href=""http://RSeek.org"" rel=""nofollow noreferrer"">RSeek</a>. The <code>sink()</code> function redirects the console to a file. <code>sink(NULL)</code> cancels the redirection. Still not entirely sure why Sweave sets up a sink. I suspect that bugs in my code were causing Sweave to abort without canceling the sink. </p>
"
1915059,163053,2009-12-16T14:40:16Z,1915001,1,FALSE,"<p>You can use the <code>chartSeries()</code> function in <code>quantmod</code> with an <code>xts</code> timeSeries and the <code>addTA()</code> function to add the background highlighting:</p>

<pre><code>addTA(xts(rep(TRUE,length(times)), times), on=-1, col=""#333333"", border=NA)
</code></pre>
"
1915361,172261,2009-12-16T15:28:12Z,1915001,11,TRUE,"<p>Using alpha transparency:</p>

<pre><code>x &lt;- seq(as.POSIXct(""1949-01-01"", tz=""GMT""), length=36, by=""months"")
y &lt;- rnorm(length(x))

plot(x, y, type=""l"", xaxt=""n"")
rect(xleft=as.POSIXct(""1950-01-01"", tz=""GMT""),
     xright=as.POSIXct(""1950-12-01"", tz=""GMT""),
     ybottom=-4, ytop=4, col=""#123456A0"") # use alpha value in col
idx &lt;- seq(1, length(x), by=6)
axis(side=1, at=x[idx], labels=format(x[idx], ""%Y-%m""))
</code></pre>

<p>or plot highlighted region behind lines:</p>

<pre><code>plot(x, y, type=""n"", xaxt=""n"")
rect(xleft=as.POSIXct(""1950-01-01"", tz=""GMT""),
     xright=as.POSIXct(""1950-12-01"", tz=""GMT""),
     ybottom=-4, ytop=4, col=""lightblue"")
lines(x, y)
idx &lt;- seq(1, length(x), by=6)
axis(side=1, at=x[idx], labels=format(x[idx], ""%Y-%m""))
box()
</code></pre>

<p><a href=""http://img341.imageshack.us/img341/7292/rect.png"">plot output http://img341.imageshack.us/img341/7292/rect.png</a></p>
"
1915512,143305,2009-12-16T15:48:25Z,1915001,5,FALSE,"<p>Here is a solution that uses <a href=""http://cran.r-project.org/web/packages/zoo"" rel=""nofollow noreferrer""><strong>zoo</strong></a> simply because that makes the subsetting easy. You could do the same with standard indexing as well:</p>

<pre><code>## create a long monthly sequence and a sub-sequence
months &lt;- seq( as.Date(""1950-01-01""), as.Date(""2009-12-12""), by=""month"")
subset &lt;- seq( as.Date(""1970-01-01""), as.Date(""1979-12-31""), by=""month"")

## generate some random values
set.seed(42)
values &lt;- cumsum(rnorm(length(months)))

## plot as a zoo object, overlay a gray background and overplot a line in red
library(zoo)
Z &lt;- zoo(values, months)
plot(Z)
rect(xleft=head(subset,1), xright=tail(subset,1),
     ybottom=par(""usr"")[3], ytop=par(""usr"")[4],
     density=NA, col=""lightgray"")
lines(Z[subset], col='red')
box()
</code></pre>

<p><a href=""http://dirk.eddelbuettel.com/misc/plotRegionExample.png"" rel=""nofollow noreferrer"">alt text http://dirk.eddelbuettel.com/misc/plotRegionExample.png</a></p>

<p>By using <code>par(""usr"")</code> we avoid the need for explicit values for upper and lower region marks. And the <code>zoo</code> indexing makes finding the start- and endpoints easy. This would work the same way for data in different time resolutions.</p>
"
1923237,23813,2009-12-17T17:16:56Z,1753950,1,FALSE,"<p>First, I agree with Eduardo Leoni's comment that we need a reproducible example, so that we have &ldquo;real&rdquo; code to work with.</p>

<p>My blind guess would be that, because in R you can abbreviate parameters, <code>g</code> is not correctly resolved between &ldquo;your&rdquo; g and an abbreviated <code>gradtol</code> from the <code>nlm</code> function.</p>

<p>On the other hand, if I try your code fragments, <code>nlm</code> proceeds with calling <code>log.pr.data</code> and fails only at the second <code>print</code> statement because <code>T.chan</code> isn't known.</p>

<p>So sadly, without a working (i.e. failing reproducibly) example, it's difficult to find out what's wrong.</p>
"
1923297,163053,2009-12-17T17:25:59Z,1923273,321,TRUE,"<p>You can just use <code>table()</code>:</p>

<pre><code>&gt; a &lt;- table(numbers)
&gt; a
numbers
  4   5  23  34  43  54  56  65  67 324 435 453 456 567 657 
  2   1   2   2   1   1   2   1   2   1   3   1   1   1   1 
</code></pre>

<p>Then you can subset it:</p>

<pre><code>&gt; a[names(a)==435]
435 
  3
</code></pre>

<p>Or convert it into a data.frame if you're more comfortable working with that:</p>

<pre><code>&gt; as.data.frame(table(numbers))
   numbers Freq
1        4    2
2        5    1
3       23    2
4       34    2
...
</code></pre>
"
1923305,37751,2009-12-17T17:27:54Z,1923273,7,FALSE,"<p>here's one fast and dirty way:</p>

<pre><code>x &lt;- 23
length(subset(numbers, numbers==x))
</code></pre>
"
1923493,234001,2009-12-17T17:55:16Z,1923273,39,FALSE,"<p>I would probably do something like this</p>

<pre><code>length(which(numbers==x))
</code></pre>

<p>But really, a better way is</p>

<pre><code>table(numbers)
</code></pre>
"
1923574,16632,2009-12-17T18:09:42Z,1923273,186,FALSE,"<p>The most direct way is <code>sum(numbers == x)</code>.  </p>

<p><code>numbers == x</code> creates a logical vector which is TRUE at every location that x occurs, and when <code>sum</code>ing, the logical vector is coerced to numeric which converts TRUE to 1 and FALSE to 0.</p>

<p>However, note that for floating point numbers it's better to use something like: <code>sum(abs(numbers - x) &lt; 1e-6)</code>.</p>
"
1925404,234233,2009-12-17T23:49:04Z,1908010,8,FALSE,"<p>As the others have said, the @ symbol is used with S4 classes, but here is a note from <a href=""http://google-styleguide.googlecode.com/svn/trunk/google-r-style.html"" rel=""noreferrer"">Google's R Style Guide</a>: ""Use S3 objects and methods unless there is a strong reason to use S4 objects or methods.""</p>
"
1931760,66353,2009-12-19T02:41:13Z,1931742,5,TRUE,"<p>Does the following help?</p>

<pre><code>#!/usr/bin/env perl

use strict;
use warnings;

my $filename = 1;
my $flag;
my $fh;

while (&lt;&gt;) {
    if (/^\d+\s+\d+\s*$/) {
        if ( $flag == 1 ) {
            $flag = 0;
            open $fh, '&gt;', $filename;
            $filename++;
        }
        print $fh $_;
    }
    elsif (/random/) {
        next;
    }
    else {
        $flag = 1;
    }
}
</code></pre>

<p><strong>Usage:</strong></p>

<p>Save the above as <code>extract</code> (or any other name, if that matters).</p>

<p>Assuming that the file with data is named <code>file</code>.</p>

<pre><code>perl extract /path/to/file
</code></pre>
"
1931840,163053,2009-12-19T03:23:00Z,1931742,2,FALSE,"<p>Here's a solution in R.  </p>

<p>Load your data:</p>

<pre><code>a &lt;- readLines(textConnection(""track type= wiggle name09
variableStep chrom=chr1
34 5 
36 7 
54 8 
variableStep chrom=chr2 
33 4 
35 2 
78 7 
this is text with the word random in it# this we need to remove
82 4 
88 6 
variableStep chrom=chr3 
78 5 
89 4 
56 7""))
</code></pre>

<p>Process it by finding the break points and only keeping rows with number space number format:</p>

<pre><code>idx &lt;- grep(""="", a)
idx &lt;- idx[c(which((idx[-1]-idx[-length(idx)])&gt;1),length(idx))]
idx &lt;- cbind(idx+1,c(idx[-1]-1,length(a)))
sapply(1:nrow(idx), function(i) {
    x &lt;- a[idx[i,1]:idx[i,2]]
    write.table(x[grep(""^\\d+\\s+\\d+\\s*"", x, perl=TRUE)], file=as.character(i), row.names=FALSE, col.names=FALSE, quote=FALSE)
})
</code></pre>
"
1934882,111690,2009-12-20T04:44:37Z,1934751,5,TRUE,"<p>Greg Snow's TeachingDemos package contains a <a href=""http://www.inside-r.org/packages/cran/TeachingDemos/docs/dots"" rel=""nofollow noreferrer"">dots(x, ...)</a> function which seems to fit your need:</p>

<pre><code>dots( round( rnorm(50, 10,3) ) )
</code></pre>

<p><img src=""https://i.stack.imgur.com/HHUGS.png"" alt=""enter image description here""></p>
"
1934924,160314,2009-12-20T05:16:56Z,1934670,4,TRUE,"<p>Your question is a bit ambiguous. What does ""of the scale to be used in jpeg outputs."" mean? Are both columns always identical? Perhaps you are looking for something like the following:</p>

<pre><code>&gt; dat&lt;-data.frame(a=c(-(1:3),0:3))
&gt; low&lt;-quantile(dat$a,.1)
&gt; high&lt;-quantile(dat$a,.9)
&gt; dat$flag&lt;-NA
&gt; dat$flag[dat$a&lt;=low]&lt;-1
&gt; dat$flag[dat$a&gt;high]&lt;-2
&gt; dat
   a flag
1 -1   NA
2 -2   NA
3 -3    1
4  0   NA
5  1   NA
6  2   NA
7  3    2
</code></pre>
"
1934969,158065,2009-12-20T05:40:36Z,1934751,5,FALSE,"<p>You can do this yourself pretty quickly:</p>

<pre><code>x &lt;- c(1,1,2,1,2,3,3,3,4,4)
plot(sort(x), sequence(table(x)))
</code></pre>
"
1935319,144157,2009-12-20T09:04:59Z,1934751,4,FALSE,"<p>The simplest answer I know is this:</p>

<pre><code>x &lt;- c(1,1,2,1,2,3,3,3,4,4)
stripchart(x,method=""stack"",at=0)
</code></pre>

<p>It's better than Jonathan Chang's suggestion because <code>stripchart</code> does proper stacking of points.</p>
"
1938731,170352,2009-12-21T07:43:03Z,1934670,2,FALSE,"<p>Thank you for the response Ian, I realize the question itself wasn't very well formed but I was having difficulty explaining what I wanted. With your assistance, I've been able to put it together: </p>

<pre><code>top &lt;- subset(data, data$column &gt; quantile(data$column, 0.85))    
bottom &lt;- subset(data, data$column &lt; quantile(data$column, 0.15))
listing &lt;- rbind(top,bottom)
label &lt;- 1:nrow(listing)
listing[sort.list(listing$Distance, decreasing=T),]
</code></pre>
"
1939524,170352,2009-12-21T11:16:29Z,1939492,1,TRUE,"<p>Adding dev.off() worked. </p>

<pre><code>for(x in 250) { 

plots &lt;- ggplot(data=dat, aes(x,y,lab=labels))
jpeg(a_paste_function)
print(plots)
dev.off()
}
</code></pre>
"
1939706,172261,2009-12-21T11:55:09Z,1939098,8,TRUE,"<p>I'm not definitely sure if this is what you were looking for:</p>

<pre><code>library(""ggplot2"")
tmp &lt;- data.frame(x=-5:5, y=rnorm(11), lab=LETTERS[1:11])
p &lt;- ggplot(aes(x=x, y=y, label=lab), data=tmp) +
     geom_point() +
     geom_text(data=subset(tmp, x &gt; 0), hjust=-0.5) +
     geom_text(data=subset(tmp, x &lt;= 0), hjust=1.5)
print(p)
</code></pre>

<p><a href=""http://img8.imageshack.us/img8/1056/geomtext.png"">geom_text labeling http://img8.imageshack.us/img8/1056/geomtext.png</a></p>
"
1950683,237354,2009-12-23T04:25:36Z,1950608,1,FALSE,"<p>Pseudo Code to start with:</p>

<pre><code>For Each ID
  If Previous month data Exists 
    compute Diff
  Else diff = 0
return diff

For Each Month
  Max 100 (Positive)
  Min 100 (Negative)
</code></pre>

<hr>

<pre><code>#Realish Code
dataset$diff &lt;- lappply(dataset,function(ID,month,value){IF dataset[month-1] = TRUE{value-(value[month-1]})})
#This gets tricky since you need to know the month and what the previous month is in a format you can test
</code></pre>
"
1950941,158065,2009-12-23T06:04:15Z,1950608,4,FALSE,"<p>Ok, first some code to generate some test data.  This makes 100 random IDs and for each one chooses 20 months from a 2 year period along with random values.  The order is then shuffled for extra fun.</p>

<pre><code>## Generate some IDs
ids &lt;- sample(1000, 100)

## Generate the data
data &lt;- do.call(rbind,
                lapply(ids,
                       function(id)
                       data.frame(ID = id,
                                  Date = sample(as.Date(paste(rep(c(2008:2009), each=12),
                                    1:12, 1, sep=""-"")),
                                    20),
                                  Value = sample(1000, 20))))

## Shuffle
data &lt;- data[sample(nrow(data), nrow(data)),]
</code></pre>

<p>Here's what it looks like for me:</p>

<pre><code>&gt; head(data)
      ID       Date Value
1007 205 2008-07-01   235
1391 840 2008-12-01   509
918  278 2009-12-01   951
1213 945 2009-03-01   842
1369 766 2009-07-01   555
798  662 2008-12-01   531
</code></pre>

<p>Ok, now let's iterate over IDs and find the diff for each month for each ID.  Before that, let's convert the month to a number so it'll be easier to take differences (this is a bit unclean, does anyone know a better way to do arithmetic on Date objects?).  This just does <code>year * 12 + month</code> so that normal arithmetic works:</p>

<pre><code>data$Month &lt;- as.POSIXlt(data$Date)$mon + as.POSIXlt(data$Date)$year * 12
</code></pre>

<p>Now compute the differences:</p>

<pre><code>by.id &lt;- by(data, data$ID, function(x) {
  ## Sort by month.
  x &lt;- x[order(x$Month),]
  ## Compute the month and value differences, taking care to pad the edge case.
  data.frame(ID=x$ID,
             Date = x$Date,             
             Month.diff=c(0, diff(x$Month)),
             Value.diff=c(0,diff(x$Value)))
})
by.id &lt;- do.call(rbind, by.id)
</code></pre>

<p>Here's what the result looks like:</p>

<pre><code>&gt; head(by.id)
    ID       Date Month.diff Value.diff
4.1  4 2008-02-01          0          0
4.2  4 2008-03-01          1        123
4.3  4 2008-05-01          2        -94
4.4  4 2008-06-01          1       -243
4.5  4 2008-08-01          2       -327
4.6  4 2008-10-01          2        656
</code></pre>

<p>If the difference between consecutive months was greater than 1, then the months were not adjacent and we should set their values to zero.</p>

<pre><code>by.id$Value.diff &lt;- ifelse(by.id$Month.diff == 1,
                           by.id$Value.diff,
                           0)
</code></pre>

<p>Finally, we iterate by month and take the top and bottom N differences (I'll set N to 10 here rather than 100 since my test data set is rather small).</p>

<pre><code>by.month &lt;- by(by.id, by.id$Date, function(x) {
  ## Sort the data in each month
  x &lt;- x[order(x$Value.diff),]
  ## Take the top and bottom and label them accordingly.
  cbind(rbind(head(x, 10), tail(x, 10)),
        type=rep(c(""min"", ""max""), each=10))
})
</code></pre>

<p>And there we have it.  Here's an example result:</p>

<pre><code>&gt; by.month[[24]]
        ID       Date Month.diff Value.diff type
130.20 130 2009-12-01          1       -951  min
415.20 415 2009-12-01          1       -895  min
662.20 662 2009-12-01          1       -878  min
107.20 107 2009-12-01          1       -744  min
824.20 824 2009-12-01          1       -731  min
170.20 170 2009-12-01          1       -719  min
502.20 502 2009-12-01          1       -714  min
247.20 247 2009-12-01          1       -697  min
789.20 789 2009-12-01          1       -667  min
132.20 132 2009-12-01          1       -653  min
64.20   64 2009-12-01          1        622  max
82.20   82 2009-12-01          1        647  max
381.20 381 2009-12-01          1        698  max
303.20 303 2009-12-01          1        700  max
131.20 131 2009-12-01          1        751  max
221.20 221 2009-12-01          1        765  max
833.20 833 2009-12-01          1        791  max
806.20 806 2009-12-01          1        806  max
780.20 780 2009-12-01          1        843  max
912.20 912 2009-12-01          1        929  max
</code></pre>
"
1951473,16632,2009-12-23T08:49:22Z,1950608,7,TRUE,"<p>Start by adding in all missing months:</p>

<pre><code>all_combs &lt;- expand.grid(
  ID = unique(data$ID),
  Date = unique(data$Date))

data &lt;- merge(data, all_combs, by = c(""ID"", ""Date""), all = T)
# Ensure data ordered by date
data &lt;- data[with(data, order(ID, Date)), ]
</code></pre>

<p>Then add a column of deltas (calculated with diff)</p>

<pre><code>library(plyr)
data &lt;- ddply(data, ""ID"", transform, delta = c(NA, diff(Value)))
</code></pre>

<p>Finally, remove missing deltas, order by their value and extract the top and bottom 10 within each group.</p>

<pre><code>changed &lt;- subset(data, !is.na(delta))
changed &lt;- changed[with(changed, order(ID, delta)), ]

# Select top 100 for each
top10 &lt;- ddply(changed, ""ID"", function(df) {
 rbind(head(df, 10), tail(df, 10))
})
</code></pre>
"
1952409,74658,2009-12-23T12:04:09Z,1476585,5,TRUE,"<p>OK, I actually had to try this, after looking at various things I wrote a function to find the roots of a quadratic equation.</p>

<pre><code>invquad&lt;-function(a,b,c,y,roots=""both"", xmin=(-Inf), xmax=(Inf),na.rm=FALSE){
#Calculate the inverse of a quadratic function y=ax^2+bx+c (ie find x when given y)
#Gives NaN with non real solutions
root1&lt;-sqrt((y-(c-b^2/(4*a)))/a)-(b/(2*a))
root2&lt;--sqrt((y-(c-b^2/(4*a)))/a)-(b/(2*a))
if (roots==""both"") {
    root1&lt;-ifelse(root1&lt;xmin,NA,root1)  
    root1&lt;-ifelse(root1&gt;xmax,NA,root1)  
    root2&lt;-ifelse(root2&lt;xmin,NA,root2)  
    root2&lt;-ifelse(root2&gt;xmax,NA,root2)      
    result&lt;-c(root1,root2)
    if (na.rm) result&lt;-ifelse(is.na(root1),root2, result)
    if (na.rm) result&lt;-ifelse(is.na(root2),root1,result)
    if (na.rm) result&lt;-ifelse(is.na(root1)&amp;is.na(root2),NA,result)
},roots=""both""
if (roots==""min"")
    result&lt;-pmin(root1,root2, NA.rm=TRUE)
if (roots==""max"")
    result&lt;-pmax(root1,root2, NA.rm=TRUE)
result
}
</code></pre>

<p>so, given the original data</p>

<pre><code>&gt; PAR
[1] 0.0000125000 0.0009603333 0.0020583333 0.0042333333 0.0322333333 0.6144333333
[7] 1.2433333333 1.8633333333
&gt; Nominal
[1]   1   3   6  10  30  50 150 250
</code></pre>

<p>we can do the analysis, find the co-efficients and then find the inverse, using some sensible limits for what Nominal values we expect back...</p>

<pre><code>lm(PAR~Nominal+I(Nominal^2))-&gt;bob
&gt; bob[[1]][[3]]
[1] -1.166904e-05 # Nominal^2
&gt; bob[[1]][[2]]
[1] 0.01061094 # Nominal
&gt; bob[[1]][[1]]
[1] -0.06395298 # Intercept
&gt; invquad(bob[[1]][[3]],bob[[1]][[2]],bob[[1]][[1]],y=PAR,xmin=-0.2,xmax=300,na.rm=TRUE)
[1]   6.068762   6.159306   6.264217   6.472106   9.157041  69.198703 146.949154
[8] 250.811211
</code></pre>

<p>Hope this helps....</p>
"
1953325,143305,2009-12-23T15:08:47Z,1953256,1,FALSE,"<p>I always use </p>

<pre><code>pdf(file=""/tmp/foo.pdf"", width=10.5, height=8)
plot(....)
dev.off()
</code></pre>

<p>which nicely fills the page with just 1/2 inch spacing.  You can flip width and height arguments to go to 'portrait' rather than 'landscape'.</p>
"
1955386,143305,2009-12-23T21:21:34Z,1955330,5,TRUE,"<p>The <a href=""http://cran.r-project.org/web/views/"" rel=""nofollow noreferrer"">CRAN Task Views</a> are reasonable starting points.  So in order</p>

<ul>
<li><p>Image processing: see <a href=""http://cran.r-project.org/web/views/Graphics.html"" rel=""nofollow noreferrer"">Graphics</a> and <a href=""http://cran.r-project.org/web/views/MedicalImaging.html"" rel=""nofollow noreferrer"">MedicalImaging</a></p></li>
<li><p>File compression: accessible from Base R, so try <code>help(connection)</code> </p></li>
<li><p>Use APIs: you will need to ask that question again, if you mean language bindings: yes, plenty, though no one single page for all</p></li>
<li><p>Interact with Internet:  see above on <code>help(connection)</code>, there are also packages that wrap curl, provide SOAP and of course the XML package.</p></li>
</ul>

<p><em>Edit:</em> And I forgot to stress that <a href=""http://www.r-project.org"" rel=""nofollow noreferrer"">R</a> as a <em>statistical language and environment</em> is more domain-specific than either Ruby or Python so the comparisons aren't entirely appropriate. But you can also code Gtk2 guis in R if you feel like it...</p>
"
1956393,163053,2009-12-24T02:34:45Z,1956337,1,FALSE,"<p>One suggestion:</p>

<p>Try running order() on the first column, and then swap every two rows by creating an index on the odd and even indexes separately, and assigning them respectively to a new vector. </p>
"
1959217,160314,2009-12-24T17:54:29Z,1956337,2,TRUE,"<p>The Deducer package has a nice sorting function for data.frames:</p>

<pre><code>&gt; library(Deducer)
&gt; text&lt;-c(""a"",""b"",""c"",""d"",""a 09"",""b 09"",""c 09"",""d 09"")
&gt; nums&lt;-c(5,7,8,9,10,3,2,1)
&gt; t1&lt;-sapply(text,function(x) strsplit(x,"" "")[[1]][1])
&gt; t2&lt;-sapply(text,function(x) strsplit(x,"" "")[[1]][2])
&gt; dat&lt;-data.frame(text,nums,t1,t2)
&gt; sort(dat,by=~-t1 -t2)
     text nums t1   t2
d       d    9  d &lt;NA&gt;
d 09 d 09    1  d   09
c       c    8  c &lt;NA&gt;
c 09 c 09    2  c   09
b       b    7  b &lt;NA&gt;
b 09 b 09    3  b   09
a       a    5  a &lt;NA&gt;
a 09 a 09   10  a   09
</code></pre>
"
1959526,66549,2009-12-24T19:46:27Z,1395517,3,TRUE,"<p>I researched this topic about six months ago. Here's what i found (which i checked/updated upon viewing your question):</p>

<p>There is <a href=""http://code.google.com/p/r-connect/"" rel=""nofollow noreferrer"">one package</a> i'm aware of directed specifically/exclusively to AOP (and which predictably is related to event monitoring/logging)--called '<strong>r-connect</strong>.' Not available in CRAN as far as i know. At the moment i haven't used it enough to give you any useful feedback. For svn checkout:</p>

<p>""svn checkout <a href=""http://r-connect.googlecode.com/svn/trunk/"" rel=""nofollow noreferrer"">http://r-connect.googlecode.com/svn/trunk/</a> r-connect-read-only""</p>
"
1960131,163053,2009-12-25T00:10:24Z,1960119,17,TRUE,"<p>Yes. Use the <code>XML</code> package. There's a function called xmlToDataFrame which will make your life easy. </p>
"
1960133,90777,2009-12-25T00:11:22Z,1960119,0,FALSE,"<p>You might find <a href=""http://www.omegahat.org/RSXML/"" rel=""nofollow noreferrer"">RSXML</a> useful. It's also in <a href=""http://cran.r-project.org/web/packages/XML/index.html"" rel=""nofollow noreferrer"">CRAN</a>.</p>
"
1961272,143305,2009-12-25T14:50:43Z,1961109,5,TRUE,"<p>R has been ported to a number of devices.  Learning to use a search engine (as has been suggested to Nathan many times) may be beneficial -- and <a href=""http://www.rseek.org"" rel=""nofollow noreferrer"">rseek.org</a> (using the 'support lists' tab) almost immediately leads me to </p>

<ul>
<li><a href=""https://stat.ethz.ch/pipermail/r-help/2004-December/062659.html"" rel=""nofollow noreferrer"">R on Sharp Zaurus</a> post from 2004</li>
<li><a href=""https://stat.ethz.ch/pipermail/r-sig-mac/2008-March/004687.html"" rel=""nofollow noreferrer"">R on iPhone</a> post from 2008 confirming you can build it, albeit sans GUI</li>
<li><a href=""http://tolstoy.newcastle.edu.au/R/e8/devel/09/10/0142.html"" rel=""nofollow noreferrer"">More on R on iPhone</a> in the context of an <em>R on Android</em> question</li>
</ul>

<p>Nathan, may I suggest that you do a bit more homework and learn to Google first?</p>
"
1961956,143305,2009-12-25T21:06:07Z,1961948,3,TRUE,"<p>The <code>as.*</code> functions are already vectorized:</p>

<pre><code>R&gt; txt &lt;- c(""123456"", ""7890123"")
R&gt; as.numeric(txt)
[1]  123456 7890123
R&gt; sum(as.numeric(txt))
[1] 8013579
R&gt; 
</code></pre>

<p>In general, 'thinking vectorized' is the way to go with R, but it takes some practice. </p>
"
1962336,143305,2009-12-26T00:45:41Z,1962278,55,TRUE,"<p>You want the (standard) <code>POSIXt</code> type from base R that can be had in 'compact form' as a <code>POSIXct</code> (which is essentially a double representing fractional seconds since the epoch) or as long form in <code>POSIXlt</code> (which contains sub-elements).  The cool thing is that arithmetic etc are defined on this -- see <code>help(DateTimeClasses)</code></p>

<p>Quick example:</p>

<pre><code>R&gt; now &lt;- Sys.time()
R&gt; now
[1] ""2009-12-25 18:39:11 CST""
R&gt; as.numeric(now)
[1] 1.262e+09
R&gt; now + 10  # adds 10 seconds
[1] ""2009-12-25 18:39:21 CST""
R&gt; as.POSIXlt(now)
[1] ""2009-12-25 18:39:11 CST""
R&gt; str(as.POSIXlt(now))
 POSIXlt[1:9], format: ""2009-12-25 18:39:11""
R&gt; unclass(as.POSIXlt(now))
$sec
[1] 11.79

$min
[1] 39

$hour
[1] 18

$mday
[1] 25

$mon
[1] 11

$year
[1] 109

$wday
[1] 5

$yday
[1] 358

$isdst
[1] 0

attr(,""tzone"")
[1] ""America/Chicago"" ""CST""             ""CDT""            
R&gt; 
</code></pre>

<p>As for reading them in, see <code>help(strptime)</code> </p>

<p>As for difference, easy too:</p>

<pre><code>R&gt; Jan1 &lt;- strptime(""2009-01-01 00:00:00"", ""%Y-%m-%d %H:%M:%S"")
R&gt; difftime(now, Jan1, unit=""week"")
Time difference of 51.25 weeks
R&gt; 
</code></pre>

<p>Lastly, the <a href=""http://cran.r-project.org/web/packages/zoo"" rel=""noreferrer"">zoo package</a> is an extremely versatile and well-documented container for matrix with associated date/time indices.</p>
"
1963055,144157,2009-12-26T10:19:06Z,1962954,8,FALSE,"<p>I tend to use png files rather than vector based graphics such as pdf or eps for this situation. The files are much smaller, although you lose resolution.</p>

<p>If it's a more conventional scatterplot, then using semi-transparent colours also helps, as well as solving the over-plotting problem. For example,</p>

<pre><code>x &lt;- rnorm(10000); y &lt;- rnorm(10000)
qplot(x, y, colour=I(alpha(""blue"",1/25)))
</code></pre>
"
1963471,143305,2009-12-26T14:14:17Z,1962954,5,FALSE,"<p>Beyond Rob's suggestions, one plot function I like as it does the 'thinning' for you is <a href=""http://cran.r-project.org/web/packages/hexbin/"" rel=""noreferrer"">hexbin</a>; an example is <a href=""http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=143"" rel=""noreferrer"">at the R Graph Gallery</a>.</p>
"
1963520,143305,2009-12-26T14:40:00Z,1963492,42,FALSE,"<p>See <code>help(grid)</code> which works with standard graphics -- short example:</p>

<pre><code>R&gt; set.seed(42)
R&gt; plot(cumsum(rnorm(100)), type='l')
R&gt; grid()
</code></pre>

<p>The <a href=""http://cran.r-project.org/web/packages/ggplot2"" rel=""noreferrer"">ggplot2</a> package defaults to showing grids due to its 'Grammar of Graphics' philosophy.  And <a href=""http://cran.r-project.org/web/packages/lattice"" rel=""noreferrer"">lattice</a> has a function <code>panel.grid()</code> you can use in custom panel functions.</p>

<p>By the way, there are search functions for help as e.g. <code>help.search(""something"")</code> and there is an entire package called <a href=""http://cran.r-project.org/web/packages/sos"" rel=""noreferrer"">sos</a> to make R web searches more fruitful. </p>
"
1964038,162436,2009-12-26T18:53:14Z,1962954,4,TRUE,"<p>Here is one possible solution for downsampling plot with respect to the x-axis, if it is log transformed.  It log transforms the x-axis, rounds that quantity, and picks the median x value in that bin:</p>

<pre><code>downsampled_qplot &lt;- function(x,y,data,rounding=0, ...) {
  # assumes we are doing log=xy or log=x
  group = factor(round(log(data$x),rounding))
  d &lt;- do.call(rbind, by(data, group, 
    function(X) X[order(X$x)[floor(length(X)/2)],]))
  qplot(x,count,data=d, ...)
}
</code></pre>

<p>Using the definition of <code>ccdf()</code> from above, we can then compare the original plot of the CCDF of the distribution with the downsampled version:</p>

<pre><code>myccdf=ccdf(rlnorm(10000,3,2.4))

qplot(x,count,data=myccdf,log='xy',main='original')
</code></pre>

<p><img src=""https://i.stack.imgur.com/2Tt9P.jpg"" alt=""""></p>

<pre><code>downsampled_qplot(x,count,data=myccdf,log='xy',rounding=1,main='rounding = 1')
</code></pre>

<p><img src=""https://i.stack.imgur.com/HglK0.jpg"" alt=""""></p>

<pre><code>downsampled_qplot(x,count,data=myccdf,log='xy',rounding=0,main='rounding = 0')
</code></pre>

<p><img src=""https://i.stack.imgur.com/EkuNh.jpg"" alt=""""></p>

<p>In PDF format, the original plot takes up 640K, and the downsampled versions occupy 20K and 8K, respectively. </p>
"
1965352,57458,2009-12-27T06:23:24Z,1962954,2,FALSE,"<p>I'd either make image files (png or jpeg devices) as <a href=""https://stackoverflow.com/users/144157/rob-hyndman"">Rob</a> already mentioned, or I'd make a <a href=""http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=70"" rel=""nofollow noreferrer"">2D histogram.</a> An alternative to the 2D histogram is a <a href=""http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=139"" rel=""nofollow noreferrer"">smoothed scatterplot</a>, it makes a similar graphic but has a more smooth cutoff from dense to sparse regions of space.</p>

<p>If you've never seen <a href=""http://addictedtor.free.fr/"" rel=""nofollow noreferrer"">addictedtor</a> before, it's worth a look. It has some very nice graphics generated in R with images and sample code.</p>

<p>Here's the sample code from the <a href=""http://addictedtor.free.fr/"" rel=""nofollow noreferrer"">addictedtor</a> site:</p>

<p>2-d histogram:</p>

<pre><code>require(gplots) 

# example data, bivariate normal, no correlation
x &lt;- rnorm(2000, sd=4) 
y &lt;- rnorm(2000, sd=1) 

# separate scales for each axis, this looks circular
hist2d(x,y, nbins=50, col = c(""white"",heat.colors(16))) 
rug(x,side=1) 
rug(y,side=2) 
box() 
</code></pre>

<p>smoothscatter:</p>

<pre><code>library(""geneplotter"")  ## from BioConductor
require(""RColorBrewer"") ## from CRAN

x1  &lt;- matrix(rnorm(1e4), ncol=2)
x2  &lt;- matrix(rnorm(1e4, mean=3, sd=1.5), ncol=2)
x   &lt;- rbind(x1,x2)

layout(matrix(1:4, ncol=2, byrow=TRUE))
op &lt;- par(mar=rep(2,4))
smoothScatter(x, nrpoints=0)
smoothScatter(x)
smoothScatter(x, nrpoints=Inf,
              colramp=colorRampPalette(brewer.pal(9,""YlOrRd"")),
              bandwidth=40)
colors  &lt;- densCols(x)
plot(x, col=colors, pch=20)

par(op)
</code></pre>
"
1965773,144157,2009-12-27T11:29:16Z,1951070,3,TRUE,"<p>Finally found the problem. I had set the variable <code>.Library.site</code> in the file <code>Rprofile.site</code> on one machine to point to a non-existent directory. It never affected R for normal use, but showed up when I tried to compile a package.</p>

<p>Thanks for the suggestions.</p>
"
1967706,199166,2009-12-28T02:12:19Z,1963492,51,FALSE,"<p>The <em>grid</em> command seems to draw grid lines where-ever it feels like. I usually use <em>abline</em> to put lines exactly where I want them. For example,</p>

<pre><code>abline(v=(seq(0,100,25)), col=""lightgray"", lty=""dotted"")
abline(h=(seq(0,100,25)), col=""lightgray"", lty=""dotted"")
</code></pre>

<p>Good luck!</p>
"
1971391,143305,2009-12-28T20:27:23Z,1971323,11,FALSE,"<p>These things do pop up on the lists, in particular on r-devel.  One fairly well-established nugget is that e.g. <code>matrix</code> operations tend to be faster than <code>data.frame</code> operations.  Then there are add-on packages that do well -- Matt's <a href=""http://cran.r-project.org/web/packages/data.table"" rel=""noreferrer"">data.table</a> package is pretty fast, and Jeff has gotten <a href=""http://cran.r-project.org/web/packages/xts"" rel=""noreferrer"">xts</a> indexing to be quick.  </p>

<p>But it ""all depends"" -- so you are usually best adviced to <em>profile on your particular code</em>. <code>R</code> has plenty of profiling support, so you should use it.  My <a href=""http://dirk.eddelbuettel.com/presentations.html"" rel=""noreferrer""><em>Intro to HPC with R</em> tutorials</a> have a number of profiling examples. </p>
"
1971399,163053,2009-12-28T20:28:00Z,1971323,6,FALSE,"<p>I will try to come back and provide more detail.  If you have any question about the efficiency of one operation over another, you would do best to profile your own code (as Dirk suggests).  The <code>system.time()</code> function is the easiest way to do this although there are many more advanced utilities (e.g. Rprof, as documented <a href=""http://wiki.r-project.org/rwiki/doku.php?id=tips:misc:profiling"" rel=""noreferrer"">here</a>).   </p>

<p>A quick response for the second part of your question:</p>

<blockquote>
  <p>What about the various flavors of apply? Are those just hidden loops? </p>
</blockquote>

<p>For the most part yes, the apply functions are just loops and can be slower than <code>for</code> statements.  Their chief benefit is clearer code.  The main exception that I have found is <code>lapply</code> which can be faster because it is coded in C directly.</p>

<blockquote>
  <p>And what about matrices vs. data frames?</p>
</blockquote>

<p>Matrices are more efficient than data frames because they require less memory for storage.  This is because data frames require additional attribute data.  From <a href=""http://cran.r-project.org/doc/manuals/R-intro.html#Data-frames"" rel=""noreferrer"">R Introduction</a>:</p>

<blockquote>
  <p>A data frame may for many purposes be regarded as a matrix with columns possibly of differing modes and attributes</p>
</blockquote>
"
1971499,163053,2009-12-28T20:52:38Z,1971461,4,TRUE,"<p>Here's how you could use the <code>lag()</code> function with <code>zoo</code> (and panel series data):</p>

<pre><code>&gt; library(plm)
&gt; library(zoo)
&gt; data(""Produc"")
&gt; dnow &lt;- pdata.frame(Produc)
&gt; x.Date &lt;- as.Date(paste(rownames(t(as.matrix(dnow$pcap))), ""-01-01"", sep=""""))
&gt; x &lt;- zoo(t(as.matrix(dnow$pcap)), x.Date)
&gt; x[1:3,1:3]
            ALABAMA  ARIZONA ARKANSAS
1970-01-01 15032.67 10148.42  7613.26
1971-01-01 15501.94 10560.54  7982.03
1972-01-01 15972.41 10977.53  8309.01
</code></pre>

<p>Lag forward by 1:</p>

<pre><code>&gt; lag(x[1:3,1:3],1)
            ALABAMA  ARIZONA ARKANSAS
1970-01-01 15501.94 10560.54  7982.03
1971-01-01 15972.41 10977.53  8309.01
</code></pre>

<p>Lag backward by 1:</p>

<pre><code>&gt; lag(x[1:3,1:3],k=-1)
            ALABAMA  ARIZONA ARKANSAS
1971-01-01 15032.67 10148.42  7613.26
1972-01-01 15501.94 10560.54  7982.03
</code></pre>

<p>As Dirk mentioned, be careful with the meaning of lag in the different time series packages.  Notice how <code>xts</code> treats this differently:</p>

<pre><code>&gt; lag(as.xts(x[1:3,1:3]),k=1)
            ALABAMA  ARIZONA ARKANSAS
1970-01-01       NA       NA       NA
1971-01-01 15032.67 10148.42  7613.26
1972-01-01 15501.94 10560.54  7982.03
</code></pre>
"
1971642,143377,2009-12-28T21:28:45Z,1971461,4,FALSE,"<p>For cross-sectional time-series data the package <a href=""http://cran.r-project.org/web/packages/plm/"" rel=""nofollow noreferrer"">plm</a> is very useful. It has a lag function that takes into account the panel nature of the data.</p>

<pre><code>library(plm)
data(""Produc"", package=""plm"")
dnow &lt;- pdata.frame(Produc)
head(lag(dnow$pcap,1))
             ALABAMA-1970 ALABAMA-1971 ALABAMA-1972 ALABAMA-1973 ALABAMA-1974 
          NA     15032.67     15501.94     15972.41     16406.26     16762.67 
</code></pre>

<p>One problem with the package is that using with (or within or transform) gives you the wrong answer.</p>

<pre><code>head(with(dnow, lag(pcap,1)))
15032.67 15501.94 15972.41 16406.26 16762.67 17316.26
</code></pre>

<p>So be careful.</p>
"
1974151,143305,2009-12-29T11:24:57Z,1973360,1,TRUE,"<p>Try this:</p>

<pre><code># I'd avoid new as a variable name
newdata &lt;- sqlQuery(newconn,""SELECT COL1, COL2 FROM TABLE1;"", errors = TRUE, 1)

# index data frame by row number and column name

if (newdata[3, ""COL1""] == ""someValue"") {
     print(""found someValue"")
} else {
     print (""failed"")
}
</code></pre>

<p>You can also do</p>

<pre><code>if (newdata[3, 2] == ""MYSTRING"")
</code></pre>

<p>to index by row and column index.  </p>

<p>Lastly, testing for NA is different than string comparison -- you need <code>is.null()</code> or <code>is.na()</code> as this may get converted by the ODBC access.</p>
"
1974986,143305,2009-12-29T14:45:44Z,1974930,0,FALSE,"<p>Maybe I am missing something here. The cost of single function(i,j,data) call should be invariant to whether it is called in a <code>for</code>-loop or via <code>foreach</code>.   Can you not try <code>foreach</code> in serial mode, then maybe try with <code>multicore</code> (unless you're on Windows) and go from there?</p>
"
1975077,163053,2009-12-29T15:01:43Z,1974998,2,TRUE,"<p>I can't test this without sample data, but this is the general idea.  You can avoid doing the <code>for()</code> loop entirely by using <code>&amp;</code> and <code>|</code> which are vectorized versions of <code>&amp;&amp;</code> and <code>||</code>.  Also, there's no need for an if-else statement if there's only one value (true or false).</p>

<pre><code>faultFinging &lt;- function(heartData){
    Group &lt;- as.numeric(c(heartData$Pulse[1] != 0,
      (heartData$Pulse[-nrow(heartData)] != 0 
        &amp; heartData$Pulse[-1] != 0
        &amp; abs(heartData$Pulse[-nrow(heartData)] - heartData$Pulse[-1])&lt;20) |
      (heartData$Pulse[-nrow(heartData)] == 0 &amp; heartData$Pulse[-1] != 0)))
    return(cbind(heartData, Group))
}
</code></pre>

<p>Putting <code>as.numeric()</code> around the index will set TRUE to 1 and FALSE to 0.</p>
"
1975132,163053,2009-12-29T15:14:13Z,1975110,0,FALSE,"<p>I think that you will need to use <code>tryCatch()</code>.  You can do whatever you want in the tryCatch() function, so it's not clear to me why you are viewing this as complex.  Maybe post your code example?</p>
"
1975134,143305,2009-12-29T15:14:25Z,1975110,4,FALSE,"<p>Have you tried the </p>

<pre><code> options(error=recover)
</code></pre>

<p>setting?  Chambers 'Software for Data Analysis' has some useful hints on debugging.</p>
"
1976103,158065,2009-12-29T18:27:31Z,1974930,2,FALSE,"<p>This is not an answer, but I thought I'd post some test results in hopes that someone else will know what's going on:</p>

<pre><code>&gt; data &lt;- matrix(rnorm(1000 * 10000), nrow=10000)
&gt; system.time(foreach(j=1:1000, .combine = function(...) NULL, .multicombine=TRUE) %do% { sum(data[,j]) })
utilisateur     système      écoulé 
      0.643       0.031       0.674 
&gt; system.time(foreach(j=1:1000, .combine = function(...) NULL, .multicombine=TRUE) %dopar% { sum(data[,j]) })
utilisateur     système      écoulé 
      0.613       0.215       0.653 
&gt; system.time(foreach(j=1:1000) %dopar% { sum(data[,j]) })
utilisateur     système      écoulé 
      0.537       0.122       0.745 
&gt; system.time(foreach(j=1:1000) %do% { sum(data[,j]) })
utilisateur     système      écoulé 
      0.650       0.028       0.681 
&gt; system.time (for (j in 1:1000) { sum(data[,j]) })
utilisateur     système      écoulé 
      0.153       0.069       0.222 
</code></pre>

<p>In short, using the builtin <code>for</code> is still way faster than serial <code>foreach</code>.  You don't really win by using <code>dopar</code>, and it doesn't seem that putting everything together is what's taking all the time (it could still be that transmitting the data back to the master takes a long time).  You can also argue that with computation this simple, the overhead will naturally dominate.  So let's do some more complicated stuff:</p>

<pre><code>&gt; data &lt;- matrix(rnorm(3000 * 10000), nrow=10000)
&gt; system.time (for(j in 1:6000) { sum(lgamma(exp(data[,(j - 1) %% 3000 + 1]))) })
utilisateur     système      écoulé 
     11.215       1.272      12.490 
&gt; system.time (foreach(j=1:6000, .combine=c) %do% { sum(lgamma(exp(data[,(j - 1) %% 3000 + 1]))) })
utilisateur     système      écoulé 
     14.304       0.468      15.788
&gt; system.time (foreach(j=1:6000, .combine=c) %dopar% { sum(lgamma(exp(data[,(j - 1) %% 3000 + 1]))) })
utilisateur     système      écoulé 
     14.377      11.839      10.358 
</code></pre>

<p>Now <code>dopar</code> is starting to win out but the three are still pretty comparable and the builtin <code>for</code> is not so bad, even with all the additional work.  But what about communication overhead? Instead of taking sums, we'll just return the transformed data (10,000 numbers per iteration).  </p>

<pre><code>&gt; system.time (for(j in 1:6000) { lgamma(exp(data[,(j - 1) %% 3000 + 1])) })
utilisateur     système      écoulé 
     11.092       1.189      12.302     
&gt; system.time (foreach(j=1:6000, .combine=function (...) NULL, .multicombine=TRUE) %do% { lgamma(exp(data[,(j - 1) %% 3000 + 1])) })
utilisateur     système      écoulé 
     14.902       1.867      22.901 
&gt; system.time (foreach(j=1:6000, .combine=function (...) NULL, .multicombine=TRUE) %dopar% { lgamma(exp(data[,(j - 1) %% 3000 + 1])) })

^C

Timing stopped at: 2.155 0.706 241.948 
&gt; 
</code></pre>

<p>Here, the <code>for</code> loop which doesn't have to bother with keeping results around takes about as long as before.  The <code>%do%</code> version took a lot longer this time. And the <code>%dopar%</code> which is probably transferring the results through shared memory?  I decided to kill it after about 4 minutes.</p>
"
1978172,143377,2009-12-30T02:54:27Z,1978006,4,TRUE,"<pre><code>a &lt;- data.frame(rep(1,5),1:5,1:5)
b &lt;- data.frame(rep(2,5),1:5,1:5)
colnames(b) &lt;- colnames(a) &lt;- paste(""a"", c(1,2,3), sep='')
d &lt;- rbind(a,b)
library(reshape)
recast(d, a2 ~ a1, measure.var=""a3"")
</code></pre>

<p>I changed your example slightly, since it had numbers as variable names. This is  not recommended because it permits the following nonsense:</p>

<pre><code> ""1"" &lt;- 3
print(1)
[1] 1
print(""1"")
[1] ""1""
print(`1`)
[1] 3
</code></pre>

<p>Need I say more?</p>
"
1982698,142454,2009-12-30T21:50:03Z,1982667,1,FALSE,"<p>One of the nicest things about R is that you can access a lot of the source code to R itself from within the environment.  If you simply type <code>arima</code> at the command prompt, you get the high-level source code for the <code>arima()</code> function.  I got several pages of code here, when I tried it.</p>

<p>You do miss out on anything implemented internally within the R executable in native code, but often the high-level code tells you everything you want to know.</p>
"
1983171,144157,2009-12-30T23:55:14Z,1982667,5,TRUE,"<p><code>arima()</code> will fit a regression model with ARMA errors. The constant is treated as the coefficient of a regression variable consisting only of 1s. So you need the covariance matrix of the regression coefficients which is usually calculated separately from the covariance matrix of the ARMA coefficients. Look at Section 8.3 of Hamilton's ""Time series analysis""</p>
"
1983446,170352,2009-12-31T01:10:21Z,1976728,1,TRUE,"<p>Assuming that all variables are categorical, you can use multiple classification analysis to gain an understanding of the associations between the variables. There was a good article on the topic from the European Consortium for Politics back in 2k7 but I can't find it on my drive, I'm sure google will have it somewhere. I can't ""see"" your data so I can't say with any certainty that MCA will be better than regression or GLM but the article I'm referring to has a discussion on this topic specifically to do with MCA vs. GLM vs. Regression.</p>

<p>Alternatively, you could use pearson product-moment correlations to identify the   coefficients. Close to 1 = positive linear relationship, close to -1 = negative linear relationship, close to 0 = no linear relationship.</p>
"
1991893,225468,2010-01-02T15:26:42Z,1395301,4,FALSE,"<p>I put the following line in front of my scripts and it allows me to work across my computers. </p>

<pre><code>setwd(path.expand(""~/path/to/working/directory/"") )
</code></pre>

<p>where ~ is = to your home directory. </p>

<p><code>Sys.setenv(HOME = ""path"")</code> or <code>Sys.setenv(R_USER = ""path"")</code> can both set the home directory.</p>

<p>In my case, I work on several windows boxes, each have fairly different directory structures, but by setting the home directory properly I can sync code between computers and have them run properly on each one since where I run my R projects have similar directory structures. </p>
"
1995984,143305,2010-01-03T19:25:53Z,1995933,32,TRUE,"<p>I was about to say that's simple, but <code>difftime()</code> stops at weeks. How odd. </p>

<p>So one possible answer would be to hack something up:</p>

<pre><code># turn a date into a 'monthnumber' relative to an origin
R&gt; monnb &lt;- function(d) { lt &lt;- as.POSIXlt(as.Date(d, origin=""1900-01-01"")); \
                          lt$year*12 + lt$mon } 
# compute a month difference as a difference between two monnb's
R&gt; mondf &lt;- function(d1, d2) { monnb(d2) - monnb(d1) }
# take it for a spin
R&gt; mondf(as.Date(""2008-01-01""), Sys.Date())
[1] 24
R&gt; 
</code></pre>

<p>Seems about right.  One could wrap this into some simple class structure. Or leave it as a hack :)</p>

<p><em>Edit:</em> Also seems to work with your examples from the Mathworks:</p>

<pre><code>R&gt; mondf(""2000-05-31"", ""2000-06-30"")
[1] 1
R&gt; mondf(c(""2002-03-31"", ""2002-04-30"", ""2002-05-31""), ""2002-06-30"")
[1] 3 2 1
R&gt; 
</code></pre>

<p>Adding the <code>EndOfMonth</code> flag is left as an exercise to the reader :)</p>

<p><em>Edit 2:</em> Maybe <code>difftime</code> leaves it out as there is no reliable way to express fractional difference which would be consistent with the <code>difftime</code> behavior for other units.</p>
"
1996404,242673,2010-01-03T21:31:39Z,1995933,5,FALSE,"<p>There is a message just like yours in the R-Help mailing list (previously I mentioned a CRAN list).</p>

<p>Here <a href=""http://tolstoy.newcastle.edu.au/R/help/04/11/7326.html"" rel=""nofollow noreferrer"">the link</a>. There are two suggested solutions:</p>

<ul>
<li>There are an average of 365.25/12 days per month so the following expression gives the number of months between d1 and d2:</li>
</ul>

<blockquote>
<pre><code>#test data 
d1 &lt;- as.Date(""01 March 1950"", ""%d %B %Y"")    
d2 &lt;- as.Date(c(""01 April 1955"", ""01 July 1980""), ""%d %B %Y"")
# calculation 
round((d2 - d1)/(365.25/12))
</code></pre>
</blockquote>

<ul>
<li>Another possibility is to get the length of <code>seq.Dates</code> like this:</li>
</ul>

<blockquote>
<pre><code>as.Date.numeric &lt;- function(x) structure(floor(x+.001), class = ""Date"")
sapply(d2, function(d2) length(seq(d1, as.Date(d2), by = ""month"")))-1
</code></pre>
</blockquote>
"
2000729,66549,2010-01-04T16:55:29Z,1866816,3,FALSE,"<p>Lattice is the graphics library most likely to be helpful here. I say that for two reasons: (i) lattice is based on the grid system, and by accessing grid's graphical primitives, you can get much finer control over, among other things, the location of your panel output; and (ii) there's more to work with--the R standard graphics package has 70 different parameters, while Lattice has 371--by my count anyway, (length(names(unlist(trellis.par.get())))), yet those 371 are not in a flat structure like they are in the base package, but instead are collected in a hierarchical structure (with 30 or so parameter groups at the top level).</p>

<p>What you want is relative positioning of your axis labels. I would recommend going down one level for this sort of task. So to do what you want, just change the relevant grob slots then just redraw the two grobs (using the R interactive prompt):</p>

<pre><code>library(lattice)
library(grid)
bwplot(~runif(200, 10, 99), xlab=""x-axis label"", ylab=""y-axis label"")
# move the x-axis label to the far left
grid.edit(""[.]xlab$"", grep=T, x=unit(0, ""npc""), just=""left"", redraw=T)
# move it to the far right
grid.edit(""[.]xlab$"", grep=T, x=unit(1, ""npc""), just=""right"", redraw=T)
# move it to the center
grid.edit(""[.]xlab$"", grep=T, x=unit(0.5, ""npc""), just=""center"", redraw=T)
# same for y-axis
grid.edit(""[.]ylab$"", grep=T, y=unit(0.5, ""npc""), just=""center"", redraw=T)
</code></pre>

<p>""[.].xlab$"", grid.edit takes a gPath object (just a path traversing a gTree, which is just a grob that contains other grobs); because i didn't know where in the gPath my object of interest resides (the x-axis/y-axis label, i used a regular expression form for the object;</p>

<p>""grep=T"", just tells grid.edit to treat the previous parameter as a regular expression;</p>

<p>""x=unit(0.5, 'npc')"", specifying viewport coordinates here (in this case, just the x value); 'npc' ('normalized parent coordinates', which is the default) treats the viewport origin as (0,0), and assigns it a width &amp; height of 1 unit each. Hence, i've specified the center of the viewport along the x axis.</p>
"
2000757,15050,2010-01-04T17:00:55Z,1975110,17,FALSE,"<p>I wrote this code about a week ago to help me track down errors that come primarily from non-interactive R sessions.  It's still a little rough, but it prints a stack trace and continues on.  Let me know if this is useful, I'd be interested in how you would make this more informative.  I'm also open into cleaner ways to get this information.</p>

<pre><code>options(warn = 2, keep.source = TRUE, error = 
  quote({ 
    cat(""Environment:\n"", file=stderr()); 

    # TODO: setup option for dumping to a file (?)
    # Set `to.file` argument to write this to a file for post-mortem debugging    
    dump.frames();  # writes to last.dump

    #
    # Debugging in R
    #   http://www.stats.uwo.ca/faculty/murdoch/software/debuggingR/index.shtml
    #
    # Post-mortem debugging
    #   http://www.stats.uwo.ca/faculty/murdoch/software/debuggingR/pmd.shtml
    #
    # Relation functions:
    #   dump.frames
    #   recover
    # &gt;&gt;limitedLabels  (formatting of the dump with source/line numbers)
    #   sys.frame (and associated)
    #   traceback
    #   geterrmessage
    #
    # Output based on the debugger function definition.

    n &lt;- length(last.dump)
    calls &lt;- names(last.dump)
    cat(paste(""  "", 1L:n, "": "", calls, sep = """"), sep = ""\n"", file=stderr())
    cat(""\n"", file=stderr())

    if (!interactive()) {
      q()
    }
  }))
</code></pre>

<p>PS: you might not want warn=2 (warnings converted to errors)</p>
"
2001477,26428,2010-01-04T19:06:23Z,2001441,0,FALSE,"<p>You have <code>path_in=</code> inside the double quotes, but <code>path_in2=</code> outside. Could this be the problem?</p>
"
2001492,143305,2010-01-04T19:08:55Z,2001441,1,TRUE,"<p>Replacing the <code>=</code> with the proper assignment operator <code>&lt;-</code> and protecting each argument with single quotes works for me:</p>

<pre><code>Rscript /tmp/RscriptArgs.R  \
  'path_in&lt;-""/Users/test/GR/web-app/Rproject/Inputs/Rmerge/Description.csv""'  \
  'path_in2&lt;-""/Users/test/IdeaProjects/Rproject/Inputs/Rmerge/Template_Auto.csv""'
</code></pre>

<p>where <code>/tmp/RscriptArgs.R</code> is what you showed from your script.</p>
"
2001544,103832,2010-01-04T19:21:32Z,2001441,0,FALSE,"<p>Thank you, I just fix my problem !</p>

<p>I should use ""path_in='/Users/test/...'"" and not ""path_in=/Users/test/..."".
Works fine with quote.</p>

<pre><code>Rscript ""/Users/test/Scripts/arg_test.R"" ""path_in='/Users/test/GR/web-app/Rproject/Inputs/Rmerge/Gene-level Description for Modules.csv'"" ""path_in2='/Users/test/IdeaProjects/Rproject/Inputs/Rmerge/Template_Auto.csv'""
</code></pre>

<p>Fix add by Dirk works fine too (thanks) !</p>
"
2001823,66549,2010-01-04T20:01:34Z,1971323,27,TRUE,"<p>Data IO was one of the features i looked into before i committed to learning R. For better or worse, here are my observations and solutions/palliatives on these issues:</p>

<p><strong>1.</strong> That <strong>R doesn't handle big data</strong> (>2 GB?) To me this is a misnomer. By default, the common data input functions load your data into RAM. Not to be glib, but to me, this is a feature not a bug--anytime my data will fit in my available RAM, that's where i want it. Likewise, one of SQLite's most popular features is the in-memory option--the user has the easy option of loading the entire dB into RAM. If your data won't fit in memory, then R makes it astonishingly easy to persist it, via connections to the common RDBMS systems (RODBC, RSQLite, RMySQL, etc.), via no-frills options like the filehash package, and via systems that current technology/practices (for instance, i can recommend <a href=""http://ff.r-forge.r-project.org/"" rel=""noreferrer"">ff</a>). In other words, the R developers have chosen a sensible (and probably optimal) default, from which it is very easy to opt out.</p>

<p><strong>2. The performance of read.table</strong> (read.csv, read.delim, et al.), the most common means for getting data into R, can be improved 5x (and often much more in my experience) just by opting out of a few of read.table's default arguments--the ones having the greatest effect on performance are mentioned in the R's Help (?read.table). Briefly, the R Developers tell us that if you provide values for the parameters 'colClasses', 'nrows', 'sep', and 'comment.char' (in particular, pass in '' if you know your file begins with headers or data on line 1), you'll see a significant performance gain. I've found that to be true.</p>

<p>Here are the snippets i use for those parameters: </p>

<p>To get the number of rows in your data file (supply this snippet as an argument to the parameter, 'nrows', in your call to read.table): </p>

<pre><code>as.numeric((gsub(""[^0-9]+"", """", system(paste(""wc -l "", file_name, sep=""""), intern=T))))
</code></pre>

<p>To get the classes for each column:</p>

<pre><code>function(fname){sapply(read.table(fname, header=T, nrows=5), class)}  
</code></pre>

<p>Note: You can't pass this snippet in as an argument, you have to call it first, then pass in the value returned--in other words, call the function, bind the returned value to a variable, and then pass in the variable as the value to to the parameter 'colClasses' in your call to read.table: </p>

<p><strong>3. Using Scan</strong>. With only a little more hassle, you can do better than that (optimizing 'read.table') by using 'scan' instead of 'read.table' ('read.table' is actually just a wrapper around 'scan'). Once again, this is very easy to do. I use 'scan' to input each column individually then build my data.frame inside R, i.e., df = data.frame(cbind(col1, col2,....)).</p>

<p><strong>4. Use R's Containers</strong> for persistence in place of ordinary file formats (e.g., 'txt', 'csv').  R's native data file '.RData' is a binary format that a little smaller than a compressed ('.gz') txt data file. You create them using <strong>save</strong>(, ). You load it back into the R namespace with <strong>load</strong>(). The difference in load times compared with 'read.table' is dramatic. For instance, w/ a 25 MB file (uncompressed size)</p>

<pre><code>system.time(read.table(""tdata01.txt.gz"", sep="",""))
=&gt;  user  system elapsed 
    6.173   0.245   **6.450** 

system.time(load(""tdata01.RData""))
=&gt; user  system elapsed 
    0.912   0.006   **0.912**   
</code></pre>

<p><strong>5. Paying attention to data types</strong> can often give you a performance boost and reduce your memory footprint. This point is probably more useful in getting data out of R. The key point to keep in mind here is that by default, numbers in R expressions are interpreted as double-precision floating point, e.g., > typeof(5)  returns ""double."" Compare the object size of a reasonable-sized array of each and you can see the significance (use object.size()). So coerce to integer when you can. </p>

<p>Finally, the 'apply' family of functions (among others) are not ""hidden loops"" or loop wrappers. They are loops implemented in C--big difference performance-wise. [edit: AWB has correctly pointed out that while 'sapply', 'tapply', and 'mapply' are implemented in C, 'apply' is simply a wrapper function.</p>
"
2002980,170792,2010-01-04T23:12:07Z,1976728,2,FALSE,"<p>Various classification or association rule mining algorithms could be of much help too. You could check the <a href=""http://www.cs.waikato.ac.nz/~ml/weka/"" rel=""nofollow noreferrer"">Weka</a> toolbench for machine learning and data mining. </p>
"
2003471,44330,2010-01-05T01:02:50Z,2003465,1,FALSE,"<p>Do you need all 3 roots or just one? If just one, I would think Newton's Method would work ok. If all 3 then it might be problematic in circumstances where two are close together.</p>
"
2003488,112950,2010-01-05T01:07:30Z,2003465,0,FALSE,"<p>The common methods are available: Newton's Method, Bisection Method, Secant, Fixed point iteration, etc. Google any one of them.</p>

<p>If you have a non-linear <em>system</em> on the other hand (e.g. a system on N polynomial eqn's in N unknowns), a method such as <a href=""http://en.wikipedia.org/wiki/Newton%27s_method#Nonlinear_systems_of_equations"" rel=""nofollow noreferrer"">high-order Newton</a> may be used.</p>
"
2003495,75170,2010-01-05T01:09:26Z,2003465,4,FALSE,"<p>The fastest known way (that I'm aware of) to find the real solutions a system of arbitrary polynomials in <em>n</em> variables is polyhedral homotopy. A detailed explanation is probably beyond a StackOverflow answer, but essentially it's a path algorithm that exploits the structure of each equation using toric geometries. Google will give you <a href=""http://scholar.google.com/scholar?hl=en&amp;q=polyhedral+homotopy+solve+polynomials&amp;btnG=Search&amp;as_sdt=2000&amp;as_ylo=&amp;as_vis=0"" rel=""nofollow noreferrer""><strong>a number of papers</strong></a>.</p>

<p>Perhaps this question is better suited for <a href=""http://mathoverflow.com""><strong>mathoverflow</strong></a>?</p>
"
2003578,104427,2010-01-05T01:29:30Z,2003465,6,FALSE,"<p>The numerical solution for doing this many times in a reliable, stable manner, involve: (1) Form the companion matrix, (2) find the eigenvalues of the companion matrix.</p>

<p>You may think this is a harder problem to solve than the original one, but this is how the solution is implemented in most production code (say, Matlab).</p>

<p>For the polynomial: </p>

<pre><code>p(t) = c0 + c1 * t + c2 * t^2 + t^3
</code></pre>

<p>the companion matrix is:</p>

<pre><code>[[0 0 -c0],[1 0 -c1],[0 1 -c2]]
</code></pre>

<p>Find the eigenvalues of such matrix; they correspond to the roots of the original polynomial.</p>

<p>For doing this very fast, download the singular value subroutines from LAPACK, compile them, and link them to your code. Do this in parallel if you have too many (say, about a million) sets of coefficients.</p>

<p>Notice that the coefficient of <code>t^3</code> is one, if this is not the case in your polynomials, you will have to divide the whole thing by the coefficient and then proceed.</p>

<p>Good luck.</p>

<p>Edit: Numpy and octave also depend on this methodology for computing the roots of polynomials. See, for instance, <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.roots.html"" rel=""nofollow noreferrer"">this link</a>.</p>
"
2003669,223783,2010-01-05T01:53:16Z,2003663,1,FALSE,"<p>try <code>paste(sqlString, collapse="" "")</code></p>
"
2003680,143305,2010-01-05T01:55:08Z,2003663,16,TRUE,"<p>The versatile <code>paste()</code> command can do that with argument <code>collapse=""""</code>:</p>

<pre><code>lines &lt;- readLines(""/tmp/sql.txt"")
lines
[1] ""SELECT TOP 100 "" "" setpoint, ""     "" tph ""           ""FROM rates""     

sqlcmd &lt;- paste(lines, collapse="""")
sqlcmd
[1] ""SELECT TOP 100  setpoint,  tph FROM rates""
</code></pre>
"
2003844,176995,2010-01-05T02:43:32Z,2003663,4,FALSE,"<p>Here's the final version of what I'm using. Thanks Dirk.</p>

<pre><code>fileconn&lt;-file(""sql.txt"",""r"")           
sqlString&lt;-readLines(fileconn)          
sqlString&lt;-paste(sqlString,collapse="""")
gsub(""\t"","""", sqlString)
library(RODBC)
sqlconn&lt;-odbcConnect(""RPM"")
results&lt;-sqlQuery(sqlconn,sqlString)
library(qcc)
tph &lt;- qcc(results$tphmean[1:50], type=""xbar.one"", ylim=c(4000,12000), std.dev=600)
close(fileconn)
close(sqlconn)
</code></pre>
"
2003983,197321,2010-01-05T03:24:57Z,2003663,0,FALSE,"<p>I use <code>sql &lt;- gsub(""\n"","""",sql)</code> and <code>sql &lt;- gsub(""\t"","""",sql)</code> together.</p>
"
2004284,NA,2010-01-05T05:08:35Z,1432867,0,FALSE,"<p>ggplot produces aesthetically pleasing graphs, but I don't have the gumption to try and publish any ggplot output yet.</p>

<p>Until the day comes, here is how I have been making the aforementioned graphs. I use a graphics package called 'gplots' in order to get the standard error bars (using data I've calculated already). Note that this code provides for two or more factors for each class/category. This requires the data to go in as a matrix and for the ""beside=TRUE"" command in the ""barplot2"" function to keep the bars from being stacked. </p>

<pre><code># Create the data (means) matrix
# Using the matrix accommodates two or more factors for each class

data.m &lt;- matrix(c(75,34,19, 39,90,41), nrow = 2, ncol=3, byrow=TRUE,
               dimnames = list(c(""Factor 1"", ""Factor 2""),
                                c(""Class A"", ""Class B"", ""Class C"")))

# Create the standard error matrix

error.m &lt;- matrix(c(12,10,7, 4,7,3), nrow = 2, ncol = 3, byrow=TRUE)

# Join the data and s.e. matrices into a data frame

data.fr &lt;- data.frame(data.m, error.m) 

# load library {gplots}

library(gplots)

# Plot the bar graph, with standard errors

with(data.fr,
     barplot2(data.m, beside=TRUE, axes=T, las=1, ylim = c(0,120),  
                main="" "", sub="" "", col=c(""gray20"",0),
                    xlab=""Class"", ylab=""Total amount (Mean +/- s.e.)"",
                plot.ci=TRUE, ci.u=data.m+error.m, ci.l=data.m-error.m, ci.lty=1))

# Now, give it a legend:

legend(""topright"", c(""Factor 1"", ""Factor 2""), fill=c(""gray20"",0),box.lty=0)
</code></pre>

<p>It is pretty plain-Jane, aesthetically, but seems to be what most journals/old professors want to see. </p>

<p>I'd post the graph produced by these example data, but this is my first post on the site. Sorry. One should be able to copy-paste the whole thing (after installing the ""gplots"" package) without problem.</p>
"
2009944,16632,2010-01-05T23:29:41Z,2007464,2,TRUE,"<p>It's pretty to compute the margins yourself:</p>

<pre><code>ddply(d, ""IDX1"", ...) 
ddply(d, c(""IDX1"", ""IDX2""), ...)
ddply(d, ""IDy1"", ...)
</code></pre>

<p>and then combine the results together with <code>rbind</code>.  It wouldn't be too hard to wrap this up into a general function.</p>

<p>Also, I'd rewrite your original code as:</p>

<pre><code>ddply(d, IDx1+IDx2~IDy1, summarise, 
  min = min(value),
  wt.mean = MyFancyWeightedHarmonicMeanFunction(value, weight),
  max = max(value)
)
</code></pre>
"
2012798,66549,2010-01-06T12:15:29Z,2010641,5,TRUE,"<p><strong>RSPlus</strong> is the only option i'm aware of. I used it almost daily for about a year, but haven't used it since R 2.7.  From your Q, it seems like you just want to run R inside SPlus, which RSPlus can certainly do (R is a separate interpreter accessible via an interface comprised of a few SPlus functions, the most-often used is '.R()', e.g., .R(""fivenum"", 1:10).</p>

<p>I think we are talking about the same thing though, because 'RinS' is one of two modules (SpinR being the other) that together comprise RSPlus (i.e., there's only a single interface, regardless of the direction you want to go--R to SPlus, or SPllus to R). Although it wasn't obvious to me at the time, i had to install both modules to get RinS to work.</p>
"
2018521,169462,2010-01-07T06:01:14Z,2018480,0,FALSE,"<p>print is probably the easiest function to use out of the box; most classes provide a customised print. They might not specifically name the type, but will often provide a distinctive form.</p>

<p>Otherwise, you might be able to write custom code to use the class and datatype functions to retrieve the information you want.</p>
"
2018582,158065,2010-01-07T06:18:42Z,2018480,5,FALSE,"<p>Check out the <code>dump</code> command:</p>

<pre><code>&gt; x &lt;- c(8,6,7,5,3,0,9)
&gt; dump(""x"", """")
x &lt;-
c(8, 6, 7, 5, 3, 0, 9)
</code></pre>
"
2019666,170792,2010-01-07T10:48:56Z,1976728,1,FALSE,"<p>I came across <a href=""http://www.jstatsoft.org/v32/i10/paper"" rel=""nofollow noreferrer"">VGAM package</a> for categorical data analysis. You could check this too</p>
"
2020322,121332,2010-01-07T12:46:01Z,1974998,1,FALSE,"<p>This can be done in a more vector way by separating your program into two parts: firstly a function which takes two time samples and determines if they meet your pulse specification:</p>

<pre><code>isPulse &lt;- function(previous, current)
{ 
  (previous != 0 &amp; current !=0 &amp; (abs(previous-current) &lt; 20)) |
  (previous == 0 &amp; current !=0)
}
</code></pre>

<p>Note the use of vector <code>|</code> instead of boolean <code>||</code>.</p>

<p>And then invoke it, supplying the two vector streams 'previous' and 'current' offset by a suitable delay, in your case, 1:</p>

<pre><code>delay &lt;- 1
samples = length(heartData$pulse)

isPulse(heartData$pulse[-(samples-(1:delay))], heartData$pulse[-(1:delay)])
</code></pre>

<p>Let's try this on some made-up data:</p>

<pre><code>sampleData = c(1,0,1,1,4,25,2,0,25,0)
heartData = data.frame(pulse=sampleData)
result = isPulse(heartData$pulse[-(samples-(1:delay))], heartData$pulse[-(1:delay)])
</code></pre>

<p>Note that the code <code>heartData$pulse[-(samples-(1:delay))]</code> trims <code>delay</code> samples from the end, for the <em>previous</em> stream, and <code>heartData$pulse[-(1:delay)]</code> trims <code>delay</code> samples from the start, for the <em>current</em> stream.</p>

<p>Doing it manually, the results should be (using <code>F</code> for false and <code>T</code> for true)</p>

<pre><code>F,T,T,T,F,F,F,T,F
</code></pre>

<p>and by running it, we find that they are!:</p>

<pre><code>&gt; print(result)
FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE
</code></pre>

<p><strong>success!</strong></p>

<p>Since you want to bind these back as a column into your original dataset, you should note that the new array is <code>delay</code> elements shorter than your original data, so you need to pad it at the start with delay FALSE elements.  You may also want to convert it into 0,1 as per your data:</p>

<pre><code>resultPadded &lt;- c(rep(FALSE,delay), result)
heartData$result = ifelse(resultPadded, 1, 0)
</code></pre>

<p>which gives</p>

<pre><code>&gt; heartData
   pulse result
1      1      0
2      0      0
3      1      1
4      1      1
5      4      1
6     25      0
7      2      0
8      0      0
9     25      1
10     0      0
</code></pre>
"
2020507,16632,2010-01-07T13:17:48Z,2018480,4,FALSE,"<p>I think you want 'str' which tells you the structure of an r object. </p>
"
2020862,7536,2010-01-07T14:11:27Z,2020790,2,FALSE,"<p>In <code>C</code>, you would use the <a href=""http://www.manpagez.com/man/3/isatty/"" rel=""nofollow noreferrer""><code>isatty</code></a> function.  If you could find an equivalent function in <code>R</code> (probably in a UNIX or file system library), that should help.</p>
"
2020980,245603,2010-01-07T14:34:45Z,2020790,11,TRUE,"<p>Perhaps you are looking for <code>interactive()</code>?</p>
"
2030966,170352,2010-01-08T21:43:53Z,2018480,7,TRUE,"<p>A few examples: </p>

<pre><code>foo &lt;- data.frame(1:12,12:1) 
foo ## What's inside?
dput(foo) ## Details on the structure, names, and class
str(foo) ## Gives you a quick look at the variable structure
</code></pre>

<p>Output on screen: </p>

<pre><code>foo &lt;- data.frame(1:12,12:1)

foo
   X1.12 X12.1
1      1    12
2      2    11
3      3    10
4      4     9
5      5     8
6      6     7
7      7     6
8      8     5
9      9     4
10    10     3
11    11     2
12    12     1

&gt; dput(foo)

structure(list(X1.12 = 1:12, X12.1 = c(12L, 11L, 10L, 9L, 8L, 
7L, 6L, 5L, 4L, 3L, 2L, 1L)), .Names = c(""X1.12"", ""X12.1""), row.names = c(NA, 
-12L), class = ""data.frame"")

&gt; str(foo)

'data.frame':   12 obs. of  2 variables:
 $ X1.12: int  1 2 3 4 5 6 7 8 9 10 ...
 $ X12.1: int  12 11 10 9 8 7 6 5 4 3 ...
</code></pre>
"
2034346,143305,2010-01-09T18:10:18Z,2034255,0,FALSE,"<p>You may want to read up on <code>help(par)</code> which is a very useful source of information for customizing standard R graphs.  This allows you to</p>

<ul>
<li>have tighter outer margins (eg <code>par(mar=c(3,3,1,1)</code>)</li>
<li>change fonts (eg <code>par(cex=0.7)</code> or some of the more specific cex alternatives</li>
<li>set colors or linetypes</li>
<li>...</li>
</ul>

<p>all of which comes close to your desired <code>loadPredefinedLayout()</code> functionality you desire.</p>

<p>Lastly, for the axes you are better off to either use a time-aware class like <code>zoo</code>, or to explicit give the x-axis argument as in the example below:</p>

<pre><code>R&gt; data &lt;- data.frame(Year=seq(as.Date(""2007-01-01""), \
                   as.Date(""2010-01-01""), by=""year""), \
                 Region1=c(17,26,53,96), Region2=c(55,43,70,58))
R&gt; data
        Year Region1 Region2
1 2007-01-01      17      55
2 2008-01-01      26      43
3 2009-01-01      53      70
4 2010-01-01      96      58
R&gt; par(mar=c(3,4,1,1)) 
R&gt; plot(data$Year, data$Region1, type='l', col='blue', ylab=""Values"")
R&gt; lines(data$Year, data$Region2, col='red')
R&gt; 
</code></pre>
"
2034621,158065,2010-01-09T19:23:34Z,2030895,2,TRUE,"<p>How about this?</p>

<pre><code>## Read in the data
foo &lt;- read.csv(""testdata.csv"")
## Normalize names so that things work later.
foo$Company &lt;- toupper(gsub("" *$"", """", foo$Company))

## Split the data into 09 and not 09.
not.09 &lt;- subset(foo, !grepl(""09$"", Company))
is.09 &lt;- subset(foo, grepl(""09$"", Company))

## Figure out where the not09 should go (even indices)
not.09$Index &lt;- 2 * order(not.09$Score, decreasing=TRUE)

## Find out where the 09s should go (odd indices)
is.09$Index &lt;- not.09[match(is.09$Company, paste(not.09$Company, ""09"")),]$Index + 1

## Combine and sort
combined &lt;- rbind(is.09, not.09)
combined &lt;- combined[order(combined$Index),-4]
</code></pre>
"
2034652,205459,2010-01-09T19:37:06Z,2003465,0,FALSE,"<p>Have you tried looking into the GSL package <a href=""http://cran.r-project.org/web/packages/gsl/index.html"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/packages/gsl/index.html</a>?</p>
"
2034959,16632,2010-01-09T21:06:12Z,2034255,3,FALSE,"<p>Yes, and in my biased opinion, you're best off using the ggplot2 package for creating graphics.  Here's how you might do so with your data (thanks to Dirk for providing a sample datset)</p>

<pre><code>data &lt;- data.frame(Year=seq(as.Date(""2007-01-01""), 
                   as.Date(""2010-01-01""), by=""year""), 
                 Region1=c(17,26,53,96), Region2=c(55,43,70,58))

library(ggplot2)

# Convert data to a form optimised for visualisation, not
# data entry
data2 &lt;- melt(data, measure = c(""Region1"", ""Region2""))

# Define the visualisation you want
ggplot(data2, aes(x = Year, y = value, colour = variable)) + 
  geom_line()
</code></pre>
"
2036387,143476,2010-01-10T06:58:55Z,2036250,4,TRUE,"<p>You actually have many options: a data frame (relational table), or list. The following code will show how to create a data frame, and then to split it into a list containing the elements {x,y} or {A,B,C,D}:</p>

<pre><code>&gt; txt &lt;- ""      Min  Max  Mean  StdDev
+ A
+   x   3    10   6.6   2.1 
+   y   2    5    3.2   1.7
+ B
+   x   3    10   6.6   2.1 
+   y   2    5    3.2   1.7
+ C
+   x   3    10   6.6   2.1 
+   y   2    5    3.2   1.7
+ D
+   x   3    10   6.6   2.1 
+   y   2    5    3.2   1.7
+ ""
&gt; 
&gt; data &lt;- head(readLines(textConnection(txt)),-1)
&gt; fields &lt;- strsplit(sub(""^[ ]+"","""",data[!nchar(data)==1]),""[ ]+"")
&gt; DF &lt;- `names&lt;-`(data.frame(rep(data[nchar(data)==1],each=2), ## letters
+                            do.call(rbind,fields[-1])),       ## data
+                 c(""Letter"",""xy"",fields[[1]]))                ## colnames
&gt; split(DF,DF$xy)
$x
  Letter xy Min Max Mean StdDev
1      A  x   3  10  6.6    2.1
3      B  x   3  10  6.6    2.1
5      C  x   3  10  6.6    2.1
7      D  x   3  10  6.6    2.1

$y
  Letter xy Min Max Mean StdDev
2      A  y   2   5  3.2    1.7
4      B  y   2   5  3.2    1.7
6      C  y   2   5  3.2    1.7
8      D  y   2   5  3.2    1.7

&gt; split(DF,DF$Letter)
$A
  Letter xy Min Max Mean StdDev
1      A  x   3  10  6.6    2.1
2      A  y   2   5  3.2    1.7

$B
  Letter xy Min Max Mean StdDev
3      B  x   3  10  6.6    2.1
4      B  y   2   5  3.2    1.7

$C
  Letter xy Min Max Mean StdDev
5      C  x   3  10  6.6    2.1
6      C  y   2   5  3.2    1.7

$D
  Letter xy Min Max Mean StdDev
7      D  x   3  10  6.6    2.1
8      D  y   2   5  3.2    1.7
</code></pre>
"
2039721,242673,2010-01-11T03:17:55Z,2034255,0,FALSE,"<p>A (in my opinion) slightly improved version of the graphic suggested by Hadley. I think now it is pretty much like the original graphic you tried to replicate (even better, actually, with direct labels).</p>

<p>After converting the data as suggested by Hadley,</p>

<pre><code>plot &lt;- ggplot(data2, aes(Year, value, group = variable,
     colour = variable)) + geom_line(size = 1) +
     opts(legend.position = ""none"")
plot &lt;- plot + geom_point () + opts(legend.position = ""none"")
plot + geom_text(data = data2[data2$year == 2010,
     ], aes(label = variable), hjust = 1.2, vjust = 1)
</code></pre>
"
2039782,163053,2010-01-11T03:40:21Z,2039763,1,FALSE,"<p>Beyond the other suggestions, you can use JRI and call R from Java. </p>

<p>Another nice option is to use the mediawiki plugin (read <a href=""http://wiki.r-project.org/rwiki/doku.php?id=developers:r_for_mediawiki"" rel=""nofollow noreferrer"">about it on the R-Wiki</a>).  It's very straightforward and gives you a simple markup for R.</p>
"
2039790,143305,2010-01-11T03:43:30Z,2039763,3,TRUE,"<p>I don't do a lot of dynamic html stuff with <a href=""http://www.project.org"" rel=""nofollow noreferrer"">R</a> but you could start with either one of these</p>

<ul>
<li><a href=""http://biostat.mc.vanderbilt.edu/rapache/index.html"" rel=""nofollow noreferrer"">rapache</a>: R inside Apache</li>
<li><a href=""http://cran.r-project.org/web/packages/Rpad/index.html"" rel=""nofollow noreferrer"">Rpad</a>: Web-based R</li>
<li><a href=""http://cran.r-project.org/doc/FAQ/R-FAQ.html#R-Web-Interfaces"" rel=""nofollow noreferrer"">R FAQ on Web interfaces</a></li>
</ul>

<p>Also of interest may be <a href=""http://www.rforge.net/brew/index.html"" rel=""nofollow noreferrer"">brew</a> for mixing R text and R code in web templates.</p>
"
2039817,16363,2010-01-11T03:53:39Z,2039763,3,FALSE,"<p>I like to call R to do statistical analysis from Python driven websites with <a href=""http://rpy.sourceforge.net/rpy2.html"" rel=""nofollow noreferrer"">rpy2</a>.</p>
"
2040753,216064,2010-01-11T09:00:54Z,2040026,0,FALSE,"<p>Maybe you could manually edit the first line (change , to "" "" and insert a line break) and then try again?</p>
"
2040916,121332,2010-01-11T09:36:42Z,2040026,-1,FALSE,"<p>Use <code>read.csv</code> instead of <code>read.table</code> and then add <code>skip=1, header=FALSE</code> to the arguments to <code>read.csv</code>.</p>
"
2042041,248069,2010-01-11T13:18:50Z,1819418,1,FALSE,"<p>I bumped into this thread by chance.  No, the aroma.* framework is not limited by the allocMatrix() limitation of ints and longs, because it does not address data using the regular address space alone - instead it subsets also via the file system.  It never hold and never loads the complete data set into memory at any time.  Basically the file system sets the limit, not the RAM nor the address space of you OS.</p>

<p>/Henrik
(author of aroma.*)</p>
"
2044056,143305,2010-01-11T18:46:34Z,2043760,1,FALSE,"<p>I do not think the Author information is in what <code>available.packages()</code> retrieves:</p>

<pre><code>R&gt; AP &lt;- available.packages()
R&gt; colnames(AP)
 [1] ""Package""    ""Version""    ""Priority""  
 [4] ""Bundle""     ""Contains""   ""Depends""   
 [7] ""Imports""    ""LinkingTo""  ""Suggests""  
[10] ""Enhances""   ""OS_type""    ""License""   
[13] ""File""       ""Repository""
R&gt; 
</code></pre>

<p>So maybe you need to combine this with a per-package lookup of the DESCRIPTION info at CRAN (or a mirror).  I do that, and a few more things, in the 200-line script driving the <a href=""http://dirk.eddelbuettel.com/cranberries"" rel=""nofollow noreferrer"">CRANberries RSS feed / html summary</a> of package updates at CRAN which stores stateful info in SQLite.  For this, I retrieve Author, Maintainer etc directly from the package I am currently looking at rather than in one big global scoop.  That said, there may of course be other meta-data at CRAN for this...</p>
"
2045763,128625,2010-01-11T23:44:07Z,2045706,1,FALSE,"<p>In the sample output from Rscript you quoted:</p>

<pre><code>Error in parse(text = args[[i]]) : 
  unexpected end of input in """"path_in='/Users/GR/web-app/Rproject/Inputs/Rscript/Gene-level""
</code></pre>

<p>I notice that there's very odd quoting going on there: two <code>""</code>s at the start, a single <code>'</code> after the <code>=</code>, and just a single <code>""</code> on the end.</p>

<p>At the very least I'd expect a matching <code>'</code> at the end of the path, and probably something to balance those <code>""</code>s as well</p>

<p>I'm guessing it's related to this line:</p>

<pre><code>RFILE=""${RFILE//$SUBSTRING}""
</code></pre>

<p>I'm guessing that you're removing one of the <code>'</code>s in that substitution.</p>

<p>Updated: As others have pointed out, your filenames contain spaces. <em>always</em> put <code>""</code>s around the name of a variable name when it contains filenames, especially when you know those filenames contain spaces. Try:</p>

<pre><code>Rscript ""$RFILE"" $ARGUMENTS
</code></pre>
"
2045782,176312,2010-01-11T23:48:57Z,2045706,1,FALSE,"<p>It appears that you are losing some of the quoting of spaces somewhere. You have this error message:</p>

<pre><code>Error in parse(text = args[[i]]) : 
  unexpected end of input in """"path_in='/Users/GR/web-app/Rproject/Inputs/Rscript/Gene-level""
</code></pre>

<p>but from other texts the file name should evidently be</p>

<pre><code>/Users/GR/web-app/Rproject/Inputs/Rscript/Gene-level Description for Modules.csv
</code></pre>

<p>This would also make sense as an explanation for your problem, as the variable substitution you are doing could well cause a space-containing argument to lose its protective quoting. Would it be possible for you to rename that file (and any other similar ones) to a name that does not contain spaces, say, by replacing spaces with underscores, and trying again?</p>
"
2045784,143305,2010-01-11T23:50:12Z,2045706,2,FALSE,"<p>Why is this a <code>bash</code> script when you have <code>Rscript</code>?  So why don't you rewrite this as an R script executed by <code>Rscript.exe</code> allowing you to test the components</p>

<ul>
<li>intialization</li>
<li>database connection</li>
<li>core work</li>
<li>...</li>
</ul>

<p>individually?</p>

<p><em>Edit (in response to your comment):</em>  R can call R, either packages via <code>library()</code> or directly via <code>source()</code>.  You have a debug problem that is comples and you should try to remove some complextity.  Moreover, R scripts can use the getopt or optparse packages to deal with command-line arguments.</p>

<p><em>Edit 2:</em> Are you aware that R has an <a href=""http://cran.r-project.org/package=RMySQL"" rel=""nofollow noreferrer"">RMySQL</a> package that would allow you to call the db from R?</p>
"
2045974,143305,2010-01-12T00:47:35Z,2045837,2,TRUE,"<p>No, the graphics devices are file-based, so your steps 1-3 are correct. You need a fourth to unlink the temporary file but that is about it.</p>
"
2046015,163053,2010-01-12T01:00:20Z,2045837,0,FALSE,"<p>If you use either lattice or ggplot, you can save the plot object (rather than the image itself) to the database (although I don't know if that meets your requirement).  The benefit of that approach is that you can easily recreate/alter the image.</p>
"
2046238,143377,2010-01-12T02:07:37Z,2041120,1,FALSE,"<p>Check whether z$lab_id is a factor (with <code>is.factor(z$lab_id)</code>). </p>

<p>If it is, try </p>

<pre><code>z$lab_id &lt;- as.character(z$lab_id)
</code></pre>

<p>if it is supposed to be a character vector; or </p>

<pre><code>z$lab_id &lt;-    as.numeric(as.character(z$lab_id))
</code></pre>

<p>if it is supposed to be a numeric vector. </p>

<p>Then order it again.</p>

<p>Ps. I had previously put these in the comments.</p>
"
2046479,129144,2010-01-12T03:15:47Z,2045706,0,FALSE,"<p>First you may want to try adding -s (updating -e to -se) to your mysql line:</p>

<pre>
RFILE=$(mysql -u$USER -p$PASS -se ""SELECT script_name FROM JobProcess WHERE script_run_id=$ID;"" $DB)
</pre>

<p>You should be able to remove your SUBSTRING replacement.
Same goes for your variables ARGUMENTS. Replace -e with -se in your mysql line and remove the SUBSTRING replace. </p>

<p>This probably won't fix your issue but it will remove any questions about your expansion replacement. Although if for some reason there was an \n or something at the end of the first line, where you replace SUBSTRING in $RFILE and $ARGUMENTS...</p>
"
2046520,15050,2010-01-12T03:27:25Z,1169330,2,FALSE,"<p>If you're specifically interested in SQL Server, the reference below is a little bit out of date but I imagine it probably still holds.</p>

<p><a href=""http://msdn.microsoft.com/en-us/library/ms811006.aspx"" rel=""nofollow noreferrer"">Using ODBC with Microsoft SQL Server</a></p>

<blockquote>
  <p>Performance of ODBC as a Native API</p>
  
  <p>One of the persistent rumors about ODBC is that it is inherently slower than a native DBMS API. This reasoning is based on the assumption that ODBC drivers must be implemented as an extra layer over a native DBMS API, translating the ODBC statements coming from the application into the native DBMS API functions and SQL syntax. This translation effort adds extra processing compared with having the application call directly to the native API. This assumption is true for some ODBC drivers implemented over a native DBMS API, but the Microsoft SQL Server ODBC driver is not implemented this way.</p>
  
  <p>The Microsoft SQL Server ODBC driver is a functional replacement of DB-Library. The SQL Server ODBC driver works with the underlying Net-Libraries in exactly the same manner as the DB-Library DLL. The Microsoft SQL Server ODBC driver has no dependence on the DB-Library DLL, and the driver will function correctly if DB-Library is not even present on the client.</p>
  
  <p><strong>Microsoft's testing has shown that the performance of ODBC-based and DB-Library–based SQL Server applications is roughly equal.</strong></p>
</blockquote>
"
2046803,60628,2010-01-12T04:59:50Z,2034255,2,FALSE,"<p>Here is <code>R</code> code that plots the data in a nice way (it is not simple code as requested, but at least the result looks good):</p>

<pre><code>test &lt;- read.table(""/tmp/test.txt"", header=TRUE)
png(filename=""/tmp/test.png"", height=750, width=1000, 
    bg=""white"", res=300)
par(mar=c(2.5,2.5,0.75,0.75), 
    family=""Gill Sans"", font=1, # font 2 would be bold
    cex=0.75, cex.lab=0.75, cex.axis=0.75) 
mymax &lt;- max(test$Region1, test$Region2)*1.25

plot(test$Region1, type=""b"", col=""#304E67"", 
     ylim=c(0, mymax), lwd=3,
     bty=""l"", axes=FALSE, ann=FALSE, cex=1.0, tck=1)

axis(1, lwd.ticks=0, at=1:length(test$Year), lab=test$Year)
axis(2, lwd=0, las=1, at=c(0,25,50,75,100), yaxp=c(0,100,4))
# grid(nx = NA, ny = 5, col = ""lightgray"") # wrong, see axTicks
for(y in c(25, 50, 75, 100)) {
  lines(rep(y, length(test$Region1)), type=""l"", col=""lightgray"", lwd=1)
}

lines(test$Region1, type=""b"", col=""#304E67"", lwd=3)
lines(test$Region2, type=""b"", col=""#974449"", lwd=3)

# title(xlab=""Year"", col.lab=rgb(0,0.5,0))
# title(ylab=""Output"", col.lab=rgb(0,0.5,0))
legend(1, mymax+8, c(""Region 1"",""Region 2""), cex=0.75, 
       col=c(""#304E67"" ,""#974449""), 
       pch=1:1, # circles
       lty=1:1, # solid 
       lwd=1.5, # line width
       bty=""n"") # no box around

dev.off()
</code></pre>

<p>The data file has this content:</p>

<pre><code>Year Region1 Region2
2007 17 55
2008 26 43
2009 53 70
2010 96 58
</code></pre>

<p>It produces the following graph:
<a href=""http://i46.tinypic.com/206gchk.png/"" rel=""nofollow noreferrer"">http://i46.tinypic.com/206gchk.png/</a></p>

<p>which comes pretty close to the graph that Keynote draws:
<a href=""http://i45.tinypic.com/b3tw08.jpg"" rel=""nofollow noreferrer"">http://i45.tinypic.com/b3tw08.jpg</a></p>
"
2046962,248627,2010-01-12T05:52:32Z,1198116,5,FALSE,"<p>With the query below you can read the data from row 61 of cloumn A, &amp; G is supposed to read all columns till G.</p>

<pre><code>SELECT * FROM [Sheet1$a61:G]
</code></pre>
"
2047400,170792,2010-01-12T07:34:25Z,1504832,1,TRUE,"<p>another option</p>

<pre><code># convert to Date
day_table$day &lt;- as.Date(day_table$day, format=""%Y/%m/%d"")
# split by user and then look for contiguous days
contig &lt;- sapply(split(day_table$day, day_table$user_id), function(.days){
    .diff &lt;- cumsum(c(TRUE, diff(.days) != 1))
    max(table(.diff))
})
</code></pre>
"
2047584,248693,2010-01-12T08:21:20Z,2047487,3,FALSE,"<p>I would simply create another column of the data frame and assign to different subsets of it conditionally.  You can also slim down the data frame indexing code.  </p>

<pre><code>RDTbase$Result = NA 
RDTbase &lt;- within(RDTbase, Result[Control==""TRUE"" &amp; Pf==""TRUE"" &amp; Pv==""FALSE""] &lt;- ""Pf"")
RDTbase &lt;- within(RDTbase, Result[Control==""FALSE""] &lt;- ""Invalid"")
</code></pre>

<p>etc.</p>

<p>""within"" just saves a little typing.</p>
"
2047877,168747,2010-01-12T09:29:41Z,2047487,2,FALSE,"<p>First of all it would be nice when you use <code>logical</code> vector instead <code>character</code>, then you could write <code>Control</code> instead <code>Control == ""TRUE""</code> and <code>!Control</code> instead <code>Control == ""FALSE""</code>. And your code will be shorter.</p>

<p>For you problem I will use several <code>ifelse</code>:</p>

<pre><code>RDTbase$Result &lt;- ifelse(
  Control == ""TRUE"",
  ifelse(
    Pf == ""TRUE"",
    ifelse(Pv == ""TRUE"",""Mixed"",""Pf""), # when Control is TRUE, Pf is TRUE
    ifelse(Pv == ""TRUE"",""Pv"",""Negative""), # when Control is TRUE, Pf is FALSE
  ),
  ""Invalid"" # when Control is FALSE
)
</code></pre>

<p>But I like magic tricks so you could do follow:</p>

<pre><code>num_code &lt;- (
  as.numeric(as.logical(Control))
  + 2*as.numeric(as.logical(Pf))
  + 4*as.numeric(as.logical(Pv))
) # values are 0,1,2,...,7
# then 
RDTbase$Result &lt;- c( 
  ""Invalid"" , # 0 = F,F,F # Control, Pf, Pv
  ""Negative"", # 1 = T,F,F
  ""Invalid"" , # 2 = F,T,F
  ""Pf""      , # 3 = T,T,F
  ""Invalid"" , # 4 = F,F,T
  ""Pv""      , # 5 = T,F,T
  ""Invalid"" , # 6 = F,T,T
  ""Mixed""   , # 7 = T,T,T
)[num_code+1]
</code></pre>

<p>It's nice trick when you need to decode several logical column to character.</p>
"
2048342,121332,2010-01-12T10:56:03Z,2048304,7,FALSE,"<h2>Other editable formats:</h2>

<p>Take a look at <code>help(devices)</code> for other formats which are available: these include <code>svg</code>, <code>pictex</code> and <code>xfig</code>, all of which are editable to greater or lesser extents.</p>

<p>Note that PDFs can be edited, for instance using the <code>Omnigraffle</code> tool available for Apple's OSX.</p>

<h2>Other ways to record plot data:</h2>

<p>In addition, you can record R's commands to the graphics subsystem for repeating it later - take a look at <code>dev.copy</code>:</p>

<pre><code> Most devices (including all screen devices) have a display list
 which records all of the graphics operations that occur in the
 device. 'dev.copy' copies graphics contents by copying the display
 list from one device to another device.  Also, automatic redrawing
 of graphics contents following the resizing of a device depends on
 the contents of the display list.
</code></pre>

<h2>Using Rscript to create a repeatable, editable plot:</h2>

<p>I typically take a third strategy, which is to copy my R session into an Rscript file, which I can run repeatedly and tweak the plotting commands until it does what I want:</p>

<pre><code>#!/usr/bin/Rscript
x = 1:10
pdf(""myplot.pdf"", height=0, width=0, paper=""a4"")
plot(x)
dev.off();
</code></pre>
"
2049312,16632,2010-01-12T13:47:28Z,2048304,4,FALSE,"<p>With ggplot and lattice, you can use <code>save</code> to save the plot object to disk and then <code>load</code> it later and modify it.  For example:</p>

<pre><code>save(testplot, file = ""test-plot.rdata"")

# Time passes and you start a new R session
load(""test-plot.rdata"")
testplot + opts(legend.position = ""none"")
testplot + geom_point()
</code></pre>
"
2049391,74658,2010-01-12T13:57:36Z,2048304,4,TRUE,"<p>Thanks for the answers, I've played around with this, and after some help from my friend Google I found the <a href=""http://cran.r-project.org/web/packages/Cairo/index.html"" rel=""nofollow noreferrer"">Cairo</a> package, which allows creation of svg files, I can then edit these in <a href=""http://www.inkscape.org/"" rel=""nofollow noreferrer"">Inkscape</a>.</p>

<pre><code>library(Cairo)
Cairo(600,600,file=""testplot.svg"",type=""svg"",bg=""transparent"",pointsize=8, units=""px"",dpi=400)
testplot
dev.off()
Cairo(1200,1200,file=""testplot12200.png"",type=""png"",bg=""transparent"",pointsize=12, units=""px"",dpi=200)
testplot
dev.off()
</code></pre>

<p>Now I just have to play around with the various settings to get my plot as good as it can be before writing the file.</p>
"
2049397,121332,2010-01-12T13:58:25Z,2047487,1,FALSE,"<p>Using transform makes this compact and elegant:</p>

<pre><code>transform(a, Result = 
 ifelse(Control,
  ifelse(Pf, 
   ifelse(Pv, ""Mixed"", ""Pf""),
   ifelse(Pv, ""Pv"", ""Negative"")),
  ""Invalid""))
</code></pre>

<p>Yields</p>

<pre><code>  Control    Pf    Pv   Result
1    TRUE  TRUE FALSE       Pf
2    TRUE FALSE  TRUE       Pv
3   FALSE FALSE FALSE  Invalid
4    TRUE  TRUE  TRUE    Mixed
5    TRUE FALSE FALSE Negative
</code></pre>

<p>Alternatively, building on Marek's version we can use logical vectors to calculate the index slightly more compactly:</p>

<pre><code>a$Result = apply(a,1,
  function(x){
    c(rep(""Invalid"", 4), ""Negative"", ""Pv"", ""Pf"", ""Mixed"")
      [1+sum(c(4,2,1)[x])]})
</code></pre>
"
2050558,169947,2010-01-12T16:38:34Z,2003465,2,TRUE,"<p>Fleshing out Arietta's answer above:</p>

<pre><code>&gt; a &lt;- c(1,3,-4)
&gt; m &lt;- matrix(c(0,0,-a[1],1,0,-a[2],0,1,-a[3]), byrow=T, nrow=3)
&gt; roots &lt;- eigen(m, symm=F, only.values=T)$values
</code></pre>

<p>Whether this is faster or slower than using the cubic solver in the GSL package (as suggested by knguyen above) is a matter of benchmarking it on your system.</p>
"
2050970,143305,2010-01-12T17:33:15Z,2050790,55,FALSE,"<p>Regarding your questions, let me address them in order and give some examples:</p>

<p><strong>1</strong>) A list is returned if and when the return statement adds one. Consider </p>

<pre><code> R&gt; retList &lt;- function() return(list(1,2,3,4)); class(retList())
 [1] ""list""
 R&gt; notList &lt;- function() return(c(1,2,3,4)); class(notList())
 [1] ""numeric""
 R&gt; 
</code></pre>

<p><strong>2</strong>) Names are simply not set:</p>

<pre><code>R&gt; retList &lt;- function() return(list(1,2,3,4)); names(retList())
NULL
R&gt; 
</code></pre>

<p><strong>3</strong>) They do not return the same thing. Your example gives</p>

<pre><code>R&gt; x &lt;- list(1,2,3,4)
R&gt; x[1]
[[1]]
[1] 1
R&gt; x[[1]]
[1] 1
</code></pre>

<p>where <code>x[1]</code> returns the first element of <code>x</code> -- which is the same as <code>x</code>. Every scalar is a vector of length one. On the other hand <code>x[[1]]</code> returns the first element of the list.</p>

<p><strong>4</strong>) Lastly, the two are different between they create, respectively, a list containing four scalars and a list with a single element (that happens to be a vector of four elements).</p>
"
2050990,37751,2010-01-12T17:35:20Z,2050790,33,FALSE,"<p>Just to take a subset of your questions:</p>

<p><a href=""http://cran.r-project.org/doc/manuals/R-lang.html#Indexing"" rel=""noreferrer"">This article</a> on indexing addresses the question of the difference between <code>[]</code> and <code>[[]]</code>.</p>

<p>In short [[]] selects a single item from a list and <code>[]</code> returns a list of the selected items. In your example, <code>x = list(1, 2, 3, 4)'</code> item 1 is a single integer but <code>x[[1]]</code> returns a single 1 and <code>x[1]</code> returns a list with only one value. </p>

<pre><code>&gt; x = list(1, 2, 3, 4)
&gt; x[1]
[[1]]
[1] 1

&gt; x[[1]]
[1] 1
</code></pre>
"
2050993,103832,2010-01-12T17:35:46Z,2045706,0,TRUE,"<p>I put a copy of script fix, but I still have some questions :</p>

<pre><code>#!/usr/bin/env bash
# Execute R process
# -----------------
### Mysql Setup ###
USER=...
PASS=...
HOST=...
DB=...

# Get Job ID process
# Use to retrieve args in my DB
ID=$1

# Get script name
RFILE=$(mysql -u$USER -p$PASS -se ""SELECT script_name FROM JobProcess WHERE script_run_id=$ID;"" $DB)

# Get script_args
ARGUMENTS=$(mysql -u$USER -p$PASS -se ""SELECT script_args FROM JobProcess WHERE script_run_id=$ID;"" $DB)

RUN=""Rscript $RFILE $ARGUMENTS""
eval ""$RUN""
</code></pre>

<p>Actually, I can't use (probably due to bad seperated token) : Rscript $RFILE $ARGUMENTS, or $RUN.</p>

<p>The purpose of this script is to run any R script by recovering all the parameters in database. Here a copy of data in my DB :</p>

<pre><code>script_run_id : 161
script_name : /Users/GR/web-app/Rproject/Scripts/arg_file_test.R
script_args : 'path_in&lt;-""/Users/GR/web-app/Rproject/Inputs/Rscript/Gene.csv""' 'path_in2&lt;-""/Users/GR/web-app/Rproject/Inputs/Rscript/Template_Auto.csv""' l=0 w=0 
</code></pre>

<p>Part script_name define R script to call, and scripts_args define arguments used with this R script. In this example, my script generate this command line :</p>

<pre><code>Rscript /Users/GR/web-app/Rproject/Scripts/arg_file_test.R 'path_in&lt;-""/Users/GR/web-app/Rproject/Inputs/Rscript/Gene.csv""' 'path_in2&lt;-""/Users/GR/web-app/Rproject/Inputs/Rscript/Template_Auto.csv""' l=0 w=0
</code></pre>

<p>Is there a better way to proceed ?<br>
For example, define a new column for files path (ex:script_Files) ?<br>
Separate arguments with comma (ex:l=0, w=3, r=6, etc), and split string into array (an example can help me) ? </p>

<p>Thanks<br>
ps : A good way, is probably to use directly Rscript.</p>
"
2051017,172261,2010-01-12T17:38:27Z,2050610,15,TRUE,"<p>To fix the second problem use</p>

<pre><code>annotate(""text"", 0.375, -1.25,
         label=paste(""rho=="", round(cor(abm.data$area, abm.data$U1_2), 2)),
         parse=TRUE)
</code></pre>

<p>i.e. <code>""rho==""</code>.</p>

<p><strong>Edit:</strong> Here is a solution to solve the first problem</p>

<pre><code>library(""plyr"")
library(""ggplot2"")

set.seed(1)
df &lt;- data.frame(x=rnorm(300), y=rnorm(300), cl=gl(3,100))   # create test data
df.cor &lt;- ddply(df, .(cl), function(val) sprintf(""rho==%.2f"", cor(val$x, val$y)))

p1 &lt;- ggplot(data=df, aes(x=x)) +
             geom_point(aes(y=y, colour=""col1"", alpha=0.4)) +
             facet_wrap(~ cl, ncol=3) +
             geom_text(data=df.cor, aes(x=0, y=3, label=V1), parse=TRUE) +
             scale_colour_manual(values=c(""col1""=""red"")) +
             opts(legend.position=""none"")
print(p1)
</code></pre>
"
2051159,163053,2010-01-12T18:01:18Z,2050790,121,TRUE,"<p>Just to address the last part of your question, since that really points out the difference between a <code>list</code> and <code>vector</code> in R:</p>

<blockquote>
  <p>Why do these two expressions not return the same result?</p>
  
  <p>x = list(1, 2, 3, 4); x2 = list(1:4)</p>
</blockquote>

<p>A list can contain any other class as each element.  So you can have a list where the first element is a character vector, the second is a data frame, etc.  In this case, you have created two different lists.  <code>x</code> has four vectors, each of length 1.  <code>x2</code> has 1 vector of length 4:</p>

<pre><code>&gt; length(x[[1]])
[1] 1
&gt; length(x2[[1]])
[1] 4
</code></pre>

<p>So these are completely different lists.  </p>

<p>R lists are very much like <a href=""http://en.wikipedia.org/wiki/Hash_table"" rel=""nofollow noreferrer"">a hash map</a> data structure in that each index value can be associated with any object.  Here's a simple example of a list that contains 3 different classes (including a function):</p>

<pre><code>&gt; complicated.list &lt;- list(""a""=1:4, ""b""=1:3, ""c""=matrix(1:4, nrow=2), ""d""=search)
&gt; lapply(complicated.list, class)
$a
[1] ""integer""
$b
[1] ""integer""
$c
[1] ""matrix""
$d
[1] ""function""
</code></pre>

<p>Given that the last element is the search function, I can call it like so:</p>

<pre><code>&gt; complicated.list[[""d""]]()
[1] "".GlobalEnv"" ...
</code></pre>

<p>As a final comment on this: it should be noted that a <code>data.frame</code> is really a list (from the <code>data.frame</code> documentation):</p>

<blockquote>
  <p>A data frame is a list of variables of the same number of rows with unique row names, given class ‘""data.frame""’</p>
</blockquote>

<p>That's why columns in a <code>data.frame</code> can have different data types, while columns in a matrix cannot.  As an example, here I try to create a matrix with numbers and characters:</p>

<pre><code>&gt; a &lt;- 1:4
&gt; class(a)
[1] ""integer""
&gt; b &lt;- c(""a"",""b"",""c"",""d"")
&gt; d &lt;- cbind(a, b)
&gt; d
 a   b  
[1,] ""1"" ""a""
[2,] ""2"" ""b""
[3,] ""3"" ""c""
[4,] ""4"" ""d""
&gt; class(d[,1])
[1] ""character""
</code></pre>

<p>Note how I cannot change the data type in the first column to numeric because the second column has characters:</p>

<pre><code>&gt; d[,1] &lt;- as.numeric(d[,1])
&gt; class(d[,1])
[1] ""character""
</code></pre>
"
2051303,121332,2010-01-12T18:19:46Z,2050790,11,FALSE,"<p>One reason lists work as they do (ordered) is to address the need for an ordered container that can contain any type at any node, which vectors do not do.  Lists are re-used for a variety of purposes in R, including forming the base of a <code>data.frame</code>, which is a list of vectors of arbitrary type (but the same length).</p>

<p>Why do these two expressions not return the same result?</p>

<pre><code>x = list(1, 2, 3, 4); x2 = list(1:4)
</code></pre>

<p>To add to @Shane's answer, if you wanted to get the same result, try:</p>

<pre><code>x3 = as.list(1:4)
</code></pre>

<p>Which coerces the vector <code>1:4</code> into a list.</p>
"
2051580,169947,2010-01-12T19:06:09Z,2045837,0,FALSE,"<p>Instead of writing to a regular file, could you write it to a FIFO that would in turn store it in the DB?  Will graphics devices write to a FIFO if it's created externally?</p>
"
2052742,83761,2010-01-12T21:56:59Z,2050790,7,FALSE,"<p>You say:</p>

<blockquote>
  <p>For another, lists can be returned
  from functions even though you never
  passed in a List when you called the
  function, and even though the function
  doesn't contain a List constructor,
  e.g.,</p>
</blockquote>

<pre><code>x = strsplit(LETTERS[1:10], """") # passing in an object of type 'character'
class(x)
# =&gt; 'list'
</code></pre>

<p>And I guess you suggest that this is a problem(?). I'm here to tell you why it's not a problem :-). Your example is a bit simple, in that when you do the string-split, you have a list with elements that are 1 element long, so you know that <code>x[[1]]</code> is the same as <code>unlist(x)[1]</code>. But what if the result of <code>strsplit</code> returned results of different length in each bin. Simply returning a vector (vs. a list) won't do at all.</p>

<p>For instance:</p>

<pre><code>stuff &lt;- c(""You, me, and dupree"",  ""You me, and dupree"",
           ""He ran away, but not very far, and not very fast"")
x &lt;- strsplit(stuff, "","")
xx &lt;- unlist(strsplit(stuff, "",""))
</code></pre>

<p>In the first case (<code>x</code> : which returns a list), you can tell what the 2nd ""part"" of the 3rd string was, eg: <code>x[[3]][2]</code>. How could you do the same using <code>xx</code> now that the results have been ""unraveled"" (<code>unlist</code>-ed)?</p>
"
2052948,169947,2010-01-12T22:31:14Z,2052174,7,FALSE,"<p>The code can actually be simplified to one short line with no loops or <code>apply()</code> at all:</p>

<pre><code>dataframe &lt;- data.frame(a = c(""No"", ""Yes"", ""No"", ""No"", ""Yes""),
                        b = c(""Hi"", ""Hi"", ""Mom"", ""Hi"", ""Mom""),
                        c = c(""Yes"", ""Yes"", ""Yes"", ""Yes"", ""No""))
cols &lt;- c(""a"",""c"")
dataframe[,cols] &lt;- as.numeric(dataframe[,cols]==""Yes"")
dataframe

  a   b c
1 0  Hi 1
2 1  Hi 1
3 0 Mom 1
4 0  Hi 1
5 1 Mom 0
</code></pre>
"
2053457,143305,2010-01-13T00:09:15Z,2053397,16,TRUE,"<p>See <code>help(integer)</code>:  </p>

<pre><code> Note that on almost all implementations of R the range of
 representable integers is restricted to about +/-2*10^9: ‘double’s
 can hold much larger integers exactly.
</code></pre>

<p>so I would recommend using <code>numeric</code> (i.e. 'double') -- a double-precision number.</p>
"
2053730,163053,2010-01-13T01:12:46Z,2053397,7,FALSE,"<p>Dirk is right.  You should be using the <code>numeric</code> type (which should be set to double).  The other thing to note is that you may not be getting back all the digits.  Look at the digits setting:</p>

<pre><code>&gt; options(""digits"")
$digits
[1] 7
</code></pre>

<p>You can extend this:</p>

<pre><code>options(digits=14)
</code></pre>

<p>Alternatively, you can reformat the number:</p>

<pre><code>format(big.int, digits=14)
</code></pre>

<p>I tested your number and am getting the same behavior (even using the <code>double</code> data type), so that may be a bug:</p>

<pre><code>&gt; as.double(""123456789123456789"")
[1] 123456789123456784
&gt; class(as.double(""123456789123456789""))
[1] ""numeric""
&gt; is.double(as.double(""123456789123456789""))
[1] TRUE
</code></pre>
"
2053906,66549,2010-01-13T02:02:01Z,2053397,21,FALSE,"<p>I understood your question a little differently vs the two who posted before i did. </p>

<p>If R's largest default value is not big enough for you, you have a few choices (disclaimer: I have used each of the libraries i mention below, but not through the R bindings, instead through other language bindings or the native library)</p>

<p>The <strong><a href=""http://cran.at.r-project.org/web/packages/Brobdingnag/index.html"" rel=""nofollow noreferrer"">Brobdingnag</a></strong> package: uses natural logs to store the values; (like Rmpfr, implemented using R's new class structure). I'm always impressed by anyone whose work requires numbers of this scale. </p>

<pre><code>library(Brobdingnag)

googol &lt;- as.brob(1e100)   
</code></pre>

<p>The <strong><a href=""http://cran.at.r-project.org/web/packages/gmp/index.html"" rel=""nofollow noreferrer"">gmp</a></strong> package: R bindings to the venerable GMP (GNU Multi-precision library). This must go back 20 years because i used it in University. This Library's motto is ""Arithmetic Without Limits,"" which is a credible claim--integers, rationals, floats, whatever, right up to the limits of the RAM on your box.</p>

<pre><code>library(gmp)

x = as.bigq(8000, 21)
</code></pre>

<p>The <strong><a href=""https://cran.r-project.org/web/packages/Rmpfr/index.html"" rel=""nofollow noreferrer"">Rmpfr</a></strong> package: R bindings which interface to both gmp (above) and MPFR, (MPFR is in turn a contemporary implementation of gmp. I have used the Python bindings ('bigfloat') and can recommend it highly. This might be your best option of the three, given its scope, given that it appears to be the most actively maintained, and and finally given what appears to be the most thorough documentation.</p>

<p>Note: to use either of the last two, you'll need to install the native libraries, <a href=""http://gmplib.org/"" rel=""nofollow noreferrer"">GMP</a> and <a href=""http://www.mpfr.org/"" rel=""nofollow noreferrer"">MPFR</a>.</p>
"
2053990,129144,2010-01-13T02:31:06Z,2045706,0,FALSE,"<p>okay, I think you may need to change what you are storing in your database.</p>

<p>your script_args field should probably look like this in your db:</p>

<p>path_in=\""/Users/GR/web-app/Rproject/Inputs/Rscript/Gene.csv\"" path_in2=\""/Users/GR/web-app/Rproject/Inputs/Rscript/Template_Auto.csv\"" l=0 w=0</p>

<p>without any other punctuation or escapes... When you populate your variable and then expand it during your eval or other method it would look like this after expansion:</p>

<pre><code>
Rscript /Users/GR/web-app/Rproject/Scripts/arg_file_test.R path_in=""/Users/GR/web-app/Rproject/Inputs/Rscript/Gene.csv"" path_in2=""/Users/GR/web-app/Rproject/Inputs/Rscript/Template_Auto.csv"" l=0 w=0
</code></pre>

<p>Also you probably don't need eval in this case and you could run $RUN by itself without eval or even just put </p>

<pre><code>Rscript $RFILE $ARGUMENTS</code></pre>

<p>on a line all by itself.</p>
"
2054850,234233,2010-01-13T06:24:31Z,2054772,0,FALSE,"<p>I figured out one decent way to do this. Here it is:</p>

<pre><code>pairs &lt;- expand.grid(alpha, beta)
names(pairs) &lt;- c(""alpha"", ""beta"")
mapply(predict, pairs$alpha, pairs$beta, 
    MoreArgs=list(object=classifier.out, data=validation.data))
</code></pre>

<p>Anyone have something simpler and more efficient? I am very eager to know because I spent a little too long on this problem. :(</p>
"
2056054,249691,2010-01-13T10:53:31Z,2055677,0,FALSE,"<p>I'm not sure to understand exactly what you want, in particular why you want to put the source code in your text file.</p>

<p>First, I'd better use the *apply family of functions to browse the data frame instead of a for loop, especially lapply and sapply.</p>

<p>With the following code you will get three different presentations of your output. The first one should be similar to what you get with your code. The second and third ones are a bit different, they output values as a list or a data frame.</p>

<pre><code>sink(""/tmp/myfile.txt"", append=TRUE, split=TRUE)

myfunc &lt;- function(v) {
  variance &lt;- var(v)
  mean &lt;- mean(v)
  return(list(variance=variance,mean=mean))
}

myotherfunc &lt;- function(v) {
  cat(""And the mean is:\n"")
  print(mean(v))
  cat(""And the variance is:\n"")
  print(var(v))
}

# results printed directly
invisible(lapply(cData, myotherfunc))

# results as a list
lapply(cData, myfunc)

# results as a data frame
sapply(cData, myfunc)


sink()
</code></pre>

<p>Anyway, I don't know if one of these outputs could be what you're looking for.</p>
"
2058348,158065,2010-01-13T16:25:27Z,2054772,3,TRUE,"<p>The problem is that outer expects its function to be vectorized (i.e., it will call predict ONCE with a vector of all the arguments it wants executed).  Therefore, when predict is called once, returning its result (which happens to be of length 4), outer complains because it doesn't equal the expected 40.</p>

<p>One way to fix this is to use <code>Vectorize</code>.  Untested code:</p>

<pre><code>outer(X=alpha, Y=beta, FUN=Vectorize(predict, vectorize.args=c(""alpha"", ""beta"")), object=classifier.out, data=validation.data)
</code></pre>
"
2059935,143305,2010-01-13T20:19:52Z,2055677,2,FALSE,"<p>I would recommend not to think in terms of </p>

<pre><code>source(""myscript.r"",echo=TRUE)
</code></pre>

<p>but rather in terms of <code>Rscript</code> (which comes with R)</p>

<pre><code>Rscript myscript.r     # on windows, linux or os x
</code></pre>

<p>or in terms of <a href=""http://dirk.eddelbuettel.com/code/littler.html"" rel=""nofollow noreferrer"">littler</a></p>

<pre><code>r myscript.r           # on linux or os x
</code></pre>

<p>This gives you the ability to query command-line parameters (via CRAN packages getopt and optparse) and much more.</p>
"
2062483,160314,2010-01-14T06:03:04Z,2062194,5,TRUE,"<p>There is no function that I am aware of that does this. One could of course be made. All that your magicFunction would need to do is create a list with elements:</p>

<pre><code>&gt; names(fakeModel)
[1] ""coefficients""  ""residuals""     ""effects""       ""rank""         
 [5] ""fitted.values"" ""assign""        ""qr""            ""df.residual""  
 [9] ""xlevels""       ""call""          ""terms""         ""model""  
</code></pre>

<p>then make it an lm object</p>

<pre><code>&gt; class(fakeModel) &lt;- c(""lm"")
</code></pre>

<p>Let me just say that I think that this is a bad idea though. Whose to say that the generic function that you apply will be applicable to a bicreg object. For example, how would you interpret AIC(fakeModel)?</p>

<p>You are better off creating your own functions to do diagnostics and prediction.</p>
"
2062865,172261,2010-01-14T08:05:09Z,2061897,77,TRUE,"<p><a href=""http://www.omegahat.org/RJSONIO/"" rel=""noreferrer"">RJSONIO</a> from Omegahat is another package which provides facilities for reading and writing data in JSON format.</p>

<p><a href=""http://cran.r-project.org/web/packages/rjson/index.html"" rel=""noreferrer"">rjson</a> does not use S4/S3 methods and so is not readily extensible, but still useful. Unfortunately, it does not used vectorized operations and so is too slow for non-trivial data. Similarly, for reading JSON data into R, it is somewhat slow and so does not scale to large data, should this be an issue. </p>

<p><strong>Update</strong> (new Package 2013-12-03):</p>

<p><a href=""http://cran.r-project.org/web/packages/jsonlite/index.html"" rel=""noreferrer"">jsonlite</a>: This package is a fork of the <code>RJSONIO</code> package. It builds on the parser from <code>RJSONIO</code>, but implements a different mapping between R objects and JSON strings. The C code in this package is mostly from the <code>RJSONIO</code> Package, the R code has been rewritten from scratch. In addition to drop-in replacements for <code>fromJSON</code> and <code>toJSON</code>, the package has functions to serialize objects. Furthermore, the package contains a lot of unit tests to make sure that all edge cases are encoded and decoded consistently for use with dynamic data in systems and applications.</p>
"
2063495,249691,2010-01-14T10:42:21Z,2062194,3,FALSE,"<p>It seems you can compute your <code>lm</code> object as usual, and then modify the coefficients afterwards by modifying the <code>$coefficients</code> attribute of your <code>lm()</code> result.</p>

<p>See this question and results for more details :</p>

<p><a href=""http://tolstoy.newcastle.edu.au/R/e2/help/07/08/24294.html"" rel=""nofollow noreferrer"">http://tolstoy.newcastle.edu.au/R/e2/help/07/08/24294.html</a></p>

<p>Not sure it corresponds to what you want to do, though...</p>
"
2064367,249691,2010-01-14T13:32:51Z,2063821,4,FALSE,"<p>You can use the <code>reshape</code> function to transform your data frame to ""long"" format. May be it is a bit faster than your code ?</p>

<pre><code>R&gt; reshape(d, direction=""long"",varying=list(c(""k1"",""k2"")),v.names=""k"",times=c(""k1"",""k2""))
     iter time   k id
1.k1    1   k1 0.2  1
2.k1    2   k1 0.6  2
1.k2    1   k2 0.3  1
2.k2    2   k2 0.4  2
</code></pre>
"
2065483,135944,2010-01-14T16:02:07Z,2063821,13,TRUE,"<p>Short answer is ""no,"" you can't avoid creating a data frame. <code>ggplot</code> requires the data to be in a data frame. If you use <code>qplot</code>, you can give it separate vectors for x and y, but internally, it's still creating a data frame out of the parameters you pass in. </p>

<p>I agree with juba's suggestion -- learn to use the <code>reshape</code> function, or better yet the <code>reshape</code> package with <code>melt</code>/<code>cast</code> functions. Once you get fast with putting your data in long format, creating amazing <code>ggplot</code> graphs becomes one step closer!</p>
"
2067224,163053,2010-01-14T20:07:37Z,2067098,34,TRUE,"<p>Ordinarily, I would suggest trying the <code>xmlToDataFrame()</code> function, but I believe that this will actually be fairly tricky because it isn't well structured to begin with.  </p>

<p>I would recommend working with this function:</p>

<pre><code>xmlToList(books)
</code></pre>

<p>One problem is that there are multiple authors per book, so you will need to decide how to handle that when you're structuring your data frame.</p>

<p>Once you have decided what to do with the multiple authors issue, then it's fairly straight forward to turn your book list into a data frame with the <code>ldply()</code> function in plyr (or just use lapply and convert the return value into a data.frame by using do.call(""rbind""...).  </p>

<p>Here's a complete example (excluding author):</p>

<pre><code>library(XML)
books &lt;- ""http://www.w3schools.com/XQuery/books.xml""
library(plyr)
ldply(xmlToList(books), function(x) { data.frame(x[!names(x)==""author""]) } )

   .id        title.text title..attrs year price   .attrs
 1 book  Everyday Italian           en 2005 30.00  COOKING
 2 book      Harry Potter           en 2005 29.99 CHILDREN
 3 book XQuery Kick Start           en 2003 49.99      WEB
 4 book      Learning XML           en 2003 39.95      WEB
</code></pre>

<p>Here's what it looks like with author included.  You need to use <code>ldply</code> in this instance since the list is ""jagged""...lapply can't handle that properly.  [Otherwise you can use <code>lapply</code> with <code>rbind.fill</code> (also courtesy of Hadley), but why bother when <code>plyr</code> automatically does it for you?]:</p>

<pre><code>ldply(xmlToList(books), data.frame)

   .id        title.text title..attrs              author year price   .attrs
1 book  Everyday Italian           en Giada De Laurentiis 2005 30.00  COOKING
2 book      Harry Potter           en        J K. Rowling 2005 29.99 CHILDREN
3 book XQuery Kick Start           en      James McGovern 2003 49.99      WEB
4 book      Learning XML           en         Erik T. Ray 2003 39.95      WEB
     author.1   author.2   author.3               author.4
1        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;                   &lt;NA&gt;
2        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;                   &lt;NA&gt;
3 Per Bothner Kurt Cagle James Linn Vaidyanathan Nagarajan
4        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;                   &lt;NA&gt;
</code></pre>
"
2071488,162832,2010-01-15T12:44:14Z,2063821,2,FALSE,"<p>So just to add to the previous answers. With qplot you could do</p>

<pre><code>p &lt;- qplot(y=d$k2, x=d$k1)
</code></pre>

<p>and then from there building it further, e.g. with</p>

<pre><code>p + theme_bw()
</code></pre>

<p>But I agree - melt/cast is genereally the way forward.</p>
"
2071896,134830,2010-01-15T13:44:48Z,2055677,0,TRUE,"<p>A few thoughts:</p>

<p>The results coming after the loop is the standard way for R to provide output &ndash; this is because R does not execute the loop until the expression is complete.  It is not a problem that is specific to <code>sink</code>.  To see this, execute all of your code snippet except the <code>sink</code> commands (so the output goes to the R console) and you will see the same effect.  For clarity, you could add a line like</p>

<pre><code>cat(""Results for iteration"", i, ""\n"")
</code></pre>

<p>at the start of your <code>for</code> loop.</p>

<hr>

<p>As juba pointed out, you don't seem to need a <code>for</code> loop in this instance &ndash; either vectorisation or <code>apply</code> would be better.</p>

<hr>

<p>The standard way of combining code and output into some form of report in R is to use <code>Sweave</code>.  This will create Latex markup that you can then compile into a PDF or PostScript document (with reasonably little effort, once you have some Latex tools set up). The obvious advantage of this is that you can include figures as well.</p>

<hr>

<p>EDIT: There is also the <a href=""http://cran.r-project.org/web/packages/brew/index.html"" rel=""nofollow noreferrer""><code>brew</code> package</a> for a mixed text and code report.</p>
"
2073074,223783,2010-01-15T16:24:35Z,2073000,0,FALSE,"<p>Re: the first part, you can filter out the rows with <code>Frequency == 0</code> by</p>

<pre><code>filteredRows &lt;- data$Frequency != 0
## restrict ourselves to looking at the data where Frequency != 0.
data[filteredRows,]
</code></pre>
"
2073120,163053,2010-01-15T16:30:58Z,2073000,3,TRUE,"<p>For the first part of your question, you can either use the <code>reshape()</code> function that's in base R or use the <code>reshape</code> package (with the <code>cast()</code> function).  </p>

<p>Here's an example of using the <code>reshape()</code> function (from the help file):</p>

<pre><code>wide &lt;- reshape(Indometh, v.names=""conc"", idvar=""Subject"",
              timevar=""time"", direction=""wide"")
</code></pre>

<p>Here's a simple example of using <code>melt</code> and <code>cast</code> (from the reshape help):</p>

<pre><code>library(reshape)
names(airquality) &lt;- tolower(names(airquality))
aqm &lt;- melt(airquality, id=c(""month"", ""day""), na.rm=TRUE)    
cast(aqm, month ~ variable, mean)
</code></pre>
"
2074719,171659,2010-01-15T20:54:23Z,2069836,11,TRUE,"<p>Jan,</p>

<p>Your shapefile has probably been read correctly. It is more likely that the plotting wasn't correct. Try spplot() instead of plot() :</p>

<pre><code>spplot(shp, col.regions=""gray"", col=""blue"")
</code></pre>

<p>If your shapefile has more than one column (which is likely), add zcol= 1 (or any other column) so you get only one panel</p>

<pre><code>spplot(shp, zcol=1, col.regions=""gray"", col=""blue"")
</code></pre>

<p>If you really want to use plot you have to set a color for the background, because otherwise it will print the holes transparent. So set pbg=""white"". Try:</p>

<pre><code>plot(shp, col=""gray"", border=""blue"", axes=TRUE, pbg=""white"")
</code></pre>

<p>By the way, the list I think Dirk was referring to is <a href=""http://n2.nabble.com/R-sig-geo-f2731867.html"" rel=""noreferrer"">R-sig-Geo</a> and you can find there many answers and ask many sig-related questions.</p>
"
2074862,16632,2010-01-15T21:17:56Z,2074606,40,TRUE,"<p>Just treat it like an array and work on each row:</p>

<pre><code>adply(df, 1, transform, max = max(x, y))
</code></pre>
"
2075243,251681,2010-01-15T22:38:13Z,1536590,6,FALSE,"<p>There is a really helpful document on subsetting R data frames at:
<a href=""http://www.ats.ucla.edu/stat/r/modules/subsetting.htm"" rel=""nofollow noreferrer"">http://www.ats.ucla.edu/stat/r/modules/subsetting.htm</a></p>

<p>Here is the relevant excerpt:</p>

<blockquote>
  <p>Subsetting rows using multiple
  conditional statements: There is no
  limit to how many logical statements
  may be combined to achieve the
  subsetting that is desired. The data
  frame x.sub1 contains only the
  observations for which the values of
  the variable y is greater than 2 and
  for which the variable V1 is greater
  than 0.6.</p>
  
  <p><code>x.sub1 &lt;- subset(x.df, y &gt; 2 &amp; V1 &gt; 0.6)</code></p>
</blockquote>
"
2075404,143305,2010-01-15T23:20:03Z,2075327,4,FALSE,"<p>The existing <a href=""http://cran.r-project.org"" rel=""nofollow noreferrer"">CRAN</a> package <a href=""http://cran.r-project.org/web/packages/profr/index.html"" rel=""nofollow noreferrer"">profr</a> and <a href=""http://cran.r-project.org/web/packages/proftools/index.html"" rel=""nofollow noreferrer"">proftools</a> are useful for this.  The latter can use Rgraphviz which isn't always installable.</p>

<p>The <a href=""http://wiki.r-project.org/rwiki/doku.php?id=tips:misc:profiling"" rel=""nofollow noreferrer"">R Wiki page on profiling</a> has additional info and a nice script by Romain which can also visualize (but requires graphviz).</p>
"
2075774,16632,2010-01-16T00:58:12Z,2075327,0,FALSE,"<p>Parsing the output that <code>Rprof</code> generates isn't too hard, and then you get access to absolutely everything.</p>
"
2076500,160314,2010-01-16T07:02:45Z,2076450,4,TRUE,"<p>close. In your example you created a subset of tonnes, but not of week.</p>

<pre><code>sql_results&lt;-structure(list(mine = structure(c(1L, 1L, 1L, 2L, 2L, 1L, 1L, 
1L, 1L), .Label = c(""AA"", ""BB""), class = ""factor""), tonnes = c(112, 
114, 119, 108, 112, 110, 109, 102, 101), week = c(41, 41, 41, 
41, 41, 42, 42, 43, 43)), row.names = c(""1"", ""2"", ""3"", ""4"", ""5"", 
""6"", ""7"", ""8"", ""9""), .Names = c(""mine"", ""tonnes"", ""week""), class = ""data.frame"")

qplot(factor(week), tonnes, data = subset(sql_results,mine==""AA""), geom = ""boxplot"")
</code></pre>
"
2079018,136328,2010-01-16T21:50:21Z,2078946,-2,FALSE,"<p>I am not sure it will work on a dataframe, but you could try one of the apply functions:</p>

<pre><code>`y1 &lt;- sapply(dataframe, gsub(guest.w$Last.Name,"""",guest.w$Party.Name.s.))`
</code></pre>
"
2079174,143377,2010-01-16T22:39:51Z,2078946,1,TRUE,"<p>Using hadleys adply:</p>

<pre><code>library(plyr)
df &lt;- data.frame(rbind(c('Smith', 'Joe Smith, Kevin Smith, Jane Smith'), c('Alter', 'Robert Alter, Mary Alter, Ronald Alter')))
names(df) &lt;- c(""last"", ""name"")
adply(df,1,transform, name=gsub(last, '', name))
</code></pre>

<p>You will probably need to clean up the spaces in your new vector.</p>
"
2079799,143305,2010-01-17T02:54:55Z,2079784,8,TRUE,"<p>Farrel, I do not exactly follow as 'item' is not an R type. Maybe you have a <code>list</code> of length 98 where each elements is a vector of character string?</p>

<p>In that case, consider this:</p>

<pre><code>R&gt; fl &lt;- list(A=c(""un"", ""deux""), B=c(""one""), C=c(""eins"", ""zwei"", ""drei""))
R&gt; lapply(fl, function(x) length(x))
$A
[1] 2

$B
[1] 1

$C
[1] 3
R&gt; do.call(rbind, lapply(fl, function(x) length(x)))
  [,1]
A    2
B    1
C    3
R&gt; 
</code></pre>

<p>So there is you vector of the length of your list, telling you many strings each list element had.  Not the last <code>do.call(rbind, someList)</code> as we got a list back from <code>lapply</code>.</p>

<p>If on the other hand you want to count the length of all the strings at each list position, replace the simply <code>length(x)</code> with a new function counting the characters:</p>

<pre><code>R&gt; lapply(fl, function(x) { sapply(x, function(y) nchar(y)) } )
$A
  un deux 
   2    4 

$B
one 
  3 

$C
eins zwei drei 
   4    4    4 

R&gt; 
</code></pre>

<p>if that is not want you want, maybe you could mock up some example input data?</p>

<p><strong><em>Edit:</em></strong>:  In response to your comments, what you wanted is probably</p>

<pre><code>R&gt; do.call(rbind, lapply(fl, length))
  [,1]
A    2
B    1
C    3
R&gt; 
</code></pre>

<p>Note that I pass in <code>length</code>, the name of a function, and not <code>length()</code>, the (displayed) body of a function.  Because that is easy to mix up, I simply apply almost always wrap an anonymous function around as in my first answer.  </p>

<p>And yes, this can also be done with just <code>sapply</code> or even some of the <code>**ply</code> functions:</p>

<pre><code>R&gt; sapply(fl, length)
A B C 
2 1 3 
R&gt; laply(fl, length)
[1] 2 1 3
R&gt; 
</code></pre>
"
2079816,226037,2010-01-17T03:01:30Z,2079772,6,TRUE,"<p>Each of the means that are generated from the bivariate Gaussian distribution are simply single points sampled in exactly the same way as any other random points that could be generated from the distribution.  The fact that they use these generated points to be the means of new distributions is irrelevant. </p>

<p>Let's say that each of the 10 means is then used to construct a new bivariate Gaussian.</p>

<blockquote>
  <p>means ~ N( (1,0), I)</p>
</blockquote>

<p>Where <strong>~</strong> indicates a value being drawn from the distribution.  Since the distribution being sampled from in this case is a bivariate Gaussian, each of the data points sampled will be a 2-dimensional point (x1, y1).</p>

<p>Each of these points sampled from the original distribution can then be used to make a new distribution.</p>

<p>Example:</p>

<pre><code>means = [ (x1,y1), (x2,y2), ..., (x10,y10) ]
</code></pre>

<p>To build new bivariate Gaussians:</p>

<pre><code>N1((x1,x2), I), N2((x2,y2), I), ..., N10((x10,y10), I)
</code></pre>

<p>They are just using the initial bivariate Gaussian distribution N((1,0), I) as an easy way to pick 10 random means that are distributed normally.</p>
"
2079836,66549,2010-01-17T03:11:58Z,2079784,0,FALSE,"<p>The code below accepts a list and returns a vector of lengths:</p>

<pre><code>x = c(""vectors"", ""matrices"", ""arrays"", ""factors"", ""dataframes"", ""formulas"", 
      ""shingles"", ""datesandtimes"", ""connections"", ""lists"")
xl = list(x)
fnx = function(xl){length(unlist(strsplit(x, """")))}
lv = sapply(x, fnx)
</code></pre>
"
2080930,163053,2010-01-17T12:24:05Z,2080774,9,FALSE,"<p>Here you go, using <code>combn</code> and <code>apply</code>:</p>

<pre><code>&gt; x2 &lt;- t(apply(x, 1, combn, 2, prod))
</code></pre>

<p>Setting the column names can be done with two <code>paste</code> commands:</p>

<pre><code>&gt; colnames(x2) &lt;- paste(""Inter.V"", combn(1:4, 2, paste, collapse=""V""), sep="""")
</code></pre>

<p>Lastly, if you want all your variables together, just <code>cbind</code> them:</p>

<pre><code>&gt; x &lt;- cbind(x, x2)
&gt;   V1 V2 V3 V4 Inter.V1V2 Inter.V1V3 Inter.V1V4 Inter.V2V3 Inter.V2V4 Inter.V3V4
1  1  9 25 18          9         25         18        225        162        450
2  2  5 20 10         10         40         20        100         50        200
3  3  4 30 12         12         90         36        120         48        360
4  4  4 34 16         16        136         64        136         64        544
</code></pre>
"
2081800,23771,2010-01-17T17:05:04Z,2075327,2,FALSE,"<p><strong>Rprof</strong> takes samples of the call stack at intervals of time - that's the good news.</p>

<p>What I would do is get access to the raw stack samples (<a href=""https://stackoverflow.com/questions/406760/whats-your-most-controversial-programming-opinion/1562802#1562802"">stackshots</a>) that it collects, and pick several at random and examine them. What I'm looking for is call sites (not just functions, but the places where one function calls another) that appear on multiple samples. For example, if a call site appears on 50% of samples, then that's what it costs, because its possible removal would save roughly 50% of total time. (Seems obvious, right? But it's not well known.)</p>

<p>Not every costly call site is optimizable, but some are, <em>unless</em> the program is already as fast as possible.</p>

<p>(Don't be distracted by issues like how many samples you need to look at. If something is going to save you a reasonable fraction of time, then it appears on a similar fraction of samples. The exact number doesn't matter. What matters is that you find it. Also don't be distracted by graph and recursion and time measurement and counting issues. What matters is, for each call site you see, the fraction of stack samples that show it.)</p>
"
2082278,160314,2010-01-17T19:21:50Z,2080774,26,TRUE,"<p>Here is a one liner for you that also works if you have factors:</p>

<pre><code>&gt; model.matrix(~(V1+V2+V3+V4)^2,x)
  (Intercept) V1 V2 V3 V4 V1:V2 V1:V3 V1:V4 V2:V3 V2:V4 V3:V4
1           1  1  9 25 18     9    25    18   225   162   450
2           1  2  5 20 10    10    40    20   100    50   200
3           1  3  4 30 12    12    90    36   120    48   360
4           1  4  4 34 16    16   136    64   136    64   544
attr(,""assign"")
 [1]  0  1  2  3  4  5  6  7  8  9 10
</code></pre>
"
2086660,170792,2010-01-18T14:30:46Z,2084192,4,FALSE,"<p>Q1. You could use</p>

<pre><code>pred&lt;-prediction(as.numeric(pred), as.numeric(iris$Species))
auc&lt;-performance(pred,""auc"")
</code></pre>

<p>BUT. number of classes is not equal to 2.
ROCR currently supports only evaluation of binary classification tasks (according to the error I got)</p>

<p>Q2. I don't think that the second can be done the way you want. I can only think to perform cross validations manualy i.e.</p>

<p>Get resample.indices (from package peperr)</p>

<pre><code>cv.ind &lt;- resample.indices(nrow(iris), sample.n = 10, method = c(""cv""))
x &lt;- lapply(cv.ind$sample.index,function(x){iris[x,1:2]})
y &lt;- lapply(cv.ind$sample.index,function(x){iris[x,5]})
</code></pre>

<p>then generate models and predictions for each cv sample</p>

<pre><code>model1&lt;-svm(x[[1]],y[[1]])
pred1&lt;-predict(model1,x[[1]])
</code></pre>

<p>etc. 
Then you could manualy construct a list like ROCR.xval </p>
"
2090903,66549,2010-01-19T03:38:55Z,2087735,0,FALSE,"<p>Is the technique you're interested in '<strong>k-means clustering</strong>'?  If so, here's how the centroids are calculated at each iteration:</p>

<ol>
<li><p>choose a k value (an integer that
specifies the number of clusters to
divide your data set);</p></li>
<li><p>random select k rows from your data
set, those are the centroids for the
1st iteration;</p></li>
<li><p>calculate the distance that each
data point is from each centroid;</p></li>
<li><p>each data point has a 'closest
centroid', that determines its
'group';</p></li>
<li><p>calculate the mean for each
    group--those are the new centroids;</p></li>
<li><p>back to step 3 (stopping criterion
is usually based on comparison with
the respective centroid values in
successive loops, i.e., if they
values change not more than 0.01%,
then quit).</p></li>
</ol>

<p>Those steps in code:</p>

<pre><code># toy data set
mx = matrix(runif60, 10, 99), nrow=12, ncol=5, byrow=F)
cndx = sample(nrow(mx), 2)
# the two centroids at iteration 1
cn1 = mx[cndx[1],]
cn2 = mx[cndx[2],]
# to calculate Pearson similarity
fnx1 = function(a){sqrt((cn1[1] - a[1])^2 + (cn1[2] - a[2])^2)}
fnx2 = function(a){sqrt((cn2[1] - a[1])^2 + (cn2[2] - a[2])^2)}
# calculate distance matrix
dx1 = apply(mx, 1, fnx1)
dx2 = apply(mx, 1, fnx2)
dx = matrix(c(dx1, dx2), nrow=2, ncol=12)
# index for extracting the new groups from the data set
ndx = apply(dx, 1, which.min)
group1 = mx[ndx==1,]
group2 = mx[ndx==2,]
# calculate the new centroids for the next iteration
new_cnt1 = apply(group1, 2, mean)
new_cnt2 = apply(group2, 2, mean)
</code></pre>
"
2092513,168747,2010-01-19T09:44:09Z,2087735,5,TRUE,"<p>At first I don't now if you distance formula is alright. I think there should be <code>sqrt(sum((x-cent)^2))</code> or <code>sum(abs(x-cent))</code>. I assumed first.
Second thought is that just printing solution is not good idea. So I first compute, then print.
Third - I recommend using plyr but I give both (with and without plyr) solutions.</p>

<pre><code># Simulated data:
n &lt;- 100
data.matrix &lt;- cbind(
  data.frame(matrix(runif(26*n), n, 26)),
  cluster=sample(letters[1:6], n, replace=TRUE)
)
cluster_col &lt;- which(names(data.matrix)==""cluster"")

# With plyr:
require(plyr)
candidates &lt;- dlply(data.matrix, ""cluster"", function(data) {
  dists &lt;- colSums(laply(data[, -cluster_col], function(x) (x-mean(x))^2))
  rownames(data)[dists==min(dists)]
})

l_ply(names(candidates), function(c_name, c_list=candidates[[c_name]]) {
    print(paste(""Candidates for cluster "",c_name))
    print(c_list)
})

# without plyr
candidates &lt;- tapply(
  1:nrow(data.matrix),
  data.matrix$cluster,
  function(id, data=data.matrix[id, ]) {
    dists &lt;- rowSums(sapply(data[, -cluster_col], function(x) (x-mean(x))^2))
    rownames(data)[dists==min(dists)]
  }
)

invisible(lapply(names(candidates), function(c_name, c_list=candidates[[c_name]]) {
    print(paste(""Candidates for cluster "",c_name))
    print(c_list)
}))
</code></pre>
"
2096494,211116,2010-01-19T19:39:16Z,2096473,6,FALSE,"<pre><code>.Platform$OS.type
</code></pre>

<p>returns</p>

<pre><code>[1] ""unix""
</code></pre>

<p>or something else.</p>
"
2096503,142879,2010-01-19T19:39:59Z,2096473,26,TRUE,"<pre><code>if(.Platform$OS.type == ""unix"") {
} else {

}
</code></pre>
"
2096510,172261,2010-01-19T19:42:15Z,2096473,10,FALSE,"<pre><code>Sys.info()[""sysname""]
</code></pre>
"
2096850,170792,2010-01-19T20:34:11Z,1563961,3,FALSE,"<p>For the top 5% also:    </p>

<pre><code>head(data[order(data$V2,decreasing=T),],.05*nrow(data))
</code></pre>
"
2096910,170792,2010-01-19T20:44:38Z,1296646,30,FALSE,"<p>or you can use package doBy</p>

<pre><code>library(doBy)
dd &lt;- orderBy(~-z+b, data=dd)
</code></pre>
"
2098442,238699,2010-01-20T01:21:13Z,2098368,297,TRUE,"<p>Try using an empty <strong>collapse</strong> argument within the paste function:</p>

<p><code>paste(sdata, collapse = '')</code></p>

<p>Thanks to <a href=""http://twitter.com/onelinetips/status/7491806343"">http://twitter.com/onelinetips/status/7491806343</a></p>
"
2100435,170792,2010-01-20T10:01:35Z,2052174,1,FALSE,"<p>Simulated data:</p>

<pre><code>data &lt;- data.frame(matrix(ifelse(runif(40)&gt;.5,""YES"",letters[1:26]), 10, 4))
</code></pre>

<p>Suppose you want to change columns X2 and X4</p>

<pre><code>cols &lt;- c(""X2"",""X4"")
data[,cols] &lt;- apply(data[cols],2,function(x) ifelse(x==""YES"",1,0))
</code></pre>
"
2102076,143305,2010-01-20T14:38:56Z,2102017,12,TRUE,"<p>Use <code>mtext</code> with option <code>outer</code>:</p>

<pre><code>set.seed(42)
oldpar &lt;- par(mfrow=c(1,2), mar=c(3,3,1,1), oma=c(0,0,3,1))  ## oma creates space 
plot(cumsum(rnorm(100)), type='l', main=""Plot A"")
plot(cumsum(rnorm(100)), type='l', main=""Plot B"")
mtext(""Now isn't this random"", side=3, line=1, outer=TRUE, cex=2, font=2)
par(oldpar)
</code></pre>
"
2103245,163053,2010-01-20T16:58:22Z,2103152,3,FALSE,"<p>There are many options for something like this.  One option is to use the <code>geom_tile</code> in ggplot2:</p>

<pre><code>library(ggplot2)
ggplot(melt(volcano), aes(x=X1, y=X2, fill=value)) + geom_tile()
</code></pre>

<p>Ends up looking like this:
<a href=""http://had.co.nz/ggplot2/graphics/75407db4900d5077b412310bc4956cb0.png"" rel=""nofollow noreferrer"">alt text http://had.co.nz/ggplot2/graphics/75407db4900d5077b412310bc4956cb0.png</a></p>

<p>Some other options include: <code>levelplot</code> (in lattice) or <code>color2D.matplot</code> (in plotrix).</p>
"
2103301,143305,2010-01-20T17:06:31Z,2103152,4,TRUE,"<p>No need to go to extra packages. Base R already has this, see</p>

<ul>
<li><p><code>help(image)</code></p></li>
<li><p><code>help(heatmap)</code></p></li>
</ul>

<p>and Romain's excellent <a href=""http://addictedtor.free.fr/graphiques/"" rel=""nofollow noreferrer"">R Graph Gallery</a> which has a searchable index.</p>
"
2103460,169947,2010-01-20T17:28:53Z,2098368,28,FALSE,"<p>Matt's answer is definitely the right answer.  However, here's an alternative solution for comic relief purposes:</p>

<pre><code>do.call(paste, c(as.list(sdata), sep=""""))
</code></pre>
"
2104532,163053,2010-01-20T20:04:31Z,2104483,12,TRUE,"<p>I can't test it on your data, but you will want to use an <code>apply</code> type function like this:</p>

<pre><code>data &lt;- do.call(""rbind"", lapply(c(""file1"", ""file2""), function(fn) 
           data.frame(Filename=fn, read.csv(fn)
))
</code></pre>

<p>Or, you can simplify it by using <code>plyr</code>.  Here's a rough simulation of how that would work (using data frames instead of files):</p>

<pre><code>&gt; df1 &lt;- data.frame(c1=1:5, c2=rnorm(5))
&gt; df2 &lt;- data.frame(c1=3:7, c2=rnorm(5))
</code></pre>

<p>In this case I will use <code>get</code> instead of <code>read.csv</code>:</p>

<pre><code>&gt; data &lt;- ldply(c(""df1"", ""df2""), function(dn) data.frame(Filename=dn, get(dn)))
&gt; data
  Filename c1          c2
1  df1  1 -0.15679732
2  df1  2 -0.19392102
3  df1  3  0.01369413
4  df1  4 -0.73942829
5  df1  5 -1.27522427
6  df2  3 -0.33944114
7  df2  4 -0.12509065
8  df2  5  0.11225053
9  df2  6  0.88460684
10 df2  7 -0.70710520
</code></pre>

<p><em>Edit</em> </p>

<p>Taking Marek's suggestion, you can either overwrite or create your own function:</p>

<pre><code>read.tables &lt;- function(file.names, ...) {
    require(plyr)
    ldply(file.names, function(fn) data.frame(Filename=fn, read.csv(fn, ...)))
}

data &lt;- read.tables(c(""filename1.csv"", ""filename2.csv""))
</code></pre>
"
2104869,184403,2010-01-20T20:51:49Z,2104483,10,FALSE,"<p>Try this:</p>

<pre><code>## take files.
files &lt;- list.files(pattern="".csv"")
## read data using loop
DF &lt;- NULL
for (f in files) {
   dat &lt;- read.csv(f, header=T, sep=""\t"", na.strings="""", colClasses=""character"")
   DF &lt;- rbind(DF, dat)
}
</code></pre>
"
2105912,213185,2010-01-20T23:39:15Z,2078468,5,FALSE,"<p>From the developer, it seems to work nicely.</p>

<pre><code>  shpFile &lt;- system.file(""shapes/sids.shp"", package=""maptools"");  
  shp&lt;-importShapefile(shpFile,projection=""LL"");  
  bb &lt;- qbbox(lat = shp[,""Y""], lon = shp[,""X""]);  
  MyMap &lt;- GetMap.bbox(bb$lonR, bb$latR, destfile = ""SIDS.jpg"");  
  #compute regularized SID rate  
  sid &lt;- 100*attr(shp, ""PolyData"")$SID74/(attr(shp, ""PolyData"")$BIR74+500)  
  b &lt;- as.integer(cut(sid, quantile(sid, seq(0,1,length=8)) ));  
  b[is.na(b)] &lt;- 1;  
  opal &lt;- col2rgb(grey.colors(7), alpha=TRUE)/255;  opal[""alpha"",] &lt;- 0.2;  
  shp[,""col""] &lt;- rgb(0.1,0.1,0.1,0.2);  
  for (i in 1:length(b)) shp[shp[,""PID""] == i,""col""] &lt;- rgb(opal[1,b[i]],opal[2,b[i]],opal[3,b[i]],opal[4,b[i]]);  
  PlotPolysOnStaticMap(MyMap, shp, lwd=.5, col = shp[,""col""], add = F);
</code></pre>
"
2108202,253216,2010-01-21T09:49:42Z,2078946,0,FALSE,"<p>you probably need to do some ""wrapping"" around your expression in order to get the apply() function working:</p>

<ul>
<li>If your working on a data.frame you should use apply() (and not sapply())</li>
<li>you must create a function for apply (with a return clause)</li>
<li>working on data.frame line as function input is a bit tricky - they are converted into vectors and loose some properties (you can't use the $ sign to call named fields) so it's better to convert it first into a list</li>
</ul>

<p>The final result looks something like this:</p>

<pre><code>df &lt;- rbind(c('Smith', 'Joe Smith, Kevin Smith, Jane Smith'), c('Alter', 'Robert Alter, Mary Alter, Ronald Alter'))
colnames(df) = c('Last.Name', 'Party.Name.s.')
apply(df,1,function(y) {y = as.list(y);return(gsub(y$Last.Name, """", y$Party.Name.s.))}) 
</code></pre>
"
2108587,170792,2010-01-21T10:55:57Z,2108484,14,TRUE,"<p>You could try</p>

<pre><code>facet_wrap( ~ location, ncol = 4)
</code></pre>
"
2110380,134830,2010-01-21T15:28:13Z,2108174,2,TRUE,"<p>A simplified, reproducible version of the linked answer is</p>

<pre><code>x &lt;- rlnorm(1000)
hx &lt;- hist(x, plot=FALSE)
plot(hx$counts, type=""h"", log=""y"", lwd=10, lend=""square"")
</code></pre>

<p>To get the axes looking more ""hist-like"", replace the last line with</p>

<pre><code>plot(hx$counts, type=""h"", log=""y"", lwd=10, lend=""square"", axes = FALSE)
Axis(side=1)
Axis(side=2)
</code></pre>

<p>Getting the bars to join up is going to be a nightmare using this method.  I suggest using trial and error with values of <code>lwd</code> (in this example, 34 is somewhere close to looking right), or learning to use <code>lattice</code> or <code>ggplot</code>.</p>

<hr>

<p>EDIT:
You can't set a border colour, because the bars aren't really rectangles &ndash; they are just fat lines.  We can fake the border effect by drawing slightly thinner lines over the top.  The updated code is</p>

<pre><code>par(lend=""square"")
bordercol &lt;- ""blue""
fillcol &lt;- ""pink""
linewidth &lt;- 24
plot(hx$counts, type=""h"", log=""y"", lwd=linewidth, col=bordercol, axes = FALSE)
lines(hx$counts, type=""h"", lwd=linewidth-2, col=fillcol)
Axis(side=1)
Axis(side=2)
</code></pre>
"
2110848,158065,2010-01-21T16:23:20Z,2108174,1,FALSE,"<p>How about using ggplot2?</p>

<pre><code>x &lt;- rnorm(1000)
qplot(x) + scale_y_log10()
</code></pre>

<p>But I agree with Hadley's comment on the other post that having a histogram with a log scale seems weird to me =).</p>
"
2114270,158065,2010-01-22T01:11:51Z,2113855,5,TRUE,"<p>One problem is that you are specifying the expression as a string so it will be evaluated into a string.  If you want to parse a string into an expression, you need to use the <code>parse</code> command:</p>

<pre><code>&gt; ""x+y""
[1] ""x+y""
&gt; parse(text=""x+y"")
expression(x+y)
attr(,""srcfile"")
&lt;text&gt; 
</code></pre>

<p>But yeah, why not use <code>apply</code> and functions instead?</p>
"
2114438,143305,2010-01-22T01:49:07Z,2113855,5,FALSE,"<p>You may be thinking about this in the wrong way and just over-complicate things. There is already an addition defined for <code>ts</code> objects:</p>

<pre><code>R&gt; set.seed(42)
R&gt; x &lt;- ts(rnorm(10), frequency = 4, start = c(1959, 2))
R&gt; y &lt;- ts(rnorm(10), frequency = 4, start = c(1959, 2))
R&gt; z &lt;- x + y
R&gt; cbind(x,y,z)
               x       y       z
1959 Q2  1.37096  1.3049  2.6758
1959 Q3 -0.56470  2.2866  1.7219
1959 Q4  0.36313 -1.3889 -1.0257
1960 Q1  0.63286 -0.2788  0.3541
1960 Q2  0.40427 -0.1333  0.2709
1960 Q3 -0.10612  0.6360  0.5298
1960 Q4  1.51152 -0.2843  1.2273
1961 Q1 -0.09466 -2.6565 -2.7511
1961 Q2  2.01842 -2.4405 -0.4220
1961 Q3 -0.06271  1.3201  1.2574
R&gt; 
</code></pre>

<p>You don't really need an expression parser to operate on R objects.</p>
"
2117080,170792,2010-01-22T12:13:49Z,1568511,100,FALSE,"<p>what about this one</p>

<pre><code>x[order(match(x,y))]
</code></pre>
"
2118649,170352,2010-01-22T16:18:14Z,2107665,2,FALSE,"<p>Sounds like a problem with the device rather than R, try reinstalling GTK+. If that doesn't work try plotting jpegs instead of png's if you can. </p>
"
2118738,163053,2010-01-22T16:31:35Z,2118698,2,FALSE,"<p>It may be a list (based on the error message).  Have you tried <code>class(d1[5])</code>?  If it's a list, then you would expect either <code>d1[[5]]</code> or <code>d1[5][[1]]</code> to be numeric.</p>

<p><em>Edit:</em></p>

<p>Given that d1[5] is itself a data frame, you need to treat it as such.  Something like this should work:</p>

<pre><code>is.numeric(d1[5][,1])
</code></pre>
"
2118751,121332,2010-01-22T16:34:06Z,2118698,4,TRUE,"<pre><code>&gt; is.numeric_data.frame=function(x)all(sapply(x,is.numeric))

&gt; is.numeric_data.frame(d1[[5]])
[1] TRUE 
</code></pre>

<h2>Why</h2>

<p><code>d1</code> is a list, hence <code>d1[5]</code> is a list of length 1, and in this case contains a <code>data.frame</code>.  to get the data frame, use <code>d1[[5]]</code>.</p>

<p>Even if a data frame contains numeric data, it isn't numeric itself:</p>

<pre><code>&gt; x = data.frame(1:5,6:10)
&gt; is.numeric(x)
[1] FALSE
</code></pre>

<p>Individual columns in a data frame are either numeric or not numeric.  For instance:</p>

<pre><code>&gt; z &lt;- data.frame(1:5,letters[1:5])

&gt; is.numeric(z[[1]])
[1] TRUE
&gt; is.numeric(z[[2]])
[1] FALSE
</code></pre>

<p>If you want to know if ALL columns in a data frame are numeric, you can use <code>all</code> and <code>sapply</code>:</p>

<pre><code>&gt; sapply(z,is.numeric)
    X1.5 letters.1.5. 
    TRUE        FALSE 

&gt; all(sapply(z,is.numeric))
[1] FALSE

&gt; all(sapply(x,is.numeric))
[1] TRUE
</code></pre>

<p>You can wrap this all up in a convenient function:</p>

<pre><code>&gt; is.numeric_data.frame=function(x)all(sapply(x,is.numeric))

&gt; is.numeric_data.frame(d1[[5]])
[1] TRUE 
</code></pre>
"
2118781,37751,2010-01-22T16:37:59Z,2118698,2,FALSE,"<p>d1[5] is not a single value. It's a vector (possibly a list?) of values. If you grab a single value I bet it is numeric. For example:</p>

<pre><code>is.numeric(d1[5][[1]])
as.numeric(d1[5][[1]])
</code></pre>

<p>So I think the confusion is between the column object and the elements in the column. R makes a distinction between those two ideas while other languages, like SQL, functionally assume that when discussing the column you're usually referring to the elements of the column. </p>

<p>This <a href=""http://cran.r-project.org/doc/manuals/R-lang.html#Indexing"" rel=""nofollow noreferrer"">discussion of indexing</a> from the R Language Definition doc really helped me wrap my head around how to reference items in R. </p>
"
2118960,163053,2010-01-22T17:01:49Z,2118929,5,TRUE,"<p>I already included my comments about debugging practices <a href=""https://stackoverflow.com/questions/1882734/what-is-your-favorite-r-debugging-trick"">in this related question</a>.  But regarding the specific message that you show here: that means that you're trying to write out 2 rows to some dataset that has 0 rows.  Something like this:</p>

<pre><code>x &lt;- data.frame(y=NULL)
x$y &lt;- 1:2
</code></pre>
"
2119523,256989,2010-01-22T18:29:22Z,1355355,9,FALSE,"<p>what about:</p>

<pre><code>t &lt;- c(""bob_smith"",""mary_jane"",""jose_chung"",""michael_marx"",""charlie_ivan"")

sub(""_.*"", """", t)
</code></pre>
"
2119951,170792,2010-01-22T19:40:13Z,2119750,1,FALSE,"<p>Take a look here. It may be helpful
<a href=""http://learnr.wordpress.com/2010/01/03/directlabels-adding-direct-labels-to-ggplot2-and-lattice-plots/"" rel=""nofollow noreferrer"">Adding direct labels to ggplot2 and lattice plots</a></p>
"
2120371,216064,2010-01-22T20:44:38Z,2113855,1,FALSE,"<p>Now I found a solution:</p>

<pre><code>window.exprTs &lt;- function(z, ...) {  
  names(z$parents)&lt;-NULL  
  do.call(z$expr, lapply(z$parents, window, ...))  
}  

plus &lt;- function(x, ...) if (nargs() == 1) x else x + Recall(...)  
z &lt;- exprTs(plus, parents=list(x=x, y=y))
</code></pre>

<p>Now  </p>

<pre><code>window(z, start=1960, end=1960.75) 
</code></pre>

<p>gives the desired result.</p>
"
2120961,194742,2010-01-22T22:20:29Z,2055947,0,TRUE,"<p>I agree that this is not an R question, but it is, nevertheless, a valid time-series question. As far as I know, the theory is not very clear on how the IC is calculated for exponential smoothing models. But there have been few studies. See, for example, <a href=""http://www.buseco.monash.edu.au/ebs/pubs/wpapers/2005/wp6-05.pdf"" rel=""nofollow noreferrer"">this paper</a>. See also Prof. Hyndman's comment to a related question <a href=""https://stackoverflow.com/questions/1401872/on-the-issue-of-automatic-time-series-fitting-using-r/1402739#1402739"">here</a>.</p>

<p>Be careful - don't use IC to compare between, for e.g., exponential smoothing and ARIMA models. See the comment in <a href=""https://stackoverflow.com/questions/1401872/on-the-issue-of-automatic-time-series-fitting-using-r/1415983#1415983"">this post</a>.</p>
"
2121496,121332,2010-01-23T00:32:46Z,2121484,7,TRUE,"<p>To do 1-d smoothing in either the vertical or horizontal axis, use apply:</p>

<pre><code>apply(myarray,1,smooth.spline)
</code></pre>

<p>or</p>

<pre><code>apply(myarray,2,smooth.spline)
</code></pre>

<p>I'm not familiar with 2-D smoothing, but a quick experiment with the fields package seemed to work.  You will need to install the package <code>fields</code> and it's dependencies.  Where <code>myMatrix</code> is the matrix you had above...  (I recreate it):</p>

<pre><code># transform data into x,y and z
m = c(1,1,2,1,1,5,6,3,2,3,2,1,1,1,1,1)
myMatrix = matrix(m,4,4,T)
myMatrix
     [,1] [,2] [,3] [,4]
[1,]    1    1    2    1
[2,]    1    5    6    3
[3,]    2    3    2    1
[4,]    1    1    1    1
Z = as.vector(myMatrix)
XY=data.frame(x=as.numeric(gl(4,1,16),Y=as.numeric(gl(4,4,16))
t=Tps(XY,Z)
surface(t)
</code></pre>

<p>Produced a pretty plot.</p>
"
2121602,143305,2010-01-23T01:02:41Z,2121484,7,FALSE,"<p><em>Smoothing</em> is a big topic, and many functions are available in R itself and via additional packages from places like <a href=""http://cran.r-project.org"" rel=""noreferrer"">CRAN</a>.  The popular book '<em>Modern Applied Statistics with S</em>' by Venables and Ripley lists a number of them in Section 8.1:
(I think -- my 4th edition is at work) and Figure 8.1:</p>

<ul>
<li>Polynomial regression: <code>lm(y ~ poly(x))</code></li>
<li>Natural splines: <code>lm(y ~ ns(x))</code></li>
<li>Smoothing splines: <code>smooth.splines(x, y)</code></li>
<li>Lowess: <code>lowess(x, y)</code> (and a newer / preferred method</li>
<li>ksmooth: <code>ksmooth(x, y)</code></li>
<li>supsmu: <code>spusmu(x, y)</code></li>
</ul>

<p>If you install the <a href=""http://cran.r-project.org/package=MASS"" rel=""noreferrer"">MASS</a> package that goes with the book, you can run this via the file  <code>scripts/ch08.R</code> and experiment yourself.</p>
"
2123204,3333,2010-01-23T13:02:43Z,2123195,1,TRUE,"<p><a href=""http://lucene.apache.org/nutch/"" rel=""nofollow noreferrer"">Nutch</a> is a decent enough crawler, but you'd have to do your own analysis on the indexed data.</p>
"
2123377,16632,2010-01-23T14:00:46Z,2082553,3,TRUE,"<p>Unfortunately ggplot2 is slow at the moment.  However, I have received a generous donation that will allow me to work on performance over summer, so I hope it will be substantially improved.</p>
"
2123916,163053,2010-01-23T17:05:28Z,2123195,2,FALSE,"<p>You could also do this in R with a combination of something like <a href=""http://www.omegahat.org/RCurl/"" rel=""nofollow noreferrer"">RCurl</a> or <a href=""http://cran.r-project.org/web/packages/XML/"" rel=""nofollow noreferrer"">XML</a> (to get the blog posts) and something like <a href=""http://cran.r-project.org/web/packages/igraph/index.html"" rel=""nofollow noreferrer"">igraph</a> (for the SNA).  You will need to parse the HTML to get all the links, and the XML package can handle that kind of processing very easily.</p>

<p>Have a look at <a href=""https://stackoverflow.com/questions/1894190/basic-sna-in-r-how-to-load-network-data"">this related question</a> for some pointers on the SNA analysis, although this is a big field of study.</p>
"
2124033,180348,2010-01-23T17:40:35Z,2123968,19,TRUE,"<p>You could use the drop elements syntax:</p>

<pre><code>&gt; (1:10)[-(1:4)]
[1]  5  6  7  8  9 10
</code></pre>
"
2124053,66549,2010-01-23T17:48:43Z,2123195,3,FALSE,"<p>By ""mapping"" I'm not sure if you are referring to mapping of raw data to an orthodox graph data structure or mapping of that data structure to an aesthetics library in order to render it. If the former, then i would guess it's a straightforward matter of writing a function to translate raw data (w/r/t which blogs link to which, and how much) into a graph data structure, such as an adjacency matrix. Mapping such a data structure for viewing can be done like this:</p>

<pre><code>library(Rgraphviz)
# create an synthetic adjacency matrix for 10 blogs
M = sapply(rep(10, 10), function(x){sample(c(0, 1), 10, T, c(0.7, 0.3))})
colnames(M) = paste(rep(""b"", 10), 1:10, sep=""-"")
rownames(M) = colnames(M) 
# 0's down the main diagonal (eliminate self-edges)
diag(M) = rep(0, 10)
# call the graphviz constructor, passing in adjacency matrix
M_gr = new(""graphAM"", adjMat=M, edgemode=""directed"")
g1 = layoutGraph(M_gr)
# (optional) aesthetic parameters for nodes &amp; edges
graph.par( list(edges = list(col=""gray"", lty=""dashed"", lwd=1), 
            nodes = list( col=""midnightblue"", shape=""ellipse"", 
               textCol=""darkred"", fill=""#B0B7C6"", fontsize=11, 
               lty=""dotted"", lwd=2)) )
# call the device driver
png(file='somefilename.png', width=600, height=460, res=128)
# call the plot function
renderGraph(g1)
# kill the device
dev.off()
</code></pre>

<p><a href=""http://img13.imageshack.us/img13/7683/bloggraph.png"" rel=""nofollow noreferrer"">alt text http://img13.imageshack.us/img13/7683/bloggraph.png</a></p>

<p>If you want to show not just connections but the <strong>strength</strong> of those connections, e.g., number, or perhaps frequency of links from one blog to another, you can do that by setting line thickness individually, through the parameter 'lwd', which i've set at 2 for all edges, for this example (another option is to show connection strength by line type, e.g., dotted, dashed, solid, color). Of course, these edge weights will have to be set in your adjacency matrix, which is simple enough--instead of '0'/'1' to represent 'not connected'/connected, you'll probably want to use '0'/'integers'.</p>
"
2125296,143305,2010-01-23T23:59:35Z,2125231,5,FALSE,"<p>A couple of things:</p>

<p>1) Mock-up data is useful as we don't know exactly what you're faced with. Please supply data if possible. Maybe I misunderstood in what follows?</p>

<p>2) Don't use <code>[[2]]</code> to index your data.frame, I think [,""colname""] is much clearer</p>

<p>3) If the only difference is a trailing ' 09' in the name, then simply regexp that out:</p>

<pre><code>R&gt; x1 &lt;- c(""foo 09"", ""bar"", ""bar 09"", ""foo"")
R&gt; x2 &lt;- gsub("" 09$"", """", x1)
[1] ""foo"" ""bar"" ""bar"" ""foo""
R&gt; 
</code></pre>

<p>Now you should be able to do your subset on the on-the-fly transformed data:</p>

<pre><code>R&gt; data &lt;- data.frame(value=1:4, name=x1)
R&gt; subset(data, gsub("" 09$"", """", name)==""foo"")
  value   name
1     1 foo 09
4     4    foo
R&gt; 
</code></pre>

<p>You could also have replace the name column with regexp'ed value.</p>
"
2126251,6372,2010-01-24T07:04:02Z,2126124,3,FALSE,"<p><code>print</code> is a generic function, with options that may be specific to the thing it's printing.  In this case, options are provided by <code>print.survfit</code>.  Although my version of <code>survival</code> actually doesn't have a <code>show.rmean</code>. Perhaps you want <code>print.rmean</code>?  See the help for <code>print.survfit</code> for more information.</p>

<pre><code>&gt; print(survfit(Surv(aml1$time,aml1$status)~1), print.rmean=T)
Call: survfit(formula = Surv(aml1$time, aml1$status) ~ 1)

   records      n.max    n.start     events     *rmean *se(rmean)     median 
     23.00      23.00      23.00      18.00      36.36       9.85      27.00 
   0.95LCL    0.95UCL 
     18.00      45.00 
    * restricted mean with upper limit =  161 
</code></pre>
"
2126956,168747,2010-01-24T12:09:44Z,2125231,14,TRUE,"<p>First of all (as Jonathan done in his comment) to reference second column you should use either <code>data[[2]]</code> or <code>data[,2]</code>. But if you are using subset you could use column name: <code>subset(data, CompanyName == ...)</code>.</p>

<p>And for you question I will do one of:</p>

<pre><code>subset(data, data[[2]] %in% c(""Company Name 09"", ""Company Name""), drop = TRUE) 
subset(data, grepl(""^Company Name"", data[[2]]), drop = TRUE)
</code></pre>

<p>In second I use <code>grepl</code> (introduced with R version 2.9) which return logical vector with <code>TRUE</code> for match. </p>
"
2127946,257861,2010-01-24T17:14:41Z,2127926,4,FALSE,"<pre><code>x = rnorm(100)
hist(x,br=10,col=c(rep(0,9),1))
</code></pre>

<p>Clearly this will color the last column so tweak the col= bit for your needs</p>

<p>Thanks</p>

<p>dangerstat</p>
"
2129961,57458,2010-01-25T02:56:39Z,2129952,12,FALSE,"<p>This will depend on the device you're using. If you're using a <a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/pdf.html"" rel=""noreferrer"">pdf device</a>, you can do this:</p>

<pre><code>pdf( ""mygraph.pdf"", width = 11, height = 8 )
plot( x, y )
</code></pre>

<p>You can then divide up the space in the pdf using the mfrow parameter like this:</p>

<pre><code>par( mfrow = c(2,2) )
</code></pre>

<p>That makes a pdf with four panels available for plotting. Unfortunately, some of the devices take different units than others. For example, I think that X11 uses pixels, while I'm certain that pdf uses inches. If you'd just like to create several devices and plot different things to them, you can use <a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/dev.html"" rel=""noreferrer"">dev.new(), dev.list(), and dev.next()</a>. </p>

<p>Other devices that might be useful include:</p>

<ul>
<li><a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/x11.html"" rel=""noreferrer"">X11</a></li>
<li><a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/postscript.html"" rel=""noreferrer"">postscript</a></li>
<li><a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/png.html"" rel=""noreferrer"">BMP, JPEG, PNG and TIFF</a></li>
<li><a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/quartz.html"" rel=""noreferrer"">quartz (OSX only)</a></li>
</ul>

<p>There's a list of all of the devices <a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/Devices.html"" rel=""noreferrer"">here</a>. </p>
"
2129970,163053,2010-01-25T03:00:18Z,2129952,41,TRUE,"<p>Use <code>dev.new()</code>.  (See <a href=""https://stackoverflow.com/questions/1801064/how-to-separate-two-plots-in-r"">this related question</a>.)</p>

<pre><code>plot(1:10)
dev.new(width=5, height=4)
plot(1:20)
</code></pre>
"
2130159,55362,2010-01-25T04:14:21Z,2083333,0,TRUE,"<p>Ended up using <a href=""http://sikuli.csail.mit.edu/"" rel=""nofollow noreferrer"">Sikuli scripts</a> for web log in. </p>
"
2130595,258198,2010-01-25T06:26:52Z,2053397,2,FALSE,"<p>I fixed few issues related to integers in rpy2 (Python can swich from int to long when needed, but R does does not seem to be able to do that.
Integer overflows should now return NA_integer_.</p>

<p>L.</p>
"
2135272,163053,2010-01-25T20:12:50Z,2134972,4,FALSE,"<p>I don't believe that there's any way to do this built into zoo or chron, but you can create your own function by using a little math.  Here you go:</p>

<pre><code>trunc.minutes &lt;- function (x, n.minutes) 
{
    if (!inherits(x, ""times"")) 
        x &lt;- as.chron(x)
    x &lt;- as.numeric(x)
    sec &lt;- round(24 * 3600 * abs(x - floor(x)))
    h &lt;- (sec%/%(n.minutes*60))/(60/n.minutes)
    hour &lt;- as.integer(h)
    minutes &lt;- (h %% hour) * 60
    chron(dates=chron::dates(x), times=times(paste(hour, minutes, ""00"", sep="":"")))
}
</code></pre>

<p>Here's an example of the usage:</p>

<pre><code>dts &lt;- chron::dates(c(""02/27/92"", ""02/27/92"", ""01/14/92"",
                ""02/28/92"", ""02/01/92""))
tms &lt;- times(c(""23:03:20"", ""23:29:56"", ""01:03:30"",
                ""18:21:03"", ""16:56:26""))     
x &lt;- chron(dates = dts, times = tms) # original dates
x
[1] (02/27/92 23:03:20) (02/27/92 22:29:56) (01/14/92 01:03:30)
[4] (02/28/92 18:21:03) (02/01/92 16:56:26)
trunc.minutes(x, 15) # new dates at 15 minute intervals
[1] (02/27/92 23:00:00) (02/27/92 22:15:00) (01/14/92 01:00:00)
[4] (02/28/92 18:15:00) (02/01/92 16:45:00)
trunc.minutes(x, 30) # new dates at 30 minute intervals
[1] (02/27/92 23:00:00) (02/27/92 22:00:00) (01/14/92 01:00:00)
[4] (02/28/92 18:00:00) (02/01/92 16:30:00)
</code></pre>

<p>Lastly, you can now use this function to aggregate the data:</p>

<pre><code>ts.zoo &lt;- zoo(rnorm(5), x) # a zoo time series
</code></pre>

<p>Or just use these new dates for aggregation (see how it rolls up the second example since there are two values in that window):</p>

<pre><code>&gt; aggregate(ts.zoo, trunc.minutes(x, 15), mean)
(01/14/92 01:00:00) (02/01/92 16:45:00) (02/27/92 23:00:00) (02/27/92 23:15:00) 
         -0.6738659          -0.4844803           0.7968155          -1.3571121 
(02/28/92 18:15:00) 
          0.7625861 
&gt; aggregate(ts.zoo, trunc.minutes(x, 30), mean)
(01/14/92 01:00:00) (02/01/92 16:30:00) (02/27/92 23:00:00) (02/28/92 18:00:00) 
         -0.6738659          -0.4844803          -0.2801483           0.7625861 
</code></pre>
"
2137563,243588,2010-01-26T04:43:47Z,2134972,1,TRUE,"<p>Given Shane's idea, I've change it a bit... The original question was how to aggregate on the minutes in the hours involved and get rid of the date. Also since the math didn't like midnight earlier, I'm using string parsing.</p>

<pre>
# Where X is a zoo obj with chron timestamps containing both time & date
# and min is like ""00:30:00"" for half hour intervals
> trunc.chrontime = function (x, min)
  {
    if (!inherits(x, ""times"")) 
        x = as.chron(x)
    s = substr(as.character(x),11,18)
    c = chron(times=s)
    trunc(c,min)
  }

> s = aggregate(d,trunc.minstr(index(d),""00:30:00""),mean)
s
00:00:00  0.5522222 0.4988889 0.006666667
00:30:00  0.5822222 0.5366667 0.012222222
01:00:00  0.3388889 0.4455556 0.000000000
01:30:00  0.3422222 0.4344444 0.000000000
02:00:00  0.3366667 0.4366667 0.000000000 ...
</pre>
"
2138953,30911,2010-01-26T11:10:28Z,1110363,2,FALSE,"<p>How about <strong>commandArgs</strong> <em>with</em> <strong>eval</strong> for a built in solution?</p>

<p><strong>test.R</strong></p>

<pre><code>## 'trailingOnly=TRUE' means only parse args after '--args'
args=(commandArgs(trailingOnly=TRUE))

## Supply default arguments
if(length(args)==0){
    print(""No arguments supplied."")
    ##supply default values
    a = 1
    b = c(1,1,1)
}else{
    for(i in 1:length(args)){
         eval(parse(text=args[[i]]))
    }
}
print(a*2)
print(b*3)
</code></pre>

<p>and to invoke it</p>

<pre><code>R CMD BATCH --no-save --no-restore '--args a=1 b=c(2,5,6)' test.R test.out
</code></pre>

<p>The usual caveats w.r.t using <strong>eval</strong> apply of course. </p>

<p>Shamelessly stolen from this <a href=""http://quantitative-ecology.blogspot.com/2007/08/including-arguments-in-r-cmd-batch-mode.html"" rel=""nofollow noreferrer"">blog post</a>.</p>
"
2140606,143305,2010-01-26T16:05:25Z,2140540,1,TRUE,"<p>See the R documentation for details on its datatypes, eg Section 4.2 of the R Import/Export Manual for <a href=""http://cran.r-project.org/doc/manuals/R-data.html#Overview-of-RDBMSs"" rel=""nofollow noreferrer"">an overview of R and RDBMS</a> and particularly Section 4.2.2 on Data Types.</p>

<p>If in doubt, try casting to a floating point number as these have a wider range, at the possible expense of precision.  Not all SQL types are mapped to all R types by all database packages. </p>
"
2141172,134830,2010-01-26T17:18:17Z,2140972,1,FALSE,"<p><strike>Writing R codes to check other R code is going to be tricky.  You'd have to find a way to determine which bits of code were variable declarations, and then try and work out whether they'd already been declared within the function.</strike>
EDIT: The previous statement would have been true, but as Aniko pointed out, the hard work has already been done in the <a href=""http://cran.r-project.org/web/packages/codetools/index.html"" rel=""nofollow noreferrer""><code>codetools</code></a> package.</p>

<p>One related thing that may be useful to you is to force a variable to be taken from the function itself (rather than from an enclosing environment).</p>

<p>This modified version of your function will always fail, since <code>n</code> is not declared.</p>

<pre><code>aha &lt;- function(p) 
{ 
   n &lt;- get(""n"", inherits=FALSE)
   return(p+n)
}
</code></pre>
"
2141248,163053,2010-01-26T17:28:44Z,2140972,5,FALSE,"<p>Richie's suggestion is very good.  </p>

<p>I would just add that you should consider creating unit test cases that would run in a clean R environment.  That will also eliminate the concern about global variables and ensures that your functions behave the way that they should.  You might want to consider using <a href=""http://cran.r-project.org/web/packages/RUnit/index.html"" rel=""noreferrer"">RUnit</a> for this.  I have my test suite scheduled to run every night in a new environment using RScript, and that's very effective and catching any kind of scope issues, etc.</p>
"
2141332,245603,2010-01-26T17:40:38Z,2140972,18,TRUE,"<p>If you want to detect such potential problems during the code-writing phase and not during run-time, then the <code>codetools</code> package is your friend.</p>

<pre><code>library(codetools)
aha&lt;-function(p){ 
  return(p+n) 
}

#check a specific function:
checkUsage(aha) 

#check all loaded functions:
checkUsageEnv(.GlobalEnv)
</code></pre>

<p>These will tell you that <code>no visible binding for global variable ‘n’</code>.</p>
"
2142758,245603,2010-01-26T21:25:08Z,2127926,7,TRUE,"<p>Expanding on dangerstat's answer, here is a little function that will automatically find which bin contains the value that you want to highlight:</p>

<pre><code>highlight &lt;- function(x, value, col.value, col=NA, ...){
   hst &lt;- hist(x, ...)
   idx &lt;- findInterval(value, hst$breaks)
   cols &lt;- rep(col, length(hst$counts))
   cols[idx] &lt;- col.value
   hist(x, col=cols, ...)
}
</code></pre>

<p>Now </p>

<pre><code>x &lt;- rnorm(100)
highlight(x, 1.2, ""red"")
</code></pre>

<p>will highlight the bin with 1.2 in it in red.</p>
"
2145052,205508,2010-01-27T06:49:23Z,2139703,1,FALSE,"<p>I know this is not what you're asking, but you might find this useful even in other cases:</p>

<p>MATLAB should use loops only when really unavoidable. for example, your code</p>

<pre><code>%// CRITERION 6
disp(' ');
disp('CRITERION 6');
for i = 1 : 1 : items
    count = 0;
    for j = 1 : 1 : factors
        if (ct(i,j) &gt; meaningful)
            count = count + 1;
        end
    end
    if (count == 0 || count &gt; 1)
        disp(['Criterion 6 is NOT MET for item ' num2str(i)])
    end
end
</code></pre>

<p>Should be written as</p>

<pre><code>%// CRITERION 6
disp(' ');
disp('CRITERION 6');
ct_lg_meaningful = sum(ct &gt; meaningful,2)   %// check where ct&gt;meaningful, and sum along 2nd axis - gives a column vector of number of times each row was larger than meaningful.
criteria_not_met = find((ct_lg_meaningful == 0)|(ct_lg_meaningful&gt;1))   %// in this vector find elements that are 0 or &gt;1
if length(criteria_not_met)&gt;0   %// if we found any elements, display them.
    disp(['Criterion 6 is NOT MET for items ' num2str(criteria_not_met')])   %' &lt;- to fix SO syntax highlighting
end
</code></pre>
"
2147552,163053,2010-01-27T14:25:10Z,2147084,0,FALSE,"<p>I would have suggested using the new <a href=""http://directlabels.r-forge.r-project.org/"" rel=""nofollow noreferrer"">directlabels package</a>, which can be used with both lattice and ggplot (and makes life very easy for these labeling problems), but unfortunately it doesn't work with barcharts.</p>
"
2147797,172261,2010-01-27T14:53:59Z,2147084,18,TRUE,"<p>Create a custom panel function, e.g.</p>

<pre><code>library(""lattice"")
p &lt;- barchart((1:10)^2~1:10, horiz=FALSE, ylim=c(0,120),
              panel=function(...) { 
                args &lt;- list(...)
                panel.text(args$x, args$y, args$y, pos=3, offset=1)
                panel.barchart(...)
              })
print(p)
</code></pre>

<p><img src=""https://i.stack.imgur.com/GaVpi.png"" alt=""lattice barchart with labels""></p>
"
2148525,143305,2010-01-27T16:24:57Z,2144968,6,TRUE,"<p>That's where it can turn into black art...   I notice that you did not list MPI or PVM -- those old workhorses of parallel computing do have monitors.  You may find solutions by going outside of R and relying on job schedulers (slurm, torque, ...)</p>

<p>If you can't do that (and hey, there are reasons why we like the simplicity of snow, foreach, ...) then maybe you can alter your jobs to log a 'heartbeat' or progress message every N steps.  You can log to text files (if you have a NFS or SMB/CIFS share), log to a database, or heck, send a <a href=""http://cran.r-project.org/web/packages/twitteR/"" rel=""noreferrer"">tweet with R</a>. It will most likely be specific to your app, and yes, it will have some cost.</p>
"
2150179,245603,2010-01-27T20:55:08Z,2150138,86,TRUE,"<p>Courtesy of the <code>?strptime</code> help file (with the example changed to your value):</p>

<pre><code> z &lt;- strptime(""2010-01-15 13:55:23.975"", ""%Y-%m-%d %H:%M:%OS"")
 z # prints without fractional seconds
 op &lt;- options(digits.secs=3)
 z
 options(op) #reset options
</code></pre>
"
2150357,244343,2010-01-27T21:24:21Z,2150138,26,FALSE,"<p>You can also use <code>strptime(time[1], ""%OSn"")</code> where 0 &lt;= n &lt;= 6, without having to set <code>digits.secs</code>.</p>
"
2151531,37751,2010-01-28T00:48:54Z,2151212,8,FALSE,"<p>you need <a href=""http://dirk.eddelbuettel.com/code/littler.html"" rel=""noreferrer"">littler</a> (pronounced 'little r')</p>

<p>Dirk will be by in about 15 minutes to elaborate ;)</p>
"
2151627,143305,2010-01-28T01:11:22Z,2151212,108,FALSE,"<p>A few points:</p>

<ol>
<li><p>Command-line parameters are
accessible via <code>commandArgs()</code>, so
see <code>help(commandArgs)</code> for an
overview.</p></li>
<li><p>You can use <code>Rscript.exe</code> on all platforms, including Windows. It will support <code>commandArgs()</code>. <a href=""http://dirk.eddelbuettel.com/code/littler.html"" rel=""noreferrer"">littler</a> could be ported to Windows but lives right now only on OS X and Linux.</p></li>
<li><p>There are two add-on packages on CRAN -- <a href=""http://cran.r-project.org/package=getopt"" rel=""noreferrer"">getopt</a> and <a href=""http://cran.r-project.org/package=optparse"" rel=""noreferrer"">optparse</a> -- which were both written for command-line parsing.</p></li>
</ol>

<p><em>Edit in Nov 2015:</em>  New alternatives have appeared and I <em>wholeheartedly</em> recommend <a href=""https://cran.r-project.org/web/packages/docopt/index.html"" rel=""noreferrer"">doctopt</a>.</p>
"
2152088,160314,2010-01-28T03:22:58Z,2151147,17,TRUE,"<p>R does not have a built in way to handle variable labels. Personally I think that this is disadvantage that should be fixed. Hmisc does provide some facilitiy for hadling variable labels, but the labels are only recognized by functions in that package. read.dta creates a data.frame with an attribute ""var.labels"" which contains the labeling information. You can then create a data dictionary from that.</p>

<pre><code>&gt; data(swiss)
&gt; write.dta(swiss,swissfile &lt;- tempfile())
&gt; a &lt;- read.dta(swissfile)
&gt; 
&gt; var.labels &lt;- attr(a,""var.labels"")
&gt; 
&gt; data.key &lt;- data.frame(var.name=names(a),var.labels)
&gt; data.key
          var.name       var.labels
1        Fertility        Fertility
2      Agriculture      Agriculture
3      Examination      Examination
4        Education        Education
5         Catholic         Catholic
6 Infant_Mortality Infant.Mortality
</code></pre>

<p>Of course this .dta file doesn't have very interesting labels, but yours should be more meaningful.</p>
"
2154190,168747,2010-01-28T11:47:22Z,2151212,181,TRUE,"<p><a href=""https://stackoverflow.com/a/2151627/8817"">Dirk's answer here</a> is everything you need. Here's a minimal reproducible example.</p>

<p>I made two files: <code>exmpl.bat</code> and <code>exmpl.R</code>.</p>

<ul>
<li><p><code>exmpl.bat</code>:</p>

<pre><code>set R_Script=""C:\Program Files\R-3.0.2\bin\RScript.exe""
%R_Script% exmpl.R 2010-01-28 example 100 &gt; exmpl.batch 2&gt;&amp;1
</code></pre>

<p>Alternatively, using <code>Rterm.exe</code>:</p>

<pre><code>set R_TERM=""C:\Program Files\R-3.0.2\bin\i386\Rterm.exe""
%R_TERM% --no-restore --no-save --args 2010-01-28 example 100 &lt; exmpl.R &gt; exmpl.batch 2&gt;&amp;1
</code></pre></li>
<li><p><code>exmpl.R</code>:</p>

<pre><code>options(echo=TRUE) # if you want see commands in output file
args &lt;- commandArgs(trailingOnly = TRUE)
print(args)
# trailingOnly=TRUE means that only your arguments are returned, check:
# print(commandArgs(trailingOnly=FALSE))

start_date &lt;- as.Date(args[1])
name &lt;- args[2]
n &lt;- as.integer(args[3])
rm(args)

# Some computations:
x &lt;- rnorm(n)
png(paste(name,"".png"",sep=""""))
plot(start_date+(1L:n), x)
dev.off()

summary(x)
</code></pre></li>
</ul>

<p>Save both files in the same directory and start <code>exmpl.bat</code>. In the result you'll get:</p>

<ul>
<li><code>example.png</code> with some plot</li>
<li><code>exmpl.batch</code> with all that was done</li>
</ul>

<p>You could also add an environment variable <code>%R_Script%</code>:</p>

<pre><code>""C:\Program Files\R-3.0.2\bin\RScript.exe""
</code></pre>

<p>and use it in your batch scripts as <code>%R_Script% &lt;filename.r&gt; &lt;arguments&gt;</code></p>

<p>Differences between <code>RScript</code> and <code>Rterm</code>:</p>

<ul>
<li><code>Rscript</code> has simpler syntax</li>
<li><code>Rscript</code> automatically chooses architecture on x64 (see <a href=""http://cran.r-project.org/doc/manuals/r-release/R-admin.html#Sub_002darchitectures"" rel=""nofollow noreferrer"">R Installation and Administration, 2.6 Sub-architectures</a> for details)</li>
<li><code>Rscript</code> needs <code>options(echo=TRUE)</code> in the .R file if you want to write the commands to the output file</li>
</ul>
"
2154581,220891,2010-01-28T12:52:32Z,2151212,79,FALSE,"<p>Add this to the top of your script: </p>

<pre><code>args&lt;-commandArgs(TRUE)
</code></pre>

<p>Then you can refer to the arguments passed as <code>args[1]</code>, <code>args[2]</code> etc.</p>

<p>Then run </p>

<pre><code>Rscript myscript.R arg1 arg2 arg3
</code></pre>

<p>If your args are strings with spaces in them, enclose within double quotes.</p>
"
2157209,143305,2010-01-28T19:13:24Z,2156935,2,FALSE,"<p>Have you seen the <a href=""http://addictedtor.free.fr/graphiques/"" rel=""nofollow noreferrer"">R Graph Gallery</a> ?</p>

<p>Other than that, you may have to index all the source code of CRAN packages to search efficiently...</p>
"
2158027,163053,2010-01-28T21:19:38Z,2158002,1,FALSE,"<p>It can't find the <code>parental</code> object that you're passing in.  You're probably referencing a field name within a data frame.  So you need to change your expression to be something like:</p>

<pre><code>boxplot(split(model$res,data.frame.name[,""parental""]))
</code></pre>
"
2158504,16632,2010-01-28T22:31:44Z,2156935,4,FALSE,"<p>If you're interested in learning about all the possible graphics you can make, you should learn about the grammar of graphics, and (my) implementation of it in R: <code>ggplot2</code>.</p>
"
2158803,160314,2010-01-28T23:31:45Z,2158780,33,TRUE,"<pre><code>t &lt;- try(pJohnson(.18, parms))
if(""try-error"" %in% class(t)) alternativeFunction()
</code></pre>
"
2159098,163053,2010-01-29T00:29:44Z,2158780,11,FALSE,"<p>Another option might be to use a <code>tryCatch</code> expression.  Here's an example:</p>

<pre><code> vari &lt;- 1
 tryCatch(print(""passes""),  error = function(e) print(vari)) # =&gt; passes
 tryCatch(stop(""fails""),  error = function(e) print(vari)) # =&gt; 1
</code></pre>

<p>You can do whatever you want within the error block, so in your case, something like this should work:</p>

<pre><code>tryCatch(pJohnson(.18, parms), error=function(e) alternativeFunction())
</code></pre>

<p>This isn't really the intended usage of the error, but it's a little more concise.</p>
"
2159101,95750,2010-01-29T00:30:43Z,1474081,6,FALSE,"<p>In addition, you can build the binary package using the --binary option.</p>

<pre><code>R CMD build --binary RJSONIO_0.2-3.tar.gz
</code></pre>
"
2160258,147427,2010-01-29T06:19:54Z,2160224,3,FALSE,"<p>I may be misunderstanding, but I think it can be done this way:</p>

<pre><code>&gt; years = c(2006, 2006, 2006, 2006, 2001, 2001, 2001, 2001, 2001)
&gt; scores = c(13, 65, 23, 34, 78, 56, 89, 98, 100)
&gt; tapply(scores, years, quantile)
$`2001`
  0%  25%  50%  75% 100% 
  56   78   89   98  100 

$`2006`
   0%   25%   50%   75%  100% 
13.00 20.50 28.50 41.75 65.00 
</code></pre>

<p>Is this right? </p>

<blockquote>
  <p>I mean the actual percentile of each
  observation. – Ryan Rosario</p>
</blockquote>

<p>Edit:</p>

<p>I think this may do it then:</p>

<pre><code>&gt; tapply(scores, years, function(x) { f = ecdf(x); sapply(x, f) })
$`2001`
[1] 0.4 0.2 0.6 0.8 1.0

$`2006`
[1] 0.25 1.00 0.50 0.75
</code></pre>

<p>With your data:</p>

<pre><code>&gt; tapply(scores, years, function(x) { f = ecdf(x); sapply(x, f) })
$`2000`
[1] 0.3333333 0.6666667 1.0000000

$`2008`
[1] 0.5 1.0
</code></pre>

<p>Edit 2:</p>

<p>This is probably faster:</p>

<pre><code>tapply(scores, years, function(x) { f = ecdf(x); f(x) })
</code></pre>

<p><code>f()</code> is vectorized :-)</p>

<p>Last, modification, I promise :-). If you want names: </p>

<pre><code>&gt; tapply(scores, years, function(x) { f = ecdf(x); r = f(x); names(r) &lt;- x; r })
$`2000`
     1000      1700      2000 
0.3333333 0.6666667 1.0000000 

$`2008`
1500 2000 
 0.5  1.0 
</code></pre>
"
2160341,144278,2010-01-29T06:43:10Z,2160224,0,FALSE,"<p>I found a method, but it requires a loop.</p>

<pre><code>group.pctiles &lt;- function(group.var, comparable) {
    unique.vals &lt;- unique(group.var)
    pctiles &lt;- vector(length = length(group.var))
    for (i in 1:length(unique.vals)) {
        slice &lt;- which(group.var == unique.vals[i])
        F &lt;- ecdf(comparable[slice])
        group.pctiles &lt;- F(comparable[slice])
        pctiles[slice] &lt;- group.pctiles
    }
    return(pctiles)
}
</code></pre>

<p>group.var is the variable that groups the data. In my example in my question, it is Year. comparable contains the values we want to find the percentiles for. In my question, comparable would be Score.</p>

<p>For the following data, I get the result below:</p>

<pre><code>Year,School,Fees
2000,10,1000
2008,1,1050
2008,4,2000
2000,3,1700
2000,1,2000

&gt; group.pctiles(dat, dat$Year, dat$Fees)
[1] 0.3333333 0.5000000 1.0000000 0.6666667 1.0000000
</code></pre>

<p>Then, I can cbind these percentiles back into the original data.frame for analysis, reporting, etc.</p>

<p>Anyone have a solution that doesn't require a loop?</p>
"
2160463,158065,2010-01-29T07:15:07Z,2160224,14,TRUE,"<p>Following up on Vince's solution, you can also do this with <code>plyr</code> or <code>by</code>:</p>

<pre><code>ddply(df, .(years), function(x) transform(x, percentile=ecdf(x$scores)(x$scores)))
</code></pre>
"
2160465,160314,2010-01-29T07:15:38Z,2160224,0,FALSE,"<p>How about something like:</p>

<pre><code>Year &lt;- c(2000,2008,2008,2000,2000)
Fees &lt;- c(1000,1050,2000,1700,2000)
dat &lt;- data.frame(Fees,Year,result=NA)
res &lt;- tapply(Fees,Year,function(x) rank(x,ties.method=""max"")/length(x))
for(i in 1:length(res))
   dat[Year==as.numeric(names(res)[i]),""result""] &lt;-res[[i]]
</code></pre>

<p>which yields:</p>

<pre><code>  Fees Year    result
1 1000 2000 0.3333333
2 1050 2008 0.5000000
3 2000 2008 1.0000000
4 1700 2000 0.6666667
5 2000 2000 1.0000000
</code></pre>
"
2161338,172261,2010-01-29T10:43:34Z,2161052,11,TRUE,"<p>You can use <code>polygon</code> in base graphics, for instance</p>

<pre><code>x &lt;- seq(as.POSIXct(""1949-01-01"", tz=""GMT""), length=36, by=""months"")
y &lt;- rnorm(length(x))
plot(x, y, type=""n"", ylim=c(-1,1)*max(abs(y)))
polygon(c(x, rev(x)), c(y, -rev(y)), col=""cornflowerblue"", border=NA)
</code></pre>

<p><strong>Update</strong>: Using <code>panel.polygon</code> from <code>lattice</code>:</p>

<pre><code>library(""lattice"")
library(""RColorBrewer"")

df &lt;- data.frame(x=rep(x,3),
                 y=rnorm(3*length(x)),
                 variable=gl(3, length(x)))

p &lt;- xyplot(y~x|variable, data=df,
            ylim=c(-1,1)*max(abs(y)),
            layout=c(1,3),
            fill=brewer.pal(3, ""Pastel2""),
            panel=function(...) {
              args &lt;- list(...)
              print(args)
              panel.polygon(c(args$x, rev(args$x)),
                            c(args$y, -rev(args$y)),
                            fill=args$fill[panel.number()],
                            border=NA)
            })
print(p)
</code></pre>
"
2161371,121332,2010-01-29T10:50:58Z,2147084,-1,FALSE,"<p>If you are using the <code>groups</code> parameter you will find the labels in @rcs's code all land on top of each other.  This can be fixed by extending panel.text to work like panel.barchart, which is easy enough if you know R.  </p>

<p>I can't post the code of the fix here for licencing reasons, sorry.</p>
"
2161917,163053,2010-01-29T12:41:21Z,2161815,5,TRUE,"<p>Use <code>rollmean</code>.  Here's an example:</p>

<pre><code>x.Date &lt;- as.Date(paste(2004, rep(1:4, 4:1), sample(1:28, 10), sep = ""-""))
x &lt;- zoo(rnorm(12), x.Date)
rollmean(x, 3)
</code></pre>

<p>In you case, you will want to lag your data after taking the mean since you the mean around a point in time.  </p>

<p>Lastly, you should know about the <code>rollapply</code> function in zoo which can do a rolling operation on any function (including one that you define).</p>
"
2162068,163053,2010-01-29T13:06:34Z,2161152,16,TRUE,"<p>I don't believe that there's a direct equivalent, so Romain Francois's suggestion (in your link) is probably the best.  You might also want to consider the following:</p>

<ol>
<li>Have a look at <a href=""http://pylit.berlios.de/"" rel=""noreferrer"">PyLit</a> and <a href=""http://gael-varoquaux.info/computers/pyreport/"" rel=""noreferrer"">PyReport</a> which are intended for literate programming with Python.</li>
<li><a href=""http://sphinx.pocoo.org/"" rel=""noreferrer"">Sphinx</a> is great for documenting with python, and can output LaTex.</li>
<li>Here's <a href=""http://www.literateprogramming.com/tools.html"" rel=""noreferrer"">a list of tools for literate programming</a>.  Some of these work with any programming language.</li>
</ol>
"
2162089,121332,2010-01-29T13:10:45Z,2161384,10,TRUE,"<p>You need to convert your dates into months using <code>cut(,""months"")</code>, then apply <code>mean</code> to each month using ggplot <code>stat_summary</code>.  Here's how to do it in <code>qplot</code>, which is a compact convenience wrapper to <code>ggplot</code>. </p>

<pre><code>qplot(as.Date(cut(date,""months"")), 
  price, data=DF, stat=""summary"", fun.y=""mean"", xlab=""date"")
</code></pre>

<p><a href=""http://www.imagechicken.com/uploads/1264786975079660800.png"">alt text http://www.imagechicken.com/uploads/1264786975079660800.png</a></p>

<hr>

<p>Base plot can also do it:</p>

<pre><code>plot(aggregate(DF$price, list(as.Date(cut(DF$date, ""month""))), mean))
</code></pre>

<p><a href=""http://www.imagechicken.com/uploads/1264786673030283100.png"">alt text http://www.imagechicken.com/uploads/1264786673030283100.png</a></p>
"
2162869,121332,2010-01-29T15:21:39Z,1644661,12,FALSE,"<p>I guess this is a reworking of @eduardo's really, but in one line.</p>

<pre><code>ggplot(df) + geom_histogram(mapping=aes(x=val)) 
  + geom_vline(data=aggregate(df[3], df[c(1,2)], mean), 
      mapping=aes(xintercept=val), color=""red"") 
  + facet_grid(cat1~cat2)
</code></pre>

<p><a href=""http://www.imagechicken.com/uploads/1264782634003683000.png"">alt text http://www.imagechicken.com/uploads/1264782634003683000.png</a></p>

<p>or using <code>plyr</code> (<code>require(plyr)</code> a package by the author of ggplot, Hadley):</p>

<pre><code>ggplot(df) + geom_histogram(mapping=aes(x=val)) 
  + geom_vline(data=ddply(df, cat1~cat2, numcolwise(mean)), 
      mapping=aes(xintercept=val), color=""red"") 
  + facet_grid(cat1~cat2)
</code></pre>

<p>It seems unsatisfying that vline isn't cut on the facets, I'm not sure why.</p>
"
2163548,216064,2010-01-29T16:58:27Z,2163108,10,TRUE,"<p>Here is an example adapted from <a href=""http://www.statmethods.net/advgraphs/parameters.html"" rel=""nofollow noreferrer"">http://www.statmethods.net/advgraphs/parameters.html</a>:</p>

<pre><code>windowsFonts(Calibri=windowsFont(""Calibri""))
par(family=""Calibri"")
plot(rnorm(1:10))
</code></pre>
"
2163858,121332,2010-01-29T17:37:43Z,2161052,11,FALSE,"<p>With a little polishing, this <code>ggplot</code> solution will look like what you want:</p>

<p><a href=""http://www.imagechicken.com/uploads/1264790429056858700.png"">alt text http://www.imagechicken.com/uploads/1264790429056858700.png</a></p>

<p>Here's how to make it from your data:</p>

<pre><code>require(ggplot2)
</code></pre>

<p>First, let's take your input data and import and restructure it into a form ggplot likes:</p>

<pre><code>rdata = read.csv(""data.csv"", 
# options: load '-' as na, ignore first comment line #Solar,
# strip whitespace that ends line, accept numbers as col headings
  na.strings=""-"", skip=1, strip.white=T, check.names=F)
# Convert to long format and check years are numeric
data = melt(rdata)
data = transform(data,year=as.numeric(as.character(variable)))
# geom_ribbon hates NAs.
data = data[!is.na(data$value),]

&gt; summary(data)
           Country       variable       value             year     
 Austria       : 12   1996   : 25   Min.   :  0.00   Min.   :1996  
 Belgium       : 12   1997   : 25   1st Qu.:  0.00   1st Qu.:1999  
 Croatia       : 12   1998   : 25   Median :  7.00   Median :2002  
 Cyprus        : 12   1999   : 25   Mean   : 36.73   Mean   :2002  
 Czech Republic: 12   2000   : 25   3rd Qu.: 30.00   3rd Qu.:2004  
 Denmark       : 12   2001   : 25   Max.   :580.00   Max.   :2007  
 (Other)       :228   (Other):150
</code></pre>

<p>Now let's plot it:</p>

<pre><code>ggplot(data=data, aes(fill=Country)) +
  facet_grid(Country~.,space=""free"", scales=""free_y"") +
  opts(legend.position=""none"") +
  geom_ribbon(aes(x=year,ymin=-value,ymax=+value))
</code></pre>
"
2164904,457898,2010-01-29T20:24:01Z,2160224,1,FALSE,"<p>You can also do something like this:</p>

<pre><code># first I'll create two dummy variables (Year, Score)
year &lt;- rep(2001:2005, 2)
score &lt;- round(rnorm(10, 35, 3))

# then coerce variables to data frame
d &lt;- data.frame(year, score)

# then you can use split() function to apply
# function to each stratum of grouping variable
sapply(split(score, year), function(x) quantile(x, probs=seq(.1, .9, .1)))
</code></pre>

<p>Output will go something like this:</p>

<pre><code>     2001 2002 2003 2004 2005
10%  34.3 32.1 34.3 29.6 36.1
20%  34.6 32.2 34.6 30.2 36.2
30%  34.9 32.3 34.9 30.8 36.3
40%  35.2 32.4 35.2 31.4 36.4
50%  35.5 32.5 35.5 32.0 36.5
60%  35.8 32.6 35.8 32.6 36.6
70%  36.1 32.7 36.1 33.2 36.7
80%  36.4 32.8 36.4 33.8 36.8
90%  36.7 32.9 36.7 34.4 36.9
</code></pre>

<p>You can utilize t() function to transpose rows and columns if you prefer. Writing a function will be a good way to tackle this kind of problems. I strongly recommend plyr package written by Hadley Wickam.</p>

<p>Hope this helps!
All the best!</p>
"
2164918,35308,2010-01-29T20:26:49Z,498932,1,FALSE,"<p>One thing you could try is adding the R files as content in the C# project - then the setup project can just copy them over for you (make sure you configure the setup file to copy content files from your project, not just the primary output). </p>

<p>You can either add the R folders into the project manually, or set up a script to modify the .csproj file (it's just an XML file) - content items are represented by these nodes: </p>

<pre><code>&lt;Content Include=""myfile"" /&gt; 
</code></pre>
"
2165185,143377,2010-01-29T21:11:02Z,2160224,7,FALSE,"<p>Using <code>ave</code></p>

<pre><code>ave(d1$scores, d1$year, FUN=function(x) ecdf(x)(x))
</code></pre>
"
2165396,163053,2010-01-29T21:48:06Z,2165342,55,TRUE,"<p>Try this:</p>

<pre><code>copula:::asCall
</code></pre>

<p>This was <a href=""http://www.mail-archive.com/r-help@r-project.org/msg77742.html"" rel=""noreferrer"">previously answered on R-help</a>.  That function was not exported in the package namespace, so you need to use the <code>:::</code> operator instead.  Typically functions are not exported when they are not intended for general usage (e.g. you don't need to document them in this case).</p>
"
2169164,143305,2010-01-30T19:40:22Z,2169118,12,FALSE,"<p>1) Why would you want to do that? There are <em>over 3500</em> (as of Feb 2012) of them?</p>

<p>2) Did you look at <a href=""http://cran.r-project.org/web/views"" rel=""noreferrer"">CRAN Task Views</a> and the <a href=""http://cran.r-project.org/package=ctv"" rel=""noreferrer"">ctv</a> package that allows you to install packages from a given task?</p>

<p>3) You bold-face question is a simple indexing query you can do by hand (and besides that, also see <code>help(sets)</code>)</p>

<pre><code>R&gt; available &lt;- LETTERS                  # a simple set
R&gt; installed &lt;- LETTERS[c(1:10, 15:26)]  # a simple subset
R&gt; available[ ! available %in% installed ]
[1] ""K"" ""L"" ""M"" ""N""
R&gt; 
</code></pre>

<p><em>Edit:</em> in response to your follow-up:</p>

<p>a) If a package does not pass 'R CMD check' on Linux and Windows, it does not get uploaded to CRAN. So that job is done.</p>

<p>b) Getting all depends at your end is work too as you will see. We did it for cran2deb which is at <a href=""http://debian.cran.r-project.org"" rel=""noreferrer"">http://debian.cran.r-project.org</a> (which does full-blown Debian package building which is more than just installing).  We get about 2050 out of 2150 packages built.  There are a few we refuse to build because of license, a few we cannot because of missing headers or libs and a few we cannot build because they need e.g. BioConductor packages.</p>
"
2169801,216064,2010-01-30T22:54:12Z,2161052,5,FALSE,"<p>Using rcs' first approach, here a solution for the sample data with base graphics:</p>

<pre><code>rawData &lt;- read.csv(""solar.csv"", na.strings=""-"")
data &lt;- ts(t(as.matrix(rawData[,2:13])), names=rawData[,1], start=1996)

inkblot &lt;- function(series, col=NULL, min.height=40, col.value=24, col.category=17, ...) {  
  # assumes non-negative values  
  # assumes that series is multivariate series  
  # assumes that series names are set, i.e. colnames(series) != NULL  

  x &lt;- as.vector(time(series))  
  if(length(col)==0){  
    col &lt;- rainbow(dim(series)[2])  
  }  

  ytotal &lt;- 0  
  for(category in colnames(series)) {  
    y &lt;- series[, category]
    y &lt;- y[!is.na(y)]
    ytotal &lt;- ytotal + max(y, min.height)  
  }  

  oldpar = par(no.readonly = TRUE)   
  par(mar=c(2,3,0,10)+0.1, cex=0.7)  

  plot(x, 1:length(x), type=""n"", ylim=c(0,1)*ytotal, yaxt=""n"", xaxt=""n"", bty=""n"", ylab="""", xlab="""", ...) 
  axis(side=1, at=x)  

  catNumber &lt;- 1  
  offset &lt;- 0  
  for(category in rev(colnames(series))) {  
    print(paste(""working on: "", category))
    y &lt;- 0.5 * as.vector(series[,category])  
    offset &lt;- offset + max(max(abs(y[!is.na(y)])), 0.5*min.height)  
    print(paste(""offset= "", str(offset)))
    polygon(c(x, rev(x)), c(offset+y, offset-rev(y)), col=col[catNumber], border=NA)  
    mtext(text=y[1], side=2, at=offset, las=2, cex=0.7, col=col.value)  
    mtext(text=y[length(y)], side=4, line=-1, at=offset, las=2, cex=0.7, col=col.value)  
    mtext(text=category, side=4, line=2, at=offset, las=2, cex=0.7, col=col.category)  
    offset &lt;- offset + max(max(abs(y[!is.na(y)])), 0.5*min.height)  
    catNumber &lt;- catNumber + 1   
  }  
}  


inkblot(data)  
</code></pre>

<p>I still need to figure out the vertical grid lines and the transparent coloring.
<img src=""https://lh4.ggpht.com/_j_5SAf7epFc/S2Wwx_Vj8nI/AAAAAAAAAMo/2LpL-zdRaC0/s512/inkblot.png"" alt=""plot""></p>
"
2170072,176922,2010-01-31T00:25:17Z,2170043,4,FALSE,"<ul>
<li><p>I'd consider using <code>apt-get</code> best practice since you will get automatic updates through the standard system tools.</p></li>
<li><p>Having 2 versions installed might get you into confusing situations: depending on your R setup you could load another package version then you expect -- your private (maybe outdated) one should in general be loaded first.</p></li>
<li><p>See above.</p></li>
</ul>
"
2170103,143305,2010-01-31T00:40:51Z,2170043,27,TRUE,"<p>It's not as easy as it seems.</p>

<ul>
<li><p><code>apt-get update</code> is good if and when</p>

<ul>
<li><p>packages exist -- but there are only around 150 or so <code>r-cran-*</code> packages out of a pool of 2100+ packages on CRAN, so rather sparse coverage</p></li>
<li><p>packages are maintained, bug free and current</p></li>
<li><p>you are happy enough with the bi-annual releases by Ubuntu</p></li>
</ul></li>
<li><p><code>install.packages()</code> and later <code>update.packages()</code> is good if and when </p>

<ul>
<li><p>you know what it takes to have built-time dependencies (besides <code>r-base-dev</code>) installed</p></li>
<li><p>you don't mind running <code>update.packages()</code> by hand as well as the <code>apt-get</code> updates.</p></li>
</ul></li>
</ul>

<p>On my Ubuntu machine at work, I go with the second solution.  But because the first one is better <em>if you have enough coverage</em>, we have built <a href=""http://debian.cran.r-project.org/"" rel=""noreferrer""><strong>cran2deb</strong></a> which provides 2050+ binary deb packages for amd64 and i386 --- but only for Debian testing.  That is what I use at home.</p>

<p>As for last question of whether you 'should you expect trouble':  No, because <code>R_LIBS_SITE</code> is set in <code>/etc/R/Renvironment</code> to be </p>

<pre><code># edd Apr 2003  Allow local install in /usr/local, also add a directory for
#               Debian packaged CRAN packages, and finally the default dir 
# edd Jul 2007  Now use R_LIBS_SITE, not R_LIBS
R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:\
/usr/lib/R/site-library:/usr/lib/R/library'}
</code></pre>

<p>which means that <em>your</em> packages go into <code>/usr/local/lib/R/site-library</code> whereas those managed by <code>apt</code> go into <code>/usr/lib/R/site-library</code> and (in the case of base packages) <code>/usr/lib/R/library</code>.</p>

<p>Hope that clarifies matters.  The r-sig-debian mailing list is a more informed place for questions like this.</p>
"
2170161,457898,2010-01-31T01:11:50Z,2169118,15,TRUE,"<p>Frankly, I think it's painstaking job... it would last for days, even weeks (depending on resources), but here's the code (I just enjoy doing trivial things):</p>

<pre><code># get names of installed packages
packs &lt;- installed.packages()
exc &lt;- names(packs[,'Package'])

# get available package names
av &lt;- names(available.packages()[,1])

# create loooong string
ins &lt;- av[!av %in% exc]
install.packages(ins)
</code></pre>

<p>I still don't get why you're doing this, but, hey... some things are just not meant to be.... 
What wonders me the most is the fact that you've already answered your own question! You got what you needed, and it's just up to you to put things together...
Are we missing the point? Did you have something else in mind?!?</p>
"
2172705,163053,2010-01-31T18:05:29Z,2172146,10,TRUE,"<p>S4 classes are relatively complicated, so I would suggest <a href=""http://cran.r-project.org/doc/contrib/Genolini-S4tutorialV0-5en.pdf"" rel=""noreferrer"">reading this introduction</a>.  </p>

<p>In this case, TSdbi is an example of a generic S4 class that gets extended by all the specific databases packages (e.g. TSMySQL, TSPostgreSQL, etc.).  There is nothing more to the TSconnect() method in TSdbi than what you're seeing: drv=""character"", dbname=""character"" are parameters to the function, not functions in and of themselves.  If you install some of the specific database packages and use showMethods(""TSconnect"") you will see all the specific instances of that function.  If you then call TSconnect() with a specific database driver it will go and use the appropriate function.  </p>

<p>This is how functions such as summary work too.  For instance, try calling <code>showMethods(summary)</code>.  Depending upon which packages are loaded, you should see multiple methods returned</p>

<p>You can easily see the source code for it on R-Forge: <a href=""http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/pkg/TSdbi/R/TSdbi.R?rev=70&amp;root=tsdbi&amp;view=markup"" rel=""noreferrer"">http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/pkg/TSdbi/R/TSdbi.R?rev=70&amp;root=tsdbi&amp;view=markup</a>.  This is the extent of that function:</p>

<pre><code>setGeneric(""TSconnect"", def= function(drv, dbname, ...) standardGeneric(""TSconnect""))

setMethod(""TSconnect"",   signature(drv=""character"", dbname=""character""),
   definition=function(drv, dbname, ...)
             TSconnect(dbDriver(drv), dbname=dbname, ...))
</code></pre>
"
2174842,135870,2010-02-01T05:32:43Z,2161152,2,FALSE,"<p>You could try <a href=""http://www.ctan.org/tex-archive/help/Catalogue/entries/sagetex.html"" rel=""nofollow noreferrer"">SageTeX</a> which implements Sweave-Like functionality for the <a href=""http://www.sagemath.org"" rel=""nofollow noreferrer"">SAGE</a> mathematics platform. I haven't played around with it as much as I would like to, but SAGE is basically a python shell and evaluates python as it's native language.</p>
"
2176026,121332,2010-02-01T10:40:56Z,2175809,21,TRUE,"<p>You may be better off working out what value type your function or code accepts, and asking for that:</p>

<pre><code>if (is.integer(aVariable))
{
  do whatever
}
</code></pre>

<p>This may be an improvement over isnull, because it provides type checking.  On the other hand, it may reduce the genericity of your code.</p>

<p>Alternatively, just make the function you want:</p>

<pre><code>is.defined = function(x)!is.null(x)
</code></pre>
"
2176610,69588,2010-02-01T12:36:53Z,2158631,1,FALSE,"<p>I don't know exactly what you're looking for, but have you looked at <a href=""http://bioconductor.org"" rel=""nofollow noreferrer"">http://bioconductor.org</a> ? The <a href=""http://www.bioconductor.org/packages/devel/bioc/html/PROcess.html"" rel=""nofollow noreferrer"">PROcess</a> package seems to have an align method. Also this site <a href=""http://www.ms-utils.org/wiki/pmwiki.php/Main/SoftwareList"" rel=""nofollow noreferrer"">here</a> has links to software and source code that may be relevant, even if not in R.</p>
"
2179267,37751,2010-02-01T18:58:32Z,2175809,7,FALSE,"<p>Ian put this in the comment, but I think it's a good answer:</p>

<pre><code>if (exists(""aVariable""))
{
  do whatever
}
</code></pre>

<p>note that the variable name is quoted. </p>
"
2180273,163053,2010-02-01T21:41:02Z,2180235,15,TRUE,"<p>Use JRI: <a href=""http://www.rforge.net/JRI/"" rel=""noreferrer"">http://www.rforge.net/JRI/</a>.  It comes bundled with rJava, including some examples of usage.  </p>

<p>A very simple example would be like this:</p>

<pre><code>import java.io.*;
import java.awt.Frame;
import java.util.Enumeration;

import org.rosuda.JRI.Rengine;
import org.rosuda.JRI.REXP;
import org.rosuda.JRI.RVector;
import org.rosuda.JRI.RMainLoopCallbacks;

public class rJavaTest {

    public static void main(String[] args) {

        Rengine re=new Rengine(args, false, new TextConsole());
        REXP x;
        re.eval(""print(1:10/3)"");
        System.out.println(x=re.eval(""iris""));
        RVector v = x.asVector();
        if (v.getNames()!=null) {
            System.out.println(""has names:"");
            for (Enumeration e = v.getNames().elements() ; e.hasMoreElements() ;) {
                System.out.println(e.nextElement());
            }
        }

        if (true) {
            System.out.println(""Now the console is yours ... have fun"");
            re.startMainLoop();
        } else {
            re.end();
            System.out.println(""end"");
        }
    }
}
</code></pre>
"
2180416,180348,2010-02-01T22:05:47Z,2180235,3,FALSE,"<p>I have found that forking R as a process, attaching to the process's stdin, stdout, and stderr streams, and sending R commands via the input stream to be quite effective.  I use the filesystem to communicate between R and my Java process.  This way, I can have multiple R processes running from different threads in Java and their environments do not conflict with each other.</p>
"
2182367,139459,2010-02-02T07:04:55Z,2182337,0,FALSE,"<p>Adding the semi-colon at the end of query sometimes creates problem. Try changing your query from:</p>

<pre><code>dbGetQuery(con, ""select surname from names WHERE age = '.foo.' ;"")
</code></pre>

<p>to:</p>

<pre><code>dbGetQuery(con, ""select surname from names WHERE age = '.foo.'"")
</code></pre>
"
2182400,44542,2010-02-02T07:12:55Z,2182337,0,FALSE,"<p>AFAIK the command has to be a string, so you should append the single components. Not being familiar with R I cant help you out HOW to do that. In MS-VBA the string concatenation operator is '&amp;'.</p>
"
2182778,121332,2010-02-02T08:49:24Z,2181902,5,FALSE,"<p>First, here's your answer:</p>

<p>To show you how to use how you might better use widgets to represent data differentiation, I refer you to the example of <a href=""http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=87"" rel=""noreferrer"">chernoff faces</a> at the R graph gallery.:</p>

<p><a href=""http://addictedtor.free.fr/graphiques/graphiques/graph_87.png"" rel=""noreferrer"">alt text http://addictedtor.free.fr/graphiques/graphiques/graph_87.png</a></p>

<p>All the code to generate this example is available at the site.</p>

<p>Alternatively, look ggplot's <a href=""http://had.co.nz/ggplot2/stat_spoke.html"" rel=""noreferrer"">stat_spoke</a> for a simple widget:
<a href=""http://had.co.nz/ggplot2/graphics/706b1badf6469940342f204b7bc98857.png"" rel=""noreferrer"">alt text http://had.co.nz/ggplot2/graphics/706b1badf6469940342f204b7bc98857.png</a></p>

<p><a href=""http://cran.r-project.org/web/packages/grImport/vignettes/import.pdf"" rel=""noreferrer"">grImport</a> provides a mechanism to import simple PDF images into your plot for use as points.</p>

<p>Now follows a critique of your example.</p>

<hr>

<p>This is not a scatterplot.  It's essentially a flowed list of ordered data points where colour is used to indicate one of the text variables, and an uninformative and redundant widget has been used to frame the data but otherwise provides no visual feedback in terms of size or shape.</p>

<p>It is not a good graph, because it completely fails to answer the stated question ""Does Paying More Lead To Better Results"", and leaves the reader to struggle draw that conclusion (and that other graph, as necessary) by themselves.</p>

<p>In addition, the authors have wasted the x, y axes - which could have been well used to position elements by outgoing and results, to provide a visual understanding of value-for-money.  Instead they have opted to order the icons by the ratio of per head cost to average graduation rate, which is sort of useful, but doesn't answer the stated question, and fails to allow a direct visual comparison of relative ratio between colleges, or the relationship between cost and value.</p>

<p>As I say, in my opinion, this is a bad graph, and your readers would not be well served by having you replicate it.</p>
"
2183125,172261,2010-02-02T09:57:21Z,2183002,3,FALSE,"<p>Try something like this:</p>

<pre><code>subset(dataframe.name, !duplicated(country.colname),
       select=c(col1.name, col2.name, ...))
</code></pre>

<p>see also this related question: <a href=""https://stackoverflow.com/questions/1769365/how-to-remove-partial-duplicates-from-a-data-frame"">how to remove partial duplicates from a data frame?</a></p>
"
2183144,30911,2010-02-02T10:00:02Z,2183002,3,TRUE,"<p><code>unique(c(1,2,3,4,4))</code></p>

<p>will give you </p>

<blockquote>
  <p>1 2 3 4</p>
</blockquote>

<p>so</p>

<p><code>unique(out[is.na(out$codeHelper),c(1,length(colnames(out)))])</code></p>

<p>should be what you're looking for?</p>
"
2184318,225468,2010-02-02T13:29:39Z,2182337,4,TRUE,"<p>This should work:</p>

<pre><code>foo = 23;

sqlStatement &lt;- paste(""select surname from names WHERE age ="",foo,'""',sep="""")

dbGetQuery(con, sqlStatement;)
</code></pre>
"
2184525,163053,2010-02-02T14:02:44Z,2182337,2,FALSE,"<p>You may want to look at the answers to this question: <a href=""https://stackoverflow.com/questions/1630724/can-i-gracefully-include-formatted-sql-strings-in-an-r-script"">Can I gracefully include formatted SQL strings in an R script?</a>.  </p>

<p>The simplest solution is to use the <code>paste</code> command as Robert suggested.</p>
"
2185524,163053,2010-02-02T16:08:00Z,2185252,27,FALSE,"<p>Here you go:</p>

<pre><code>x &lt;- read.table(textConnection(
""Code Country        1950    1951    1952    1953    1954
AFG  Afghanistan    20,249  21,352  22,532  23,557  24,555
ALB  Albania        8,097   8,986   10,058  11,123  12,246""), header=TRUE)
library(reshape)
x2 &lt;- melt(x,id=c(""Code"",""Country""),variable_name=""Year"")
x2[,""Year""] &lt;- as.numeric(gsub(""X"","""",x2[,""Year""]))
</code></pre>
"
2185525,245603,2010-02-02T16:07:59Z,2185252,46,TRUE,"<p><code>reshape()</code> takes a while to get used to, just as <code>melt</code>/<code>cast</code>. Here is a solution with reshape, assuming your data frame id called <code>d</code>:</p>

<pre><code>reshape(d, direction=""long"", varying=list(names(d)[3:7]), v.names=""Value"", 
        idvar=c(""Code"",""Country""), timevar=""Year"", times=1950:1954)
</code></pre>
"
2186159,171659,2010-02-02T17:22:39Z,2175809,14,FALSE,"<p>If it's just a matter of easy reading, you could always define your own function :</p>

<pre><code>is.not.null &lt;- function(x) ! is.null(x)
</code></pre>

<p>So you can use it all along your program.</p>

<pre><code>is.not.null(3)
is.not.null(NULL)
</code></pre>
"
2186533,118402,2010-02-02T18:21:04Z,2185958,3,FALSE,"<p>You could always just use a clustered bootstrap.  Resample across families, which you believe are independent. That is, keep families together when you resample. Compute <code>p2 - p1</code> for each sample.  After 1000 iterations or so, compute the upper and bottom 2.5% quantiles.  This will give you a bootstrapped 95% confidence interval. Alternatively compute the fraction of samples above zero, or whatever your hypothesis is.  The procedure should have good pretty good properties unless the number of families is small.</p>

<p>It's probably easiest to do this by hand in R rather than relying on any package.</p>
"
2187503,143305,2010-02-02T20:36:33Z,2187448,4,FALSE,"<p>Just add a new column each time plyr calls you:</p>

<pre><code>R&gt; DF &lt;- data.frame(kmer=sample(1:3, 50, replace=TRUE), \
                    cvCut=sample(LETTERS[1:3], 50, replace=TRUE))
R&gt; library(plyr)
R&gt; ddply(DF, .(kmer, cvCut), function(X) data.frame(X, newId=1:nrow(X)))
   kmer cvCut newId
1     1     A     1
2     1     A     2
3     1     A     3
4     1     A     4
5     1     A     5
6     1     A     6
7     1     A     7
8     1     A     8
9     1     A     9
10    1     A    10
11    1     A    11
12    1     B     1
13    1     B     2
14    1     B     3
15    1     B     4
16    1     B     5
17    1     B     6
18    1     C     1
19    1     C     2
20    1     C     3
21    2     A     1
22    2     A     2
23    2     A     3
24    2     A     4
25    2     A     5
26    2     B     1
27    2     B     2
28    2     B     3
29    2     B     4
30    2     B     5
31    2     B     6
32    2     B     7
33    2     C     1
34    2     C     2
35    2     C     3
36    2     C     4
37    3     A     1
38    3     A     2
39    3     A     3
40    3     A     4
41    3     B     1
42    3     B     2
43    3     B     3
44    3     B     4
45    3     C     1
46    3     C     2
47    3     C     3
48    3     C     4
49    3     C     5
50    3     C     6
R&gt; 
</code></pre>
"
2187505,163053,2010-02-02T20:36:51Z,2187448,2,FALSE,"<p>I think that this is what you want:</p>

<p>Load the data:</p>

<pre><code>x &lt;- read.table(textConnection(
""id      size kmer cvCut   cumsum
1      8132   23    10     8132
10000   778   23    10 13789274
30000   324   23    10 23658740
50000   182   23    10 28534840
100000   65   23    10 33943283
200000   25   23    10 37954383
250000  584   23    12 16546507
300000  110   23    12 29435303
400000   28   23    12 34697860
600000  127   23     2 47124443
600001  127   23     2 47124570""), header=TRUE)
</code></pre>

<p>Use ddply:</p>

<pre><code>library(plyr)
ddply(x, .(kmer, cvCut), function(x) cbind(x, 1:nrow(x)))
</code></pre>
"
2187775,16632,2010-02-02T21:16:22Z,2187448,13,TRUE,"<p>I'd do it like this:</p>

<pre><code>library(plyr)
ddply(df, c(""kmer"", ""cvCut""), transform, newID = seq_along(kmer))
</code></pre>
"
2188489,169947,2010-02-02T23:12:38Z,2186015,1,FALSE,"<p>Hey hey - I just discovered that RSQLite, which is what I'm using in this case, does indeed have bound-variable support:</p>

<p><a href=""http://cran.r-project.org/web/packages/RSQLite/NEWS"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/packages/RSQLite/NEWS</a></p>

<p>See the entry about <code>dbSendPreparedQuery()</code> and <code>dbGetPreparedQuery()</code>.</p>

<p>So in theory, that turns this nastiness:</p>

<pre><code>df &lt;- data.frame()
for (x in data$guid) {
  query &lt;- paste(""SELECT uuid, cites, score FROM mytab WHERE uuid='"",
                 x, ""'"", sep="""")
  df &lt;- rbind(df, dbGetQuery(con, query))
}
</code></pre>

<p>into this:</p>

<pre><code>df &lt;- dbGetPreparedQuery(
     con, ""SELECT uuid, cites, score FROM mytab WHERE uuid=:guid"", data)
</code></pre>

<p>Unfortunately, when I actually try it, it seems that it's only for <code>INSERT</code> statements and the like, not for <code>SELECT</code> statements, because I get an error: <code>RS-DBI driver: (cannot have bound parameters on a SELECT statement)</code>.</p>

<p>Providing that capability would be fantastic.</p>

<p>The next step would be to hoist this up into DBI itself so that all DBs can take advantage of it, and provide a default implementation that just pastes it into the string like we're all doing ourselves now.</p>
"
2188498,168657,2010-02-02T23:14:42Z,2188414,5,TRUE,"<p>You can use backslash to escape otherwise unprintable characters:</p>

<pre><code>print(""\245"")
</code></pre>

<p>displays the Yen character (<kbd>¥</kbd>) on my gui. The 245 is in octal format, so the above expression is printing out ASCII (or whatever encoding the GUI is using) character 165.</p>

<p>219 is 333 in octal, but</p>

<pre><code>print(""\333"")
</code></pre>

<p>prints out the <kbd>Û</kbd> character on my gui.</p>

<hr>

<p>A few (but by no means all) unicode characters are also supported on the R gui:</p>

<pre><code>cyrillic_d &lt;- ""\u0414""
print(cyrillic_d)
</code></pre>

<p>outputs <kbd>Д</kbd>.</p>
"
2188771,257636,2010-02-03T00:12:58Z,2187910,0,FALSE,"<p>If I understand right you want to estimate a function given some (x,y) values of it. If yes check the following links.</p>

<p>Read about this:</p>

<pre><code>http://en.wikipedia.org/wiki/Spline_%28mathematics%29
http://en.wikipedia.org/wiki/Polynomial_interpolation
http://en.wikipedia.org/wiki/Newton_polynomial
http://en.wikipedia.org/wiki/Lagrange_polynomial
</code></pre>

<p>Googled it:</p>

<pre><code>http://www.stat.wisc.edu/~xie/smooth_spline_tutorial.html
http://stat.ethz.ch/R-manual/R-patched/library/stats/html/smooth.spline.html
http://www.image.ucar.edu/GSP/Software/Fields/Help/splint.html
</code></pre>

<p>I never used R so I am not sure if that works or not, but if you have Matlab i can explain you more.</p>
"
2189904,212593,2010-02-03T05:29:14Z,2188414,4,FALSE,"<p>Following mobrule, the following works on R running in a UTF-8 locale on Linux:</p>

<pre><code>&gt; ""\u258A""
[1] ""▊""
</code></pre>
"
2190262,258334,2010-02-03T07:10:20Z,2190154,5,FALSE,"<p>So one quick version would be to transform the data.frame and use the <code>rbind()</code> function
to get what you want.</p>

<pre><code>dataNEW &lt;- data.frame(bnames[,1],c(""m""), bnames[,c(2,3)], c(""f""), bnames[,4])
colnames(dataNEW) &lt;- c(""name"", ""gender"", ""value"", ""name"", ""gender"", ""value"")
</code></pre>

<p>This will give you:</p>

<pre><code>          name gender value      name gender value
1        Jacob      m 22272      Emma      f 18587
2      Michael      m 20298  Isabella      f 18377
3        Ethan      m 20004     Emily      f 17217
4       Joshua      m 18924   Madison      f 16853
5       Daniel      m 18717       Ava      f 16850
6    Alexander      m 18423    Olivia      f 16845
7      Anthony      m 18158    Sophia      f 15887
8      William      m 18149   Abigail      f 14901
9  Christopher      m 17783 Elizabeth      f 11815
10     Matthew      m 17337     Chloe      f 11699
</code></pre>

<p>Now you can use <code>rbind()</code>:</p>

<pre><code>dataNGV &lt;- rbind(dataNEW[1:3],dataNEW[4:6])
</code></pre>

<p>which leads to:</p>

<pre><code>      name gender value
1        Jacob      m 22272
2      Michael      m 20298
3        Ethan      m 20004
4       Joshua      m 18924
5       Daniel      m 18717
6    Alexander      m 18423
7      Anthony      m 18158
8      William      m 18149
9  Christopher      m 17783
10     Matthew      m 17337
11        Emma      f 18587
12    Isabella      f 18377
13       Emily      f 17217
14     Madison      f 16853
15         Ava      f 16850
16      Olivia      f 16845
17      Sophia      f 15887
18     Abigail      f 14901
19   Elizabeth      f 11815
20       Chloe      f 11699
</code></pre>
"
2190537,170792,2010-02-03T08:19:45Z,2190154,3,FALSE,"<p>I think (if I have understood correctly) that mropa's solution needs one more step to get what you want </p>

<pre><code>library(plyr)
data &lt;- ddply(dataNGV, .(name,gender), 
      function(x) data.frame(name=rep(x[,1],x[,3]),gender=rep(x[,2],x[,3])))
</code></pre>
"
2191185,168747,2010-02-03T10:21:47Z,2190154,3,TRUE,"<p>Direct vector-based solution (replace the loop) will be</p>

<pre><code># your data:
bnames &lt;- read.table(textConnection(
""male.name n.male female.name n.female
Jacob 22272 Emma 18587
Michael 20298 Isabella 18377
Ethan 20004 Emily 17217
Joshua 18924 Madison 16853
Daniel 18717 Ava 16850
Alexander 18423 Olivia 16845
Anthony 18158 Sophia 15887
William 18149 Abigail 14901
Christopher 17783 Elizabeth 11815
Matthew 17337 Chloe 11699
""), sep="" "", header=TRUE, stringsAsFactors=FALSE)

# how to avoid loop
bnames$male.name[ rep(1:nrow(bnames), times=bnames$n.male) ]
</code></pre>

<p>It's based on fact that <code>rep</code> can do at once thing you do in loop.</p>

<p>But for final result you should combine mropa and gd047 answers.</p>

<p>Or with my solution:</p>

<pre><code>data_final &lt;- data.frame(
  name = c(
    bnames$male.name[ rep(1:nrow(bnames), times=bnames$n.male) ],
    bnames$female.name[ rep(1:nrow(bnames), times=bnames$n.female) ]
  ),
  gender = rep(
    c(""m"", ""f""),
    times = c(sum(bnames$n.male), sum(bnames$n.female))
  ),
  stringsAsFactors = FALSE
)
</code></pre>

<p>[EDIT] Simplify:</p>

<pre><code>data_final &lt;- data.frame(
  name = rep(
    c(bnames$male.name, bnames$female.name),
    times = c(bnames$n.male, bnames$n.female)
  ),
  gender = rep(
    c(""m"", ""f""),
    times = c(sum(bnames$n.male), sum(bnames$n.female))
  ),
  stringsAsFactors = FALSE
)
</code></pre>
"
2191752,60617,2010-02-03T12:17:38Z,2190756,9,FALSE,"<p>Another way is </p>

<pre><code>&gt; length(z[z==TRUE])
[1] 498
</code></pre>

<p>While <code>sum(z)</code>  is nice and short, for me <code>length(z[z==TRUE])</code> is more self explaining. Though, I think with a simple task like this it does not really make a difference...</p>

<p>If it is a large vector, you probably should go with the fastest solution, which is <code>sum(z)</code>. <code>length(z[z==TRUE])</code> is about 10x slower and <code>table(z)[TRUE]</code> is about 200x slower than <code>sum(z)</code>.</p>

<p>Summing up, <code>sum(z)</code> is the fastest to type and to execute.</p>
"
2191824,168747,2010-02-03T12:29:46Z,2190756,115,TRUE,"<p>There are some problems when logical vector contains <code>NA</code> values.<br>
See for example:</p>

<pre><code>z &lt;- c(TRUE, FALSE, NA)
sum(z) # gives you NA
table(z)[""TRUE""] # gives you 1
length(z[z==TRUE]) # f3lix answer, gives you 2 (because NA indexing returns values)
</code></pre>

<p>So I think safe is</p>

<pre><code>sum(z, na.rm=TRUE) # best way to count TRUE values
</code></pre>

<p>(which gives 1). I think that <code>table</code> solution is less efficient (look at the code of <code>table</code> function).</p>

<p>Also, you should be careful with the ""table"" solution, in case there are no TRUE values in the logical vector. Suppose <code>z &lt;- c(NA, FALSE, NA)</code> or simply <code>z &lt;- c(FALSE, FALSE)</code></p>

<pre><code>table(z)[""TRUE""] # gives you NA for both cases.
</code></pre>
"
2191892,163053,2010-02-03T12:44:19Z,2190756,68,FALSE,"<p>Another option which hasn't been mentioned is to use <code>which</code>:</p>

<pre><code>length(which(z))
</code></pre>

<p>Just to actually provide some context on the ""which is faster question"", it's always easiest just to test yourself.  I made the vector much larger for comparison:</p>

<pre><code>z &lt;- sample(c(TRUE,FALSE),1000000,rep=TRUE)
system.time(sum(z))
   user  system elapsed 
   0.03    0.00    0.03
system.time(length(z[z==TRUE]))
   user  system elapsed 
   0.75    0.07    0.83 
system.time(length(which(z)))
   user  system elapsed 
   1.34    0.28    1.64 
system.time(table(z)[""TRUE""])
   user  system elapsed 
  10.62    0.52   11.19 
</code></pre>

<p>So clearly using <code>sum</code> is the best approach in this case.  You may also want to check for <code>NA</code> values as Marek suggested.</p>

<p>Just to add a note regarding NA values and the <code>which</code> function:</p>

<pre><code>&gt; which(c(T, F, NA, NULL, T, F))
[1] 1 4
&gt; which(!c(T, F, NA, NULL, T, F))
[1] 2 5
</code></pre>

<p>Note that which only checks for logical <code>TRUE</code>, so it essentially ignores non-logical values.</p>
"
2191933,168747,2010-02-03T12:51:24Z,2189184,3,FALSE,"<p>There is nice function <code>file.path</code> which allow to create file paths OS independent. You could use it in your code as:</p>

<pre><code>inPath = file.path(""D:"",""R_Analysis"")
inFile = ""sample.txt""
outPath = file.path(""D:"",""R_Analysis"")
outFile = ""processed_sample.txt""
pdfOutPath = file.path(""D:"",""R_Analysis"")
pdfOutFile = ""processed_sample.pdf""
</code></pre>

<p>and then use</p>

<pre><code>read.table(file.path(inPath, inFile))
pdf(file.path(pdfOutPath, pdfOutFile))
</code></pre>

<p>Your path is ""windows-depended"" (reference to disk label), but if you use relatives paths then it could be more useful.</p>

<p>And second hint - you should open graphics device as late as possible, e.g.</p>

<pre><code>pdf(file.path(pdfOutPath, pdfOutFile),height=6,width=9)
print(p + composite)  
dev.off()
</code></pre>

<p>Then it's easier to search for proper line when you want to see plot in window and not in file.</p>
"
2192099,245603,2010-02-03T13:20:17Z,2185958,2,FALSE,"<p>Check out the <code>survey</code> package: it is designed to take into account correlations induced by clustered sampling.</p>
"
2192397,168747,2010-02-03T14:00:29Z,2192316,17,FALSE,"<p>Maybe</p>

<pre><code>gsub(""[^0-9]"", """", ""aaa12xxxx"")
# [1] ""12""
</code></pre>
"
2192448,225468,2010-02-03T14:08:53Z,2192316,2,FALSE,"<p>One way would be this: </p>

<pre><code>test &lt;- regexpr(""[0-9]+"",""aaa12456xxx"")
</code></pre>

<p>Now, notice regexpr gives you the starting and ending indices of the string:</p>

<pre><code>    &gt; test
[1] 4
attr(,""match.length"")
[1] 5
</code></pre>

<p>So you can use that info with substr function</p>

<pre><code>substr(""aaa12456xxx"",test,test+attr(test,""match.length"")-1)
</code></pre>

<p>I'm sure there is a more elegant way to do this, but this was the fastest way I could find. Alternatively, you can use sub/gsub to strip out what you don't want to leave what you do want. </p>
"
2192606,163053,2010-02-03T14:30:11Z,2192360,4,TRUE,"<p>Yes.  You can use the <code>.onLoad</code>, <code>.onAttach</code>, or <code>.First.lib</code> functions to do whatever you want when the package is loaded.  I suggest looking at the help for those functions.  You would use <code>.onLoad</code> with a namespace, and <code>.First.lib</code> without.</p>

<p>One convention is that people will frequently put these commands in a separate <code>zzz.R</code> file, which is just used for package related code.  </p>
"
2192621,143305,2010-02-03T14:31:57Z,2192576,1,FALSE,"<p>I think style guidelines (as discussed before on SO) help for programmer efficiency.  R Core seems to agree by providing some hints (and Emacs parameters for consistent indenting).</p>

<p>Execution efficiency is more difficult to achieve by decree. You may have to fall back to rules of thumb ('vectorise') as well as profiling.</p>
"
2192671,143305,2010-02-03T14:38:48Z,2192360,18,FALSE,"<p>Quick points:</p>

<ul>
<li><p>if your package has a NAMESPACE, then <code>.onLoad()</code> is where you do this</p></li>
<li><p>if your package does not have NAMESPACE, then <code>.First.lib()</code> is where you do this</p></li>
<li><p>either way, use <code>packageStartupMessage()</code> instead of <code>cat()</code> so that users have a choice of suppressing this.</p></li>
</ul>
"
2192672,16632,2010-02-03T14:38:54Z,2190154,2,FALSE,"<p>Alternatively, download the full (cleaned up) baby names dataset from <a href=""http://github.com/hadley/data-baby-names"" rel=""nofollow noreferrer"">http://github.com/hadley/data-baby-names</a>.</p>
"
2192731,23771,2010-02-03T14:46:16Z,2192576,2,FALSE,"<p>What you can count on being slow is anything that, in a loop, rebuilds data, like appending elements to a vector, <em>if it is done a lot</em>.</p>
"
2192732,16632,2010-02-03T14:46:21Z,2192316,110,TRUE,"<p>Use the new stringr package which wraps all the existing regular expression operates in a consistent syntax and adds a few that are missing:</p>

<pre><code>library(stringr)
str_locate(""aaa12xxx"", ""[0-9]+"")
#      start end
# [1,]     4   5
str_extract(""aaa12xxx"", ""[0-9]+"")
# [1] ""12""
</code></pre>
"
2193940,134830,2010-02-03T17:30:26Z,2192576,19,FALSE,"<blockquote>
<pre><code>Programmer efficient                 |   Computationally efficient
                                     |
Write everything in R                |   Call C/Fortran routines
Reuse code                           |   Custom create everything 
  (functions not scripts,            |
  packages not individual functions) |
Use high level functions             |   Use low-level functions
Write things that work               |   Write it, profile it, optimise it.
                                     |     Repeat ad infinitum.
</code></pre>
</blockquote>
"
2194852,212593,2010-02-03T19:34:06Z,2192316,8,FALSE,"<p>You can use PERL regexs' lazy matching:</p>

<pre><code>&gt; sub("".*?([0-9]+).*"", ""\\1"", ""aaa12xx99"",perl=TRUE)
[1] ""12""
</code></pre>

<p>Trying to substitute out non-digits will lead to an error in this case.</p>
"
2197326,16632,2010-02-04T04:31:24Z,2196985,5,FALSE,"<p>See <a href=""http://learnr.wordpress.com/2009/04/09/ggplot2-sales-dashboard/"" rel=""noreferrer"">http://learnr.wordpress.com/2009/04/09/ggplot2-sales-dashboard/</a> for an example with code.</p>
"
2197348,163053,2010-02-04T04:39:08Z,2196985,3,FALSE,"<p>I would suggest also having a look at <a href=""http://cran.r-project.org/web/packages/brew/index.html"" rel=""nofollow noreferrer"">the brew package</a>, as in this <a href=""http://learnr.wordpress.com/2009/09/09/brew-creating-repetitive-reports/"" rel=""nofollow noreferrer"">example on the learnr blog</a>.</p>
"
2201248,245603,2010-02-04T16:21:21Z,2194516,1,FALSE,"<p>My suggestion might be completely off, because you don't give enough details about the contents of your files, and I had to guess from the code. Anyway, here it goes.</p>

<p>You don't state it, but I would assume that your code crashes on the second line, when you read in the big matrix. The loop reads the lines one-at-a-time, and should not crash. The only reason you need that big matrix is to calculate the WordProbs vector. So why don't you rewrite that part using the same looping using <code>scan</code>? In fact, you could probably don't even need to store the <code>WordProbs</code> vector, just <code>sum(WordFreq)</code> - you can get that using an initial run through hte file. Then rewrite the formula within the loop to calculate the current <code>WordProb</code>.</p>
"
2203628,12677,2010-02-04T22:12:25Z,2196985,4,TRUE,"<p>I think your html/css-direction might be a really smart move.</p>

<p>It might be easier to get an awesome layout using using Open Office draw and just link to the images, checking off the link box when insterting them for the first time. Open Office supports export to pdf making it usefull for reporting.</p>

<p>Even if it was straight forward to programaticly create a stunning document layout in R, I'm not sure it would be worth the time and effort.</p>

<p>Regards</p>
"
2204613,457898,2010-02-05T02:02:28Z,2190756,6,FALSE,"<p><code>which</code> is good alternative, especially when you operate on matrices (check <code>?which</code> and notice the <code>arr.ind</code> argument). But I suggest that you stick with <code>sum</code>, because of <code>na.rm</code> argument that can handle <code>NA</code>'s in logical vector.
For instance:</p>

<pre><code># create dummy variable
set.seed(100)
x &lt;- round(runif(100, 0, 1))
x &lt;- x == 1
# create NA's
x[seq(1, length(x), 7)] &lt;- NA
</code></pre>

<p>If you type in <code>sum(x)</code> you'll get <code>NA</code> as a result, but if you pass <code>na.rm = TRUE</code> in <code>sum</code> function, you'll get the result that you want.</p>

<pre><code>&gt; sum(x)
[1] NA
&gt; sum(x, na.rm=TRUE)
[1] 43
</code></pre>

<p>Is your question strictly theoretical, or you have some practical problem concerning logical vectors?</p>
"
2209371,163053,2010-02-05T18:20:28Z,2209258,36,TRUE,"<p>You may want to look at the closely <a href=""https://stackoverflow.com/questions/1562124/merge-many-data-frames-from-csv-files"">related question on stackoverflow</a>.</p>

<p>I would approach this in two steps: import all the data (with <code>plyr</code>), then merge it together:</p>

<pre><code>filenames &lt;- list.files(path="".../tempDataFolder/"", full.names=TRUE)
library(plyr)
import.list &lt;- llply(filenames, read.csv)
</code></pre>

<p>That will give you a list of all the files that you now need to merge together.  There are many ways to do this, but here's one approach (with <code>Reduce</code>):</p>

<pre><code>data &lt;- Reduce(function(x, y) merge(x, y, all=T, 
    by=c(""COUNTRYNAME"", ""COUNTRYCODE"", ""Year"")), import.list, accumulate=F)
</code></pre>

<p>Alternatively, you can do this with the <code>reshape</code> package if you aren't comfortable with <code>Reduce</code>:</p>

<pre><code>library(reshape)
data &lt;- merge_recurse(import.list)
</code></pre>
"
2209540,457898,2010-02-05T18:51:00Z,2190756,0,FALSE,"<p>I've been doing something similar a few weeks ago. Here's a possible solution, it's written from scratch, so it's kind of beta-release or something like that. I'll try to improve it by removing loops from code...</p>

<p>The main idea is to write a function that will take 2 (or 3) arguments. First one is a <code>data.frame</code> which holds the data gathered from questionnaire, and the second one is a numeric vector with correct answers (this is only applicable for single choice questionnaire). Alternatively, you can add third argument that will return numeric vector with final score, or data.frame with embedded score.</p>

<pre><code>fscore &lt;- function(x, sol, output = 'numeric') {
    if (ncol(x) != length(sol)) {
        stop('Number of items differs from length of correct answers!')
    } else {
        inc &lt;- matrix(ncol=ncol(x), nrow=nrow(x))
        for (i in 1:ncol(x)) {
            inc[,i] &lt;- x[,i] == sol[i]
        }
        if (output == 'numeric') {
            res &lt;- rowSums(inc)
        } else if (output == 'data.frame') {
            res &lt;- data.frame(x, result = rowSums(inc))
        } else {
            stop('Type not supported!')
        }
    }
    return(res)
}
</code></pre>

<p>I'll try to do this in a more elegant manner with some *ply function. Notice that I didn't put <code>na.rm</code> argument... Will do that</p>

<pre><code># create dummy data frame - values from 1 to 5
set.seed(100)
d &lt;- as.data.frame(matrix(round(runif(200,1,5)), 10))
# create solution vector
sol &lt;- round(runif(20, 1, 5))
</code></pre>

<p>Now apply a function:</p>

<pre><code>&gt; fscore(d, sol)
 [1] 6 4 2 4 4 3 3 6 2 6
</code></pre>

<p>If you pass data.frame argument, it will return modified data.frame.
I'll try to fix this one... Hope it helps!</p>
"
2209579,169947,2010-02-05T18:56:25Z,2209258,1,FALSE,"<p>If I'm not mistaken, a pretty simple change could eliminate the <code>3:length(FileNames)</code> kludge:</p>

<pre><code>FileNames &lt;- list.files(path="".../tempDataFolder/"", full.names=TRUE)
dataMerge &lt;- data.frame()
for(f in FileNames){ 
  ReadInMerge &lt;- read.csv(file=f, header=T, na.strings=""NULL"")
  dataMerge &lt;- merge(dataMerge, ReadInMerge, 
               by=c(""COUNTRYNAME"", ""COUNTRYCODE"", ""Year""), all=T)
}
</code></pre>
"
2218768,23903,2010-02-07T23:13:06Z,2218395,1,FALSE,"<p>If you have access to the underlying distance matrix that generated each dendrogram (you probably do if you generated the dendorograms in R), couldn't you just use correlation between the corresponding values of the two matrices?  I know this doesn't address the letter of what you asked, but it's a good solution to the spirit of what you asked.</p>
"
2219383,413049,2010-02-08T03:25:17Z,2156935,2,FALSE,"<p>these are good memory-joggers.  I second the ggplot2 recommend, also recommend looking thru CRAN views:</p>

<p><a href=""http://cran.r-project.org/web/views/"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/views/</a></p>

<p><a href=""http://cran.fhcrc.org/web/views/Graphics.html"" rel=""nofollow noreferrer"">http://cran.fhcrc.org/web/views/Graphics.html</a></p>

<p>(this mirror seems faster in west coast US)</p>

<hr>

<p><a href=""http://dataspora.com/archive/2009/seminar/Survey_of_R_Graphics_by_Driscoll_Dataspora_Jun2009.pdf"" rel=""nofollow noreferrer"">http://dataspora.com/archive/2009/seminar/Survey_of_R_Graphics_by_Driscoll_Dataspora_Jun2009.pdf</a></p>

<hr>

<p><a href=""http://zoonek2.free.fr/UNIX/48_R/04.html"" rel=""nofollow noreferrer"">http://zoonek2.free.fr/UNIX/48_R/04.html</a></p>

<p>(possibly world's longest webpage)</p>

<p><a href=""http://www.stat.auckland.ac.nz/~ihaka/120/lectures.html"" rel=""nofollow noreferrer"">http://www.stat.auckland.ac.nz/~ihaka/120/lectures.html</a></p>

<p>Ihaka's lectures notes</p>
"
2219651,218160,2010-02-08T04:51:04Z,2123195,0,FALSE,"<p>For the record, I highly recommend the mechanize library in Python- it makes building your own personalized crawler/scraper a snap.</p>
"
2219766,185908,2010-02-08T05:29:12Z,2218395,4,FALSE,"<p>As you know, <a href=""https://www.centerspace.net/drawing-dendrograms"" rel=""nofollow noreferrer"">Dendrograms</a> arise from hierarchical clustering - so what you are really asking is how can I compare the results of two hierarchical clustering runs.  There are no standard metrics I know of, but I would be looking at the number of clusters found and comparing membership similarity between like clusters.  <a href=""http://www.centerspace.net/clustering-analysis-part-iii-hierarchical-cluster-analysis"" rel=""nofollow noreferrer"">Here</a> is a good overview of hierarchical clustering that my colleague wrote on clustering scotch whiskey's.</p>
"
2221067,168168,2010-02-08T11:00:02Z,2219626,4,FALSE,"<p>ggplot2 doesn't support ts-objects: only dates of class date and times of class POSIXct are supported. So you would need to convert your data first to a suitable class.</p>

<p>Have a look at <a href=""http://had.co.nz/ggplot2/scale_date.html"" rel=""nofollow noreferrer"">http://had.co.nz/ggplot2/scale_date.html</a> for examples.</p>
"
2221349,134830,2010-02-08T11:57:16Z,2189184,1,FALSE,"<p>Be consistent with your style.  For example, always use <code>&lt;-</code>, or always use <code>=</code>; don't mix and match.  Here are some example style guides from <a href=""http://google-styleguide.googlecode.com/svn/trunk/google-r-style.html"" rel=""nofollow noreferrer"">Google</a> and <a href=""http://had.co.nz/stat405/resources/r-style-guide.html"" rel=""nofollow noreferrer"">Hadley Wickham</a>.</p>

<p>If you are using <code>read.table</code> with <code>sep=','</code> and <code>header=TRUE</code>, you can probably call <code>read.csv</code> instead.</p>

<p>Wherever possible, try to place things in functions rather than having one long script. This can help make the code more readable, and as a bonus you may end up with bits of code you can reuse for later analyses.  In this case, I'd be tempted to move all the code that creates the plot into a function (possibly with subfunctions for initialising the plot and for doing the drawing part).</p>

<p>The <a href=""http://www.burns-stat.com/pages/Tutor/R_inferno.pdf"" rel=""nofollow noreferrer"">R Inferno</a> contains lots of ideas on good R programming practice.</p>
"
2222103,245603,2010-02-08T14:16:06Z,2218395,11,TRUE,"<p>Comparing dendrograms is not quite the same as comparing hierarchical clusterings, because the former includes the lengths of branches as well as the splits, but I also think that's a good start. I would suggest you read E. B. Fowlkes &amp; C. L. Mallows (1983). ""A Method for Comparing Two Hierarchical Clusterings"". Journal of the American Statistical Association 78 (383): 553–584 <a href=""http://dx.doi.org/10.2307/2288117"" rel=""noreferrer"">(link)</a>. </p>

<p>Their approach is based on cutting the trees at each level <em>k</em>, getting a measure <em>Bk</em> that compares the groupings into <em>k</em> clusters, and then examining the <em>Bk</em> vs <em>k</em> plots. The measure <em>Bk</em> is based upon looking at pairs of objects and seeing whether they fall into the same cluster or not. </p>

<p>I am sure that one can write code based on this method, but first we would need to know how the dendrograms are represented in R.</p>
"
2224237,163053,2010-02-08T19:24:00Z,2224196,9,TRUE,"<p>Use <code>unlist()</code>:</p>

<pre><code>&gt; x &lt;- list(l1=c(""a"",""b"",""c""), l2=c(""b"",""d""))
&gt; unlist(x)
l11 l12 l13 l21 l22 
""a"" ""b"" ""c"" ""b"" ""d"" 
</code></pre>

<p>And to get the unique values, just use <code>unique</code>:</p>

<pre><code>&gt; unique(unlist(x))
[1] ""a"" ""b"" ""c"" ""d""
</code></pre>
"
2226636,212593,2010-02-09T03:54:17Z,2219626,4,FALSE,"<p>My crude attempt at a function to generate POSIX dates from a ts object, assuming that periods are years:</p>

<pre><code>tsdates &lt;- function(ts){
  dur&lt;-12%/%frequency(ts)
  years&lt;-trunc(time(ts))
  months&lt;-(cycle(ts)-1)*dur+1
  yr.mn&lt;-as.data.frame(cbind(years,months))
  dt&lt;-apply(yr.mn,1,function(r){paste(r[1],r[2],'01',sep='/')})
  as.POSIXct(dt,tz='UTC')
}
</code></pre>

<p>This can be used with ggplot as:</p>

<pre><code>qplot(tsdates(presidents),presidents,geom='line')
</code></pre>

<p>A more complete solution would need to be able to lay out multiple time series. Also it would be nice to be able to automatically align points according to the time of observation so that we can do things like:</p>

<pre><code>qplot(presidents,lag(presidents))
</code></pre>
"
2226767,163080,2010-02-09T04:32:51Z,2226526,3,TRUE,"<p>Do you need something like this?</p>

<p><a href=""http://cran.r-project.org/web/packages/gap/vignettes/rnews.pdf"" rel=""nofollow noreferrer"">""Drawing pedigree diagrams with R and graphviz""</a> by Jing Hua Zhao</p>

<p>If you search for the name of the paper by author it is available from many sources.</p>
"
2226880,57458,2010-02-09T05:09:40Z,2226867,30,TRUE,"<p>You can read a file using pipes like this:</p>

<pre><code>d = read.table( pipe( 'cat data.txt' ), header = T )
</code></pre>

<p>If you wanted to read from an SSH connection, try this:</p>

<pre><code>d = read.table( pipe( 'ssh hostname ""cat data.txt""' ), header = T )
</code></pre>

<p>There's also no reason to confine this to just ssh commands, you could also do something like this:</p>

<pre><code>d = read.table( pipe( 'cat *.txt' ) )
</code></pre>

<p>See the <a href=""http://cran.r-project.org/doc/manuals/R-data.html#Connections"" rel=""noreferrer"">R Data Import/Export</a> page for more information, specifically the <a href=""http://cran.r-project.org/doc/manuals/R-data.html#Connections"" rel=""noreferrer"">Connections</a> section.</p>
"
2229655,134830,2010-02-09T14:15:58Z,2228544,2,FALSE,"<p>There is a function called Curry in the <a href=""http://cran.r-project.org/web/packages/roxygen/index.html"" rel=""nofollow noreferrer"">roxygen</a> package.<br>
Found via <a href=""https://stat.ethz.ch/pipermail/r-help/2009-December/221224.html"" rel=""nofollow noreferrer"">this conversation</a> on the R Mail Archive.</p>
"
2229663,163053,2010-02-09T14:16:50Z,2228544,28,FALSE,"<p>Both of these functions actually exist in <a href=""http://cran.r-project.org/web/packages/roxygen/index.html"" rel=""noreferrer"">the <code>roxygen</code> package</a> (<a href=""http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/pkg/R/functional.R?rev=71&amp;root=roxygen&amp;view=markup"" rel=""noreferrer"">see the source code here</a>) from Peter Danenberg (was originally based on <a href=""https://stat.ethz.ch/pipermail/r-devel/2007-November/047318.html"" rel=""noreferrer"">Byron Ellis's solution on R-Help</a>):</p>

<pre><code>Curry &lt;- function(FUN,...) {
  .orig = list(...);
  function(...) do.call(FUN,c(.orig,list(...)))
}

Compose &lt;- function(...) {
  fs &lt;- list(...)
  function(...) Reduce(function(x, f) f(x),
                       fs,
                       ...)
}
</code></pre>

<p>Note the usage of the <code>Reduce</code> function, which can be very helpful when trying to do functional programming in R.  See ?Reduce for more details (which also covers other functions such as <code>Map</code> and <code>Filter</code>).</p>

<p>And your example of Curry (slightly different in this usage):</p>

<pre><code>&gt; library(roxygen)
&gt; p &lt;- Curry(paste, collapse="""")
&gt; p(letters[1:10])
[1] ""abcdefghij""
</code></pre>

<p>Here's an example to show the utility of <code>Compose</code> (applying three different functions to letters):</p>

<pre><code>&gt; Compose(function(x) x[length(x):1], Curry(paste, collapse=""""), toupper)(letters)
[1] ""ZYXWVUTSRQPONMLKJIHGFEDCBA""
</code></pre>

<p>And your final example would work like this:</p>

<pre><code>&gt; aggregate(df[,""t""], df[""l""], Compose(Curry(paste, collapse=""""), toupper))
  l    x
1 1  ABG
2 2 DEFH
3 3  CIJ
</code></pre>

<p>Lastly, here's a way to do the same thing with <code>plyr</code> (could also easily be done with <code>by</code> or <code>aggregate</code> as already shown):</p>

<pre><code>&gt; library(plyr)
&gt; ddply(df, .(l), function(df) paste(toupper(df[,""t""]), collapse=""""))
  l   V1
1 1  ABG
2 2 DEFH
3 3  CIJ
</code></pre>
"
2230883,217595,2010-02-09T16:59:46Z,2161152,2,FALSE,"<p>I have also thought about the same thing many times. After reading your questions and looking into your link I made small modifications to the custom python Sweave driver, that you link to. I modified it to also keep the source code  and produce the output as well the same way that Sweave does for R.</p>

<p>I posted the modified version and an example here: <a href=""http://mpastell.com/2010/02/09/python-in-sweave-document/"" rel=""nofollow noreferrer"">http://mpastell.com/2010/02/09/python-in-sweave-document/</a></p>

<p>Granted, it is not optimal but I'm quite happy with the output and I like the ability to include both R and Python in the same document. </p>

<p><strong>Edit about PyLit</strong>:</p>

<p>I also like PyLit and contrary to my original answer you can catch ouput with it as well, although it not as elegant as Sweave! Here is a small example how to do it:</p>

<pre><code>import sys

# Catch PyLit output

a = range(3)
sys.stdout = open('output.txt', 'w')
print a
sys.stdout = sys.__stdout__

# .. include:: output.txt
</code></pre>
"
2232434,163053,2010-02-09T20:50:52Z,2231993,7,FALSE,"<p>One quick suggestion: try to do some matching on the different fields separately before using merge.  The simplest approach is with the <code>pmatch</code> function, although R has no shortage of text matching functions (e.g. <code>agrep</code>).  Here's a simple example:</p>

<pre><code>pmatch(c(""med"", ""mod""), c(""mean"", ""median"", ""mode""))
</code></pre>

<p>For your dataset, this matches all the fund names out of <code>a</code>:</p>

<pre><code>&gt; nrow(merge(a,b,x.by=""Fund.Name"", y.by=""Fund.name""))
[1] 58
&gt; length(which(!is.na(pmatch(a$Fund.Name, b$Fund.name))))
[1] 238
</code></pre>

<p>Once you create matches, you can easily merge them together using those instead.</p>
"
2232558,269831,2010-02-09T21:11:23Z,2231993,1,FALSE,"<p>I'm a Canada local as well, recognize the fund names.</p>

<p>This is a difficult one as each of the data providers picks their own form for the individual fund names. Some use different structure like all end in either Fund or Class others are all over the place. Each seems to choose their own short-forms as well and these change regularly.</p>

<p>That's why so many people like you are doing this by hand on a regular basis. Some of the consulting firms do list indexes to link various sources, not sure if you've explored that route?</p>

<p>As Shane and Marek pointed out this is a matching task more than a straight join. Many companies are struggling with this one. I'm in the middle of my work on this...</p>

<p>Jay</p>
"
2232659,269831,2010-02-09T21:27:04Z,2219626,2,FALSE,"<p>Time series data from the ?ts example.</p>

<pre><code>gnp &lt;- ts(cumsum(1 + round(rnorm(100), 2)), start = c(1954, 7), frequency = 12)

new.date &lt;- seq(as.Date(paste(c(start(gnp),1), collapse = ""/"")), by = ""month"", length.out = length(gnp))
</code></pre>

<p>The seq function can work with date objects. The example above provides the starting date, specifies a monthly frequency and signifies how long of a date vector to create.</p>

<p>Hopefully this is helpfully in your data preparation before using ggplot2 or something else.</p>

<p>You can combine the example above into a data.frame like this:</p>

<pre><code>dat &lt;- data.frame(date=new.date, value=gnp) 
</code></pre>

<p>This can be plotted in ggplot like this:</p>

<pre><code>ggplot(data=dat) + geom_line(aes(date, gnp))
</code></pre>

<p>All the best,</p>

<p>Jay</p>
"
2233599,135944,2010-02-10T00:38:19Z,2233584,90,TRUE,"<p><code>stopifnot()</code></p>

<p>You may also be interested in packages like <a href=""http://cran.r-project.org/web/packages/RUnit/index.html"" rel=""noreferrer"">Runit</a> and <a href=""http://cran.r-project.org/web/packages/testthat/index.html"" rel=""noreferrer"">testthat</a> for unit testing.</p>
"
2234366,212593,2010-02-10T04:13:38Z,2231993,2,TRUE,"<p>Approximate string matching is not a good idea since an incorrect match would invalidate the whole analysis. If the names from each source is the same each time, then building indexes seems the best option to me too. This is easily done in R:</p>

<p>Suppose you have the data:</p>

<pre><code>a&lt;-data.frame(name=c('Ace','Bayes'),price=c(10,13))
b&lt;-data.frame(name=c('Ace Co.','Bayes Inc.'),qty=c(9,99))
</code></pre>

<p>Build an index of names for each source one time, perhaps using pmatch etc. as a starting point and then validating manually.</p>

<pre><code>a.idx&lt;-data.frame(name=c('Ace','Bayes'),idx=c(1,2))
b.idx&lt;-data.frame(name=c('Ace Co.','Bayes Inc.'), idx=c(1,2))
</code></pre>

<p>Then for each run merge using:</p>

<pre><code>a.rich&lt;-merge(a,a.idx,by=""name"")
b.rich&lt;-merge(b,b.idx,by=""name"")
merge(a.rich,b.rich,by=""idx"")
</code></pre>

<p>Which would give us:</p>

<pre><code>  idx name.x price     name.y qty
1   1    Ace    10    Ace Co.   9
2   2  Bayes    13 Bayes Inc.  99
</code></pre>
"
2237269,60617,2010-02-10T13:58:44Z,2232699,24,TRUE,"<p>You are quoting the wrong part of documentation. If you have a look at the doc of <code>[.data.table</code> you will read:</p>

<blockquote>
  <p>When i is a data.table, x must have a
  key, meaning join i to x and <strong>return
  the rows in x that match</strong>. An equi-join
  is performed between each column in i
  to each column in x’s key in order.
  This is similar to base R
  functionality of sub- setting a matrix
  by a 2-column matrix, and in higher
  dimensions subsetting an n-dimensional
  array by an n-column matrix</p>
</blockquote>

<p>I admit the description of the package (the part you quoted) is somewhat confusing, because it seems to say that the ""[""-operation can be used instead of merge. But I think what it says is: if x and y are both data.tables we use a join on an index (which is invoked like merge) instead of binary search.</p>

<hr>

<p><em>One more thing:</em></p>

<p>The data.table library I installed via <code>install.packages</code> was missing the <code>merge.data.table method</code>, so using <code>merge</code> would call <code>merge.data.frame</code>. After installing the <a href=""http://r-forge.r-project.org/projects/datatable/"" rel=""nofollow noreferrer"">package from R-Forge</a> R used the faster <code>merge.data.table</code> method. </p>

<p>You can check if you have the merge.data.table method by checking the output of:</p>

<pre><code>methods(generic.function=""merge"")
</code></pre>

<hr>

<p><strong>EDIT [Answer no longer valid]:</strong> This answer refers to data.table version 1.3. In version 1.5.3 the behaviour of data.table changed and x[y] returns the expected results. Thank you <a href=""https://stackoverflow.com/users/403310/matthew-dowle"">Matthew Dowle</a>, author of data.table, for pointing this out in the comments.</p>
"
2237317,163053,2010-02-10T14:06:45Z,2232699,3,FALSE,"<p>I think that f3lix is correct and that the documentation is a little misleading.  The benefit is in doing a fast join to subset the data.  You still ultimately need to use the <code>merge</code> function afterwards as in your above example. </p>

<p>You will see in <a href=""http://files.meetup.com/1406240/Data%20munging%20with%20SQL%20and%20R.pdf"" rel=""nofollow noreferrer"">Josh's presentation on using data.table</a> that this is how his example runs.  He first subsets one of the data.tables, then does a merge: </p>

<pre><code>library(data.table)
sdt &lt;- DT(series, key='series_id')
ddt &lt;- DT(data, key='series_id')
u &lt;- sdt[ grepl('^[A-Z]{2}URN', fred_id) &amp; !grepl('DSURN', fred_id) ]
d &lt;- ddt[ u, DT(min=min(value)), by='series_id', mult='all']
data &lt;- merge(d,series)[,c('title','min','mean','max')]
</code></pre>
"
2238414,269831,2010-02-10T16:21:55Z,1260965,17,FALSE,"<p>Thought I would add some new information here since there has been some activity around this topic since the posting. Here are two great links to ""Choropleth Map R Challenge"" on the Revolutions blog:</p>

<p><a href=""http://blog.revolutionanalytics.com/2009/11/choropleth-map-r-challenge.html"" rel=""nofollow noreferrer"">Choropleth Map R Challenge</a></p>

<p><a href=""http://blog.revolutionanalytics.com/2009/11/choropleth-challenge-result.html"" rel=""nofollow noreferrer"">Choropleth Challenge Results</a></p>

<p>Hopefully these are useful for people viewing this question.</p>

<p>All the best,</p>

<p>Jay</p>
"
2239978,250839,2010-02-10T20:08:23Z,2239851,3,TRUE,"<p>It's usually better in R if you use the various apply-like functions, rather than a loop.  I think this solves your problem; the only drawback is that you have to use string keys.</p>

<pre><code>&gt; descriptions &lt;- c(""foo//bar"", """")
&gt; probes &lt;- c(10, 20)
&gt; probe2gene &lt;- lapply(strsplit(descriptions, ""//""), function (x) x[2])
&gt; names(probe2gene) &lt;- probes
&gt; probe2gene &lt;- probe2gene[!is.na(probe2gene)]
&gt; probe2gene[[""10""]]
[1] ""bar""
</code></pre>

<p>Unfortunately, R doesn't have a good dictionary/map type.  The closest I've found is using lists as a map from string-to-value.  That seems to be idiomatic, but it's ugly.</p>
"
2240060,269831,2010-02-10T20:21:54Z,2239851,2,FALSE,"<p>If I understand correctly you are looking to save each probe-description combination where the there is more than one (split) value in description?</p>

<p>Probe and Description are the same length?</p>

<p>This is kind of messy but a quick first pass at it?</p>

<pre><code>a &lt;- list(""a"",""b"",""c"")
b &lt;- list(c(""a"",""b""),c(""DEF"",""ABC""),c(""Z""))

names(b) &lt;- a
matches &lt;- which(lapply(b, length)&gt;1) #several ways to do this
b &lt;- lapply(b[matches], function(x) x[2]) #keeps the second element only
</code></pre>

<p>That's my first attempt. If you have a sample dataset that would be very useful.</p>

<p>Best regards,</p>

<p>Jay</p>
"
2240504,160314,2010-02-10T21:32:25Z,2237600,5,TRUE,"<p>Here is an implementation of Hadley's idea.</p>

<pre><code>library(ggplot2)
funcs &lt;- list(log,function(x) x,function(x) x*log(x),function(x) x^2,  exp)
cols &lt;-heat.colors(5,1)
p &lt;-ggplot()+xlim(c(1,10))+ylim(c(1,10))
for(i in 1:length(funcs))
    p &lt;- p + stat_function(aes(y=0),fun = funcs[[i]], colour=cols[i])
print(p)
</code></pre>
"
2241406,142879,2010-02-11T00:23:36Z,2241290,2,FALSE,"<p>ggplot(data.set, aes(x = Time, y = Value, colour = Type)) +
geom_area(aes(fill = Type), position = 'stack')</p>

<p>you need to give the geom_area a fill element and also stack it (though that might be a default)</p>

<p>found here <a href=""http://www.mail-archive.com/r-help@r-project.org/msg84857.html"" rel=""nofollow noreferrer"">http://www.mail-archive.com/r-help@r-project.org/msg84857.html</a></p>
"
2241505,143305,2010-02-11T00:51:56Z,2241369,3,TRUE,"<p>The directory <code>/usr/share/local/lib/R</code> is the default location; the directory is has ownership <code>root:staff</code> by default.  If you add yourself to group <code>staff</code> (easiest: by editing <code>/etc/group</code> and <code>/etc/gshadow</code>) you can write there and you do not need <code>sudo</code> powers for the installation of packages. That is what I do.</p>

<p>Alternatively, do <code>apt-get install littler</code> and copy the example file <code>/usr/share/doc/littler/examples/install.r</code> to <code>/usr/local/bin</code> and <code>chmod 755</code> it.  The you can just do <code>sudo install.r lattice ggplot2</code> to take two popular examples.</p>

<p>BTW Ubuntu 8.1 does not exist as a version. Maybe you meant 8.10?  Consider upgrading to 9.10 ...</p>

<p><em>Edit:</em> Also have a look at <a href=""https://stackoverflow.com/questions/2170043/r-apt-get-install-r-cran-foo-vs-install-packagesfoo"">this recent SO question</a>.</p>
"
2241626,143305,2010-02-11T01:26:37Z,2241583,5,TRUE,"<p>There are many possibilities. Here is a simple one using <code>merge()</code> and a simple column-wise subtraction in the enlarged <code>data.frame</code>:</p>

<pre><code>R&gt; DF1 &lt;- data.frame(trial=rep(1:3,2), \
                     Person=rep(c(""John"",""Bill""), each=3), \
                     Time=c(1.2,1.3,1.1,2.3,2.5,2.7))
R&gt; DF2 &lt;- data.frame(Person=c(""John"",""Bill""), Offset=c(0.5,1.0))
R&gt; DF &lt;- merge(DF1, DF2)
R&gt; DF
  Person trial Time Offset
1   Bill     1  2.3    1.0
2   Bill     2  2.5    1.0
3   Bill     3  2.7    1.0
4   John     1  1.2    0.5
5   John     2  1.3    0.5
6   John     3  1.1    0.5
R&gt; DF$NewTime &lt;- DF$Time - DF$Offset
R&gt; DF
  Person trial Time Offset NewTime
1   Bill     1  2.3    1.0     1.3
2   Bill     2  2.5    1.0     1.5
3   Bill     3  2.7    1.0     1.7
4   John     1  1.2    0.5     0.7
5   John     2  1.3    0.5     0.8
6   John     3  1.1    0.5     0.6
R&gt; 
</code></pre>
"
2241679,13969,2010-02-11T01:46:38Z,2241290,2,FALSE,"<p>I was able to get my result with this:</p>

<p>I loaded the stackedPlot() function from <a href=""https://stat.ethz.ch/pipermail/r-help/2005-August/077475.html"" rel=""nofollow noreferrer"">https://stat.ethz.ch/pipermail/r-help/2005-August/077475.html</a></p>

<p>The function (not mine, see link) was:</p>

<pre><code>
stackedPlot = function(data, time=NULL, col=1:length(data), ...) {

  if (is.null(time))
    time = 1:length(data[[1]]);

  plot(0,0
       , xlim = range(time)
       , ylim = c(0,max(rowSums(data)))
       , t=""n"" 
       , ...
       );

  for (i in length(data):1) {

    # Die Summe bis zu aktuellen Spalte
    prep.data = rowSums(data[1:i]);

    # Das Polygon muss seinen ersten und letzten Punkt auf der Nulllinie haben
    prep.y = c(0
                , prep.data
                , 0
                )

    prep.x = c(time[1]
                , time
                , time[length(time)]
                )

    polygon(prep.x, prep.y
            , col=col[i]
            , border = NA
            );
  }
}
</code></pre>

<p>Then I reshaped my data to wide format.  Then it worked!</p>

<pre><code>
wide = reshape(data, idvar=""ThingAge"", timevar=""VisitWeek"", direction=""wide"");
stackedPlot(wide);
</code></pre>
"
2242463,212593,2010-02-11T05:24:36Z,2241369,0,FALSE,"<p>If you are the only user who needs those packages, then the easiest and neatest way is to let R create a personal library for you. That way you don't need to mess with the system directories managed by the package management system.</p>

<p>Another way to install <em>some</em> packages in Ubuntu is to look for Ubuntu packages with names like <code>r-cran-*</code>. This way you do not have to worry about dependencies, the packages become available to all users, and updates are taken care of by the Ubuntu package management system. But only a small proportion of CRAN packages are available this way and you may not get the latest version.</p>
"
2242506,212593,2010-02-11T05:38:25Z,2241290,2,FALSE,"<p>Turning integers into factors and using geom_bar rather than geom_area worked for me:</p>

<pre><code>df&lt;-expand.grid(x=1:10,y=1:6)
df&lt;-cbind(df,val=runif(60))
df$fx&lt;-factor(df$x)
df$fy&lt;-factor(df$y)
qplot(fy,val,fill=fx,data=df,geom='bar')
</code></pre>
"
2242579,212593,2010-02-11T05:59:10Z,2239851,0,FALSE,"<p>Another way. </p>

<pre><code>probe&lt;-c(4,3,1)
gene&lt;-c('red//hair','strange','blue//blood')
probe2gene&lt;-character()
probe2gene[probe]&lt;-sapply(strsplit(gene,'//'),'[',2)
probe2gene
[1] ""blood"" NA      NA      ""hair"" 
</code></pre>

<p>In the sapply, we take advantage of the fact that in R the subsetting operator is also a function named '[' to which we can pass the index as an argument. Also, an out-of-range index does not cause an error but gives a NA value. On the left hand of the same line, we use the fact that we can pass a vector of indices in any order and with gaps.</p>
"
2243210,457898,2010-02-11T08:46:46Z,2241369,0,FALSE,"<p>Well, I prefer to install packages into local R folder <code>~/R/</code>, but it's just a matter of an individual preference... you can also grant yourself a write permission to default library, it doesn't make any difference.</p>

<p>Be sure to add up-to-date packages. Those packages available in default repos are quite old. R v.2.9.0 is available by default in 9.10, while v.2.10.1 is now available.
So stay up-to-date, add this line to file <code>/etc/apt/sources.list</code> (replace <code>&lt;text&gt;</code> with CRAN server address, you can find server addresses on <code>www.r-project.org</code> > CRAN > Linux > Ubuntu):</p>

<p><code>deb http://&lt;my.favorite.cran.mirror&gt;/bin/linux/ubuntu karmic/</code></p>

<p>then run this line in terminal:</p>

<p><code>gpg --keyserver subkeys.pgp.net --recv-key E2A11821 &amp;&amp; gpg -a --export E2A11821 | sudo apt-key add -</code></p>

<p>and if keys are imported properly, run:</p>

<p><code>sudo apt-get install r-base-core</code></p>

<p>or if you already installed R, run:</p>

<p><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade</code></p>

<p>you should also check for <code>alias</code> functions (try <code>man alias</code> in terminal) to automatize repetitive tasks... feel comfortable in terminal, Synaptic is indeed a good tool, but most Linux users prefer command-line approach for a good reason - it's highly customizable =)</p>

<p>I recommend that you stick with one server (be advised when choosing the default server - I prefer UCLA's server, Berkeley works just fine, Main server is usually busy as hell... so there...)</p>

<p>Alternatively, you can add default CRAN server to .First() function:</p>

<pre><code># replace '&lt;server address&gt;'

.First() &lt;- function() {
options(""repos"" = c(CRAN = ""&lt;my.favorite.cran.mirror&gt;""))
}
</code></pre>

<p>now you can just type:</p>

<p><code>&gt; install.packages('&lt;somepackage&gt;')</code></p>

<p>and you'll lose <b>the</b> boring Tcl/Tk serverlist window! Oh, what a relief!</p>

<p>Welcome to Ubuntu!<br>
Cheers, mate!</p>
"
2243329,168168,2010-02-11T09:09:03Z,2241290,5,TRUE,"<pre><code>library(ggplot2)
set.seed(134)
df &lt;- data.frame(
    VisitWeek = rep(as.Date(seq(Sys.time(),length.out=5, by=""1 day"")),3),
    ThingAge = rep(1:3, each=5),
    MyMetric = sample(100, 15))

ggplot(df, aes(x=VisitWeek, y=MyMetric)) + 
    geom_area(aes(fill=factor(ThingAge)))
</code></pre>

<p>gives me the image below. I suspect your problem lies in correctly specifying the fill mapping for the area plot: <code>fill=factor(ThingAge)</code></p>

<p><a href=""http://www.imageurlhost.com/images/wbc5alknt1apvg3czzmb.png"">alt text http://www.imageurlhost.com/images/wbc5alknt1apvg3czzmb.png</a></p>
"
2246451,172382,2010-02-11T17:56:13Z,2186015,3,FALSE,"<p>Below is a summary of what's currently supported in RSQLite for bound
parameters.  You are right that there is not currently support for
SELECT, but there is no good reason for this and I would like to add
support for it.</p>

<p>If you feel like hacking, you can get a read-only checkout of all of
the DBI related packages here:</p>

<pre><code>use --user=readonly --password=readonly

https://hedgehog.fhcrc.org/compbio/r-dbi/trunk
https://hedgehog.fhcrc.org/compbio/r-dbi/trunk/DBI
https://hedgehog.fhcrc.org/compbio/r-dbi/trunk/SQLite/RSQLite
</code></pre>

<p>I like to receive patches, especially if they include tests and
documentation.  Unified diff, please.  I actually do all my
development using git and so best case is to create a git clone of say
RSQLite and then send me diffs as <code>git format-patch -n
git-svn..</code></p>

<p>Anyhow, here are some examples:</p>

<pre><code>library(""RSQLite"")

make_data &lt;- function(n)
{
    alpha &lt;- c(letters, as.character(0:9))
    make_key &lt;- function(n)
    {
        paste(sample(alpha, n, replace = TRUE), collapse = """")
    }
    keys &lt;- sapply(sample(1:5, replace=TRUE), function(x) make_key(x))
    counts &lt;- sample(seq_len(1e4), n, replace = TRUE)
    data.frame(key = keys, count = counts, stringsAsFactors = FALSE)
}

key_counts &lt;- make_data(100)


db &lt;- dbConnect(SQLite(), dbname = "":memory:"")

sql &lt;- ""
create table keys (key text, count integer)
""

dbGetQuery(db, sql)

bulk_insert &lt;- function(sql, key_counts)
{
    dbBeginTransaction(db)
    dbGetPreparedQuery(db, sql, bind.data = key_counts)
    dbCommit(db)
    dbGetQuery(db, ""select count(*) from keys"")[[1]]
}

##  for all styles, you can have up to 999 parameters

## anonymous
sql &lt;- ""insert into keys values (?, ?)""
bulk_insert(sql, key_counts)


## named w/ :, $, @
## names are matched against column names of bind.data

sql &lt;- ""insert into keys values (:key, :count)""
bulk_insert(sql, key_counts[ , 2:1])

sql &lt;- ""insert into keys values ($key, $count)""
bulk_insert(sql, key_counts)

sql &lt;- ""insert into keys values (@key, @count)""
bulk_insert(sql, key_counts)

## indexed (NOT CURRENTLY SUPPORTED)
## sql &lt;- ""insert into keys values (?1, ?2)""
## bulk_insert(sql)
</code></pre>
"
2247131,144537,2010-02-11T19:30:22Z,2247045,5,FALSE,"<p>Total hack, JD, but it gets it done</p>

<pre><code>x &lt;- ""xxyyxyxy""
c&lt;-strsplit(x,"""")[[1]]
sapply(seq(2,nchar(x),by=2),function(y) paste(c[y-1],c[y],sep=""""))
[1] ""xx"" ""yy"" ""xy"" ""xy""
</code></pre>
"
2247185,169947,2010-02-11T19:37:53Z,2247045,5,FALSE,"<p>Here's one way, but not using regexen:</p>

<pre><code>a &lt;- ""xxyyxyxy""
n &lt;- 2
sapply(seq(1,nchar(a),by=n), function(x) substr(a, x, x+n-1))
</code></pre>
"
2247217,144642,2010-02-11T19:44:23Z,2247045,9,FALSE,"<p>strsplit is going to be problematic, look at a regexp like this  </p>

<pre><code>strsplit(z, '[[:alnum:]]{2}')  
</code></pre>

<p>it will split at the right points but nothing is left.</p>

<p>You could use substring &amp; friends</p>

<pre><code>z &lt;- 'xxyyxyxy'  
idx &lt;- 1:nchar(z)  
odds &lt;- idx[(idx %% 2) == 1]  
evens &lt;- idx[(idx %% 2) == 0]  
substring(z, odds, evens)  
</code></pre>
"
2247388,172382,2010-02-11T20:12:13Z,2247045,18,FALSE,"<p>How about</p>

<pre><code>strsplit(gsub(""([[:alnum:]]{2})"", ""\\1 "", x), "" "")[[1]]
</code></pre>

<p>Basically, add a separator (here "" "") and <em>then</em> use <code>strsplit</code></p>
"
2247488,170792,2010-02-11T20:27:33Z,2247111,2,TRUE,"<p>Check this one:</p>

<pre><code># Create a list to hold the functions
funcs &lt;- list()
funcs[]

# loop through to define functions
for(i in 1:21){

    # Make function name
    funcName &lt;- paste( 'func', i, sep = '' )

    # make function
    func = paste('function(x){x * ', i,'}',sep = '')

    funcs[[funcName]] = eval(parse(text=func))

    }
</code></pre>
"
2247574,163053,2010-02-11T20:43:15Z,2247045,48,TRUE,"<p>Using substring is the best approach:</p>

<pre><code>substring(x, seq(1,nchar(x),2), seq(2,nchar(x),2))
</code></pre>

<p>But here's a solution with plyr:</p>

<pre><code>library(""plyr"")
laply(seq(1,nchar(x),2), function(i) substr(x, i, i+1))
</code></pre>
"
2247984,172382,2010-02-11T21:51:14Z,2239851,0,FALSE,"<p>Here's another approach that should be fast.  Note that this doesn't
remove the empty descriptions.  It could be adapted to do that or you
could clean those in a post processing step using lapply.  Is it the
case that you'll never have a valid description of length one?</p>

<pre><code>make_desc &lt;- function(n)
{
    word &lt;- function(x) paste(sample(letters, 5, replace=TRUE), collapse = """")
    if (runif(1) &lt; 0.70)
        paste(sapply(seq_len(n), word), collapse = ""//"")
    else
        ""----""
}

description &lt;- sapply(seq_len(10), make_desc)
probes &lt;- seq_len(length(description))

desc_parts &lt;- strsplit(description, ""//"", fixed=TRUE, useBytes=TRUE)
lens &lt;- sapply(desc_parts, length)
probes_expand &lt;- rep(probes, lens)
ans &lt;- split(unlist(desc_parts), probes_expand)


&gt; description
 [1] ""fmbec""                                                               
 [2] ""----""                                                                
 [3] ""----""                                                                
 [4] ""frrii//yjxsa//wvkce//xbpkc""                                          
 [5] ""kazzp//ifrlz//ztnkh//dtwow//aqvcm""                                   
 [6] ""stupm//ncqhx//zaakn//kjymf//swvsr//zsexu""                            
 [7] ""wajit//sajgr//cttzf//uagwy//qtuyh//iyiue//xelrq""                     
 [8] ""nirex//awvnw//bvexw//mmzdp//lvetr//xvahy//qhgym//ggdax""              
 [9] ""----""                                                                
[10] ""ubabx//tvqrd//vcxsp//rjshu//gbmvj//fbkea//smrgm//qfmpy//tpudu//qpjbu""


&gt; ans[[3]]
[1] ""----""
&gt; ans[[4]]
[1] ""frrii"" ""yjxsa"" ""wvkce"" ""xbpkc""
</code></pre>
"
2248254,16632,2010-02-11T22:27:54Z,2247111,6,FALSE,"<p>Use a closure (a function that write functions):</p>

<pre><code>multiply &lt;- function(i) {
  force(i)
  function(x) x * i
}

funcs &lt;- list()
for(i in 1:21){
  funcName &lt;- paste( 'func', i, sep = '' )
  funcs[[funcName]] = multiply(i)
}

# OR in one line, with lapply
funcs &lt;- lapply(1:21, multiply)
</code></pre>
"
2248533,66549,2010-02-11T23:20:38Z,2248261,9,TRUE,"<p>This should work:</p>

<pre><code>x = 1:4
y = x
plot(x, y, ann=F, axis=F, col=""blue"", pch=16)
text(x, y, labels=c(""1st"", ""2nd"", ""3rd"", ""4th""), col=""red"", pos=c(3,4,4,1), offset=0.6)
</code></pre>

<p>Just convert your non-data vector (the one containing the labels) to a character vector:
    labels = as.character(label_vector)</p>

<p>and then substitute this for the third argument in line 4 above.</p>

<p>The 'Text' function is fairly versatile because of the various arguments you can pass in--e.g., (as in the example above) you can set the text to a different color than your data points using ""col""; You can also specify the position (relative to the data point annotated by a given text label) for each text label separately. That's often useful to keep text labels from, for instance, overlapping one of the axes, which is what happened the first time i ran this example without setting 'pos'. So by setting 'pos' (as c(3, 4, 4, 1)) i set the position of the text labels as ""above"", ""right"", ""right"", and ""below""--moving the first data point up so it doesn't hit the bottom x-axis, and moving the fourth one down so it doesn't hit the top x-axis. Additionally, using 'offset' (which has a default value of 0.5) you can specify the magnitude of the position adjustment.</p>
"
2248544,160314,2010-02-11T23:21:52Z,2248261,3,FALSE,"<p>Here is a way to do this using the <code>ggplot2</code> package:</p>

<pre><code>library(ggplot2)
x &lt;- rnorm(10)
y &lt;- rnorm(10)
labs &lt;- 1:10
ggplot()+geom_text(aes(x=x,y=y,label=labs))
</code></pre>

<p><img src=""https://i.stack.imgur.com/3oXPN.png"" alt=""enter image description here""></p>
"
2249311,163053,2010-02-12T02:31:00Z,2249181,1,FALSE,"<p>I have never done this myself, but there is an example <a href=""http://www.rforge.net/doc/packages/rJava/with.html"" rel=""nofollow noreferrer"">in the rJava documentation</a> of creating and working with a HashMap using the <code>with</code> function:</p>

<pre><code>HashMap &lt;- J(""java.util.HashMap"")
with( HashMap, new( SimpleEntry, ""key"", ""value"" ) )
with( HashMap, SimpleEntry )
</code></pre>
"
2249622,269831,2010-02-12T04:10:31Z,2249457,5,TRUE,"<p>This will get rid of your border:</p>

<pre><code>p + opts(legend.background = theme_rect(col = 0))
</code></pre>

<p>other options in addition to col (which applies to the border) are fill (background) and size (which is the border size).</p>

<p>Hope that helps!</p>

<p>All the best,</p>

<p>Jay</p>
"
2251996,271848,2010-02-12T13:20:27Z,2241583,1,FALSE,"<p>One liner:</p>

<pre><code>transform(merge(d1,d2), Time=Time - Offset, Offset=NULL)
</code></pre>
"
2253749,900119,2010-02-12T17:29:42Z,2243046,6,TRUE,"<p>Well I found the answer to my problem. The starting values for the real data are completely different from the dummy values: a=500 and b=.1 result in a nice fit. Just thought it might be useful to mention that here.</p>
"
2254669,163053,2010-02-12T19:49:58Z,2254608,3,TRUE,"<p>It is probably best to use an l*ply function (from plyr) or lapply when dealing with lists like this. </p>

<p>The easiest way to do the import is probably like so:</p>

<pre><code>library(plyr)
runs &lt;- llply(paste(""run"",1:30,"".csv"",sep=""""), read.csv)
</code></pre>

<p>Here's one way to plot them:</p>

<pre><code># some dummy data
runs &lt;- list(a=data.frame(x=1:5, y=rnorm(5)), b=data.frame(x=1:5, y=rnorm(5)))
par(mfrow=c((length(runs)/2),2));
l_ply(1:length(runs), function(i) { plot(runs[[i]]$x, runs[[i]]$y) })
</code></pre>

<p>Of course, you can also output this to another device (e.g. pdf) and not use <code>par()</code>.</p>
"
2254685,245603,2010-02-12T19:53:31Z,2254608,5,FALSE,"<p>You would probably be much better off creating <em>one</em> data frame with all the data. For example, add the run number when importing (<code>runs[[i]] = data.frame(read.csv(paste(""run"", i-1, "".csv"")), Run=i)</code>), and then do <code>alldata &lt;- do.call(rbind, runs)</code>.</p>

<p>Now you can use <code>lattice</code> or <code>ggplot2</code> to make plots. For example to get a scatterplot of all runs using different colors by run do:</p>

<pre><code>library(ggplot2)
qplot(x, y, colour=Run, data=alldata, geom=""point"")
</code></pre>
"
2254721,84378,2010-02-12T19:59:30Z,2254701,0,FALSE,"<p>Ok, so I found a solution, if I do</p>

<pre><code>(d$trial&gt;=3)+1
</code></pre>

<p>It converts the boolean to an integer and it works ... however, is there a better way to do this?</p>
"
2254757,245603,2010-02-12T20:04:55Z,2254701,1,TRUE,"<p>If you want to be explicit with the assignment (and hard-coding the cutoff of 3), you can use</p>

<pre><code>d$Day &lt;- ifelse(d$trial&lt;3, 1, 2)
</code></pre>

<p>This is a bit more transparent. Otherwise, as you discovered, doing an arithmetic operation will convert the logical value to numeric. You can do it yourself by using <code>as.numeric</code> or <code>as.integer</code>:</p>

<pre><code>as.integer(FALSE)  #0
as.integer(TRUE)   #1
</code></pre>
"
2254790,163053,2010-02-12T20:11:06Z,2254701,1,FALSE,"<p>Get the data:</p>

<pre><code>x &lt;- read.table(textConnection(
""Trial Person 
1     John   
2     John   
3     John   
4     John
1     Bill 
2     Bill
3     Bill
4     Bill""), header=TRUE)
</code></pre>

<p>I think that your current approach is the right one (note: you don't need as.numeric, because it's automatically cast when doing addition in this case):</p>

<pre><code>(x$Trial &gt;= 3) + 1
</code></pre>

<p>Otherwise, here's a way to do it with plyr.  </p>

<pre><code>library(plyr)
ddply(x, .(Person), transform, Day=rep(c(1,2), each=2))
</code></pre>
"
2254865,135944,2010-02-12T20:21:20Z,2254701,1,FALSE,"<p>More generally, if you're trying to convert a vector of the form <code>c(1,2,3,4,5,6)</code> to <code>c(1,1,2,2,3,3)</code>, as if you had two trials per day, then you might want to express this using integer division: </p>

<pre><code>&gt; x &lt;- 1:6
&gt; x
[1] 1 2 3 4 5 6
&gt; (x-1) %/% 2 + 1
[1] 1 1 2 2 3 3
</code></pre>
"
2255101,245603,2010-02-12T21:05:40Z,2254986,28,FALSE,"<p>The answer probably depends on what format your date is in, but here is an example using the <code>Date</code> class:</p>

<pre><code>dt &lt;- as.Date(""2010/02/10"")
new.dt &lt;- dt - as.difftime(2, unit=""days"")
</code></pre>

<p>You can even play with different units like weeks.</p>
"
2255187,163053,2010-02-12T21:18:44Z,2254986,58,TRUE,"<p>Just subtract a number:</p>

<pre><code>&gt; as.Date(""2009-10-01"")
[1] ""2009-10-01""
&gt; as.Date(""2009-10-01"")-5
[1] ""2009-09-26""
</code></pre>

<p>Since the <code>Date</code> class only has days, you can just do basic arithmetic on it.</p>

<p>If you want to use POSIXlt for some reason, then you can use it's slots:</p>

<pre><code>&gt; a &lt;- as.POSIXlt(""2009-10-04"")
&gt; names(unclass(as.POSIXlt(""2009-10-04"")))
[1] ""sec""   ""min""   ""hour""  ""mday""  ""mon""   ""year""  ""wday""  ""yday""  ""isdst""
&gt; a$mday &lt;- a$mday - 6
&gt; a
[1] ""2009-09-28 EDT""
</code></pre>
"
2258549,143305,2010-02-13T18:17:26Z,2258511,7,TRUE,"<p>JD, we do that in the <a href=""http://cran.r-project.org/package=digest"" rel=""noreferrer"">digest</a> package via <code>serialize()</code> to/from <code>raw</code>. That is nice as you can store serialized objects in SQL and other places. I would actually store this as RData as well which is way quicker to <code>load()</code> (no parsing!) and <code>save()</code>.</p>

<p>Or, if it has to be <code>RawToChar()</code> and ascii then use something like this (taken straight from <code>help(digest)</code> where we compare serialization of the file COPYING:</p>

<pre><code> # test 'length' parameter and file input
 fname &lt;- file.path(R.home(),""COPYING"")
 x &lt;- readChar(fname, file.info(fname)$size) # read file
 for (alg in c(""sha1"", ""md5"", ""crc32"")) {
   # partial file
   h1 &lt;- digest(x    , length=18000, algo=alg, serialize=FALSE)
   h2 &lt;- digest(fname, length=18000, algo=alg, serialize=FALSE, file=TRUE)
   h3 &lt;- digest( substr(x,1,18000) , algo=alg, serialize=FALSE)
   stopifnot( identical(h1,h2), identical(h1,h3) )
   # whole file
   h1 &lt;- digest(x    , algo=alg, serialize=FALSE)
   h2 &lt;- digest(fname, algo=alg, serialize=FALSE, file=TRUE)
   stopifnot( identical(h1,h2) )
 }
</code></pre>

<p>so with that your example becomes this:</p>

<pre><code>R&gt; outCon &lt;- file(""/tmp/jd.txt"", ""w"")
R&gt; mychars &lt;- rawToChar(serialize(1:10, NULL, ascii=T))
R&gt; cat(mychars, file=outCon); close(outCon)
R&gt; fname &lt;- ""/tmp/jd.txt""
R&gt; readChar(fname, file.info(fname)$size)
[1] ""A\n2\n133633\n131840\n13\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n""
R&gt; unserialize(charToRaw(readChar(fname, file.info(fname)$size)))
[1]  1  2  3  4  5  6  7  8  9 10
R&gt; 
</code></pre>
"
2258866,158065,2010-02-13T20:09:43Z,2258784,1,FALSE,"<p>All the options I've ever used have been explained in hadley's great ggplot2 book.</p>
"
2258924,172261,2010-02-13T20:26:45Z,2258844,6,TRUE,"<pre><code>R&gt; set.seed(1)
R&gt; vec1 &lt;- runif(100)
R&gt; vec2 &lt;- 1:20

R&gt; vec1[1:length(vec2)] &lt;- vec2
R&gt; vec1
  [1]  1.00000  2.00000  3.00000  4.00000  5.00000  6.00000  7.00000  8.00000
  [9]  9.00000 10.00000 11.00000 12.00000 13.00000 14.00000 15.00000 16.00000
 [17] 17.00000 18.00000 19.00000 20.00000  0.93471  0.21214  0.65167  0.12556
 [25]  0.26722  0.38611  0.01339  0.38239  0.86969  0.34035  0.48208  0.59957
 [33]  0.49354  0.18622  0.82737  0.66847  0.79424  0.10794  0.72371  0.41127
 [41]  0.82095  0.64706  0.78293  0.55304  0.52972  0.78936  0.02333  0.47723
 [49]  0.73231  0.69273  0.47762  0.86121  0.43810  0.24480  0.07068  0.09947
 [57]  0.31627  0.51863  0.66201  0.40683  0.91288  0.29360  0.45907  0.33239
 [65]  0.65087  0.25802  0.47855  0.76631  0.08425  0.87532  0.33907  0.83944
 [73]  0.34668  0.33377  0.47635  0.89220  0.86434  0.38999  0.77732  0.96062
 [81]  0.43466  0.71251  0.39999  0.32535  0.75709  0.20269  0.71112  0.12169
 [89]  0.24549  0.14330  0.23963  0.05893  0.64229  0.87627  0.77891  0.79731
 [97]  0.45527  0.41008  0.81087  0.60493
</code></pre>
"
2259872,135870,2010-02-14T01:59:45Z,2258784,13,FALSE,"<p>The <code>ggplot2</code> package does not contain much reference information-- this is probably because Hadley has put a lot of work into developing and polishing the package, creating a website full of examples and writing an excellent book that describes the system in detail.</p>

<p>The first place I would look for answers would be the <code>ggplot2</code> website:</p>

<p><a href=""http://docs.ggplot2.org/"" rel=""nofollow noreferrer"">http://docs.ggplot2.org/</a></p>

<p>However, since <code>opts()</code> is not really a geom, stat or scale there are no examples that focus specifically on it.  The next place to look would be the section of the website that contains material from the book-- including source code for examples:</p>

<p><a href=""http://ggplot2.org/book/"" rel=""nofollow noreferrer"">http://ggplot2.org/book/</a></p>

<p>The example code for the chapter ""Polishing your plots for publication"" contains some well commented examples of using <code>set_theme()</code> and <code>opts()</code> to alter plot appearance.</p>

<p>The ultimate source for information is of course the book it's self.  If you find <code>ggplot2</code> has simplified and streamlined your workflow for producing statistical graphics, buying a copy of the book is a great way to say ""thank you"" and support the further development of the package.</p>

<blockquote>
  <p><strong>Update</strong></p>
</blockquote>

<p>After some further investigation, you may be able to find a relatively complete list of options by listing out the source of one of the ""theme"" functions such as <code>theme_bw</code>.  The source doesn't provide a description of the possible <code>key=value</code> pairs, but it does at least show most of the <code>key</code> names that have an effect when set via <code>opts()</code>.</p>
"
2260203,144278,2010-02-14T05:05:38Z,2260147,4,FALSE,"<p>This is interesting. The easiest way would be to fix the Python code so that the dict can be transformed more easily.</p>

<p>But, how about this?</p>

<pre><code>k1 &lt;- unlist(lapply(data,FUN=function(x){return(x[[1]])}))
k2 &lt;- unlist(lapply(data,FUN=function(x){return(x[[2]])}))
data.frame(k1,k2)
</code></pre>

<p>You will need to cast k1 and k2 into the correct data type still, but this should accomplish what you are looking for.</p>
"
2260395,NA,2010-02-14T07:06:50Z,1297698,-1,FALSE,"<p>I've had some trouble with lme4::lmList. For example, summary doesn't seem to work. So you might run into some problems because of that.</p>

<p>So even though I use lmer, instead of lme, I've been explicitly calling nlme::lmList instead. Then summary etc. will work.</p>
"
2260991,163053,2010-02-14T12:11:28Z,2260147,11,TRUE,"<p>The <code>l*ply</code> functions can be your best friend when doing with list processing.  Try this:</p>

<pre><code>&gt; library(plyr)
&gt; ldply(data, data.frame)
  k1 k2
1 v1 v2
2 v3 v4
</code></pre>

<p><code>plyr</code> does some very nice processing behind the scenes to deal with things like irregular lists (e.g. when each list doesn't contain the same number of elements).  This is very common with JSON and XML, and is tricky to handle with the base functions.</p>

<p>Or alternatively using base functions:</p>

<pre><code>&gt; do.call(""rbind"", lapply(data, data.frame))
</code></pre>

<p>You can use <code>rbind.fill</code> (from <code>plyr</code>) instead of <code>rbind</code> if you have irregular lists, but I'd advise just using <code>plyr</code> from the beginning to make your life easier.</p>

<p><em>Edit:</em></p>

<p>Regarding your more complicated example, using Hadley's suggestion deals with this easily:</p>

<pre><code>&gt; x&lt;-list(list(k1=2,k2=3),list(k2=100,k1=200),list(k1=5, k3=9))
&gt; ldply(x, data.frame)
   k1  k2 k3
1   2   3 NA
2 200 100 NA
3   5  NA  9
</code></pre>
"
2261149,60617,2010-02-14T13:13:12Z,2261079,366,TRUE,"<p>Probably the best way is to handle the trailing whitespaces when you read your data file. If you use <code>read.csv</code> or <code>read.table</code> you can set the parameter<code>strip.white=TRUE</code>.</p>

<p>If you want to clean strings afterwards you could use one of these functions:</p>

<pre><code># returns string w/o leading whitespace
trim.leading &lt;- function (x)  sub(""^\\s+"", """", x)

# returns string w/o trailing whitespace
trim.trailing &lt;- function (x) sub(""\\s+$"", """", x)

# returns string w/o leading or trailing whitespace
trim &lt;- function (x) gsub(""^\\s+|\\s+$"", """", x)
</code></pre>

<p>To use one of these functions on <code>myDummy$country</code>:</p>

<pre><code> myDummy$country &lt;- trim(myDummy$country)
</code></pre>

<hr>

<p>To 'show' the whitespace you could use:</p>

<pre><code> paste(myDummy$country)
</code></pre>

<p>which will show you the strings surrounded by quotation marks ("") making whitespaces easier to spot.</p>
"
2261349,212593,2010-02-14T14:13:46Z,2261079,7,FALSE,"<p>Use grep or grepl to find observations with whitespaces and sub to get rid of them.</p>

<pre><code>names&lt;-c(""Ganga Din\t"",""Shyam Lal"",""Bulbul "")
grep(""[[:space:]]+$"",names)
[1] 1 3
grepl(""[[:space:]]+$"",names)
[1]  TRUE FALSE  TRUE
sub(""[[:space:]]+$"","""",names)
[1] ""Ganga Din"" ""Shyam Lal"" ""Bulbul""  
</code></pre>
"
2262252,16363,2010-02-14T18:36:57Z,2249181,4,TRUE,"<p>Try this:</p>

<pre><code>library(rJava)
.jinit()
# create a hash map
hm&lt;-.jnew(""java/util/HashMap"")
# using jrcall instead of jcall, since jrcall uses reflection to get types 
.jrcall(hm,""put"",""one"", ""1"")
.jrcall(hm,""put"",""two"",""2"")
.jrcall(hm,""put"",""three"", ""3"")

# convert to R list
keySet&lt;-.jrcall(hm,""keySet"")
an_iter&lt;-.jrcall(keySet,""iterator"")
aList &lt;- list()
while(.jrcall(an_iter,""hasNext"")){
  key &lt;- .jrcall(an_iter,""next"");
  aList[[key]] &lt;- .jrcall(hm,""get"",key)
}
</code></pre>

<p>Note that using .jrcall is less efficient than .jcall.  But for the life of me I can not get the method signature right with .jcall.  I wonder if it has something to do with the lack of generics. </p>
"
2264062,212593,2010-02-15T04:40:50Z,2253179,3,FALSE,"<p>Setting the colour aesthetic for each geom to a constant may help. Here is a small example:</p>

<pre><code>require(ggplot2)
set.seed(666)
N&lt;-20
foo&lt;-data.frame(x=1:N,y=runif(N),z=runif(N))
p&lt;-ggplot(foo)
p&lt;-p+geom_line(aes(x,y,colour=""Theory""))
p&lt;-p+geom_point(aes(x,z,colour=""Practice""))

#Optional, if you want your own colours
p&lt;-p+scale_colour_manual(""Source"",c('blue','red'))

print(p)
</code></pre>

<p><a href=""http://img59.imageshack.us/img59/5460/fooplot.png"" rel=""nofollow noreferrer"">alt text http://img59.imageshack.us/img59/5460/fooplot.png</a></p>
"
2265113,168747,2010-02-15T10:00:20Z,2261079,8,FALSE,"<p>ad1) To see white spaces you could directly call <code>print.data.frame</code> with modified arguments:</p>

<pre><code>print(head(iris), quote=TRUE)
#   Sepal.Length Sepal.Width Petal.Length Petal.Width  Species
# 1        ""5.1""       ""3.5""        ""1.4""       ""0.2"" ""setosa""
# 2        ""4.9""       ""3.0""        ""1.4""       ""0.2"" ""setosa""
# 3        ""4.7""       ""3.2""        ""1.3""       ""0.2"" ""setosa""
# 4        ""4.6""       ""3.1""        ""1.5""       ""0.2"" ""setosa""
# 5        ""5.0""       ""3.6""        ""1.4""       ""0.2"" ""setosa""
# 6        ""5.4""       ""3.9""        ""1.7""       ""0.4"" ""setosa""
</code></pre>

<p>See also <code>?print.data.frame</code> for other options.</p>
"
2265949,144565,2010-02-15T12:42:48Z,1395266,2,FALSE,"<p>Check out <a href=""http://github.com/jolby/rincanter"" rel=""nofollow noreferrer"">Rincanter</a>.</p>
"
2266386,163053,2010-02-15T14:00:56Z,2266200,4,FALSE,"<p>All <a href=""http://cran.r-project.org/"" rel=""nofollow noreferrer"">the packages on CRAN</a> are open source, so you can download all the source code from there. I recommend starting there by looking at the packages you use regularly to see how they're implemented. </p>

<p>Beyond that, <a href=""http://rosettacode.org/wiki/Category:R"" rel=""nofollow noreferrer"">Rosetta Code</a> has many R examples.  And you may want to follow <a href=""http://www.r-bloggers.com/"" rel=""nofollow noreferrer"">R-Bloggers</a>.</p>
"
2266576,66549,2010-02-15T14:35:04Z,2266200,8,TRUE,"<p>I'll mention a few that i think are excellent resources but that i haven't seen mentioned on SO. They are all free and freely available on the Web (links supplied).</p>

<p><strong><em><a href=""http://www.ats.ucla.edu/stat/dae/"" rel=""noreferrer"">Data Analysis Examples</a></em></strong>
A collection of individual examples from the UCLA Statistics Dept. which you can browse by major category (e.g., ""Count Models"", ""Multivariate Analysis"", ""Power Analysis"") then download examples with complete R code under any of these rubrics (e.g., under ""Count Models"" are ""Poisson Regression"", ""Negative Binomial Regression"", and so on). </p>

<p><strong><em><a href=""http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf"" rel=""noreferrer"">Verzani: SimpleR: Using R for Introductory Statistics</a></em></strong>  A little over 100 pages, and just outstanding. It's easy to follow but very dense. It is a few years old, still i've only found one deprecated function in this text. This is a resource for a brand new R user; it also happens to be an excellent statistics refresher. This text probably contains 20+ examples (with R code and explanation) directed to fundamental statistics (e.g., hypothesis testing, linear regression, simple simulation, and descriptive statistics).</p>

<p><strong><em><a href=""http://zoonek2.free.fr/UNIX/48_R/all.html"" rel=""noreferrer"">Statistics with R</a></em></strong>   (Vincent Zoonekynd)  You can read it online or print it as a pdf. Printed it's well over 1000 pages. The author obviously got a lot of the information by reading the source code for the various functions he discusses--a lot of the information here, i haven't found in any other source. This resource contains large sections on Graphics, Basic Statistics, Regression, Time Series--all w/ small examples (R code + explanation). The final three sections contain the most exemplary code--very thorough application sections on Finance (which seems to be the author's professional field), Genetics, and Image Analysis.  </p>
"
2267798,170792,2010-02-15T17:42:35Z,2266200,9,FALSE,"<p>Just to add some more</p>

<p><a href=""http://zoonek2.free.fr/UNIX/48_R/02.html"" rel=""nofollow noreferrer"">Programming in R</a></p>

<p><a href=""http://www.statslab.cam.ac.uk/~pat/redwsheets.pdf"" rel=""nofollow noreferrer"">INTRODUCTION TO STATISTICAL MODELLING IN R</a></p>

<p><a href=""http://gbi.agrsci.dk/statistics/courses/mixed07/block2material/LinearAlgebraR-Handout.pdf"" rel=""nofollow noreferrer"">Linear algebra in R</a></p>

<p><a href=""http://www.burns-stat.com/pages/Tutor/R_inferno.pdf"" rel=""nofollow noreferrer"">The R Inferno</a></p>

<p><a href=""http://www.mayin.org/ajayshah/KB/R/index.html"" rel=""nofollow noreferrer"">R by example</a></p>

<p><a href=""http://biostat.mc.vanderbilt.edu/wiki/Main/RClinic"" rel=""nofollow noreferrer"">The R Clinic</a> </p>

<p><a href=""http://faculty.washington.edu/tlumley/survey/"" rel=""nofollow noreferrer"">Survey analysis in R</a></p>

<p><a href=""http://manuals.bioinformatics.ucr.edu/home/R_BioCondManual"" rel=""nofollow noreferrer"">R &amp; Bioconductor Manual</a></p>

<p><a href=""http://pj.freefaculty.org/R/Rtips.html"" rel=""nofollow noreferrer"">Rtips</a></p>

<p><a href=""http://www.ats.ucla.edu/stat/R/"" rel=""nofollow noreferrer"">Resources to help you learn and use R</a></p>

<p><a href=""http://www.cerebralmastication.com/r-resources/"" rel=""nofollow noreferrer"">General R Links</a></p>
"
2268077,66549,2010-02-15T18:28:17Z,2266408,3,TRUE,"<p>The R Bundle Developer is apparently working on this (see this <a href=""http://old.nabble.com/R-bundle-respecting-startup-directory---.Rprofile--to25585521.html#a25603844"" rel=""nofollow noreferrer"">Post</a> on the Mailing List) but it's not available at the moment.</p>

<p>In the meantime, you have a couple of choices.</p>

<p>First, you can create a new bundle (e..g, ""briandk-R"") then create a snippet w/in that bundle either with 'source($1)' or just hardcode the file you want to source instead of the placeholder (so, e.g., ""source(""~/some_file_to_source.R""). If you do the latter, then you can configure TM to source your file via a tab trigger (in the Bundle Editor, toggle over to 'settings' (upper left hand corner) and type ""source.r, source.rd.console"" in the 'Scope Selector' field then choose a few letters for your tab trigger (e.g., ""src."")</p>

<p>If you don't want to do that, go to the 'Rdaemon' Directory (which is either in your home directory or in ~/Library/Application Support/Rdaemon). Look in this directory and you will see another directory called ""daemon.' In there is a file called ""start.r"" which lists the files that are sourced upon starting R from the Rdaemon. You know what to do from there. (Note: This directory also contains a couple of other scripts which contain initial settings; you might wish to have a look at those as well)</p>
"
2268155,256662,2010-02-15T18:41:22Z,2266200,2,FALSE,"<h2><strong>Book like tutorials</strong></h2>

<p>Book like tutorials are usually spread in the form of PDF. Many of them are available on the R-project homepage here:</p>

<p><a href=""http://cran.r-project.org/other-docs.html#english"" rel=""nofollow noreferrer"">http://cran.r-project.org/other-docs.html#english</a></p>

<p>(This link includes many of the texts others have mentioned)</p>

<h2><strong>Article like tutorials</strong></h2>

<p>These are usually present inside blogs. The biggest list of R-bloggers I know of exists here:</p>

<p><a href=""http://www.r-bloggers.com/"" rel=""nofollow noreferrer"">http://www.r-bloggers.com/</a></p>

<p>And many of these bloggers posts (many of which are tutorials) are listed here:</p>

<p><a href=""http://www.r-bloggers.com/archive/"" rel=""nofollow noreferrer"">http://www.r-bloggers.com/archive/</a></p>

<p>(although inside each blog there are usually more tutorials).</p>
"
2269182,245603,2010-02-15T21:51:59Z,2269084,15,TRUE,"<p><code>unique</code> does not appear to have an <code>na.rm</code> argument, but you can remove the missing values yourself before calling it:</p>

<pre><code>A &lt;- matrix(c(NA,""A"",""A"",
             ""B"", NA, NA,
              NA, NA, ""C""), nr=3, byrow=TRUE)
apply(A, 1, function(x)unique(x[!is.na(x)]))
</code></pre>

<p>gives</p>

<pre><code>[1] ""A"" ""B"" ""C""
</code></pre>
"
2269272,143305,2010-02-15T22:09:16Z,2269084,6,FALSE,"<p>You were very, very close in your initial solution. But as Aniko remarked, you not to remove <code>NA</code> values before you can use unique.</p>

<p>An example where we first create a similar <code>data.frame</code> and then use <code>apply()</code> as you did -- but with an additional anonymous function that is used to combine <code>na.omit()</code> and <code>unique()</code>:</p>

<pre><code>R&gt; DF &lt;- t(data.frame(foo=sample(c(NA, ""Foo""), 5, TRUE), 
                      bar=sample(c(NA, ""Bar""), 5, TRUE)))
R&gt; DF
    [,1]  [,2] [,3]  [,4]  [,5] 
foo ""Foo"" NA   ""Foo"" ""Foo"" ""Foo""
bar NA    NA   NA    ""Bar"" ""Bar""
R&gt; apply(DF, 1, function(x) unique(na.omit(x)))
  foo   bar 
""Foo"" ""Bar"" 
</code></pre>
"
2270883,212593,2010-02-16T05:33:06Z,2270201,3,FALSE,"<p>I don't know what is causing the error, but a fix that I could come up with is to replace the for loop with a data frame like this:</p>

<pre><code>date.df&lt;-data.frame(d=dates,t=text)
p &lt;- p + geom_text(aes(x=d,label=t),y=0,
                   data = date.df,
                   size = 3, hjust = 1, vjust = 0)
p&lt;-p+geom_vline(aes(xintercept=d),data=date.df,alpha=.4)
</code></pre>
"
2274298,258334,2010-02-16T16:12:44Z,2274186,0,FALSE,"<p>Take the answer from Aniko and reshape it to your specific problem. I used the data <code>submitted</code> as you have posted.</p>

<p>Define your variables:</p>

<pre><code>y1 &lt;- submitted[30:(length(submitted)-1)]
x1 &lt;- seq(length(y1))
</code></pre>

<p>It is enough just using the <code>seq()</code> function this way. It already does the job for you. Then you do the fit and capture the x-values of your <code>barplot()</code> as mentioned by Aniko. That will save the x-values in a matrix so I use <code>as.vector()</code> afterwards to turn it into a vector and make things a little easier.</p>

<pre><code>fit1 &lt;- nls(y1~a*x1*exp(-b*x1^2),start=list(a=500,b=.01),trace=TRUE)
bar &lt;- barplot(submitted, las=2, cex.axis=0.8, cex=0.8)
bar2 &lt;- as.vector(bar)
</code></pre>

<p>If you simply print your <code>bar2</code> you will see the exact values and now you can specify where to place your fit in the plot. Be sure that in your following <code>lines()</code> function the x-vector is of the same length as the y-vector. Eg you can simply do some additional checks with the <code>length()</code> function.
Here is the your fit placed into the second half of your barplot:</p>

<pre><code>lines(x = bar2[30:(length(bar2)-1)], y = predict(fit1))
</code></pre>
"
2274348,245603,2010-02-16T16:18:34Z,2274186,3,TRUE,"<p>A reproducible example would have been helpful, but probably the issue is that the bars are not located at the x-coordinates that you expect. You can find out the x-coordinates of the bars by capturing the output of the <code>barplot</code> function:</p>

<pre><code>dat &lt;- 1:5                   # fake data for barplot
fit &lt;- dat+rnorm(5, sd=0.1)  # fake fitted values

bp &lt;- barplot(dat)           # draw plot and capture x-coordinates
lines(bp, fit)               # add line
</code></pre>

<p><strong>Edit:</strong><br>
The same principle can be used for adding a partial line. Rewriting your code a bit to get an index <code>idx</code> showing the parts of the data that you want to model:</p>

<pre><code>x &lt;- 0:(length(submitted)-1) 
idx &lt;- 30:(length(submitted)-1)  # the part of the data to be modeled
y1 &lt;- submitted[idx] 
x1 &lt;- idx-30 
fit1 &lt;- nls(y1~a*x1*exp(-b*x1^2),start=list(a=500,b=.01),trace=TRUE) 
# capture the midpoints from the barplot
bp &lt;- barplot(submitted,names.arg=x, las=2, cex.axis=0.8, cex=0.8) 
# subset the midpoints to the range of the fit
lines(bp[idx], predict(fit1)) 
</code></pre>

<p>(note that I also changed <code>seq(0:n)</code> to <code>0:n</code>, because the first does not give you a sequence of 0 to n.)</p>
"
2274554,66549,2010-02-16T16:44:59Z,2274487,8,FALSE,"<p>Self-hosting is an option if you insist on using RApache. This might be easier than you think. Here's a link to a <a href=""http://blog.leahculver.com/2009/07/my-own-server.html"" rel=""noreferrer"">blog post</a> i read a month ago before i decided to buy the hardware and server my own files. i just watched this seven minute YouTube video tutorial entitled ""<a href=""http://www.r-statistics.com/2010/02/r-web-application-hello-world-using-rapache-7min-video-tutorial/"" rel=""noreferrer"">R Web Application–'Hello World' using RApache</a>""  I believe this was just posted today.</p>

<p>In seven minutes, the author walks through building a ""hello world"" Site using RApache then walks through a more ambitious example, building a user-input form to collect inputs then deliver them to a particular R function--pretty much a exemplary slice of what i suspect most people would want to use RApache for.</p>

<p>A second option is using a web framework. My recommendation here is <a href=""http://www.djangoproject.com/"" rel=""noreferrer"">Django</a>. Why? It's written in Python so you can access R functionality via the python bindings (RPy2). Second, if you are not an experienced web developer, Django is in many ways, a great framework to begin with because it's truly a ""full-stack"" solution--it works more or less out of the box. In addition, there is a substantial and growing body of quality step-by-setp tutorials, code snippets, and even packaged django Sites, to learn from.</p>
"
2274577,111466,2010-02-16T16:48:11Z,2274487,3,FALSE,"<p>it seems they provide a VMWare image to get up and running quickly.</p>

<p>I suggest you download VMWare player and try the image. Since RApache isn't available for Windows, this is the most simple way, I guess. I wouldn't use that for hosting, but I would first try whether this stack is actually the right thing for your app. Also, this allows you testing things locally.</p>
"
2275731,197321,2010-02-16T19:36:19Z,2274487,1,FALSE,"<p>Doug,
Should I read your suggestion as saying that a Django app can call the RPy2 functionality without RApache? If so, that sounds like a solution for folks on shared hosting who can't install the RAPache module. </p>
"
2276001,163053,2010-02-16T20:15:10Z,2275896,127,TRUE,"<p>The <code>apply</code> functions in R don't provide improved performance over other looping functions (e.g. <code>for</code>).  One exception to this is <code>lapply</code> which can be a little faster because it does more work in C code than in R (see <a href=""https://stackoverflow.com/questions/1692336/applying-a-function-to-a-distance-matrix-in-r"">this question for an example of this</a>).  </p>

<p>But in general, the rule is that <strong><em>you should use an apply function for clarity, not for performance</em></strong>.  </p>

<p>I would add to this that <strong><em>apply functions have <a href=""http://en.wikipedia.org/wiki/Side_effect_(computer_science)"" rel=""nofollow noreferrer"">no side effects</a></em></strong>, which is an important distinction when it comes to functional programming with R.  This can be overridden by using <code>assign</code> or <code>&lt;&lt;-</code>, but that can be very dangerous.  Side effects also make a program harder to understand since a variable's state depends on the history.</p>

<p><em>Edit:</em></p>

<p>Just to emphasize this with a trivial example that recursively calculates the Fibonacci sequence; this could be run multiple times to get an accurate measure, but the point is that none of the methods have significantly different performance:</p>

<pre><code>&gt; fibo &lt;- function(n) {
+   if ( n &lt; 2 ) n
+   else fibo(n-1) + fibo(n-2)
+ }
&gt; system.time(for(i in 0:26) fibo(i))
   user  system elapsed 
   7.48    0.00    7.52 
&gt; system.time(sapply(0:26, fibo))
   user  system elapsed 
   7.50    0.00    7.54 
&gt; system.time(lapply(0:26, fibo))
   user  system elapsed 
   7.48    0.04    7.54 
&gt; library(plyr)
&gt; system.time(ldply(0:26, fibo))
   user  system elapsed 
   7.52    0.00    7.58 
</code></pre>

<p><em>Edit 2:</em></p>

<p>Regarding the usage of parallel packages for R (e.g. rpvm, rmpi, snow), these do generally provide <code>apply</code> family functions (even the <code>foreach</code> package is essentially equivalent, despite the name).  Here's a simple example of the <code>sapply</code> function in <code>snow</code>:</p>

<pre><code>library(snow)
cl &lt;- makeSOCKcluster(c(""localhost"",""localhost""))
parSapply(cl, 1:20, get(""+""), 3)
</code></pre>

<p>This example uses a socket cluster, for which no additional software needs to be installed; otherwise you will need something like PVM or MPI (see <a href=""http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html"" rel=""nofollow noreferrer"">Tierney's clustering page</a>).  <code>snow</code> has the following apply functions:</p>

<pre><code>parLapply(cl, x, fun, ...)
parSapply(cl, X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)
parApply(cl, X, MARGIN, FUN, ...)
parRapply(cl, x, fun, ...)
parCapply(cl, x, fun, ...)
</code></pre>

<p>It makes sense that <code>apply</code> functions should be used for parallel execution since they <em>have no <strong><a href=""http://en.wikipedia.org/wiki/Side_effect_(computer_science)"" rel=""nofollow noreferrer"">side effects</a></em></strong>.  When you change a variable value within a <code>for</code> loop, it is globally set.  On the other hand, all <code>apply</code> functions can safely be used in parallel because changes are local to the function call (unless you try to use <code>assign</code> or <code>&lt;&lt;-</code>, in which case you can introduce side effects).  Needless to say, it's critical to be careful about local vs. global variables, especially when dealing with parallel execution.</p>

<p><em>Edit:</em></p>

<p>Here's a trivial example to demonstrate the difference between <code>for</code> and <code>*apply</code> so far as side effects are concerned:</p>

<pre><code>&gt; df &lt;- 1:10
&gt; # *apply example
&gt; lapply(2:3, function(i) df &lt;- df * i)
&gt; df
 [1]  1  2  3  4  5  6  7  8  9 10
&gt; # for loop example
&gt; for(i in 2:3) df &lt;- df * i
&gt; df
 [1]  6 12 18 24 30 36 42 48 54 60
</code></pre>

<p>Note how the <code>df</code> in the parent environment is altered by <code>for</code> but not <code>*apply</code>.</p>
"
2280284,163053,2010-02-17T11:48:21Z,2280276,19,TRUE,"<p>A vector has 1 dimension while a data frame has 2. I can't think of a good reason to convert a single vector into a data frame. That question generally arises when you have multiple vectors. </p>

<p>A better question is what is the difference between a data frame and a matrix: a data frame can have different data types for each column, while a matrix is all one data type. Behind the scenes, a data frame is really a list with equal length vectors at each index.  </p>
"
2280874,256662,2010-02-17T13:16:32Z,2280276,3,FALSE,"<p>Another good point to note is that when running code, operations on matrixes are (most of the time) much faster then on data frames.</p>

<p>Tal</p>
"
2281246,170792,2010-02-17T14:03:54Z,2280724,3,TRUE,"<p>As far as I can remember, the new version overwrites the existing one without a problem. In my case, however, the new version had many more problems in communicating with R console. For example it was very often the case that I was pressing the <code>R send: selection</code> button and nothing was happening.</p>

<p>I considered myself very lucky to have kept the Tinn-R 2.2.0.2 installer, and this is what I am using until now.</p>
"
2281627,143305,2010-02-17T14:53:19Z,2281353,50,TRUE,"<p>As Oscar Wilde said</p>

<blockquote>
  <p>Consistency is the last refuge of the
  unimaginative.</p>
</blockquote>

<p>R is more of an evolved rather than designed language, so these things happen. <code>names()</code> and <code>colnames()</code> work on a <code>data.frame</code> but <code>names()</code> does not work on a matrix:</p>

<pre><code>R&gt; DF &lt;- data.frame(foo=1:3, bar=LETTERS[1:3])
R&gt; names(DF)
[1] ""foo"" ""bar""
R&gt; colnames(DF)
[1] ""foo"" ""bar""
R&gt; M &lt;- matrix(1:9, ncol=3, dimnames=list(1:3, c(""alpha"",""beta"",""gamma"")))
R&gt; names(M)
NULL
R&gt; colnames(M)
[1] ""alpha"" ""beta""  ""gamma""
R&gt; 
</code></pre>
"
2281760,163053,2010-02-17T15:09:30Z,2281353,8,FALSE,"<p>Just to expand a little on Dirk's example:</p>

<p>It helps to think of a data frame as a list with equal length vectors.  That's probably why <code>names</code> works with a data frame but not a matrix.</p>

<p>The other useful function is <code>dimnames</code> which returns the names for every dimension.  You will notice that the <code>rownames</code> function actually just returns the first element from <code>dimnames</code>.</p>

<p>Regarding <code>rownames</code> and <code>row.names</code>: I can't tell the difference, although <code>rownames</code> uses <code>dimnames</code> while <code>row.names</code> was written outside of R.  They both also seem to work with higher dimensional arrays:</p>

<pre><code>&gt;a &lt;- array(1:5, 1:4)
&gt; a[1,,,]
&gt; rownames(a) &lt;- ""a""
&gt; row.names(a)
[1] ""a""
&gt; a
, , 1, 1    
  [,1] [,2]
a    1    2

&gt; dimnames(a)
[[1]]
[1] ""a""

[[2]]
NULL

[[3]]
NULL

[[4]]
NULL
</code></pre>
"
2281974,134830,2010-02-17T15:35:37Z,2281353,4,FALSE,"<p>I think that using <code>colnames</code> and <code>rownames</code> makes the most sense; here's why.</p>

<p>Using <code>names</code> has several disadvantages.  You have to remember that it means ""column names"", and it only works with data frame, so you'll need to call <code>colnames</code> whenever you use matrices. By calling <code>colnames</code>, you only have to remember one function.  Finally, if you look at the code for <code>colnames</code>, you will see that it calls <code>names</code> in the case of a data frame anyway, so the output is identical.</p>

<p><code>rownames</code> and <code>row.names</code> return the same values for data frame and matrices; the only difference that I have spotted is that where there aren't any names, <code>rownames</code> will print ""NULL"" (as does <code>colnames</code>), but <code>row.names</code> returns it invisibly.  Since there isn't much to choose between the two functions, <code>rownames</code> wins on the grounds of aesthetics, since it pairs more prettily with<code>colnames</code>.  (Also, for the lazy programmer, you save a character of typing.)</p>
"
2282132,245603,2010-02-17T15:54:15Z,2281561,7,TRUE,"<p>You are almost there. Assuming you have the data frame <code>dt</code> with the x, y, and pij values, just sample the rows!</p>

<pre><code>dt &lt;- expand.grid(X=1:3, Y=1:2)
dt$p &lt;- runif(6)
dt$p &lt;- dt$p / sum(dt$p)  # get fake probabilities
idx &lt;- sample(1:nrow(dt), size=8, replace=TRUE, prob=dt$p)
sampled.x &lt;- dt$X[idx]
sampled.y &lt;- dt$Y[idx]
</code></pre>
"
2282786,256662,2010-02-17T17:10:04Z,2280724,0,FALSE,"<p>I went through what you just did and switched because of it to notepad++ combined with NppToR.</p>

<p>Good luck,
Tal</p>
"
2282941,37751,2010-02-17T17:31:07Z,2282892,3,TRUE,"<p>Notepad++ isn't really configured to offer these types of features. You'll do better to explore StatET + Eclipse or ESS (Emacs speaks statistics). </p>

<p>Links:</p>

<ul>
<li><a href=""http://www.walware.de/goto/statet"" rel=""nofollow noreferrer"">StatET</a></li>
<li><a href=""http://www.splusbook.com/R_Eclipse_StatET.pdf"" rel=""nofollow noreferrer"">StatET install guide</a> (PDF)</li>
<li><a href=""http://ess.r-project.org/"" rel=""nofollow noreferrer"">ESS</a></li>
</ul>
"
2283430,457898,2010-02-17T18:40:52Z,2281353,2,FALSE,"<p>And another expansion:</p>

<pre><code># create dummy matrix
set.seed(10)
m &lt;- matrix(round(runif(25, 1, 5)), 5)
d &lt;- as.data.frame(m)
</code></pre>

<p>If you want to assign new column names you can do following on <code>data.frame</code>:</p>

<pre><code># an identical effect can be achieved with colnames()   
names(d) &lt;- LETTERS[1:5]
&gt; d
  A B C D E
1 3 2 4 3 4
2 2 2 3 1 3
3 3 2 1 2 4
4 4 3 3 3 2
5 1 3 2 4 3
</code></pre>

<p>If you, however run previous command on <code>matrix</code>, you'll mess things up:</p>

<pre><code>names(m) &lt;- LETTERS[1:5]
&gt; m
     [,1] [,2] [,3] [,4] [,5]
[1,]    3    2    4    3    4
[2,]    2    2    3    1    3
[3,]    3    2    1    2    4
[4,]    4    3    3    3    2
[5,]    1    3    2    4    3
attr(,""names"")
 [1] ""A"" ""B"" ""C"" ""D"" ""E"" NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
[20] NA  NA  NA  NA  NA  NA 
</code></pre>

<p>Since matrix can be regarded as two-dimensional vector, you'll assign names only to first five values (you don't want to do that, do you?). In this case, you should stick with <code>colnames()</code>.</p>

<p>So there...</p>
"
2283653,163053,2010-02-17T19:14:40Z,2050790,11,FALSE,"<p>Just to add one more point to this: </p>

<p>R does have a data structure equivalent to the Python dict in <a href=""http://cran.r-project.org/web/packages/hash/"" rel=""noreferrer"">the <code>hash</code> package</a>.  You can read about it in <a href=""http://opendatagroup.com/2009/07/26/hash-package-for-r/"" rel=""noreferrer"">this blog post from the Open Data Group</a>.  Here's a simple example:</p>

<pre><code>&gt; library(hash)
&gt; h &lt;- hash( keys=c('foo','bar','baz'), values=1:3 )
&gt; h[c('foo','bar')]
&lt;hash&gt; containing 2 key-value pairs.
  bar : 2
  foo : 1
</code></pre>

<p>In terms of usability, the <code>hash</code> class is very similar to a list.  But the performance is better for large datasets.</p>
"
2284218,135870,2010-02-17T20:38:50Z,2278951,5,TRUE,"<p>Well, it seems like you would first need to generate n-tuples from your vector.  The following function should accomplish that:</p>

<pre><code>makeTuples &lt;- function( x, n ){

  # Very inefficient way to loop... but what the heck
  tuples &lt;- list()

  for( i in 1:n ){

    tuples[[i]] &lt;- x[i:(length(x)-n+i)]

  }

  return(tuples)

}
</code></pre>

<p>Then you could feed the results of <code>makeTuples()</code> to <code>table()</code> using <code>do.call()</code>:</p>

<pre><code>do.call( table, makeTuples(s,3) )

, ,  = 0


    0 1
  0 4 1
  1 3 1

, ,  = 1


    0 1
  0 2 1
  1 0 1
</code></pre>

<p>This works because the <code>makeTuples()</code> function returns the tuples as a list of lists. The output isn't quite as nice as you wanted, but you could write a function to reformat, say:</p>

<pre><code>, ,  = 0


    0 1
  0 4 1
  1 3 1
</code></pre>

<p>To:</p>

<pre><code>     0 1
  00 4 1
  01 3 1
</code></pre>

<p>It would require looping over the outer n-2 dimensions of the n-dimensional array returned by <code>table</code>, creating row names and concatenating things together.</p>

<blockquote>
  <p><strong>Update</strong></p>
</blockquote>

<p>So, I was just sitting in a Stochastic processes class when I figured out a more or less straight-forward way to produce the output you want without trying to unwind the output of <code>table()</code>.  First you will need a function that generates all possible permutations of n selections from your population.  The generation of permutations can be done with <code>expand.grid()</code>, but it needs a little sugar-coating:</p>

<pre><code>permute &lt;- function( population, n ){

  permutations &lt;- do.call( expand.grid, rep( list(population), n ) )

  permutations &lt;- apply( permutations, 1, paste, collapse = '' )

  return( permutations )

}
</code></pre>

<p>The basic idea is to iterate over the list of permutations and count the number of tuples that match the given permutation.  Since you want the results split out into a table, we should select a permutation of n-1 elements from the population and let the last position form the columns of the table.  Here's a function that takes a permutation of size n-1, a list of tuples, and the population the tuples were drawn from and produces a named vector of match counts:</p>

<pre><code>countFrequency &lt;- function(permutation,tuples,population){

  permutations &lt;- paste( permutation, population, sep = '' )

  # Inner lapply applies the equality operator `==` to each
  # permutation and returns a list of TRUE/FALSE vectors.
  # Outer lapply sums the number of TRUE values in each vector. 
  frequencies &lt;- lapply(lapply(permutations,`==`,tuples),sum)

  names( frequencies ) &lt;- as.character( population )

  return( unlist(frequencies) )

}
</code></pre>

<p>Finally, all three functions can be combined into a bigger function that takes a vector, splits it into n-tuples and returns a frequency table.  The final aggregation operation is done using <code>ldply()</code> from Hadley Wickham's <code>plyr</code> package as it does a nice job of preserving information such as which permutation corresponds to which row of output matches:</p>

<pre><code>permutationFrequency &lt;- function( vector, n, population = unique( vector ) ){

  # Split the vector into tuples.
  tuples &lt;- makeTuples( vector, n )

  # Coerce and compact the tuples to a vector of strings.
  tuples &lt;- do.call(cbind,tuples)
  tuples &lt;- apply( tuples, 1, paste, collapse = '' )

  # Generate permutations of n-1 elements from the population.
  # Turn into a named list for ldply() to work it's magic.
  permutations &lt;- permute( population, n-1 )
  names( permutations ) &lt;- permutations

  frequencies &lt;- ldply( permutations, countFrequency,
    tuples = tuples, population = population )

  return( frequencies )

}
</code></pre>

<p>And there you go:</p>

<pre><code>require( plyr )
permutationFrequency( s, 2 )
  .id 1 0
1   1 2 3
2   0 2 7

permutationFrequency( s, 3 )
  .id 1 0
1  11 1 1
2  01 1 1
3  10 0 3
4  00 2 4

permutationFrequency( s, 4 )
  .id 1 0
1 111 0 1
2 011 1 0
3 101 0 0
4 001 1 1
5 110 0 1
6 010 0 1
7 100 0 2
8 000 2 2

permutationFrequency( sample( -1:1, 10, replace = T ), 2 )
  .id 1 -1 0
1   1 1  2 0
2  -1 0  1 2
3   0 1  0 2
</code></pre>

<p>Apologies to my stochastic processes teacher, but functional programming problems in R were just more interesting than the Gambler's Ruin today...</p>
"
2284486,163053,2010-02-17T21:18:31Z,2284446,41,TRUE,"<p>This question is very closely related to: <a href=""https://stackoverflow.com/questions/1266279/how-to-organize-large-r-programs"">""How to organize large R programs?""</a></p>

<p>You should consider creating an R package.  You can use the <code>package.skeleton</code> function to start with given a set of R files.  I also strongly recommend using <code>roxygen</code> to document the package at the beginning, because it's much more difficult to do it after the fact.</p>

<p>Read <a href=""http://cran.r-project.org/doc/manuals/R-exts.html"" rel=""nofollow noreferrer""><strong>""Writing R Extensions""</strong></a>.  The online book ""Statistics with R"" has <a href=""http://zoonek2.free.fr/UNIX/48_R/02.html#7"" rel=""nofollow noreferrer"">a section on this subject</a>.  Also take a look at <a href=""http://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf"" rel=""nofollow noreferrer"">Creating R Packages: A Tutorial</a> by Friedrich Leisch.  Lastly, if you're in NY, come to the upcoming NY use-R group meeting on <a href=""http://www.meetup.com/nyhackr/calendar/12556884/"" rel=""nofollow noreferrer"">""Authoring R Packages: a gentle introduction with examples""</a>.</p>

<p>Just to rehash some suggestions about good practices:</p>

<ul>
<li>A package allows you to use <code>R CMD check</code> which is very helpful at catching bugs; separately you can look at using the <code>codetools</code> package.</li>
<li>A package also forces you to do a minimal amount of documentation, which leads to better practices in the long run.</li>
<li>You should also consider doing unit testing (e.g. with <a href=""http://cran.r-project.org/package=RUnit"" rel=""nofollow noreferrer"">RUnit</a>) if you want your code to be robust/maintainable.</li>
<li>You should consider using a style guide (e.g. <a href=""https://google.github.io/styleguide/Rguide.xml"" rel=""nofollow noreferrer"">Google Style Guide</a>).</li>
<li>Use a version control system from the beginning, and if you're going to make your code open source, then consider using github or r-forge.</li>
</ul>

<p><em>Edit:</em></p>

<p>Regarding how do make incremental changes without rebuilding and installing the full package: I find the easiest thing to do is to make changes in your relevant R file and then use the <code>source</code> command to load those changes.  Once you load your library into an R session, it will always be lower in the environment (and lower in priority) than the .GlobalEnv, so any changes that you source or load in directly will be used first (use the <code>search</code> command to see this).  That way you can have your package underlying and you are overwriting changes as you're testing them in the environment.</p>

<p>Alternatively, you can use an IDE like StatET or ESS.  They make loading individual lines or functions out of an R package very easy.  StatET is particularly well designed to handle managing packages in a directory-like structure.</p>
"
2284692,118402,2010-02-17T21:55:15Z,2281561,7,FALSE,"<p>It's not clear to me why you should care that it is bivariate.  The probabilities sum to one and the outcomes are discrete, so you are just sampling from a <a href=""http://en.wikipedia.org/wiki/Categorical_distribution"" rel=""noreferrer"">categorical distribution</a>.  The only difference is that you are indexing the observations using rows and columns rather than a single position.  This is just notation. </p>

<p>In R, you can therefore easily sample from your distribution by reshaping your data and sampling from a categorical distribution.  Sampling from a categorical can be done using <code>rmultinom</code> and using <code>which</code> to select the index, or, as Aniko suggests, using <code>sample</code> to sample the rows of the reshaped data.  Some bookkeeping can take care of your exact case.</p>

<p>Here's a solution:</p>

<pre><code>library(reshape)

# Reshape data to long format.
data &lt;- matrix(data = c(.25,.5,.1,.4), nrow=2, ncol=2)
pmatrix &lt;- melt(data)

# Sample categorical n times.
rcat &lt;- function(n, pmatrix) {
    rows &lt;- which(rmultinom(n,1,pmatrix$value)==1, arr.ind=TRUE)[,'row']
    indices &lt;- pmatrix[rows, c('X1','X2')]
    colnames(indices) &lt;- c('i','j')
    rownames(indices) &lt;- seq(1,nrow(indices))
    return(indices)
}

rcat(3,pmatrix)
</code></pre>

<p>This returns 3 random draws from your matrix, reporting the <code>i</code> and <code>j</code> of the rows and columns:</p>

<pre><code>  i j
1 1 1
2 2 2
3 2 2
</code></pre>
"
2285126,168139,2010-02-17T23:05:40Z,2280724,0,FALSE,"<p>Perhaps I am just a sheep who blindly follows but I stick to the instructions in the Tinn-R help file.
C:/Tinn-R/doc/english/user_guide/user_guide.html#basic_configuration_installconfigure
The easiest way to access the user guide is from the Tinn-R menu.
Help>Main>User guide>HTML</p>

<p>In the user guide it clearly states that one should uninstall the prior Tinn-R and then install the new version.</p>
"
2286227,143305,2010-02-18T04:09:13Z,2286085,4,FALSE,"<p>Package <a href=""http://cran.r-project.org/package=multcomp"" rel=""nofollow noreferrer"">multcomp</a> has e.g. <code>plot.cld()</code> -- you could try</p>

<pre><code>library(multcomp)
example(plot.cld)
</code></pre>

<p>Also, a quick ""multiple comparison plot"" search at <a href=""http://rseek.org"" rel=""nofollow noreferrer"">http://rseek.org</a> reveals a few more packages and Task Views.</p>
"
2287177,275887,2010-02-18T08:24:39Z,2286831,5,FALSE,"<p>I'm using git for version control. My typical directory structure (e.g. for articles) is as follows.</p>

<pre><code>.
..
.git
README
README.html
ana
dat
doc
org
</code></pre>

<p>Most directories/files (ana, doc, org) are under version control. Of course, large binary datasets are excluded from version control (via .gitignore). README is an Emacs org-mode file.</p>
"
2287210,165787,2010-02-18T08:32:29Z,2286831,1,FALSE,"<p>I use git, myself. Local repositories, stored in the same directory as the R project. That way, if I eliminate a project down the road, the repository goes with it; I can work offline; and I don't have IRB, FERPA, HIPPA issues to deal with.</p>

<p>If I need added backup assurance, I can git to a remote (secured!) repository.</p>

<p>-Wil</p>
"
2287275,165787,2010-02-18T08:44:27Z,2266408,2,FALSE,"<p>The first part of Doug's response offers the simplest immediate solution... add</p>

<pre><code>source('/Users/briandk/.Rprofile')
</code></pre>

<p>to the head of any .r files you want those functions in... with that one line of code, you get your utility functions. Of course, that only helps if you're running the whole TM file.</p>

<p>Ideally, the bundle will be updated... perhaps to support a shell variable via TM's preferences???</p>

<pre><code>TM_RPROFILE 
</code></pre>

<p>which could be set to the path to your .Rprofile file. </p>

<p>I just hacked this into tmR.rb with just 2 lines of code. To implement this, go to ~/Library/Application Support/TextMate/Pristine Copy/Bundles/ and Show the Contents of R.tmbundle</p>

<p>In there, you'll find support/tmR.rb</p>

<p>in my version, near line 112, you should change</p>

<pre><code>tmpDir = File.join(ENV['TMP'] || ""/tmp"", ""TM_R"")
recursive_delete(tmpDir) if File.exists?(tmpDir) # remove the temp dir if it's already there
Dir::mkdir(tmpDir)

# Mechanism for dynamic reading
# stdin, stdout, stderr = popen3(""R"", ""--vanilla"", ""--no-readline"", ""--slave"", ""--encoding=UTF-8"")
stdin, stdout, stderr, pid = my_popen3(""R --vanilla --slave --encoding=UTF-8 2&gt;&amp;1"")
# init the R slave
stdin.puts(%{options(device=""pdf"")})
stdin.puts(%{options(repos=""#{cran}"")})
</code></pre>

<p>to</p>

<pre><code>tmpDir = File.join(ENV['TMP'] || ""/tmp"", ""TM_R"")
recursive_delete(tmpDir) if File.exists?(tmpDir) # remove the temp dir if it's already there
Dir::mkdir(tmpDir)

rprofile = (ENV['TM_RPROFILE'] == nil) ? """" : ""source('"" + ENV['TM_RPROFILE'] + ""')""

# Mechanism for dynamic reading
# stdin, stdout, stderr = popen3(""R"", ""--vanilla"", ""--no-readline"", ""--slave"", ""--encoding=UTF-8"")
stdin, stdout, stderr, pid = my_popen3(""R --vanilla --slave --encoding=UTF-8 2&gt;&amp;1"")
# init the R slave
stdin.puts(""#{rprofile}"")
stdin.puts(%{options(device=""pdf"")})
stdin.puts(%{options(repos=""#{cran}"")})
</code></pre>

<p>Just added 2 lines there... the one that begins ""rprofile ="" and the one that includes ""#{rprofile}""</p>

<p>-Wil</p>
"
2287427,212593,2010-02-18T09:13:15Z,2278951,1,FALSE,"<p>One approach is to create a data frame of the subsequences and then use the table function:</p>

<pre><code>s&lt;-c(1,0,0,0,1,0,0,0,0,0,1,1,1,0,0)
n&lt;-length(s)
k&lt;-3
subseqs&lt;-t(sapply(1:(n-k+1),function(i){s[i:(i+k-1)]}))
colnames(subseqs)&lt;-paste('Y',1:k,sep="""")
subseqs&lt;-data.frame(subseqs)
table(subseqs)
</code></pre>

<p>This produces</p>

<pre><code>, , Y3 = 0

   Y2
Y1  0 1
  0 4 1
  1 3 1

, , Y3 = 1

   Y2
Y1  0 1
  0 2 1
  1 0 1
</code></pre>

<p>Use ftable instead of table or on the output of table for a display similar to the one in your question:</p>

<pre><code>ftable(subseqs)
          Y3 0 1
    Y1 Y2       
    0  0     4 2
       1     1 1
    1  0     3 0
       1     1 1
</code></pre>
"
2287947,212593,2010-02-18T10:49:03Z,2287616,26,FALSE,"<p>If you are producing the entire output yourself, you can use sprintf</p>

<pre><code>&gt; sprintf(""%.10f"",0.25)
[1] ""0.2500000000""
</code></pre>

<p>I don't know of any way of forcing R's higher level functions to print an exact number of digits.</p>

<p>Displaying 100 digits does not make sense if you are printing R's usual numbers, since the best accuracy you can get using 64-bit doubles is around 16 decimal digits (look at .Machine$double.eps on your system). The remaining digits will just be junk.</p>
"
2288013,134830,2010-02-18T10:58:10Z,2287616,37,TRUE,"<p>The reason it is only a suggestion is that you could quite easily write a print function that ignored the options value.  The built-in printing and formatting functions do use the <code>options</code> value as a default.</p>

<p>As to the second question, since R uses finite precision arithmetic, your answers aren't accurate beyond 15 or 16 decimal places, so in general, more aren't required.  The <a href=""http://cran.r-project.org/web/packages/gmp/index.html"" rel=""noreferrer"">gmp</a> and <a href=""http://cran.r-project.org/web/packages/rcdd/index.html"" rel=""noreferrer"">rcdd</a> packages deal with multiple precision arithmetic (via an interace to the gmp library), but this is mostly related to big integers rather than more decimal places for your doubles.</p>

<p><a href=""http://www.wolfram.com/products/mathematica/index.html"" rel=""noreferrer"">Mathematica</a> or <a href=""http://www.maplesoft.com/Products/Maple/"" rel=""noreferrer"">Maple</a> will allow you to give as many decimal places as your heart desires.</p>

<p>EDIT:<br>
It might be useful to think about the difference between decimal places and significant figures.  If you are doing statistical tests that rely on differences beyond the 15th significant figure, then your analysis is almost certainly junk.</p>

<p>On the other hand, if you are just dealing with very small numbers, that is less of a problem, since R can handle number as small as <code>.Machine$double.xmin</code> (usually 2e-308).</p>

<p>Compare these two analyses.</p>

<pre><code>x1 &lt;- rnorm(50, 1, 1e-15)
y1 &lt;- rnorm(50, 1 + 1e-15, 1e-15)
t.test(x1, y1)  #Should throw an error

x2 &lt;- rnorm(50, 0, 1e-15)
y2 &lt;- rnorm(50, 1e-15, 1e-15)
t.test(x2, y2)  #ok
</code></pre>

<p>In the first case, differences between numbers only occur after many significant figures, so the data are ""nearly constant"".  In the second case, Although the size of the differences between numbers are the same, compared to the magnitude of the numbers themselves they are large.</p>

<hr>

<p>As mentioned by e3bo, you can use multiple-precision floating point numbers using the <code>Rmpfr</code> package.</p>

<pre><code>mpfr(""3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825"")
</code></pre>

<p>These are slower and more memory intensive to use than regular (double precision) <code>numeric</code> vectors, but can be useful if you have a poorly conditioned problem or unstable algorithm.</p>
"
2288510,NA,2010-02-18T12:22:30Z,2288485,56,FALSE,"<p>if <code>x</code> is the column name of dataframe <code>dat</code>, and <code>x</code> is of type factor, use:</p>

<pre><code>as.numeric(as.character(dat$x))
</code></pre>
"
2289369,143305,2010-02-18T14:41:14Z,2288485,14,FALSE,"<p>Tim is correct, and Shane has an omission. Here are additional examples:</p>

<pre><code>R&gt; df &lt;- data.frame(a = as.character(10:15))
R&gt; df &lt;- data.frame(df, num = as.numeric(df$a), 
                        numchr = as.numeric(as.character(df$a)))
R&gt; df
   a num numchr
1 10   1     10
2 11   2     11
3 12   3     12
4 13   4     13
5 14   5     14
6 15   6     15
R&gt; summary(df)
  a          num           numchr    
 10:1   Min.   :1.00   Min.   :10.0  
 11:1   1st Qu.:2.25   1st Qu.:11.2  
 12:1   Median :3.50   Median :12.5  
 13:1   Mean   :3.50   Mean   :12.5  
 14:1   3rd Qu.:4.75   3rd Qu.:13.8  
 15:1   Max.   :6.00   Max.   :15.0  
R&gt; 
</code></pre>

<p>Our <code>data.frame</code> now has a summary of the factor column (counts) and numeric summaries of the <code>as.numeric()</code> --- which is <em>wrong</em> as it got the numeric factor levels --- and the (correct) summary of the <code>as.numeric(as.character())</code>.</p>
"
2290107,269831,2010-02-18T16:15:19Z,2288485,101,FALSE,"<p>Something that has helped me: if you have ranges of variables to convert (or just more then one), you can use <code>sapply</code>.</p>

<p>A bit nonsensical but just for example:</p>

<pre><code>data(cars)
cars[, 1:2] &lt;- sapply(cars[, 1:2], as.factor)
</code></pre>

<p>Say columns 3, 6-15 and 37 of you dataframe need to be converted to numeric one could:</p>

<pre><code>dat[, c(3,6:15,37)] &lt;- sapply(dat[, c(3,6:15,37)], as.numeric)
</code></pre>
"
2290194,37751,2010-02-18T16:24:20Z,2286831,18,TRUE,"<p>My workflow is not that different than Bernd's. I usually have a main directory where I put all my *.R code files. As soon as I have more than about 5 lines in a text file I start version control, in my case git. Most of my work is not in a team context meaning that I'm the only one changing my code. As soon as I make a substantive change (yes that is subjective) I do a check in. I agree with Dirk that this process is orthogonal to the workflow. </p>

<p>I use Eclipse + StatET and while there is a plugin for git in Eclipse (<a href=""http://www.eclipse.org/egit/"" rel=""nofollow noreferrer"">EGit</a> and probably others), I don't use it. I'm in Windows and just use git-gui for Windows. Here's <a href=""https://stackoverflow.com/questions/157476/what-guis-exist-for-git-on-windows"">some more options</a>. </p>

<p>There's a lot of room for personal idiosyncrasies in version control, but I recommend this one tip as a best practice: if you report results to others (i.e. journal article, your team, management in your firm) <strong>ALWAYS</strong> do a version control check in right before running results that go out to others. Invariably, 3 months later someone will look at your results and ask some question about the code which you can't answer unless you know the EXACT state of the code when you produced those results. So make it a practice and put in the comments ""this is the version of the code that I used for 4th quarter financials"" or whatever your use case is. </p>

<p>Also keep in mind that version control is no replacement for a good backup plan. My motto is: ""3 copies. 2 geographies. 1 mind at peace."" </p>

<p><strong>EDIT (Feb 24, 2010):</strong> Joel Spolsky, one of the founders of Stack Overflow, just released a <a href=""http://hginit.com/"" rel=""nofollow noreferrer"">highly visual and very cool intro to Mercurial</a>. This tutorial alone may be reason to adopt Mercurial if you have not already chosen a revision control system. I think when it comes to Git vs. Mercurial the most important advice is to chose one and use it. Maybe use what your friends/coworkers use or use the one with the best tutorial. But just use one already! ;) </p>
"
2291593,245603,2010-02-18T19:32:23Z,2253179,8,TRUE,"<p>See below for a solution. The main idea is the following: imagine the points having an invisible line under them, and the lines having invisible points. So each ""series"" gets color and shape and linetype attributes, and at the end we will manually set them to invisible values (0 for lines, NA for points) as necessary. ggplot2 will merge the legends for the three attributes automatically.</p>

<pre><code># make plot 
p &lt;- qplot(data = d, x=xval, y=yval, colour=""Measured"", shape=""Measured"",
          linetype=""Measured"",  xlab = x_label,   ylab = y_label, xlim = x_range,
          geom=""point"") 

#add lines for functions 
for(j in 1:length(funcs)){ 
   p &lt;- p + stat_function(aes(colour=""All"", shape=""All"", linetype=""All""), 
                          fun = funcs[[j]],  alpha=I(1/5), geom=""line"")  
} 

# make one function special 
p &lt;- p + stat_function(fun = funcs[[1]], aes(colour=""Fitted"", shape=""Fitted"",
                       linetype=""Fitted""), size = 1, geom=""line"")

# modify look 
 p &lt;- p +  scale_colour_manual("""", values=c(""green"", ""blue"", ""red"")) + 
           scale_shape_manual("""", values=c(19,NA,NA)) + 
           scale_linetype_manual("""", values=c(0,1,1)) 

print(p) 
</code></pre>
"
2292913,163053,2010-02-18T23:04:46Z,2286831,13,FALSE,"<p>Rather than focusing on revision control in particular, it sounds like you're really asking a bigger question about how statistical analysis compares to software development.  That's an interesting question.  Here are some thoughts:</p>

<p>Data analysis <em>can be</em> more like an art than a science.  In a sense, you might want to look for inspiration to the process that an author would follow when writing a book more than the process that a software developer would follow.  On the other hand, I have yet to encounter a software project that followed a straight line.  And even at a theoretical level, there is a great amount of variance in <a href=""http://en.wikipedia.org/wiki/Software_development_methodologies"" rel=""noreferrer"">software development methodologies</a>.  Of these, given that a statistical analysis can be a discovery process (i.e. one that can't be fully planned up front), it would make sense to follow something like an <a href=""http://en.wikipedia.org/wiki/Agile_software_development"" rel=""noreferrer""><strong>agile methodology</strong></a> (much more so that something like the waterfall methodology).  In other words, you need to plan for your analysis to be iterative and self-reflective.</p>

<p>That said, I think the notion that statistical analysis is purely exploratory with no goal in mind is potentially problematic.  That can lead to the point where you are 5 steps past your eureka moment, and have no way to get back to it.  There is always a goal of some sort, even if the goal itself is changing.  Moreover, if there is no goal, how will you know when you've reached the end?</p>

<p>One approach is to start off with one R file as you start a project (or a set of files like in the Josh and Bernd examples), and progressively add to it (so that it grows in size) as you make discoveries.  This is also especially true when you have data that needs to be kept as part of the analysis.  This file should be version controlled regularly to ensure that you can always step backwards if you make mistakes (allowing to incremental gains).  Version control systems are immensely helpful in development not just because they ensure that you don't lose things, but also because they provide you with a timeline.  And tag your check-ins so that you know what's in them at a glance, and note major milestones.  I love JD's point about checking in before submitting something.</p>

<p>Once you have reached your final set of conclusions, it's often best to create a final version of your file that summarizes your analysis from start to end.  You might even consider putting this into a Sweave document so that it's fully self-contained and literate.</p>

<p>You should also give serious thought to what others around you are doing.  Nothing makes me cringe more than to see people reinventing the wheel, especially when it means extra work for the group as a whole to integrate with.</p>

<p>Your decisions about which version control system to use, which IDE, etc. (implementation issues) are ultimately extremely low on the totem pole in relation to the overall project management.  Just use <em>any</em> one of them properly and you're already 95% of the way there, and the differences between them are small in comparison to the alternative of using nothing.  </p>

<p>Lastly, if you are using something like github, google code, or R-forge, you will note something that they all have in common: a suite of tools beyond just a version control system.  Namely, you should consider using things like the issue tracking system and the wiki to document progress and log open issues/tasks.  The more organized you are with your analysis, the greater the likelihood of success.</p>
"
2293313,457898,2010-02-19T00:31:30Z,2288485,199,TRUE,"<p>Since (still) nobody got check-mark, I assume that you have some practical issue in mind, mostly because you haven't specified what type of vector you want to convert to <code>numeric</code>. I suggest that you should apply <code>transform</code> function in order to complete your task.</p>

<p>Now I'm about to demonstrate certain ""conversion anomaly"":</p>

<pre><code># create dummy data.frame
d &lt;- data.frame(char = letters[1:5], 
                fake_char = as.character(1:5), 
                fac = factor(1:5), 
                char_fac = factor(letters[1:5]), 
                num = 1:5, stringsAsFactors = FALSE)
</code></pre>

<p>Let us have a glance at <code>data.frame</code></p>

<pre><code>&gt; d
  char fake_char fac char_fac num
1    a         1   1        a   1
2    b         2   2        b   2
3    c         3   3        c   3
4    d         4   4        d   4
5    e         5   5        e   5
</code></pre>

<p>and let us run:</p>

<pre><code>&gt; sapply(d, mode)
       char   fake_char         fac    char_fac         num 
""character"" ""character""   ""numeric""   ""numeric""   ""numeric"" 
&gt; sapply(d, class)
       char   fake_char         fac    char_fac         num 
""character"" ""character""    ""factor""    ""factor""   ""integer"" 
</code></pre>

<p>Now you probably ask yourself <i>""Where's an anomaly?""</i> Well, I've bumped into quite peculiar things in R, and this is not <b>the</b> most confounding thing, but it can confuse you, especially if you read this before rolling into bed.</p>

<p>Here goes: first two columns are <code>character</code>. I've deliberately called 2<sup>nd</sup> one <code>fake_char</code>. Spot the similarity of this <code>character</code> variable with one that Dirk created in his reply. It's actually a <code>numerical</code> vector converted to <code>character</code>. 3<sup>rd</sup> and 4<sup>th</sup> column are <code>factor</code>, and the last one is ""purely"" <code>numeric</code>.</p>

<p>If you utilize <code>transform</code> function, you can convert the <code>fake_char</code> into <code>numeric</code>, but not the <code>char</code> variable itself.</p>

<pre><code>&gt; transform(d, char = as.numeric(char))
  char fake_char fac char_fac num
1   NA         1   1        a   1
2   NA         2   2        b   2
3   NA         3   3        c   3
4   NA         4   4        d   4
5   NA         5   5        e   5
Warning message:
In eval(expr, envir, enclos) : NAs introduced by coercion
</code></pre>

<p>but if you do same thing on <code>fake_char</code> and <code>char_fac</code>, you'll be lucky, and get away with no NA's:</p>

<pre><code>&gt; transform(d, fake_char = as.numeric(fake_char), 
               char_fac = as.numeric(char_fac))

  char fake_char fac char_fac num
1    a         1   1        1   1
2    b         2   2        2   2
3    c         3   3        3   3
4    d         4   4        4   4
5    e         5   5        5   5
</code></pre>

<p>If you save transformed <code>data.frame</code> and check for <code>mode</code> and <code>class</code>, you'll get:</p>

<pre><code>&gt; D &lt;- transform(d, fake_char = as.numeric(fake_char), 
                    char_fac = as.numeric(char_fac))

&gt; sapply(D, mode)
       char   fake_char         fac    char_fac         num 
""character""   ""numeric""   ""numeric""   ""numeric""   ""numeric"" 
&gt; sapply(D, class)
       char   fake_char         fac    char_fac         num 
""character""   ""numeric""    ""factor""   ""numeric""   ""integer""
</code></pre>

<p>So, the conclusion is: <b><i>Yes, you can convert <code>character</code> vector into a <code>numeric</code> one, but only if it's elements are ""convertible"" to <code>numeric</code>.</b></i> If there's just one <code>character</code> element in vector, you'll get error when trying to convert that vector to <code>numerical</code> one.</p>

<p>And just to prove my point:</p>

<pre><code>&gt; err &lt;- c(1, ""b"", 3, 4, ""e"")
&gt; mode(err)
[1] ""character""
&gt; class(err)
[1] ""character""
&gt; char &lt;- as.numeric(err)
Warning message:
NAs introduced by coercion 
&gt; char
[1]  1 NA  3  4 NA
</code></pre>

<p>And now, just for fun (or practice), try to guess the output of these commands:</p>

<pre><code>&gt; fac &lt;- as.factor(err)
&gt; fac
???
&gt; num &lt;- as.numeric(fac)
&gt; num
???
</code></pre>

<p>Kind regards to Patrick Burns! =)</p>
"
2293871,135870,2010-02-19T03:24:32Z,2286831,3,FALSE,"<p>After reading your update, it seems like you are viewing the choice and use of a version control systems as dictating the <strong>structure</strong> of your repository and <strong>workflow</strong>.  In my opinion, version control is more akin to an <strong>insurance policy</strong> as it provides the following services:</p>

<ol>
<li><p>Backups.  If something gets accidentally deleted or the whims of fate fry your hard drive your work can be recovered from the repository.  With distributed version control nothing short of the apocalypse can cause you to loose work-- in which case you'll probably have other things to worry about anyway.</p></li>
<li><p>The mother of all undo buttons.  Was the analysis looking better an hour ago? a day ago? a week ago?  Version control provides a rewind button that allows you to travel back in time.</p></li>
</ol>

<p>If you are the only person working on a project, the above two points probably outline how version control systems will affect the way you work.</p>

<p>The other side of version control systems is that they foster collaborative efforts by allowing people to experiment on an isolated copy or ""branch"" of the project material and then ""merge"" any positive changes back into the master copy.  It also provides a means for project members to keep tabs on who's changes affected which lines of which files.</p>

<p>As an example, I keep all of my college coursework under version control in a <em>Subversion</em> repository.  I am the only one who works on this repository so I never branch or merge the source-- I just commit and occasionally rewind.  The ability to rewind my work reduces the risks of trying some sort of new analysis-- I just do it.  If two hours later it looks like it wasn't such a good idea, I just revert the project files and try something different.</p>

<p>In contrast, most all of my non-coursework package/program development is hosted under <em>git</em>.  In this sort of a setting I frequently want to experiment on a branch while having a stable master copy available.  I use <em>git</em> rather than <em>Subversion</em> in these situations because <em>git</em> makes branching and merging an effortless task.</p>

<p>The important point is that in both of these cases the <strong>structure</strong> of my repository and the <strong>workflow</strong> I use are not decided by my version control system-- they are decided by me.  The only impact the version control has on my workflow is that it frees me from worrying about trying something new, deciding I don't like it, and then having to undo all the changes to get back to where I started.  Because I use version control, I can follow Yogi Berra's advice:</p>

<blockquote>
  <p><strong>When you come to a fork in the road, take it.</strong></p>
</blockquote>

<p>Because I can always go back and take it the other way.</p>
"
2295579,212593,2010-02-19T10:30:11Z,2287374,6,TRUE,"<p>As far as I understand, this happens not because of the nested ... but because of the first ... in barchart. So even this does not work:</p>

<pre><code>f.barchart &lt;- function(...) {
    barchart(...)
}

x &lt;- data.frame(a = c(1,1,2,2), b = c(1,2,3,4), d = c(1,2,2,1))
print(f.barchart(a ~ b, data = x, groups = d))
</code></pre>

<p>I think this is because ... is a pairlist while barchart is expecting individual arguments. We need to unpack the pairlist, while making sure that we do not evaluate it too soon. The following is my solution:</p>

<pre><code>f.barchart &lt;- function(...) {
  cl&lt;-match.call()
  cl$panel=function(x, y, ...) {
            panel.barchart(x, y, ...)
          }
  cl[[1]]=barchart
  eval(cl)
}
</code></pre>

<p>We capture the call to f.barchart using match.call which expands the dots, add the panel argument to the call, set the function to be called to barchart and then evaluate the call. Just as we added the panel argument, we could delete arguments which are used by f.barchart but which do not need to be passed to barchart.</p>
"
2296527,66549,2010-02-19T13:24:38Z,2296451,6,FALSE,"<p>Yes same methods will work (matrix/data.frame)--see below:</p>

<pre><code>A = matrix(1:12, nrow=4)
colnames(A) = c(""col1"", ""col2"", ""col3"")
row.names(A) = c(""row1"", ""row2"", ""row3"", ""row4"")

dfA = as.data.frame(A)
row.names(dfA) = c(""r1"", ""r2"", ""r3"", ""r4"")
colnames(A) = c(""C1"", ""C2"", ""C3"")
</code></pre>

<p>And to save time, you can do this:</p>

<pre><code>x = rep(""col"", dim(M)[2])
y = 1:dim(M)[2]
colnames(M) = paste(x, y, sep="""")
</code></pre>
"
2296889,168747,2010-02-19T14:16:51Z,2296451,11,TRUE,"<p>From comment to answer:</p>

<pre><code>row.names(x) &lt;- paste(""SS"", 1:nrow(x), sep="""")
colnames(x) &lt;-  paste(""M"" , 1:ncol(x), sep="""")
</code></pre>

<p>As @doug wrote, it works for matrices and data frames.</p>
"
2297003,163053,2010-02-19T14:34:17Z,2296694,15,FALSE,"<p>As mentioned in the other answer, there is now a pure Java implementation of R: <a href=""http://code.google.com/p/renjin"" rel=""nofollow noreferrer"">http://code.google.com/p/renjin</a>.  Exciting development, and looking forward to see how it evolves.</p>

<p><em>Former answer below:</em></p>

<p>Bill is correct: there is no pure implementation of the R langauge (or the S langauge) on either the JVM or CLR, so the best that you can do is to use some kind of interface through C.</p>

<p><a href=""http://www.rforge.net/JRI/"" rel=""nofollow noreferrer""><strong>JRI</strong></a> allows you to call R from any Java program, so any language on the JVM should be able to call the JRI jars.  See my example of how to use this in this question: <a href=""https://stackoverflow.com/questions/2180235/r-from-within-java"">R from within Java</a>.  As an example, there is a clojure library that is currently being developed around JRI by <a href=""http://joelboehland.com/"" rel=""nofollow noreferrer"">Joel Boehland</a>: <a href=""http://github.com/jolby/rincanter"" rel=""nofollow noreferrer""><strong>Rincanter</strong></a> (<a href=""http://data-sorcery.org/category/rincanter/"" rel=""nofollow noreferrer"">read about it here</a>).</p>
"
2297157,245603,2010-02-19T14:54:01Z,2294163,4,TRUE,"<p>if <code>a</code> is the anova table object, then <code>attr(a,""heading"")</code> does contain the information you are looking for, but I couldn't figure out a nice way of extracting it. So I looked up the code of <code>anova.glm</code>, which directed me to the code of <code>anova.lmlist</code> to figure out how they put that information into the heading. This inspired to following solution:</p>

<pre><code># fake data
x &lt;- 1:10
y &lt;- x+ rnorm(10)

# two models
m1 &lt;- glm(y~x)
m2 &lt;- glm(y~x+I(x^2))
a &lt;- anova(m1, m2)  # anova object to be printed

# get model formulas
flas &lt;- sapply(list(m1,m2), function(x)paste(deparse(x$formula)))
rownames(a) &lt;- flas  # add formulas as rownames

# convert to latex
xtable(a)
</code></pre>

<p><strong>Edit to cater for long formulas:</strong><br>
If you have long formulas, two changes are needed: first we have to make sure that <code>deparse</code> does not break it into lines, and then we need to make latex to wrap the formula in the table. The first can be achieved by using the <code>cutoff.width</code> argument of deparse, and the second by using a <code>p{width}</code> column type in latex. For example:</p>

<pre><code># add long formula
m2$formula &lt;- freq ~ sex + attend + birth + politics + sex:attend + sex:birth + 
              sex:politics + attend:birth + attend:politics + birth:politics + 
              sex:attend:birth + sex:attend:politics + sex:birth:politics +
              attend:birth:politics
a &lt;- anova(m1, m2) 

# use a large width
flas &lt;- sapply(list(m1,m2), 
               function(x)paste(deparse(x$formula, cutoff.width=500)))
rownames(a) &lt;- flas  # add formulas as rownames

# convert to latex with first column wrapped in a 5cm wide parbox
xtable(a, align=""p{5cm}rrrr"")
</code></pre>

<p>The result is not overly pretty, but your formula is not pretty either. In this particular case I would use <code>(sex + attend + birth + politics)^3</code> - gets the point across and is much shorter. </p>
"
2297838,37751,2010-02-19T16:25:16Z,2296325,0,FALSE,"<p>I have not tried this with a database, but I had some challenges recently when serializing to/from a text file. Here's a <a href=""https://stackoverflow.com/questions/2258511/r-serialize-objects-to-text-file-and-back-again"">question I asked that might be related</a>. Have you tried using the ascii=T switch with serialize? Then try it both with and without rawToChar. </p>

<p>I don't have an easy environment to test your code, but I am interested in what you come up with. I'm working on some code where I will eventually be serializing objects and putting them in a DB. I'm just not to that point yet. </p>
"
2298057,457898,2010-02-19T16:50:56Z,2294163,1,FALSE,"<p>I reckon that you want to get LaTeX table, but you can easily get HTML table with model formula.</p>

<pre><code># if we presuppose that &lt;b&gt;a&lt;/b&gt; is object from @Aniko's reply
&gt; class(a)
[1] ""anova""      ""data.frame""
# after doing a bit of that sapply magic you get
&gt; a
Analysis of Deviance Table

Model 1: y ~ x
Model 2: y ~ x + I(x^2)
               Resid. Df Resid. Dev Df Deviance
y ~ x                  8     15.503            
y ~ x + I(x^2)         7     12.060  1   3.4428
</code></pre>

<p>You can do something like this:</p>

<pre><code># load xtable library
library(xtable)
# sink output to html file
sink(""~/anova_specs.html"")  # suppose you're running R on Linux ""~/""
print(xtable(a), type = ""html"")
sink()
</code></pre>

<p>It's not as pretty as LaTeX table, but it has model formula...</p>
"
2300678,143305,2010-02-20T01:32:45Z,2300629,7,FALSE,"<p>What you ask for is not always proper praxis, but you can force it via <code>par(new=TRUE)</code>:</p>

<pre><code>x &lt;- 1:20
plot(x, log(x), type='l') 
par(new=TRUE)              # key: ask for new plot without erasing old
plot(x, sqrt(x), type='l', col='red', xlab="""", yaxt=""n"")
axis(4)
</code></pre>

<p>The x-axsis is plotted twice, but as you have the same x-coordinates that is not problem. The second y-axis is suppressed and plotted on the right.  But the labels show you that you are now mixing to different levels. </p>
"
2300915,211450,2010-02-20T03:25:09Z,2300767,2,TRUE,"<p>I've found workaround: add <code>colClasses</code> description</p>

<pre><code>data_raw &lt;- read.table(""data"",
                     colClasses=c(""character"",""character"",""character""),
                     sep=""\t"",
                     quote=""\"""")  
</code></pre>

<p>How to convert character to vector:</p>

<p>It's easier to use separators in data, because finally you'll get character and to convert it to e.g. vector you should do smth like:    </p>

<pre><code>m &lt;- as.integer(unlist(strsplit(""0,1,1,1,0"",split="","")))
m
[1] 0 1 1 1 0
</code></pre>

<p>or</p>

<pre><code>m&lt;-as.integer(scan(textConnection(""0,1,1,1,0""),sep="",""))
</code></pre>
"
2300956,16363,2010-02-20T03:46:31Z,2296325,2,TRUE,"<p>Perhaps if you collapse the pngfilecontents vector into a single string.  Something like:</p>

<pre><code>update.query &lt;- ""update df_DemandPatternMaster set ""
update.query &lt;- paste( update.query, "" pngFile = '"", paste(pngfilecontents, collapse="""") , ""' where DemandPatternID = "", sep="""")
</code></pre>
"
2301569,158065,2010-02-20T08:20:37Z,2300629,3,TRUE,"<p>R doesn't seem to be rejecting your axes.  What error are you getting?  Your command will put ticks way off the graph (since it uses the first axis to position them).  What I think you want is the following:</p>

<pre><code>&gt; plot(x = dataset$x, y = dataset$y)
&gt; axis(4, at = axTicks(2), label = axTicks(2) * 10)
</code></pre>
"
2307454,163053,2010-02-21T20:59:05Z,2307443,12,TRUE,"<p>I would make them equal length then add them:</p>

<pre><code>&gt; length(x) &lt;- length(y)
&gt; x
[1]  1  2 NA
&gt; x + y
[1]  4  6 NA
&gt; x[is.na(x)] &lt;- 0
&gt; x + y
[1] 4 6 5
</code></pre>

<p>Or, as a function:</p>

<pre><code>add.uneven &lt;- function(x, y) {
    l &lt;- max(length(x), length(y))
    length(x) &lt;- l
    length(y) &lt;- l
    x[is.na(x)] &lt;- 0
    y[is.na(y)] &lt;- 0
    x + y
}

&gt; add.uneven(x, y)
[1] 4 6 5
</code></pre>

<p>Given that you're just adding two vectors, it may be more intuitive to work with it like this:</p>

<pre><code>&gt; `%au%` &lt;- add.uneven
&gt; x %au% y
[1] 4 6 5
</code></pre>

<p>Here's another solution using rep:</p>

<pre><code>x &lt;- c(x, rep(0, length(y)-length(x)))
x + y
</code></pre>
"
2307960,143305,2010-02-21T23:42:35Z,2307925,8,TRUE,"<p>Not sure I understand but is this use of <code>order()</code> what you want:</p>

<pre><code>R&gt; foo &lt;- c(1,3,2, 5,4)
R&gt; bar &lt;- c(2,6,4,10,8)
R&gt; fooind &lt;- order(foo)   # index of ordered 
R&gt; foo[fooind]
[1] 1 2 3 4 5
R&gt; bar[fooind]
[1]  2  4  6  8 10
R&gt; 
</code></pre>
"
2309116,266609,2010-02-22T06:05:32Z,2308991,0,FALSE,"<p>This page explains it for you
<a href=""http://www.statmethods.net/advgraphs/trellis.html"" rel=""nofollow noreferrer"">http://www.statmethods.net/advgraphs/trellis.html</a></p>

<p>You basically want to alter the equation for the graphs.
They should be more like </p>

<p>csalary ~ bsalary|gender</p>

<p>should break the graphs apart based on different values of gender. There is a bunch of control language for continuous conditional variables.</p>
"
2309143,160314,2010-02-22T06:12:15Z,2308991,2,TRUE,"<p>you can use the cut() function to slice your data into ordinal categories. Then ggplot2's qplot function can then very easily create your desired plots.</p>

<pre><code>library(ggplot2)

#fake data
csalary &lt;- rnorm(100,,100)
bsalary &lt;- csalary +rnorm(100,,10)

#Regular Scatter Plot
qplot(bsalary,csalary)

#Stacked dot plot
qplot(cut(bsalary,10),csalary)

#box-plot
qplot(cut(bsalary,10),csalary,geom=""boxplot"")
</code></pre>
"
2309245,212593,2010-02-22T06:42:45Z,2308991,2,FALSE,"<p>Do you really want to do that? Turning a continuous variable into an ordinal one throws away information since different values of the X variable end up in the same bin. I think your boxplot graphic conveys much less information than your scatterplot. </p>

<p>If you are dissatisfied with the scatterplot because of points overlapping, one way to preserve information would be to add a smooth curve that captures the trend. Look at the documentation for <code>lowess</code> for an example. </p>

<p>In your graph the three observations with salaries higher than $20,000 are pushing the remaining observations into a corner. Dropping those and replotting would give a better graph.</p>

<p>Another approach for skewed data like yours is to plot the logarithms of the variables instead of the variables themselves.</p>
"
2309932,170792,2010-02-22T09:31:30Z,2309826,1,FALSE,"<p>Here is something that works for me. However I' m not sure if this is the optimum solution</p>

<pre><code>a &lt;- function(variable) {
print(deparse(substitute(variable)))
my_command &lt;- paste('internala(',substitute(variable),')',sep = '')
eval(parse(text = my_command))
}

internala &lt;- function(variableXX) {
namex=deparse(substitute(variableXX))
print(namex)
}
</code></pre>
"
2309953,168747,2010-02-22T09:34:35Z,2309826,3,TRUE,"<p>You could change <code>a</code> function to substitute argument of an <code>internala</code> function and <code>eval</code> it:</p>

<pre><code>a &lt;- function(variable) {
    print(deparse(substitute(variable)))
    eval(substitute(internala(variable))) # this is only change
}

internala &lt;- function(variableXX) {
    namex=deparse(substitute(variableXX))
    print(namex)
}

a(whatever)
</code></pre>

<p>As hadley suggest its better to directly pass names. I usually do something like that:</p>

<pre><code>a &lt;- function(variable, vname=deparse(substitute(variable))) {
    print(vname)
    internala(variable, vname)
}

internala &lt;- function(variableXX, namex=deparse(substitute(variableXX))) {
    print(namex)
}

a(whatever)
</code></pre>

<p>Each function could be call without passing name, but you can override it. For example:</p>

<pre><code>a(whatever, ""othername"")
</code></pre>
"
2311498,163053,2010-02-22T14:35:38Z,2310409,32,TRUE,"<p>Roxygen can be used anywhere within an R file (in other words, it doesn't have to be followed by a function).  It can also be used to document any docType in the R documentation. </p>

<p>So you can just document your data in a separate block (something like this):</p>

<pre><code>#' This is data to be included in my package
#'
#' @name data-name
#' @docType data
#' @author My Name \email{blahblah@@roxygen.org}
#' @references \url{data_blah.com}
#' @keywords data
NULL
</code></pre>
"
2311701,245603,2010-02-22T15:02:38Z,2310913,21,TRUE,"<p>I think you are better of creating an <code>hclust</code> object, and then converting it to a dendrogram using <code>as.dendrogram</code>, then trying to create a dendrogram directly. Look at the <code>?hclust</code> help page to see the meaning of the elements of an <code>hclust</code> object. </p>

<p>Here is a simple example with four leaves A, B, C, and D, combining first A-B, then C-D, and finally AB-CD:</p>

<pre><code>a &lt;- list()  # initialize empty object
# define merging pattern: 
#    negative numbers are leaves, 
#    positive are merged clusters (defined by row number in $merge)
a$merge &lt;- matrix(c(-1, -2,
                    -3, -4,
                     1,  2), nc=2, byrow=TRUE ) 
a$height &lt;- c(1, 1.5, 3)    # define merge heights
a$order &lt;- 1:4              # order of leaves(trivial if hand-entered)
a$labels &lt;- LETTERS[1:4]    # labels of leaves
class(a) &lt;- ""hclust""        # make it an hclust object
plot(a)                     # look at the result   

#convert to a dendrogram object if needed
ad &lt;- as.dendrogram(a)
</code></pre>
"
2311994,16632,2010-02-22T15:41:46Z,2309826,6,FALSE,"<p>You're better off not messing around with substitute and friends - you are likely to create a function that will be very hard to program against.  Why not just pass in the name of the variable explicitly as a string?</p>
"
2314280,144157,2010-02-22T21:18:11Z,2308991,2,FALSE,"<p>Rather than slice the data by the value of the conditioning variable (turning a continuous variable into a discrete variable), it is more efficient to condition using a kernel function. There is package that does this: <a href=""http://cran.r-project.org/web/packages/hdrcde/"" rel=""nofollow noreferrer"">hdrcde</a>. Check out the examples in the help files.</p>
"
2315622,37213,2010-02-23T01:46:40Z,2315601,66,TRUE,"<p><a href=""https://stat.ethz.ch/pipermail/r-help/2001-November/016802.html"" rel=""noreferrer"">This</a> seems to explain it.</p>

<blockquote>
  <p>The definition of <code>order</code> is that <code>a[order(a)]</code> is in
  increasing order. This works with your example, where the correct
  order is the fourth, second, first, then third element.</p>
  
  <p>You may have been looking for <code>rank</code>, which returns the rank of the
  elements<br>
  <code>R&gt; a &lt;- c(4.1, 3.2, 6.1, 3.1)</code><br>
  <code>R&gt; order(a)</code><br>
  <code>[1] 4 2 1 3</code><br>
  <code>R&gt; rank(a)</code><br>
  <code>[1] 3 2 4 1</code><br>
  so <code>rank</code> tells you what order the numbers are in,
  <code>order</code> tells you how to get them in ascending order.</p>
  
  <p><code>plot(a, rank(a)/length(a))</code> will give a graph of the CDF.  To see why
  <code>order</code> is useful, though, try <code>plot(a, rank(a)/length(a),type=""S"")</code>
  which gives a mess, because the data are not in increasing order</p>
  
  <p>If you did<br>
  <code>oo&lt;-order(a)</code><br>
  <code>plot(a[oo],rank(a[oo])/length(a),type=""S"")</code><br>
  or simply<br>
  <code>oo&lt;-order(a)</code><br>
  <code>plot(a[oo],(1:length(a))/length(a)),type=""S"")</code><br>
  you get a line graph of the CDF.</p>
</blockquote>

<p>I'll bet you're thinking of rank.</p>
"
2315770,66549,2010-02-23T02:30:08Z,2315601,29,FALSE,"<p>To sort a 1D vector or a single column of data, just call the <strong><em>sort</em></strong> function and pass in your sequence.</p>

<p>On the other hand, the <strong><em>order</em></strong> function is necessary to sort data <em>two</em>-dimensional data--i.e., multiple columns of data collected in a matrix or dataframe.</p>

<pre><code>Stadium Home Week Qtr Away Off Def Result       Kicker Dist
751     Out  PHI   14   4  NYG PHI NYG   Good      D.Akers   50
491     Out   KC    9   1  OAK OAK  KC   Good S.Janikowski   32
702     Out  OAK   15   4  CLE CLE OAK   Good     P.Dawson   37
571     Out   NE    1   2  OAK OAK  NE Missed S.Janikowski   43
654     Out  NYG   11   2  PHI NYG PHI   Good      J.Feely   26
307     Out  DEN   14   2  BAL DEN BAL   Good       J.Elam   48
492     Out   KC   13   3  DEN  KC DEN   Good      L.Tynes   34
691     Out  NYJ   17   3  BUF NYJ BUF   Good     M.Nugent   25
164     Out  CHI   13   2   GB CHI  GB   Good      R.Gould   25
80      Out  BAL    1   2  IND IND BAL   Good M.Vanderjagt   20
</code></pre>

<p>Here is an excerpt of data for field goal attempts in the 2008 NFL season, a dataframe i've called 'fg'. suppose that these 10 data points represent all of the field goals attempted in 2008; further suppose you want to know the the distance of the longest field goal attempted that year, who kicked it, and whether it was good or not; you also want to know the second-longest, as well as the third-longest, etc.; and finally you want the shortest field goal attempt.</p>

<p>Well, you could just do this:</p>

<pre><code>sort(fg$Dist, decreasing=T)
</code></pre>

<p>which returns: 50 48 43 37 34 32 26 25 25 20</p>

<p>That is correct, but not very useful--it does tell us the distance of the longest field goal attempt, the second-longest,...as well as the shortest; however, but that's all we know--eg, we don't know who the kicker was, whether the attempt was successful, etc. Of course, we need the entire dataframe sorted on the ""Dist"" column (put another way, we want to sort all of the data rows on the single attribute <em>Dist</em>.
that would look like this:</p>

<pre><code>Stadium Home Week Qtr Away Off Def Result       Kicker Dist
751     Out  PHI   14   4  NYG PHI NYG   Good      D.Akers   50
307     Out  DEN   14   2  BAL DEN BAL   Good       J.Elam   48
571     Out   NE    1   2  OAK OAK  NE Missed S.Janikowski   43
702     Out  OAK   15   4  CLE CLE OAK   Good     P.Dawson   37
492     Out   KC   13   3  DEN  KC DEN   Good      L.Tynes   34
491     Out   KC    9   1  OAK OAK  KC   Good S.Janikowski   32
654     Out  NYG   11   2  PHI NYG PHI   Good      J.Feely   26
691     Out  NYJ   17   3  BUF NYJ BUF   Good     M.Nugent   25
164     Out  CHI   13   2   GB CHI  GB   Good      R.Gould   25
80      Out  BAL    1   2  IND IND BAL   Good M.Vanderjagt   20
</code></pre>

<p><strong>This is what <em>order</em> does.</strong> It is 'sort' for two-dimensional data; put another way, it returns a 1D integer index comprised of the row numbers such that sorting the rows according to that vector, would give you a correct row-oriented sort on the column, <em>Dist</em></p>

<p>Here's how it works. Above, <em>sort</em> was used to sort the Dist column; to sort the entire dataframe on the Dist column, we use 'order' <em>exactly the same way as 'sort' is used above</em>:</p>

<pre><code>ndx = order(fg$Dist, decreasing=T)
</code></pre>

<p>(i usually bind the array returned from 'order' to the variable 'ndx', which stands
for 'index', because i am going to use it as an index array to sort.)</p>

<p>that was step 1, here's step 2:</p>

<p>'ndx', what is returned by 'sort' is then used as an index array to <strong>re-order</strong> the dataframe, 'fg':</p>

<pre><code>fg_sorted = fg[ndx,]
</code></pre>

<p>fg_sorted is the re-ordered dataframe immediately above.</p>

<p>In sum, 'sort' is used to create an index array (which specifies the sort order of the column you want sorted), which then is used as an index array to re-order the dataframe (or matrix).</p>
"
2316106,279194,2010-02-23T04:26:44Z,2296694,5,FALSE,"<p>If you needs stats on the pure JVM, you might try Incanter (www.incanter.org).  It's an R-like library for Clojure, which brings lispy functional programming to the JVM.  It looks like a really cool start-up with some potential.  They have a pretty active user community already on their google group.</p>
"
2316224,16632,2010-02-23T05:04:31Z,2253179,1,FALSE,"<p>This isn't supported natively in ggplot2, but I'm hoping I'll figure out how for a future version.</p>
"
2316736,258334,2010-02-23T08:39:11Z,2316630,2,FALSE,"<p>You can do it without the <code>paste()</code> command in your <code>for</code> loop. Simply assign the columns via the function <code>colnames()</code> in your loop:</p>

<pre><code>column_to_plot &lt;- colnames(dataframeNAME)[j]
</code></pre>

<p>Hope that helps as a first kludge.</p>
"
2316768,133234,2010-02-23T08:46:18Z,2316630,1,FALSE,"<p>Here is one way to do it:</p>

<pre><code>tmp.df &lt;- data.frame(col_1=rnorm(10),col_2=rnorm(10),col_3=rnorm(10))
x &lt;- seq(2,20,by=2)
plot(x, tmp.df$col_1)
for(j in 2:3){
  name.list &lt;- list(""x"",paste(""col_"",j,sep=""""))
  with(tmp.df, do.call(""lines"",lapply(name.list,as.name))) }
</code></pre>

<p>You can also do colnames(tmp.df)[j] instead of paste(..) if you'd like.</p>
"
2316950,168747,2010-02-23T09:21:31Z,2316630,6,TRUE,"<p>I do:</p>

<pre><code>x &lt;- runif(100)
column_2 &lt;-
    column_3 &lt;-
    column_4 &lt;-
    column_5 &lt;-
    column_6 &lt;-
    column_7 &lt;-
    column_8 &lt;-
    column_9 &lt;-
    column_10 &lt;-
    column_11 &lt;-
    column_12 &lt;- rnorm(100)

for (j in 2:12) {
    column_to_plot = paste(""column_"", j, sep = """")
    do.call(""plot"", list(x, as.name(column_to_plot)))
}
</code></pre>

<p>And I have no errors. Maybe you could provide hard-code which (according to your question) works, then will be simpler to find a reason of the error. </p>

<p>(I know that I can generate vectors using loop and <code>assign</code>, but I want to provide clear example)</p>
"
2317393,143476,2010-02-23T10:42:35Z,2316630,2,FALSE,"<p>Are you trying to retrieve an object in the workspace by a character string? In that case, parse() might help:</p>

<pre><code>for (j in 2:12) {
    column_to_plot = paste(""column_"", j, sep = """")
    plot(x, eval(parse(text=column_to_plot)))
}
</code></pre>

<p>In this case you could use do.call(), but it would not be required.</p>

<p><strong>Edit: wrapp parse() in eval()</strong></p>
"
2318134,170792,2010-02-23T12:43:00Z,2317446,4,TRUE,"<p>Without being sure what exactly causes the problem, I would try to transform the <code>Class</code> column to a factor (so defining the type as <code>C-classification</code> will no longer be necessary) using something like this:</p>

<pre><code>data$Class &lt;- as.factor(data$Class)
</code></pre>

<p>or in one step:</p>

<pre><code>model &lt;- svm(as.factor(Class)~.,data, kernel = ""linear"")
</code></pre>
"
2318409,279497,2010-02-23T13:26:56Z,2318368,18,TRUE,"<p>Return them in a list:</p>

<p><code>return(list(v1=v1,v2=v2))</code></p>
"
2319737,16632,2010-02-23T16:15:19Z,2316356,2,FALSE,"<p>This is on their to do list - in the next version, only functions with roxygen documentation will create man files.  </p>
"
2320066,135870,2010-02-23T16:57:52Z,2316356,14,TRUE,"<p>Well, I finally found and browsed the Roxygen-devel list at <a href=""http://r-forge.r-project.org/projects/roxygen/"" rel=""noreferrer"">R-forge</a> to see when this would be implemented, and it appears to already be in the version of Roxygen that is on CRAN.  The key is to specify <code>use.Rd2=TRUE</code> when calling <code>roxygenize()</code>.  Under this mode, Roxygen will skip creating documentation for any functions that are not preceded by Roxygen comments.</p>
"
2321431,143305,2010-02-23T20:21:22Z,2321333,4,TRUE,"<p>Aniko's comment has almost all you need (along with <code>header=TRUE</code>):</p>

<pre><code>R&gt; data &lt;- read.table(""test.table.1.0"", header=TRUE)
R&gt; data
  id Source
1  1    A10
2  2    A32
3  3    A10
4  4    A25
R&gt; 
</code></pre>

<p>In other words, if you have the data in a file, read from a file. A textConnection is useful if and when you have the data 'right there' along with the command as in the email you referenced.</p>
"
2321512,269831,2010-02-23T20:35:59Z,2321333,6,FALSE,"<p>One can go directly into SQLITE using read.csv.sql() OR read.csv2.sql() from the sqldf package.</p>

<p>From the online manual:</p>

<p><a href=""http://code.google.com/p/sqldf/#Example_13._read.csv.sql_and_read.csv2.sql"" rel=""noreferrer"">Link</a></p>

<blockquote>
  <p>Example 13. read.csv.sql and
  read.csv2.sql read.csv.sql is an
  interface to sqldf that works like
  read.csv in R except that it also
  provides an sql= argument and not all
  of the other arguments of read.csv are
  supported. It uses (1) SQLite's import
  facility via RSQLite to read the input
  file into a temporary disk-based
  SQLite database which is created on
  the fly. (2) Then it uses the provided
  SQL statement to read the table so
  created into R. As the first step
  imports the data directly into SQLite
  without going through R it can handle
  larger files than R itself can handle
  as long as the SQL statement filters
  it to a size that R can handle. Here
  is Example 6c redone using this
  facility:</p>
</blockquote>

<pre><code># Example 13a. 
library(sqldf) 

write.table(iris, ""iris.csv"", sep = "","", quote = FALSE, row.names = FALSE) 
iris.csv &lt;- read.csv.sql(""iris.csv"",  
        sql = ""select * from file where Sepal_Length &gt; 5"") 

# Example 13b.  read.csv2.sql.  Commas are decimals and ; is sep. 

library(sqldf) 
Lines &lt;- ""Sepal.Length;Sepal.Width;Petal.Length;Petal.Width;Species 
5,1;3,5;1,4;0,2;setosa 
4,9;3;1,4;0,2;setosa 
4,7;3,2;1,3;0,2;setosa 
4,6;3,1;1,5;0,2;setosa 
"" 
cat(Lines, file = ""iris2.csv"") 

iris.csv2 &lt;- read.csv2.sql(""iris2.csv"", sql = ""select * from file where Sepal_Length &gt; 5"") 
</code></pre>
"
2323307,143476,2010-02-24T02:38:08Z,2321786,1,FALSE,"<p>Since the R data frame structure is based loosely on the SQL table, having each element of the data frame be anything other than an atomic data type is uncommon. However, it can be done, as you've shown, and this linked <a href=""http://tolstoy.newcastle.edu.au/R/e2/help/07/07/21284.html"" rel=""nofollow noreferrer"">post</a> describes such an application implemented on a larger scale.</p>

<p>An alternative is to store your data as a string and have a function to retrieve it, or create a separate function to which the data is attached and extract it using indices stored in your data frame.</p>

<pre><code>&gt; ## alternative 1
&gt; tokens &lt;- function(x,i=TRUE) Map(as.numeric,strsplit(x[i],"",""))
&gt; d &lt;- data.frame(id=c(1,2,3), token_lengths=c(""5,5"", ""9"", ""4,2,2,4,6""))
&gt; 
&gt; tokens(d$token_lengths)
[[1]]
[1] 5 5

[[2]]
[1] 9

[[3]]
[1] 4 2 2 4 6

&gt; tokens(d$token_lengths,2:3)
[[1]]
[1] 9

[[2]]
[1] 4 2 2 4 6

&gt; 
&gt; ## alternative 2
&gt; retrieve &lt;- local({
+   token_lengths &lt;- list(c(5,5), 9, c(4,2,2,4,6))
+   function(i) token_lengths[i]
+ })
&gt; 
&gt; d &lt;- data.frame(id=c(1,2,3), token_lengths=1:3)
&gt; retrieve(d$token_lengths[2:3])
[[1]]
[1] 9

[[2]]
[1] 4 2 2 4 6
</code></pre>
"
2325749,134830,2010-02-24T12:01:59Z,2321786,4,FALSE,"<p>Trying to shoehorn the data into a data frame seems hackish to me.  Far better to consider each row as an individual object, then think of the dataset as an array of these objects.</p>

<p>This function converts your data strings to an appropriate format.  (This is S3 style code; you may prefer to use one of the 'proper' object oriented systems.)</p>

<pre><code>as.mydata &lt;- function(x)
{
   UseMethod(""as.mydata"")
}

as.mydata.character &lt;- function(x)
{
   convert &lt;- function(x)
   {
      md &lt;- list()
      md$phrase = x
      spl &lt;- strsplit(x, "" "")[[1]]
      md$num_words &lt;- length(spl)
      md$token_lengths &lt;- nchar(spl)
      class(md) &lt;- ""mydata""
      md
   }
   lapply(x, convert)
}
</code></pre>

<p>Now your whole dataset looks like</p>

<pre><code>mydataset &lt;- as.mydata(c(""hello world"", ""greetings"", ""take me to your leader""))

mydataset
[[1]]
$phrase
[1] ""hello world""

$num_words
[1] 2

$token_lengths
[1] 5 5

attr(,""class"")
[1] ""mydata""

[[2]]
$phrase
[1] ""greetings""

$num_words
[1] 1

$token_lengths
[1] 9

attr(,""class"")
[1] ""mydata""

[[3]]
$phrase
[1] ""take me to your leader""

$num_words
[1] 5

$token_lengths
[1] 4 2 2 4 6

attr(,""class"")
[1] ""mydata""
</code></pre>

<p>You can define a print method to make this look prettier.</p>

<pre><code>print.mydata &lt;- function(x)
{
   cat(x$phrase, ""consists of"", x$num_words, ""words, with"", paste(x$token_lengths, collapse="", ""), ""letters."")
}
mydataset
[[1]]
hello world consists of 2 words, with 5, 5 letters.
[[2]]
greetings consists of 1 words, with 9 letters.
[[3]]
take me to your leader consists of 5 words, with 4, 2, 2, 4, 6 letters.
</code></pre>

<p>The sample operations you wanted to do are fairly straightforward with data in this format.</p>

<pre><code>sapply(mydataset, function(x) nchar(x$phrase) &gt; 10)
[1]  TRUE FALSE  TRUE
</code></pre>
"
2326853,143377,2010-02-24T14:51:31Z,2321786,4,FALSE,"<p>I would just use the data in the ""long"" format.</p>

<p>E.g.</p>

<pre><code>&gt; d1 &lt;- data.frame(id=1:3, num_words=c(2,1,4), phrase=c(""hello world"", ""greetings"", ""take me to your leader""))
&gt; d2 &lt;- data.frame(id=c(rep(1,2), rep(2,1), rep(3,5)), token_length=c(5,5,9,4,2,2,4,6))
&gt; d2$tokenid &lt;- with(d2, ave(token_length, id, FUN=seq_along))
&gt; d &lt;- merge(d1,d2)
&gt; subset(d, nchar(phrase) &gt; 10)
  id num_words                 phrase token_length tokenid
1  1         2            hello world            5       1
2  1         2            hello world            5       2
4  3         4 take me to your leader            4       1
5  3         4 take me to your leader            2       2
6  3         4 take me to your leader            2       3
7  3         4 take me to your leader            4       4
8  3         4 take me to your leader            6       5
&gt; with(d, tapply(token_length, id, mean))
  1   2   3 
5.0 9.0 3.6 
</code></pre>

<p>Once the data is in the long format, you can use sqldf or plyr to extract what you want from it.</p>
"
2328295,37751,2010-02-24T18:03:37Z,2321333,6,FALSE,"<p>If you're data is not all that big, read.table() works great. If you have gigs of data you may find read.table or read.csv to be a little slow. In that case you <strong>can</strong> read data directly into sqlite from R using the sqldf package. Here's an example:</p>

<pre><code>library(sqldf)
f &lt;- file(“test.table.1.0”)
bigdf &lt;- sqldf(“select * from f”, dbname = tempfile(),
   file.format = list(header = T, row.names = F))
</code></pre>

<p>A few months ago I wrote a <a href=""http://www.cerebralmastication.com/2009/11/loading-big-data-into-r/"" rel=""noreferrer"">personal anecdote about my experience using this method</a>. </p>

<p>In my experience pulling data directly into sqlite is a LOT faster than reading it into R. But it's not worth the extra code if a simple read.csv() or read.table() works well for you.</p>
"
2328332,143476,2010-02-24T18:08:13Z,2321786,4,FALSE,"<p>Another option would be to convert your data frame into a matrix of mode list - each element of the matrix would be a list. standard array operations (slicing with <code>[</code>, apply(), etc. would be applicable).</p>

<pre><code>&gt; d &lt;- data.frame(id=c(1,2,3), num_tokens=c(2,1,4), token_lengths=as.array(list(c(5,5), 9, c(4,2,2,4,6))))
&gt; m &lt;- as.matrix(d)
&gt; mode(m)
[1] ""list""
&gt; m[,""token_lengths""]
[[1]]
[1] 5 5

[[2]]
[1] 9

[[3]]
[1] 4 2 2 4 6

&gt; m[3,]
$id
[1] 3

$num_tokens
[1] 4

$token_lengths
[1] 4 2 2 4 6
</code></pre>
"
2329206,168747,2010-02-24T20:22:33Z,2329036,4,FALSE,"<p>To get all columns with specified name you could do:</p>

<pre><code>names_with_mm &lt;- grep(""mm$"", names(series_to_plot.df), value=TRUE)
series_to_plot.df[, names_with_mm]
</code></pre>

<p>But if your base <code>data.frame</code>'s all have the same structure then you can <code>rbind</code> them, something like:</p>

<pre><code>series_to_plot.df &lt;- rbind(
  cbind(name=""p3c3"", p3c3),
  cbind(name=""p3c4"", p3c4),
  cbind(name=""p3c5"", p3c5)
)
</code></pre>

<p>Then <code>mm</code> values are in one column and its easier to plot.</p>
"
2329237,245603,2010-02-24T20:27:47Z,2326351,2,FALSE,"<p>You are producing some very strange plots. By using <code>position=""fill""</code> you are stretching out each bar to have height 1 (because the one observation corresponding to the letter is 100% of all observations corresponding to the letter within a panel), completely loosing whatever information you are trying to plot. My guess is that some of your questions stem from this mistake, but I am not sure.  </p>

<p>(Q1) Do you want the bar width to be the same in plots for the different branches? Since you are changing the number of levels of the x variable, the bars have to get wider to fill up the plot. Some solution options: </p>

<ul>
<li>Do some smart resizing of the width of the plot to go around that. </li>
<li>Leave all the x levels in - I think this is the cleanest way</li>
<li>You can get your bars narrower and centered on the plot by using the <code>expand</code> option of <code>scale_x_discrete</code>. So if you have N total x-values (here N=26 letters), but a particular plot only uses k of them, then add <code>+ scale_x_discrete(expand=c(0.05, (N-k)/2))</code> to your plot. The first term is a multiplicative expansion factor, and this is the default value, and the second term is an additive factor. </li>
</ul>

<p>(Q2) i=7 is the only group that has multiple <code>number</code> values corresponding to the same letter/ltrgroup combination. The bar geom does not know what to do with that. I agree that the error message is really cryptic.  </p>

<p>(Q3) One option is to avoid using factors - use <code>data.frame(...,stringsAsFactors=FALSE)</code> when combining character vectors, and then subsetting will not keep unused levels around.</p>
"
2329289,37751,2010-02-24T20:35:10Z,2329036,1,FALSE,"<p>The <a href=""http://docs.google.com/viewer?a=v&amp;q=cache:sNuYdCH4aFwJ:cran.r-project.org/doc/manuals/R-lang.pdf+r+language+definition&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEESimnt1hKjEDxoFSU7XYxXBy8-XHZjM8fxZdo9sgWeQJ_9eHJLbNETr9YFCcUwU2KPKO921c_MX4gbTBlbIaFU4sTmorkTzPe_ymWqnWt1EeQRBry4-XmVQl4m-2sb20N8kKtiBk&amp;sig=AHIEtbTX7nuWBs1BVGYXVFa_BVAU5VSE2Q"" rel=""nofollow noreferrer"">R Language Definition</a> has some good info on indexing (sec 3.4.1), which is pretty helpful. </p>

<p>You can then pull the names matching a sequence with the grep() command. Then string it all together like this:</p>

<pre><code> dataWithMM &lt;- series_to_plot.df[,grep(""[P]"", names(series_to_plot.df))]
</code></pre>

<p>to deconstruct it a little, this gets the number of the columns that match the ""mm"" pattern:</p>

<pre><code> namesThatMatch &lt;- grep(""[mm]"", names(series_to_plot.df)
</code></pre>

<p>Then we use that list to call the columns we want:</p>

<pre><code>  dataWithMM &lt;- series_to_plot.df[, namesThatMatch ]
</code></pre>
"
2329536,168168,2010-02-24T21:08:27Z,2326351,1,FALSE,"<p>(Q1) I don't think it is possible to fix bar width. Aniko's suggestion to keep all the levels makes most sense to me.</p>

<p>(Q2) replace <code>binwidth = 1</code> with <code>stat=""identity""</code>, as I don't think you need <code>stat=""bin""</code>.</p>

<p>(Q3) Other options include <code>drop.levels</code> in <code>gdata</code>-package, and <code>dropUnusedLevels</code> in <code>Hmisc</code>-package.</p>
"
2330825,16632,2010-02-25T00:49:26Z,2329851,42,TRUE,"<pre><code>rubies  &lt;- data.frame(carat = c(3, 4, 5), price= c(5000, 5000, 5000))

ggplot(diamonds, aes(carat, price)) + 
  geom_point() + 
  geom_point(data = rubies, colour = ""red"")
</code></pre>
"
2331247,224346,2010-02-25T02:45:24Z,2331237,0,FALSE,"<p>If the processes are competing for resources, then the benefits of parallization are reduced.</p>

<p>If the disk is running constantly (and so the processes are IO-bound), you won't notice any benefit. If they share the same instances of data structures (resulting in a lot of time wasted in synchronization), you'll notice a greatly reduced performance increase. If the ""reduce"" part of the operation is what takes most of the time, parallizing the ""map"" won't produce significant performance increases.</p>

<p>You haven't given us enough data to say for sure what the cause is in your case.</p>
"
2331249,13,2010-02-25T02:45:27Z,2331237,2,FALSE,"<p>There is one usual answer to questions about performance, and this applies whether you're doing serial or parallel programming. Use a profiler. :-)</p>
"
2331258,271599,2010-02-25T02:48:39Z,2331237,2,FALSE,"<p>Your assumption about being memory bound is correct. You need to get your working sets down to the size of the cache or increase your memory bandwidth. One way to do that would be to distribute your program on to several machines. Then you need to make sure that your chunks are coarse enough to overcome the communication expense between the machines. GPUs also have very high memory bandwidth. Your problem is still small enough that it could fit within the memory on a graphics card.</p>
"
2331309,247533,2010-02-25T03:03:01Z,2331237,0,FALSE,"<p>5 minutes does sound like a long time even for R to read a gigabyte file, so I'll assume that you're not I/O bound.  In that case, the answer is that you are, most likely, memory bound.  If so, if you read only half a chunk, parallelizing should help you.  (But are you sure the computations are actually occurring in separate threads instead of being time-sliced between the same thread?  What happens if you start two separate instances of R, one which processes one chunk, and the other which processes another?)</p>
"
2331322,246211,2010-02-25T03:05:38Z,2331070,4,TRUE,"<p>Okay that was a dumb question :)</p>

<p>Here's the obvious answer.</p>

<pre><code>Week = c(rep(1:8,2))
Total = rnorm(16,1000,600)
Alarm = c(rep(""BELTWEIGHER HIGH HIGH"",8), rep(""MICROWAVE LHS"",8))
spark &lt;- data.frame(Week, Alarm, Total)


s &lt;- ggplot(spark, aes(Week, Total)) +         
     theme(
        panel.background = element_rect(size = 1, colour = ""lightgray""),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(), 
        axis.ticks = element_blank(),
        strip.background = element_blank(),
        strip.text.y = element_blank()
        #strip.text.y = element_text(size = 7, colour = ""red"", angle = 90)
    )

s + facet_grid(Alarm ~.) + geom_line()
</code></pre>
"
2331656,212593,2010-02-25T04:32:39Z,2329036,2,TRUE,"<p>To add to the other answers, I don't think it is a good idea to have useful information encoded in variable names. Much better to rearrange your data so that all useful information is in the value of some variable. I don't know enough about your data set to suggest the right format, but it might be something like</p>

<pre><code>p c         rd day date mm sd ...
3 3 2010-10-04 ...
</code></pre>

<p>Once you have done this the answer to your question becomes the simple <code>df$mm</code>.</p>

<p>If you are getting the data in a less useful form from an external source, you can rearrange it in a more useful form like the above within R using the <code>reshape</code> function or functions from the <code>reshape</code> package.</p>
"
2332543,281025,2010-02-25T08:16:37Z,2330169,3,FALSE,"<p>what about </p>

<p>mtext(subtitle,outer=T,side=1,line=-1)</p>
"
2333860,170792,2010-02-25T12:18:29Z,2333436,6,TRUE,"<p>I would use something like this:</p>

<pre><code>stsample&lt;-sqlQuery(odcon,
    paste(""
####DATASET CONSTRUCTION QUERY #########
    select 
    *  
    from 
    BOB.DESIGNSAMPLE T1, 
    BOB.DESIGNSUBJECTGROUP T2, 
    BOB.DESIGNEVENT T3, 
    BOB.CONFIGSAMPLETYPES T4 
    WHERE 
    T1.SUBJECTGROUPID = T2.SUBJECTGROUPID 
    AND T1.TREATMENTEVENTID = T3.TREATMENTEVENTID 
    AND T1.SAMPLETYPEKEY = T4.SAMPLETYPEKEY 
    AND T1.STUDYID = T2.STUDYID 
    AND T1.STUDYID = T3.STUDYID 
    AND T1.STUDYID = 
###################################   
    "", as.character(chstudid), sep="""")
    )
</code></pre>
"
2335108,216064,2010-02-25T15:21:47Z,2333436,2,FALSE,"<p>What about using gsub(""\n"", "" "", ""long multiline select string"") instead of paste?</p>
"
2336442,121332,2010-02-25T18:09:03Z,2336056,0,FALSE,"<p>I'm no expert, but this looks to me like what you want.  It's automated, short to code, gives the same correlations as your example above, and produces p-values.</p>

<pre><code>&gt; df = data.frame(block=block, Unemployed=Unemployed.3,
+ GNP.deflator=GNP.deflator.3)
&gt; require(plyr)
Loading required package: plyr
&gt; ddply(df, ""block"", function(x){
+   as.data.frame(
+     with(x,cor.test(Unemployed, GNP.deflator))[c(""p.value"",""estimate"")]
+ )})
  block    p.value  estimate
1     a 0.01030636 0.6206334
2     b 0.01030636 0.6206334
3     c 0.01030636 0.6206334
</code></pre>

<p>To see all the details, do this:</p>

<pre><code>&gt; dlply(df, ""block"", function(x){with(x,cor.test(Unemployed, GNP.deflator))})
$a

    Pearson's product-moment correlation

data:  Unemployed and GNP.deflator 
t = 2.9616, df = 14, p-value = 0.01031
alternative hypothesis: true correlation is not equal to 0 
95 percent confidence interval:
 0.1804410 0.8536976 
sample estimates:
      cor 
0.6206334 


$b

    Pearson's product-moment correlation

data:  Unemployed and GNP.deflator 
t = 2.9616, df = 14, p-value = 0.01031
alternative hypothesis: true correlation is not equal to 0 
95 percent confidence interval:
 0.1804410 0.8536976 
sample estimates:
      cor 
0.6206334 


$c

    Pearson's product-moment correlation

data:  Unemployed and GNP.deflator 
t = 2.9616, df = 14, p-value = 0.01031
alternative hypothesis: true correlation is not equal to 0 
95 percent confidence interval:
 0.1804410 0.8536976 
sample estimates:
      cor 
0.6206334 


attr(,""split_type"")
[1] ""data.frame""
attr(,""split_labels"")
  block
1     a
2     b
3     c
</code></pre>
"
2337087,143305,2010-02-25T19:46:15Z,2337018,2,FALSE,"<p>I would start by fixing the formula: remove the redundant <code>EuropeWater</code> as you already supply the <code>data=</code> argument:</p>

<pre><code>res &lt;- rpart(water_class ~ population + GDPpercapita + area_km2 + 
                           populationdensity + climate, 
             data=EuropeWater, method=""class"")
</code></pre>

<p>Also, make sure that all columns of your <code>data.frame</code> are of the appropriate type.  Maybe some of the data read from the csv file was mistakenly read as a factor?  A quick <code>summary(EuropeWater)</code> may reveal this.</p>
"
2337180,168747,2010-02-25T20:01:03Z,2337018,15,FALSE,"<p>Does this work?</p>

<pre><code>EuropeWater_tree &lt;- rpart(EuropeWater$water_class ~ 
 population+GDPpercapita + area_km2 + populationdensity + EuropeWater$climate, 
 data=EuropeWater, method=""class"")
</code></pre>

<p>I think you should quote method type.</p>
"
2338523,118402,2010-02-25T23:54:12Z,2336056,1,FALSE,"<p>If I understand your question correctly, you are interested in computing the <a href=""http://en.wikipedia.org/wiki/Intraclass_correlation"" rel=""nofollow noreferrer"">intraclass correlation</a> between multiple tests.  There is an implementation in the <a href=""http://pbil.univ-lyon1.fr/library/psy/html/icc.html"" rel=""nofollow noreferrer"">psy</a> package, although I have not used it.</p>

<p>If you want to perform inference on the correlation estimate, you could bootstrap the subjects.  Just make sure to keep the tests together for each sample.</p>
"
2338858,255531,2010-02-26T01:05:40Z,2333521,4,TRUE,"<p>It seems to be the bug reported here:
<a href=""http://groups.google.com/group/ggplot2/browse_thread/thread/6a7929d2b122efb2"" rel=""nofollow noreferrer"">http://groups.google.com/group/ggplot2/browse_thread/thread/6a7929d2b122efb2</a></p>
"
2340007,255531,2010-02-26T07:11:29Z,2339953,5,TRUE,"<p>maybe you can relabel your factor?</p>

<pre><code>growth_series$series_id &lt;- factor(
     growth_series$series_id, 
     labels=c('treatment 1', 't2', 't3'))
</code></pre>

<p>Or if you are still looking for scale_something, it should be scale_colour_hue() </p>

<pre><code>... + scale_colour_hue('my legend', 
   breaks = levels(growth_series$series_id), 
   labels=c('t1', 't2', 't3'))
</code></pre>
"
2342008,66549,2010-02-26T13:46:40Z,1812702,5,TRUE,"<p>The ""<a href=""http://mirrors.softliste.de/cran/"" rel=""nofollow noreferrer"">sos</a>"" package is an excellent resource. It's primary interface is the ""<strong>findFn</strong>"" command, which accepts a string (your search term) and scans the ""function"" entries in Johnathan Baron's site search database, and returns the entries that contain the search term in a data frame (of class ""findFn"").</p>

<p>The columns of this data frame are: <strong>Count</strong>, <strong>MaxScore</strong>, <strong>TotalScore</strong>, <strong>Package</strong>, <strong>Function</strong>, <strong>Date</strong>, <strong>Score</strong>, <strong>Description</strong>, and <strong>Link</strong>. Clicking on ""Link"" in any entry's row will immediately pull up the help page.</p>

<p>An example: suppose you wanted to find all convolution filters across all 1800+ R packages. </p>

<pre><code>library(sos)
cf = findFn(""convolve"") 
</code></pre>

<p>This query will look the term ""convolve"", in other words, that doesn't have to be the function name. </p>

<p>Keying in ""cf"" returns an HTML table of all matches found (23 in this case). This table is an HTML rendering of the data frame i mentioned just above. What is particularly convenient is that each column (""Count"", ""MaxScore"", etc.) is sortable by clicking on the column header, so you can view the results by ""Score"", by ""Package Name"", etc.</p>

<p>(As an aside: when running that exact query, one of the results was the function ""panel.tskernel"" in a package called ""latticeExtra"". I was not aware this package had any time series filters in it and i doubt i would have discovered it otherwise.</p>
"
2342512,143305,2010-02-26T15:06:32Z,2342472,3,FALSE,"<p>Yes, just assign to <code>levels</code>:</p>

<pre><code>R&gt; set.seed(100)
R&gt; x &lt;- as.factor(round(runif(100,1,7)))
R&gt; table(x)
x
 1  2  3  4  5  6  7 
 3 16 20 19 18 17  7 
R&gt; levels(x) &lt;- LETTERS[1:7]
R&gt; table(x)
x
 A  B  C  D  E  F  G 
 3 16 20 19 18 17  7 
R&gt; 
</code></pre>
"
2342549,245603,2010-02-26T15:11:10Z,2336056,4,TRUE,"<p>I agree with Tristan - you are looking for ICC. The only difference from standard implementations is that the two raters (tests) evaluate each subject repeatedly. There might be an implementation that allows that. In the meanwhile here is another approach to get the correlation.</p>

<p>You can use ""general linear models"", which are generalizations of linear models that explicitly allow correlation between residuals.  The code below implements this using the <code>gls</code> function of the <code>nlme</code> package. I am sure there are other ways as well. To use this function we have to first reshape the data into a ""long"" format. I also changed the variable names to <code>x</code> and <code>y</code> for simplicity. I also used <code>+rnorm(N)</code> instead of <code>+rnorm(1)</code> in your code, because that's what I think you meant.</p>

<pre><code>library(reshape)
library(nlme)
dd &lt;- data.frame(x=Unemployed.3, y=GNP.deflator.3, block=factor(block))
dd$occasion &lt;- factor(rep(1:N, 3))  # variable denoting measurement occasions
dd2 &lt;- melt(dd, id=c(""block"",""occasion""))  # reshape

# fit model with the values within a measurement occasion correlated
#   and different variances allowed for the two variables
mod &lt;- gls(value ~ variable + block, data=dd2, 
           cor=corSymm(form=~1|block/occasion), 
           weights=varIdent(form=~1|variable))  
# extract correlation
mod$modelStruct$corStruct
</code></pre>

<p>In the modeling framework you can use a likelihood ratio test to get a p-value. <code>nlme</code> can also give you a confidence interval:</p>

<pre><code>mod2 &lt;- gls(value ~ variable + block, data=dd2, 
           weights=varIdent(form=~1|variable))  
anova(mod, mod2)   # likelihood-ratio test for corr=0

intervals(mod)$corStruct  # confidence interval for the correlation
</code></pre>
"
2342777,168747,2010-02-26T15:45:07Z,2342472,7,TRUE,"<p>You must provide <code>levels</code> argument to factor (as Dirk wrote):</p>

<pre><code>set.seed(2342472)
( x &lt;- round(runif(10,1,7)) )
#  [1] 7 5 5 3 1 2 5 3 3 2
( xf &lt;- as.factor(x) )
# [1] 7 5 5 3 1 2 5 3 3 2
# Levels: 1 2 3 5 7
( yf &lt;- factor(x,levels=7:1) )
# [1] 7 5 5 3 1 2 5 3 3 2
# Levels: 7 6 5 4 3 2 1
</code></pre>

<p>you could do this on existing factor too</p>

<pre><code>( yxf &lt;- factor(xf,levels=7:1) )
# [1] 7 5 5 3 1 2 5 3 3 2
#Levels: 7 6 5 4 3 2 1
</code></pre>

<p>As you see levels were extended in desire order.</p>
"
2342778,143377,2010-02-26T15:45:09Z,2342472,2,FALSE,"<p>If you complete the factor levels you're good to go:</p>

<pre><code>df &lt;- data.frame(x=factor(c(2,4,5,6)))
df$x &lt;- factor(df$x, levels = 7:1)
table(df$x)

7 6 5 4 3 2 1 
0 1 1 1 0 1 0 
</code></pre>
"
2343008,269476,2010-02-26T16:18:08Z,2342472,1,FALSE,"<p>In this case, since you have numbers, why not just transform the numbers using modular arithmetic?</p>

<p>eg</p>

<pre><code>levels(x) &lt;- as.character((6*as.numeric(levels(x)))%%7+1)
</code></pre>

<p>Modify the 6 and 7 as appropriate if using larger ranges.</p>
"
2344686,170792,2010-02-26T20:42:17Z,2343783,10,TRUE,"<ol>
<li><p>One of the MDS methods is <code>distance scaling</code> and it is divided in metric and non-metric. Another one is the <code>classical scaling</code> (also called <code>distance geometry</code> by those in bioinformatics). Classical scaling can be carried out in R by using the command <code>cmdscale</code>. Kruskal's method of nonmetric distance scaling (using the stress function and isotonic regression) can be carried out by using the command <code>isoMDS</code> in library MASS. 
The standard treatment of <code>classical scaling</code> yields an eigendecomposition problem and as such is the same as PCA if the goal is dimensionality reduction. The <code>distance scaling</code> methods, on the other hand, use iterative procedures to arrive at a solution. </p></li>
<li><p>If you refer to the distance structure, I guess you should pass a structure of the class <code>dist</code> which is an object with distance information. Or a (symmetric) matrix of distances, or an object which can be coerced to such a matrix using as.matrix(). (As I read in the help, only the lower triangle of the matrix is used, the rest is ignored).</p></li>
<li><p>(for classical scaling method): One way of determining the dimensionality of the resulting configuration is to look at the eigenvalues of the <code>doubly centered</code> symmetric matrix B (= HAH). The usual strategy is to plot the ordered eigenvalues (or some function of them) against dimension and then identify a dimension at which the  eigenvalues become “stable” (i.e., do not change perceptively). At that dimension, we may observe an “elbow” that shows where stability occurs (for points of a n-dimensional space, stability in the plot should occur at dimension n+1). For easier graphical interpretation of a classical scaling solution, we usually choose n to be small, of the order 2 or 3. </p></li>
</ol>
"
2346185,255531,2010-02-27T04:07:22Z,2344950,5,TRUE,"<pre><code>library(igraph)
g &lt;- graph.empty()
g &lt;- add.vertices(g, 4, 
    label=c('a', 'b', 'c', 'd'), 
    shape=c('rectangle', 'rectangle', 'circle', 'circle'))
g &lt;- add.edges(g, c(1, 2, 2, 3))
plot(g)
</code></pre>
"
2347435,237733,2010-02-27T13:17:46Z,2347410,1,FALSE,"<p>Looking at that set of data you could parse it using "", "" (note the extra space) as the seperator intead of "",""</p>
"
2347470,125214,2010-02-27T13:28:05Z,2347410,-4,FALSE,"<p>How about doing it as a two step process.
1. Replace the "","" with a TAB character
2. Split on tab.</p>

<p>I'm assuming .NET here but the sample principle would apply in any language</p>
"
2347478,42774,2010-02-27T13:30:23Z,2347410,0,FALSE,"<p>You could use the following regular expression to remove the commas and any surrounding quote marks to leave plain csv content</p>

<pre><code>,(?=[0-9])|""
</code></pre>

<p>then process it as normal</p>
"
2347548,255531,2010-02-27T13:53:36Z,2347410,17,TRUE,"<p>since there is an ""r"" tag under the question, I assume this is an R question.
In R, you do not need to do anything to handle the quoted commas:</p>

<pre><code>&gt; read.csv('t.csv', header=F)
     V1          V2          V3          V4
1 Sudan  15,276,000  14,098,000  13,509,000
2  Chad      209000      196000      190000

# if you want to convert them to numbers:
&gt; df &lt;- read.csv('t.csv', header=F, stringsAsFactor=F)
&gt; df$V2 &lt;- as.numeric(gsub(',', '', df$V2))
</code></pre>
"
2347966,203692,2010-02-27T16:04:30Z,2338228,8,TRUE,"<p>You could use the melt() function and then geom_boxplot().
First reproduce the data (thanks to Excel):</p>

<pre><code>vec &lt;- c(c(27, 32, 31, 28, 20, 28, 10, 29, 15, 29), 
 + c(31, 33, 20, 28, 21,  9, 14, 21, 34, 33), 
 + c(27, 33, 28, 23, 26, 33, 19, 11, 26, 30), 
 + c(33, 17, 10, 31, 10, 32, 10, 29, 31, 28), 
 + c(25, 10, 29, 34, 32, 33, 28, 32, 32, 32), 
 + c(32, 19, 13, 32, 26, 20,  9, 26, 32, 33), 
 + c(33, 32, 18, 26, 27, 28, 29, 32, 24, 25), 
 + c(33, 34, 32, 30, 27, 31, 22, 32, 33, 30), 
 + c(32, 34, 31, 22, 17, 31, 11, 27, 18, 23), 
 + c(26, 10, 30, 30, 27,  4,  4,  4,  4,  4))
myMatrix &lt;- matrix(data=vec, ncol=10, byrow=TRUE)
</code></pre>

<p>Then melt and plot:</p>

<pre><code>library(reshape)
library(ggplot2)
ggplot(data=melt(as.data.frame(myMatrix)), aes(variable, value)) + geom_boxplot()
</code></pre>

<p>Also, as Hadley suggested (long ago) in the comments, there is no need to cast the matrix to a data frame:</p>

<pre><code>ggplot(data=melt(myMatrix), aes(as.factor(X2), value)) + geom_boxplot()
</code></pre>
"
2349320,143305,2010-02-27T23:06:33Z,2349205,14,TRUE,"<p>Because of the thousand separator, the data will have been read as 'non-numeric'. So you need to convert it:</p>

<pre><code> we &lt;- gsub("","", """", we)   # remove comma
 we &lt;- as.numeric(we)      # turn into numbers
</code></pre>

<p>and now you can do</p>

<pre><code> hist(we)
</code></pre>

<p>and other numeric operations.</p>
"
2349531,143305,2010-02-28T00:28:06Z,2349511,7,TRUE,"<p>Do you mean 'merge' or 'rearrange' or simply concatenate?  If it is the latter then</p>

<pre><code>R&gt; pop2 &lt;- data.frame(pop[,1:4], rs123=paste(pop[,5],pop[,6],sep=""""), 
+                                rs157=paste(pop[,7],pop[,8],sep=""""), 
+                                rs132=paste(pop[,9],pop[,10], sep=""""))
R&gt; pop2
   status sex age disType rs123 rs157 rs132
1       0   0  42       0    11    24    44
2       1   1  37       0    31    44    44
3       1   0  38       0    11    24    44
4       0   1  45       0    31    22    44
5       1   1  25       0    31    24    44
6       0   1  31       0    11    44    44
7       1   0  43       0    11    44    44
8       0   0  41       0    11    44    44
9       1   1  57       0    31    22    24
10      1   1  40       0    11    22    24
</code></pre>

<p>and now you can do counts and whatnot on pop2:</p>

<pre><code>R&gt; sapply(pop2[,5:7], table)
$rs123

11 31 
 6  4 

$rs157

22 24 44 
 3  3  4 

$rs132

24 44 
 2  8 

R&gt; 
</code></pre>
"
2349973,163053,2010-02-28T03:30:16Z,2349820,2,TRUE,"<p>Can you write Java code (or access a Jar file) in Processing?  If so, then you can absolutely do this.  JRI provides a low level interface to R and I have yet to encounter something in R that couldn't be run through its functions.  </p>

<p>See <a href=""https://stackoverflow.com/questions/2180235/r-from-within-java"">this related question</a> for simple example of how to use it.</p>

<p>I haven't really used Processing other than to look at it a few times, but it was my understanding that it had its own language.</p>
"
2351302,170792,2010-02-28T14:24:07Z,2351204,2,FALSE,"<p>after loading the plyr package, replace </p>

<pre><code>subs &lt;- list()
    for (i in 1:length(lst)) {
            # apply function on each part, by row
            subs[[i]] &lt;- apply(dt[ , lst[[i]]], 1, fun)
    }
</code></pre>

<p>with</p>

<pre><code>subs &lt;- llply(lst,function(x) apply(dt[,x],1,fun))
</code></pre>
"
2351508,16632,2010-02-28T15:31:01Z,2351204,7,TRUE,"<p>I'd take a different approach and keep everything as data frames so that you can use merge and ddply.  I think you'll find this approach is a little more general, and it's easier to check that each step is performed correctly.</p>

<pre><code># Convert everything to long data frames
m$id &lt;- 1:nrow(m)

library(reshape)
obs &lt;- melt(m, id = ""id"")
obs$variable &lt;- as.numeric(gsub(""V"", """", obs$variable))

varinfo &lt;- melt(lst)
names(varinfo) &lt;- c(""variable"", ""scale"")

# Merge and summarise
obs &lt;- merge(obs, varinfo, by = ""variable"")

ddply(obs, c(""id"", ""scale""), summarise, 
  mean = mean(value), 
  sum = sum(value))
</code></pre>
"
2352006,255531,2010-02-28T18:08:14Z,2351744,33,TRUE,"<p>How about this:</p>

<pre><code>gsub('(.{1,90})(\\s|$)', '\\1\n', s)
</code></pre>

<p>It will break string ""s"" into lines with maximum 90 chars (excluding the line break character ""\n"", but including inter-word spaces), unless there is a word itself exceeding 90 chars, then that word itself will occupy a whole line. </p>

<p>By the way, your function seems broken --- you should replace </p>

<pre><code>lineLen &lt;- 0
</code></pre>

<p>with</p>

<pre><code>lineLen &lt;- wordLen[i]
</code></pre>
"
2352010,143305,2010-02-28T18:09:13Z,2351744,3,FALSE,"<p>You can look at e.g. the <code>write.dcf()</code> FUNCTION in R itself; it also uses a loop so nothing to be ashamed of here. </p>

<p>The first goal is to get it right --- see Chambers (2008).  </p>
"
2353036,NA,2010-02-28T22:54:37Z,2312913,5,FALSE,"<p>For large ""sparse"" datasets i would seriously recommend ""Affinity propagation"" method.
It has superior performance compared to k means and it is deterministic in nature.</p>

<p><a href=""http://www.psi.toronto.edu/affinitypropagation/"" rel=""noreferrer"">http://www.psi.toronto.edu/affinitypropagation/</a>
  It was published in journal ""Science"".</p>

<p>However the choice of optimal clustering algorithm depends on the data set under consideration. K Means is a text book method and it is very likely that some one has developed a better algorithm more suitable for your type of dataset/</p>

<p>This is a good tutorial by Prof. Andrew Moore (CMU, Google) on K Means and Hierarchical Clustering.
<a href=""http://www.autonlab.org/tutorials/kmeans.html"" rel=""noreferrer"">http://www.autonlab.org/tutorials/kmeans.html</a></p>
"
2353106,163053,2010-02-28T23:23:14Z,2352813,9,TRUE,"<p>You can just do this:</p>

<pre><code>merge(test_growth_series_LUT, test_growth_series)
</code></pre>

<p>It will automatically match the names.  If you need to specify the column, you do it like this:</p>

<pre><code>merge(test_growth_series_LUT, test_growth_series, by = ""series_id"")
</code></pre>

<p>Or this way if you need to specify on both sides (only needed if they have different names that you want to match on):</p>

<pre><code>merge(test_growth_series_LUT, test_growth_series, by.x = ""series_id"", by.y = ""series_id"")
</code></pre>

<p>I recommend looking at the examples (and walking through them) by going to the help for merge (<code>?merge</code>) or by calling <code>example(""merge"", ""base"")</code> (less useful that actually walking through it yourself.</p>

<p>Two notes:</p>

<ol>
<li>You would never need to use the intersect function here.  Use <code>c()</code> to specify multiple column names explicitly.  Or use the <code>all</code>, <code>all.x</code>, and <code>all.y</code> parameters to specify what kind of join you want.</li>
<li>You would use quotes to specify a column name in most cases unless you have attached the data.  Otherwise it will complain about not being able to locate the name.  In particular, the name needs to be in the search path when you aren't using quotes.</li>
</ol>
"
2354304,212593,2010-03-01T07:03:50Z,2352617,27,TRUE,"<p>Contrasts are needed when you fit linear models with factors (i.e. categorical variables) as explanatory variables. The contrast specifies how the levels of the factors will be coded into a family of numeric dummy variables for fitting the model. </p>

<p>Here are some good notes for the different varieties of contrasts used:
<a href=""http://www.unc.edu/courses/2006spring/ecol/145/001/docs/lectures/lecture26.htm"" rel=""noreferrer"">http://www.unc.edu/courses/2006spring/ecol/145/001/docs/lectures/lecture26.htm</a></p>

<p>When the contrasts used are changed, the model remains the same in terms of the underlying joint probability distributions allowed. Only its parametrization changes. The fitted values remain the same as well. Also, once you have the value of the parameters for one choice of contrasts, it is easy to derive what the value of the parameters for another choice of contrasts would have been. </p>

<p>Therefore the choice of contrasts has no statistical consequence. It is purely a matter of making coefficients and hypothesis tests easier to interpret.</p>
"
2354781,170792,2010-03-01T09:20:26Z,2352617,6,FALSE,"<p>Take a look <a href=""http://books.google.gr/books?id=8D4HVx0apZQC&amp;lpg=PP1&amp;dq=%22the%20R%20book%22&amp;pg=PA365#v=onepage&amp;q=The%20writers%20of%20R%20agree%20that%20treatment%20contrasts%20represent%20the%20best%20solution&amp;f=false"" rel=""nofollow noreferrer"">here</a> (pages 365-370, which are free to view). On page 364 starts a one-way analysis of variance of a plant competition experiment. The code from page 364 that is missing is:</p>

<pre><code>comp&lt;-read.table(""c:\\temp\\competition.txt"",header=T)
attach(comp)
names(comp)
[1] ""biomass"" ""clipping""
The categorical explanatory variable is clipping and it has five levels as follows:
levels(clipping)
...
</code></pre>

<p>The definition of contrasts is given later, on page 368 (in the homonym paragraph). If you want to follow the examples, you can download the datasets. See <a href=""https://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on/2252232#2252232"">my post here</a>. </p>

<p>It is out of the question that the book is highly recommended. </p>
"
2356283,143377,2010-03-01T14:03:28Z,2355727,6,TRUE,"<p>Plm will work fine for this sort of data. The time component is not required.</p>

<pre><code>&gt; library(plm)
&gt; data(""Produc"", package=""plm"")
&gt; zz &lt;- plm(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, data=Produc, index=c(""state""))
&gt; zz2 &lt;- lm(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp+factor(state), data=Produc)
&gt; summary(zz)$coefficients[,1:3]
              Estimate   Std. Error    t-value
log(pcap) -0.026149654 0.0290015755 -0.9016632
log(pc)    0.292006925 0.0251196728 11.6246309
log(emp)   0.768159473 0.0300917394 25.5272539
unemp     -0.005297741 0.0009887257 -5.3581508
&gt; summary(zz2)$coefficients[1:5,1:3]
                Estimate   Std. Error    t value
(Intercept)  2.201617056 0.1760038727 12.5089126
log(pcap)   -0.026149654 0.0290015755 -0.9016632
log(pc)      0.292006925 0.0251196728 11.6246309
log(emp)     0.768159473 0.0300917394 25.5272539
unemp       -0.005297741 0.0009887257 -5.3581508
</code></pre>
"
2358834,165384,2010-03-01T20:25:44Z,2344368,2,FALSE,"<p>I'm not sure why you're getting this error on just loading a library, but we've seen this issue on Leopard due to Apple's freetype/fontconfig/cairo libraries calling CoreFoundation calls:</p>

<p><a href=""http://finzi.psych.upenn.edu/R/Rhelp02/archive/118681.html"" rel=""nofollow noreferrer"">http://finzi.psych.upenn.edu/R/Rhelp02/archive/118681.html</a></p>

<p>How did you install R?</p>

<p>I tried reproducing your issue on Snow Leopard with stock apache2, binary R 2.10.1 from CRAN, Xcode 3.2.1, and rapche 1.1.8 but no luck. My setup worked like a champ.</p>

<p>I don't have access to Leopard anymore, so my best suggestion is to use gdb to break on that really long system call. You will want to remove the REvalonStartup directive and replace it with a handler call which does the same, configure apache to spawn only one child process (hence the whole fork thing) and connect gdb to it. Then point your web browser to your handler url and see if gdb breaks on the function and then look at the stack trace.... Pretty lame I know, but that's all I got for now.</p>
"
2359839,133234,2010-03-01T23:17:44Z,2359723,19,TRUE,"<p>As others pointed out, your example seems to work fine for the cases where the variable chart_title is a string or an expression.  Sometimes it's tricky to construct the title variable; for instance, a confusing scenario might arise if chart_title uses some other variables, and if in addition you are using some greek characters so a simple <code>paste(...)</code> doesn't suffice.  To construct a title like that, you could use something like the following:</p>

<pre><code>foo &lt;- rnorm(100)
number &lt;- 1
chart_title &lt;- substitute(paste(""Chart no. "",number,"": "",alpha,"" vs "",beta,sep=""""), list(number = number))
qplot(foo,foo) + opts(title = chart_title)
</code></pre>

<p>Another function that comes in handy when constructing titles is <code>bquote()</code>.  Programmatic title construction can be a messy business; R FAQ 7.13 (<a href=""http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-substitute-into-a-plot-label_003f"" rel=""noreferrer"">http://cran.r-project.org/doc/FAQ/R-FAQ.html</a>) can get you started, but even that FAQ basically tells you to search R-Help when in doubt.</p>
"
2360480,16632,2010-03-02T02:11:05Z,2359723,15,FALSE,"<p>Please provide a reproducible example.  The following works fine for me:</p>

<pre><code>title &lt;- ""My title""
qplot(mpg, wt, data = mtcars) + opts(title = title)
</code></pre>

<p>Since version 0.9.2, <code>opts</code> has been <a href=""http://docs.ggplot2.org/current/theme.html"" rel=""noreferrer"">replace</a> by <code>theme</code>:</p>

<pre><code>qplot(mpg, wt, data = mtcars) + theme(title = title)
</code></pre>

<p>Also, see <code>?ggtitle</code>.</p>
"
2360895,212593,2010-03-02T04:25:11Z,2359444,1,FALSE,"<p>Does this do what you want?</p>

<pre><code>quality = new(class(old.quality.obj)[[1]]))
</code></pre>
"
2360967,163053,2010-03-02T04:50:00Z,2359444,0,FALSE,"<p>You might want the get function:</p>

<pre><code>a &lt;- get(class(object))
a(...)
</code></pre>
"
2361859,170792,2010-03-02T08:56:55Z,2360490,1,FALSE,"<p>Let me try an approach</p>

<pre><code>x &lt;- rnbinom(100, mu = 10, size = 10)
y &lt;- rnbinom(100, mu = 10, size = 10)
z &lt;- rep(1:3, length=100)
A &lt;- as.data.frame(cbind(x,y,z))
</code></pre>

<p>At first load the <code>plyr</code> library</p>

<pre><code>library(plyr)
</code></pre>

<p>The following code returns the results for each z </p>

<pre><code>dlply(A, .(z), function(x) {
    hdev &lt;- function(par, mydata) {-sum(dnbinom(mydata, mu = par[1], size = par[2], log = TRUE))}
    nlminb(c(9, 12), hdev, mydata=t(as.vector(x[1] + as.vector(x[2]))))
}
)
</code></pre>

<p>Now, with this one you will get a 3x2 dataframe with the $par results</p>

<pre><code>ddply(A, .(z), function(x) {
    hdev &lt;- function(par, mydata) {-sum(dnbinom(mydata, mu = par[1], size = par[2], log = TRUE))}
    res &lt;- nlminb(c(9, 12), hdev, mydata=t(as.vector(x[1] + as.vector(x[2]))))
    return(res$par)
}
)
</code></pre>
"
2361964,203692,2010-03-02T09:12:13Z,2361557,2,FALSE,"<p>also you can just use a function:</p>

<pre><code>ff &lt;- function(x){ifelse(x &lt; 4, 5, 2)}
</code></pre>

<p>and then change </p>

<pre><code>geom_point(aes(size = n_in_stat)) +
</code></pre>

<p>with</p>

<pre><code>geom_point(aes(size = ff(n_in_stat))) +
</code></pre>
"
2364095,284485,2010-03-02T15:09:05Z,2363881,7,TRUE,"<p>I found the solution. The problem is that there aren't values for each type for each date, i.e. there are x-values for which certain levels of type don't have an entry.</p>

<p>For instance, type=V has visitors=20 at 2010-01-17, and visitors=22 at 2010-01-19, so I would add visitors=20 for 2010-01-18 as well.</p>

<p>My data was generated by using cast from the reshape package, so just setting add.missing=T as a flag fixed my issues:</p>

<pre><code>cx &lt;- cast(visitors.melt, type+date~., length, add.missing=T)
names(cx)[3] &lt;- ""visitors""
cx &lt;- ddply(cx, .(type), function(x) data.frame(date=x$date, visitors=cumsum(x$visitors)))
</code></pre>
"
2365350,160314,2010-03-02T17:44:27Z,2361557,3,TRUE,"<p>scale_size_manual sets the sizes for a discrete variable.</p>

<pre><code>geom_point(aes(size =n_in_stat&gt;4)) + scale_size_manual(values=c(2,5))
</code></pre>
"
2365923,199166,2010-03-02T19:08:59Z,2352813,0,FALSE,"<blockquote>
  <p>The error I'm getting is ""Error in as.vector(y) : object 'series_id' not found""</p>
</blockquote>

<p>A column in your data.frame can be referred to like this: <code>test_growth_series$series_id</code>, which returns the vector of series_id's. Doing the <strong>intersect is unnecessary</strong>, but would be correctly written like this:</p>

<pre><code>intersect(test_growth_series$series_id, test_growth_series_LUT$series_id)
</code></pre>

<p>To be slightly more correct, you probably want to do a <strong>left join</strong> by using <code>all.x=TRUE</code>. This covers you in case a series_id from test_growth_series doesn't appear in your look up table. Without it, you might end up missing rows in your result.</p>

<pre><code>merge(test_growth_series, test_growth_series_LUT, by = ""series_id"", all.x=TRUE)
</code></pre>

<p>This topic is also discussed in <a href=""https://stackoverflow.com/questions/1299871/how-to-join-data-frames-in-r-inner-outer-left-right"">How to join data frames in R (inner, outer, left, right)?</a></p>
"
2366386,264696,2010-03-02T20:10:35Z,2359444,0,TRUE,"<p>thanks for your responses. they lead me to a solution that works</p>

<pre><code>newObject&lt;-ShortReadQ(sread=...,
             quality=new(Class=class(quality(anotherObject)),theFirstParameter=...), 
             id=...)
</code></pre>
"
2367512,271844,2010-03-02T22:55:22Z,2338152,1,TRUE,"<p>Can you see if the error message is correct, i.e., check ""by hand"" whether the covariance matrix from the failing model is actually positive semi-definite? I'm not sure, but I imagine there are some checks you could perform - maybe there's something here: <a href=""http://en.wikipedia.org/wiki/Positive-definite_matrix"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Positive-definite_matrix</a></p>
"
2368659,217595,2010-03-03T04:01:04Z,2161152,21,FALSE,"<p>I have written a Python implementation of Sweave called Pweave that implements basic functionality and some options of Sweave for Python code embedded in reST or Latex document. You can get it here: <a href=""http://mpastell.com/pweave"" rel=""noreferrer"">http://mpastell.com/pweave</a> and see the original blog post here:  <a href=""http://mpastell.com/2010/03/03/pweave-sweave-for-python/"" rel=""noreferrer"">http://mpastell.com/2010/03/03/pweave-sweave-for-python/</a></p>
"
2370644,269476,2010-03-03T11:11:49Z,2370515,9,FALSE,"<p>It not quite clear what exactly you are trying to do.</p>

<p>To reference a row in a data frame use <code>df[row,]</code></p>

<p>To get the first position in a vector of something use <code>match(item,vector)</code>, where the vector could be one of the columns of your data frame, eg <code>df$cname</code> if the column name is cname.</p>

<p>Edit:</p>

<p>To combine these you would write:</p>

<p><code>df[match(item,df$cname),]</code></p>

<p>Note that the match gives you the first item in the list, so if you are not looking for a unique reference number, you may want to consider something else.</p>
"
2370770,163053,2010-03-03T11:34:00Z,2370515,50,FALSE,"<p>I'm interpreting your question to be about getting row numbers. </p>

<ul>
<li>You can try <code>as.numeric(rownames(df))</code> if you haven't set the rownames.  Otherwise use a sequence of <code>1:nrow(df)</code>.  </li>
<li>The <code>which()</code> function converts a TRUE/FALSE row index into row numbers. </li>
</ul>
"
2370935,66549,2010-03-03T11:59:09Z,2370515,2,FALSE,"<p>If i understand your question, you just want to be able to access items in a data frame (or list) <strong>by row</strong>:</p>

<pre><code>x = matrix( ceiling(9*runif(20)), nrow=5  )   
colnames(x) = c(""col1"", ""col2"", ""col3"", ""col4"")
df = data.frame(x)      # create a small data frame

df[1,]                  # get the first row
df[3,]                  # get the third row
df[nrow(df),]           # get the last row

lf = as.list(df)        

lf[[1]]                 # get first row
lf[[3]]                 # get third row
</code></pre>

<p>etc.</p>
"
2373917,256662,2010-03-03T18:30:25Z,2367328,6,TRUE,"<p>Here is a my solution to this:</p>

<pre><code>resize.win &lt;- function(Width=6, Height=6)
{
        # works for windows
    dev.off(); # dev.new(width=6, height=6)
    windows(record=TRUE, width=Width, height=Height)
}
resize.win(5,5)
plot(rnorm(100))
resize.win(10,10)
plot(rnorm(100))
</code></pre>
"
2374018,169947,2010-03-03T18:44:55Z,2373334,2,TRUE,"<p>The easiest way I know of is like this:</p>

<pre><code>plot(graph.tree(20, 2), layout=layout.reingold.tilford, ylim=c(1,-1))
</code></pre>

<p>I don't know whether that's officially supported though.</p>
"
2375157,163053,2010-03-03T21:34:18Z,2374947,1,TRUE,"<p>Have you tried this:</p>

<pre><code>d[,rd &lt; 100]
</code></pre>

<p>Here's a self-contained example:</p>

<pre><code>d &lt;- data.frame(matrix(1:100, ncol=10))
rd &lt;- as.list(1:10)
d[,rd &lt; 5]
</code></pre>

<p>To get the shape of a dataframe, use <code>nrow</code> and <code>ncol</code>.</p>

<p><em>Edit:</em></p>

<p>Based on your response to my <code>NA</code> question, it sounds like you have non-logical values in your index that result from missing values in your list.  The best thing to do is to first decide how you want to treat a missing value.  Then deal with them using the <code>is.na</code> function (here I extend my example from above):</p>

<pre><code>rd[[3]] &lt;- NA
d[,rd &lt; 5]
# =&gt; Error in `[.data.frame`(d, , rd &lt; 5) : undefined columns selected
</code></pre>

<p>To deal with this, I will set that NA value to 0 (which means that it the respective column will be included in the final data.frame):</p>

<pre><code>rd[is.na(rd)] &lt;- 0
d[,rd &lt; 5]
</code></pre>

<p>You need to decide for yourself what to do with the <code>NA</code> values.</p>
"
2375197,66549,2010-03-03T21:40:19Z,2374947,1,FALSE,"<p>For the bonus Q, you get ""shape"" of a data frame or matrix using the ""<strong>dim</strong>"" command.</p>

<pre><code>A = matrix( ceiling(10*runif(40)), nrow=8)
colnames(A) = c(""col1"", ""col2"", ""col3"", ""col4"", ""col5"")
df = data.frame(A)
b = ceiling(100*runif(5))

ndx = b &lt; 50          
result = df[,ndx]     # just the columns of df corresponding to b &lt; 50
</code></pre>
"
2375643,66549,2010-03-03T22:55:15Z,2375587,7,FALSE,"<p>so what you want, in R lexicon, is to change only the <strong><em>labels</em></strong> for a given factor variable (ie, leave the data as well as the factor <em>levels</em>, unchanged).</p>

<pre><code>df$letters = factor(df$letters, labels=c(""d"", ""c"", ""b"", ""a""))
</code></pre>

<p>given that you want to change only the <em>datapoint-to-label mapping</em> and not the data or the factor schema (how the datapoints are binned into individual bins or factor values, it might help to know how the mapping is originally set when you initially create the factor.</p>

<p>the rules are simple:</p>

<ul>
<li>labels are mapped to levels by index value (ie, the value
at levels[2] is given the label, label[2]);</li>
<li>factor levels can be set explicitly by passing them in via the the
<em>levels</em> argument; or</li>
<li>if no value is supplied for the levels argument, the default
value is used which is the result calling <em>unique</em> on the data vector
passed in (for the <em>data</em> argument);</li>
<li>labels can be set explicitly via the labels argument; or</li>
<li>if no value is supplied for the labels argument, the default value is
used which is just the <em>levels</em> vector</li>
</ul>
"
2375877,158065,2010-03-03T23:34:53Z,2375587,97,TRUE,"<p>Use the <code>levels</code> argument of <code>factor</code>:</p>

<pre><code>df &lt;- data.frame(f = 1:4, g = letters[1:4])
df
#   f g
# 1 1 a
# 2 2 b
# 3 3 c
# 4 4 d

levels(df$g)
# [1] ""a"" ""b"" ""c"" ""d""

df$g &lt;- factor(df$g, levels = letters[4:1])
# levels(df$g)
# [1] ""d"" ""c"" ""b"" ""a""

df
#   f g
# 1 1 a
# 2 2 b
# 3 3 c
# 4 4 d
</code></pre>
"
2376045,163053,2010-03-04T00:17:20Z,2376034,2,TRUE,"<p>You could define the color first:</p>

<pre><code>color &lt;- rep(""black"", length(bin))
color[is.null(color)] &lt;- ""red""
</code></pre>

<p>Otherwise you can use an ifelse statement:</p>

<pre><code>colour=ifelse(is.null(bin), ""red"", ""black"")
</code></pre>
"
2376349,271844,2010-03-04T01:39:36Z,2161152,4,FALSE,"<p>You might consider noweb, which is language independent and is the basis for Sweave. I've used it for Python and it works well. </p>

<p><a href=""http://www.cs.tufts.edu/~nr/noweb/"" rel=""nofollow noreferrer"">http://www.cs.tufts.edu/~nr/noweb/</a></p>
"
2376560,212593,2010-03-04T02:45:45Z,2370648,6,TRUE,"<p>There is some weirdness in using ... as an argument in a function call that I don't fully understand (it has something to do with ... being a list-type object). </p>

<p>Here is a version that works by taking the function call as an object, setting the function to be called to lm and then evaluating the call in the context of our own caller. The result of this evaluation is our return value (in R the value of the last expression in a function is the value returned, so we do not need an explicit <code>return</code>).</p>

<pre><code>foo &lt;- function(formula,data,...){
   print(head(data))
   x&lt;-match.call()
   x[[1]]&lt;-quote(lm)
   eval.parent(x)
}
</code></pre>

<p>If you want to add arguments to the lm call, you can do it like this:</p>

<pre><code>x$na.action &lt;- 'na.exclude'
</code></pre>

<p>If you want to drop arguments to foo before you call lm, you can do it like this</p>

<pre><code>x$useless &lt;- NULL
</code></pre>

<p>By the way, <code>geom_smooth</code> and <code>stat_smooth</code> pass any extra arguments to the smoothing function, so you need not create a function of your own if you only need to set some extra arguments</p>

<pre><code>qplot(data=diamonds, carat, price, facets=~clarity) + 
  stat_smooth(method=""loess"",span=0.5)
</code></pre>
"
2376672,143305,2010-03-04T03:22:43Z,2376614,4,TRUE,"<p>One way would be:</p>

<pre><code>myfile &lt;- tempfile()                   # portable across OSs
pdf(file=myfile, height=20, width=20)  # 20x20 inches, adjust at will
plot(....)                             # or print(....) for lattice + ggplot2
dev.off()                              # finalize and close file
cat(""Look at"", myfile, ""\n"")
</code></pre>

<p>and now inspect the chart in the temp. file just created with a proper pdf viewer allowing you to zoom at will.</p>
"
2377528,74658,2010-03-04T07:33:39Z,2376614,4,FALSE,"<p>Check out <a href=""https://stackoverflow.com/questions/2048304/create-editable-plots-from-r"">this previous question</a> and the answers: </p>

<p>Basically, you can use the Cairo package to create <a href=""http://en.wikipedia.org/wiki/Scalable_Vector_Graphics"" rel=""nofollow noreferrer"">svg</a> files, which are vector based, not pixel based, I can then edit these in <a href=""http://www.inkscape.org/"" rel=""nofollow noreferrer"">Inkscape</a> and i think you can view them direct in firefox (???).</p>

<pre><code>library(Cairo)
Cairo(600,600,file=""testplot.svg"",type=""svg"",bg=""transparent"",pointsize=8, units=""px"",dpi=400)
testplot
dev.off()
Cairo(1200,1200,file=""testplot12200.png"",type=""png"",bg=""transparent"",pointsize=12, units=""px"",dpi=200)
testplot
dev.off()
</code></pre>

<p>Now I had to play around with the various settings to get my plot as good as it can be before writing the file. (critical settings seem to be the pointsize, which varies the size of the points on the graph, the size, obviously, and the dpi)</p>
"
2378726,170792,2010-03-04T11:10:58Z,2375587,17,FALSE,"<p>some more, just for the record</p>

<pre><code>## reorder is a base function
df$letters &lt;- reorder(df$letters, new.order=letters[4:1])

library(gdata)
df$letters &lt;- reorder.factor(df$letters, letters[4:1])
</code></pre>

<p>You may also find useful <a href=""http://finzi.psych.upenn.edu/R/library/Epi/html/Relevel.html"" rel=""noreferrer"">Relevel</a> and <a href=""http://finzi.psych.upenn.edu/R/library/reshape/html/combine-factor-9x.html"" rel=""noreferrer"">combine_factor</a>.</p>
"
2379151,457898,2010-03-04T12:19:15Z,2375587,5,FALSE,"<p>Dealing with factors in R is quite peculiar job, I must admit... While reordering the factor levels, you're not reordering underlying numerical values. Here's a little demonstration:</p>

<pre><code>&gt; numbers = 1:4
&gt; letters = factor(letters[1:4])
&gt; dtf &lt;- data.frame(numbers, letters)
&gt; dtf
  numbers letters
1       1       a
2       2       b
3       3       c
4       4       d
&gt; sapply(dtf, class)
  numbers   letters 
""integer""  ""factor"" 
</code></pre>

<p>Now, if you convert this factor to numeric, you'll get:</p>

<pre><code># return underlying numerical values
1&gt; with(dtf, as.numeric(letters))
[1] 1 2 3 4
# change levels
1&gt; levels(dtf$letters) &lt;- letters[4:1]
1&gt; dtf
  numbers letters
1       1       d
2       2       c
3       3       b
4       4       a
# return numerical values once again
1&gt; with(dtf, as.numeric(letters))
[1] 1 2 3 4
</code></pre>

<p>As you can see... by changing levels, you change levels only (who would tell, eh?), not the numerical values! But, when you use <code>factor</code> function as @Jonathan Chang suggested, something different happens: you change numerical values themselves.</p>

<p>You're getting error once again 'cause you do <code>levels</code> and then try to relevel it with <code>factor</code>. Don't do it!!! Do <b>not</b> use <code>levels</code> or you'll mess things up (unless you know exactly what you're doing). </p>

<p><i>
One lil' suggestion: avoid naming your objects with an identical name as R's objects (<code>df</code> is density function for F distribution, <code>letters</code> gives lowercase alphabet letters). In this particular case, your code would not be faulty, but sometimes it can be... but this can create confusion, and we don't want that, do we?!? =)
</i></p>

<p>Instead, use something like this (I'll go from the beginning once again):</p>

<pre><code>&gt; dtf &lt;- data.frame(f = 1:4, g = factor(letters[1:4]))
&gt; dtf
  f g
1 1 a
2 2 b
3 3 c
4 4 d
&gt; with(dtf, as.numeric(g))
[1] 1 2 3 4
&gt; dtf$g &lt;- factor(dtf$g, levels = letters[4:1])
&gt; dtf
  f g
1 1 a
2 2 b
3 3 c
4 4 d
&gt; with(dtf, as.numeric(g))
[1] 4 3 2 1
</code></pre>

<p>Note that you can also name you <code>data.frame</code> with <code>df</code> and <code>letters</code> instead of <code>g</code>, and the result will be OK. Actually, this code is identical with the one you posted, only the names are changed. This part <code>factor(dtf$letter, levels = letters[4:1])</code> wouldn't throw an error, but it can be confounding!</p>

<p>Read the <code>?factor</code> manual thoroughly! What's the difference between <code>factor(g, levels = letters[4:1])</code> and <code>factor(g, labels = letters[4:1])</code>? What's similar in <code>levels(g) &lt;- letters[4:1]</code> and <code>g &lt;- factor(g, labels = letters[4:1])</code>?</p>

<p>You can put ggplot syntax, so we can help you more on this one!</p>

<p>Cheers!!!</p>

<p>Edit:</p>

<p><code>ggplot2</code> actually requires to change both levels and values? Hm... I'll dig this one out...</p>
"
2379969,279497,2010-03-04T14:26:11Z,2321786,0,FALSE,"<p>I would also use strings for the variable length data, but as in the following example: ""c(5,5)"" for the first phrase. One needs to use <code>eval(parse(text=...))</code> to carry out computations.</p>

<p>For example, the <code>mean</code> can be computed as follows:</p>

<p><code>sapply(data$token_lengths,function(str) mean(eval(parse(text=str))))</code></p>
"
2380501,245603,2010-03-04T15:36:03Z,2379701,3,TRUE,"<p>I would ""clean up"" the original data frame replacing the values that are equal to the previous value by NA. <code>xtable</code> will render these missing values as empty spaces. This will look good if you don't have horizontal border lines.</p>

<pre><code>cleanf &lt;- function(x){ 
   oldx &lt;- c(FALSE, x[-1]==x[-length(x)])  # is the value equal to the previous?
   res &lt;- x
   res[oldx] &lt;- NA        
   res}
</code></pre>

<p>Now we can apply this function to the columns that you want cleaned up:</p>

<pre><code>clean.cols &lt;- c( ""RUNID"", ""ANALYTEINDEX"")
calqc_table[clean.cols] &lt;- lapply(calqc_table[clean.cols], cleanf)
</code></pre>
"
2382067,172261,2010-03-04T19:21:20Z,2381618,6,TRUE,"<p>See R-Help: <a href=""http://www.mail-archive.com/r-help@stat.math.ethz.ch/msg50892.html"">Adding error bars to lattice plots</a></p>

<pre><code>prepanel.ci &lt;- function(x, y, lx, ux, subscripts, ...) {
    x &lt;- as.numeric(x)
    lx &lt;- as.numeric(lx[subscripts])
    ux &lt;- as.numeric(ux[subscripts])
    list(xlim = range(0, x, ux, lx, finite = TRUE))
}


panel.ci &lt;- function(x, y, lx, ux, subscripts, ...) {
    x &lt;- as.numeric(x)
    y &lt;- as.numeric(y)
    lx &lt;- as.numeric(lx[subscripts])
    ux &lt;- as.numeric(ux[subscripts])
    panel.barchart(x, y, ...)
    panel.arrows(lx, y, ux, y, col = 'black',
                 length = 0.25, unit = ""native"",
                 angle = 90, code = 3)
}

p &lt;- barchart(reorder(var, mean) ~ mean, data=plot.data,
              lx=plot.data$mean-plot.data$error,
              ux=plot.data$mean+plot.data$error,
              panel=panel.ci,
              prepanel=prepanel.ci)
print(p)
</code></pre>

<p><a href=""http://img689.imageshack.us/img689/9011/errorbar.png"">lattice barchart with error bar http://img689.imageshack.us/img689/9011/errorbar.png</a></p>
"
2382479,170792,2010-03-04T20:26:34Z,2381618,5,FALSE,"<p>If it doesn't have to be lattice here is a simple function that uses base R functionality, that is supplied with three arguments: the widths of the bars (xv), the lengths (up and down) of the error bars (z) and the labels for the bars on the y axis (nn). </p>

<pre><code>error.bars&lt;-function(xv,z,nn){
par(las = 1)
yv &lt;- barplot(xv,horiz = TRUE,col=""cyan"",xlim=c(0,(max(xv)+max(z))),names=nn,xlab=deparse(substitute(xv)))
g &lt;- (max(yv)-min(yv))/(3*length(yv)) 
for (i in 1:length(yv)) {
lines(c(xv[i]+z[i],xv[i]-z[i]),c(yv[i],yv[i]))
lines(c(xv[i]+z[i],xv[i]+z[i]),c(yv[i]+g,yv[i]-g))
lines(c(xv[i]-z[i],xv[i]-z[i]),c(yv[i]+g,yv[i]-g))
}}

plot.data &lt;- plot.data[order(plot.data$mean),] # reorder data
mean&lt;-as.vector(plot.data$mean)
se&lt;-as.vector(plot.data$error)
labels&lt;-as.character(plot.data$var)

error.bars(mean,se,labels)
</code></pre>

<p><img src=""https://lh4.ggpht.com/_QHi-vzRmFVU/S5An3i41PyI/AAAAAAAACnQ/rth8TZT4BYE/s512/error-bars.png"" alt=""alt text""></p>
"
2384570,143305,2010-03-05T04:33:16Z,2384517,11,FALSE,"<p>You can </p>

<ul>
<li>use <code>system()</code> to fire off a command as if it was on shell, incl globbing</li>
<li>use <code>list.files()</code> aka <code>dir()</code> to do the globbing / reg.exp matching yourself and the copy the files individually</li>
<li>use <code>file.copy</code> on individual files as shown in mjv's answer</li>
</ul>
"
2384621,166686,2010-03-05T04:46:22Z,2384517,38,TRUE,"<p>I don't think there is a direct way (shy of shelling-out), but something like the following usually works for me.    </p>

<pre><code>flist &lt;- list.files(""patha"", ""^filea.+[.]csv$"", full.names = TRUE)
file.copy(flist, ""pathb"")
</code></pre>

<p><strong>Notes:</strong></p>

<ul>
<li>I purposely decomposed in two steps, they can be combined.</li>
<li>See the regular expression: R uses true regex, and also separates the file pattern from the path, in two separate arguments.</li>
<li>note the <code>^</code> and <code>$</code> (beg/end of string) in the regex -- this is a common gotcha, as these are implicit to wildcard-type patterns, but required with regexes (lest some file names which match the wildcard pattern but also start and/or end with additional text be selected as well).</li>
<li>In the Windows world, people will typically add the <code>ignore.case = TRUE</code> argument to <code>list.files</code>, in order to emulate the fact that directory searches are case insensitive with this OS.</li>
<li>R's <code>glob2rx()</code> function provides a convenient way to convert wildcard patterns to regular expressions.  For example  <code>fpattern = glob2rx('filea*.csv')</code> returns a different but equivalent regex.</li>
</ul>
"
2386142,170792,2010-03-05T10:54:32Z,2386086,2,TRUE,"<p>Give</p>

<pre><code>head(dataset)
</code></pre>

<p>and watch the name it has been given. Perhaps it would be something like:</p>

<pre><code>dataset$data_transfer.Jingle.TCP.total_size_kb
</code></pre>
"
2386221,168747,2010-03-05T11:09:13Z,2386086,1,FALSE,"<p>Two ways:</p>

<pre><code>dataset[[""data_transfer.Jingle/TCP.total_size_kb""]]
</code></pre>

<p>or</p>

<pre><code>dataset$`data_transfer.Jingle/TCP.total_size_kb`
</code></pre>
"
2388998,136862,2010-03-05T18:23:45Z,2388974,5,FALSE,"<p>My bad. Neal Richter pointed out to me <a href=""http://www.omegahat.org/RCurl/RCurlJSS.pdf"" rel=""noreferrer"">http://www.omegahat.org/RCurl/RCurlJSS.pdf</a> - which better explains the difference between <code>cookiefile</code> and <code>cookiejar</code>. The sample script in the question actually <em>does</em> work. But it only writes the file to disk when it is no longer being used. </p>
"
2390389,118402,2010-03-05T22:30:48Z,2390338,4,TRUE,"<p>Should you using + instead of * in the formula?</p>

<pre><code>&gt; summary(aov(x~fertilizer + crop))
            Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
fertilizer   2 13.6800  6.8400  6.2371 0.03426 *
crop         3  2.8200  0.9400  0.8571 0.51218  
Residuals    6  6.5800  1.0967                  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
</code></pre>
"
2391447,212593,2010-03-06T05:29:20Z,2391364,0,FALSE,"<p>Here is one solution using reshape</p>

<pre><code>s&lt;-""A: 1
B: 2
C: 10
A: 34
B: 20
C: 6.7
A: 2
B: 78
C: 35
""
d&lt;-d&lt;-read.delim(textConnection(s),header=FALSE,sep="":"",strip.white=TRUE)
N&lt;-nrow(d)%/%3
d$id&lt;-rep(1:N,each=3)
reshape(d,dir=""wide"",timevar=""V1"",idvar=""id"")
</code></pre>

<p>Which produces</p>

<pre><code>  id V2.A V2.B V2.C
1  1    1    2 10.0
4  2   34   20  6.7
7  3    2   78 35.0
</code></pre>
"
2391501,133234,2010-03-06T05:53:37Z,2391364,4,FALSE,"<p>Here is how to do it with the plyr package:</p>

<pre><code>require(""plyr"")
my.data &lt;- ""A: 1
            B: 2
            C: 10
            A: 34
            B: 20
            C: 6.7
            A: 2
            B: 78
            C: 35""   
df &lt;- read.delim(textConnection(my.data),header=FALSE,sep="":"",strip.white=TRUE)

as.data.frame(dlply(df,.(V1),function(x) x[[2]]))
</code></pre>

<p>You get</p>

<pre><code>   A  B    C
1  1  2 10.0
2 34 20  6.7
3  2 78 35.0
</code></pre>

<p>You can see what magic plyr is doing just by playing with <code>dlply(df,.(V1))</code> or <code>dlply(df,.(V1),function(x) x)</code></p>
"
2391752,6372,2010-03-06T07:41:13Z,2391739,10,TRUE,"<p>Use system(...).</p>
"
2392026,20972,2010-03-06T09:35:32Z,2392017,15,TRUE,"<p>If all the languages support SQLite - use it. The power of SQL might not be useful to you right now, but it probably will be at some point, and it saves you having to rewrite things later when you decide you want to be able to query your data in more complicated ways.</p>

<p>SQLite will also probably be substantially faster if you only want to access certain bits of data in your datastore - since doing that with a flat-text file is challenging without reading the whole file in (though it's not impossible).</p>
"
2392131,136264,2010-03-06T10:19:04Z,2392017,5,FALSE,"<p>A flat text file (e.g. in csv format) would be the most portable solution. Almost every program/library can work with this format: R and Python have good csv support and if your data set isn't too large you can even import the csv into Excel for smaller tasks.</p>

<p>However, text files are unhandily for larger data sets since you need to read them completely for almost all operations (depending on the structure of your data).</p>

<p>SQLite allows you to filter the data very easily (even without much SQL experties) and as you already mentioned can do some computation on its own (AVG, SUM, ...). Using the Firefox Plug-in <a href=""https://addons.mozilla.org/en-US/firefox/addon/5817"" rel=""noreferrer"">SQLiteManager</a> you can work with the DB on every computer without any installation/configuration trouble and thus easily manage your data (import/export, filter). </p>

<p>So I would recommend to use SQLite for larger data sets that needs a lot of filtering to extract the data that you need. For smaller data sets or if there is no need to select subsets of your data a flat (csv) text file should be fine.</p>
"
2392343,168747,2010-03-06T11:42:21Z,2392216,25,TRUE,"<p><code>apply</code> converts your data.frame to character matrix. Use <code>lapply</code>:</p>

<pre><code>lapply(a, class)
# $x1
# [1] ""numeric""
# $x2
# [1] ""factor""
# $x3
# [1] ""factor""
</code></pre>

<p>In second command apply converts result to character matrix, using <code>lapply</code>:</p>

<pre><code>a2 &lt;- lapply(a, as.factor)
lapply(a2, class)
# $x1
# [1] ""factor""
# $x2
# [1] ""factor""
# $x3
# [1] ""factor""
</code></pre>

<p>But for simple lookout you could use <code>str</code>:</p>

<pre><code>str(a)
# 'data.frame':   100 obs. of  3 variables:
#  $ x1: num  -1.79 -1.091 1.307 1.142 -0.972 ...
#  $ x2: Factor w/ 2 levels ""a"",""b"": 2 1 1 1 2 1 1 1 1 2 ...
#  $ x3: Factor w/ 2 levels ""a"",""b"": 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>

<p>Additional explanation according to comments:</p>

<h2>Why does the lapply works while apply doesn't?</h2>

<p>First thing what <code>apply</code> do is convert an argument to a matrix. So <code>apply(a)</code> is equivalent of <code>apply(as.matrix(a))</code>. As you can see <code>str(as.matrix(a))</code> gives you:</p>

<pre><code>chr [1:100, 1:3] "" 0.075124364"" ""-1.608618269"" ""-1.487629526"" ...
- attr(*, ""dimnames"")=List of 2
  ..$ : NULL
  ..$ : chr [1:3] ""x1"" ""x2"" ""x3""
</code></pre>

<p>There are no more factors, so <code>class</code> return <code>""character""</code> for all columns.<br>
<code>lapply</code> works on columns so gives you what you want (it do something like <code>class(a$column_name)</code> for each column).</p>

<p>Why <code>apply</code> and <code>as.factor</code> doesn't work you can see in help to <code>apply</code>:</p>

<blockquote>
  <p>In all cases the result is coerced by
  as.vector to one of the basic vector
  types before the dimensions are set,
  so that (for example) factor results
  will be coerced to a character array.</p>
</blockquote>

<p>Why <code>sapply</code> and <code>as.factor</code> doesn't work you can see in help to <code>sapply</code>:</p>

<blockquote>
  <p>Value (...) An atomic vector or matrix
  or list of the same length as X (...)
  If simplification occurs, the output
  type is determined from the highest
  type of the return values in the
  hierarchy NULL &lt; raw &lt; logical &lt;
  integer &lt; real &lt; complex &lt; character &lt;
  list &lt; expression, after coercion of
  pairlists to lists.</p>
</blockquote>

<p>You never get matrix of factors or data.frame.</p>

<h2>How to convert output to <code>data.frame</code>?</h2>

<p>Simple one to use <code>as.data.frame</code> as you wrote in comment:</p>

<pre><code>a2 &lt;- as.data.frame(lapply(a, as.factor))
str(a2)
'data.frame':   100 obs. of  3 variables:
 $ x1: Factor w/ 100 levels ""-2.49629293159922"",..: 60 6 7 63 45 93 56 98 40 61 ...
 $ x2: Factor w/ 2 levels ""a"",""b"": 1 1 2 2 2 2 2 1 2 2 ...
 $ x3: Factor w/ 2 levels ""a"",""b"": 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>

<p>But if you want to replace selected character columns with <code>factor</code> there is a trick:</p>

<pre><code>a3 &lt;- data.frame(x1=letters, x2=LETTERS, x3=LETTERS, stringsAsFactors=FALSE)
str(a3)
'data.frame':   26 obs. of  3 variables:
 $ x1: chr  ""a"" ""b"" ""c"" ""d"" ...
 $ x2: chr  ""A"" ""B"" ""C"" ""D"" ...
 $ x3: chr  ""A"" ""B"" ""C"" ""D"" ...

columns_to_change &lt;- c(""x1"",""x2"")
a3[, columns_to_change] &lt;- lapply(a3[, columns_to_change], as.factor)
str(a3)
'data.frame':   26 obs. of  3 variables:
 $ x1: Factor w/ 26 levels ""a"",""b"",""c"",""d"",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ x2: Factor w/ 26 levels ""A"",""B"",""C"",""D"",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ x3: chr  ""A"" ""B"" ""C"" ""D"" ...
</code></pre>

<p>You could use it to replace all columns using:</p>

<pre><code>a3 &lt;- data.frame(x1=letters, x2=LETTERS, x3=LETTERS, stringsAsFactors=FALSE)
a3[, ] &lt;- lapply(a3, as.factor)
str(a3)
'data.frame':   26 obs. of  3 variables:
 $ x1: Factor w/ 26 levels ""a"",""b"",""c"",""d"",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ x2: Factor w/ 26 levels ""A"",""B"",""C"",""D"",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ x3: Factor w/ 26 levels ""A"",""B"",""C"",""D"",..: 1 2 3 4 5 6 7 8 9 10 ...
</code></pre>
"
2392951,190597,2010-03-06T15:22:39Z,2391364,7,TRUE,"<p>How about :</p>

<pre><code>s&lt;-""A: 1
B: 2
C: 10
A: 34
B: 20
C: 6.7
A: 2
B: 78
C: 35
""
d&lt;-read.delim(textConnection(s),header=FALSE,sep="":"",strip.white=TRUE)
cols&lt;-levels(d[,'V1'])
d&lt;-data.frame(sapply(cols,function(x) {d['V2'][d['V1']==x]}, USE.NAMES=TRUE))
</code></pre>

<p>which yields:</p>

<pre><code>   A  B    C
1  1  2 10.0
2 34 20  6.7
3  2 78 35.0
</code></pre>
"
2394211,142477,2010-03-06T21:46:32Z,2391364,2,FALSE,"<p>I posted this question on R-help as well, and got a response from Phil Spector suggesting <code>unstack</code>.  </p>

<p>This is a modification of Leo Alekseyev's response</p>

<pre><code>my.data &lt;- ""A: 1
            B: 2
            C: 10
            A: 34
            B: 20
            C: 6.7
            A: 2
            B: 78
            C: 35""   
df &lt;- read.delim(textConnection(my.data),header=FALSE,sep="":"",strip.white=TRUE)
unstack(df, V2 ~ V1)
</code></pre>

<p>This results in:</p>

<pre><code>   A  B    C
1  1  2 10.0
2 34 20  6.7
3  2 78 35.0
</code></pre>

<p>Some advantages of this approach compared to the other thoughtful answers is that you don't need to specify the number of columns ahead of time.  It also doesn't require any additional packages.</p>
"
2395912,212593,2010-03-07T10:24:19Z,2394902,2,FALSE,"<p>You can try out the <code>read.spss</code> function from the <code>foreign</code> package.</p>

<p>A rough and ready way to get rid of the <code>labelled</code> class created by <code>spss.get</code></p>

<pre><code>for (i in 1:ncol(x)) {
    z&lt;-class(x[[i]])
    if (z[[1]]=='labelled'){
       class(x[[i]])&lt;-z[-1]
       attr(x[[i]],'label')&lt;-NULL
    }
}
</code></pre>

<p>But can you please give an example where <code>labelled</code> causes problems? </p>

<p>If I have a variable <code>MAED</code> in a data frame <code>x</code> created by <code>spss.get</code>, I have:</p>

<pre><code>&gt; class(x$MAED)
[1] ""labelled"" ""factor""  
&gt; is.factor(x$MAED)
[1] TRUE
</code></pre>

<p>So well-written code that expects a factor (say) should not have any problems.</p>
"
2396630,66549,2010-03-07T14:46:05Z,2156935,3,FALSE,"<p>Your question, or the general pattern anyway, was clearly a primary use case for the design of the <strong>sos package</strong>.</p>

<p>sos actually goes one step further that your question requires by identifying particular functions with packages; in addition, it ranks the results by relevance (by default, you can change the default behavior via the ""sortby"" parameter, e.g., sortby=""Date"")</p>

<p>Here's how it works:</p>

<p>most of this package's functionality is exposed via the ""<strong>findFn</strong>"" command</p>

<p>for instance, if you want <em>a list of all functions and the parent package related to <strong>scatter plots</em></strong>:</p>

<pre><code>findFn(""scatter plot"", maxPages=2, sortby=""TotalScore"")
</code></pre>

<p>This returns a dataframe formatted as HTML table and delivered in your default browser (if you don't want it to pop-up immediately, then just bind the function call to a variable and then call the variable when you're ready)</p>

<p>The right-most column of the dataframe/HTML page is ""Description and Link"". Clicking an entry in that column opens another tab in your browser (according to the user-set preferences set in your browser) with the complete R help page for that function.</p>

<p>The results from the function call above show, for instance, that the functions for plotting data in a 'scatter plot' format are found in the following packages: </p>

<ul>
<li>ade4 (function: scatter) </li>
<li>IDPmisc (functions: ipairs, iplots)</li>
<li>GGally (function: ggally_points)</li>
<li>PerformanceAnalytics (function:
chart.Scatter)</li>
<li>mclust (function: clPairs)</li>
</ul>

<p>Another example:</p>

<pre><code>findFn(""boxplot"", maxPages=2, sortby=""TotalScore"")
</code></pre>

<p>identifies these (among others) packages/functions for plotting boxplots:</p>

<ul>
<li>sfsmisc (function: boxplot.matrix)</li>
<li>aplpack (function: boxplot2D)</li>
<li>NADA (function: boxplot-methods)</li>
<li>StatDA (function: rg.boxplot)</li>
<li>plotrix (function: gap.boxplot)</li>
<li>gplots (function: boxplot.n)</li>
<li>multcompView (function:
multcompBoxplot)</li>
<li>oligo (function: boxplot)</li>
</ul>
"
2397721,170792,2010-03-07T19:59:17Z,2397349,3,TRUE,"<p>Of course anova would be impossible, as anova involves calculating the total variation in the response variable and partitioning it into informative components (SSA, SSE). I can't see how one could calculate sum of squares for a categorical variable like Kyphosis. </p>

<p>I think that what you actually talking about is Attribute Selection (or evaluation). I would use the <code>information gain</code> measure for example. I think that this is what is used to select the test attribute at each node in the tree  and the attribute with the highest information gain (or greatest entropy reduction) is chosen as the test attribute for the current node. This attribute minimizes the information needed to classify the samples in the resulting partitions.</p>

<p>I am not aware whether there is a method of ranking attributes according to their information gain in R, but I know that there is in <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow noreferrer"">WEKA</a> and is named <a href=""http://wiki.pentaho.com/display/DATAMINING/InfoGainAttributeEval"" rel=""nofollow noreferrer"">InfoGainAttributeEval</a> It evaluates the worth of an attribute by measuring the information gain with respect to the class. And if you use <code>Ranker</code> as the <code>Search Method</code>, the attributes are ranked by their individual evaluations.</p>

<p><strong>EDIT</strong>
I finally found a way to do this in R using Library <code>CORElearn</code></p>

<pre><code>estInfGain &lt;- attrEval(Kyphosis ~ ., kyphosis, estimator=""InfGain"")
print(estInfGain)
</code></pre>
"
2397927,287778,2010-03-07T20:51:57Z,1801064,4,FALSE,"<p>You could also try the layout command:</p>

<p>Try 
       <code>layout(1:2)</code></p>

<pre><code>plot(A)    
plot(B)
</code></pre>
"
2399061,143305,2010-03-08T03:08:54Z,2399027,3,FALSE,"<p>After editing your post for readability / formatting it seems that you have no Java system in your path. I don't use the OS you're trying to use this on, but on mine <code>rJava</code> only works if I also install a Java Run-Time Environment or, better still, a Java SDK.  </p>

<p>Note that the package clearly lists</p>

<pre><code>SystemRequirements: java
</code></pre>

<p>and that the <a href=""http://www.rforge.net/rJava/"" rel=""nofollow noreferrer""><strong>rJava site</strong></a> clearly states the following</p>

<blockquote>
  <p><strong>Installation</strong> </p>
  
  <p>First, make sure you have
  JDK 1.4 or higher installed (some
  platforms require hgher version see R
  Wiki). On unix systems make sure that
  R was configured with Java support. If
  not, you can re-configure R by using R
  CMD javareconf (you may have to
  prepend sudo or run it as root
  depending on your installation - see
  R-ext manual A.2.2 for details). On
  Windows Java is detected at run-time
  from the registry.</p>
  
  <p>rJava can be installed as any other R
  package from CRAN using
  install.packages('rJava'). See the
  files section in the left menu for
  development versions.</p>
  
  <p>JRI is only compiled if supported,
  i.e. if R was configured as a
  framework or with --enable-R-shlib.</p>
</blockquote>

<p>so I think we have a few smoking guns pointing the same way.</p>
"
2401961,163053,2010-03-08T14:30:32Z,2399027,4,TRUE,"<p>Following up on some of Dirk's sage advice:</p>

<ul>
<li>Your path should probably say <code>C:\Program Files\Java\jre6\bin\</code> (remove ""client""). </li>
<li>Your path should also have <code>C:\Program Files\R\R-2.10.1\bin\</code>.  You don't need that explicit reference to the rJava libs.  I would also then remove <code>C:\Program Files\R</code> from the path.</li>
<li>Lastly, confirm that Java is accessible by either going to your command prompt and typing <code>java -version</code>, or from within R by typing <code>system(""java -version"")</code>.</li>
<li>If you install the SDK, then you will also want a system variable <code>JAVA_HOME</code> which in my case points to <code>C:\Sun\SDK\</code>.</li>
</ul>
"
2402950,88198,2010-03-08T16:46:36Z,2402885,0,FALSE,"<p>I haven't done this myself, but I do know of the R package that a lot of people use for the step of putting it in the second array there.  It's called <code>reshape</code>:</p>

<p><a href=""http://www.statmethods.net/management/reshape.html"" rel=""nofollow noreferrer"">http://www.statmethods.net/management/reshape.html</a></p>

<p><a href=""http://had.co.nz/reshape/introduction.pdf"" rel=""nofollow noreferrer"">http://had.co.nz/reshape/introduction.pdf</a></p>

<p>As for the plotting part, I think that <code>lattice</code> or <code>ggplot</code> probably both have functions for doing exactly what you want, but again I am an R newbie so I can't say much more...</p>
"
2403261,143305,2010-03-08T17:28:27Z,2402885,3,TRUE,"<p>That aggregation is just a simple call to <code>table</code> inside of <code>apply</code>:</p>

<pre><code>R&gt; foo &lt;- data.frame(online=sample(c(""S"",""W"",""U""),10,TRUE), 
                     offline=sample(c(""S"",""W"",""U""),10,TRUE))
R&gt; apply(foo,2,table)
  online offline
S      3       1
U      4       5
W      3       4
</code></pre>

<p>which you can feed into <code>barplot</code>.</p>
"
2403558,133234,2010-03-08T18:10:24Z,2402885,1,FALSE,"<p>Dirk's answer is the way to go, but on the OP's data a simple <code>apply(foo,2,table)</code> won't work -- you need to deal with the 0 entry, perhaps like so:</p>

<pre><code>my.data &lt;- ""online              offline
1         sehrwichtig             wichtig
2             wichtig           unwichtig
3         sehrwichtig           unwichtig
4         sehrwichtig         sehrwichtig
5         sehrwichtig         sehrwichtig
6         sehrwichtig           unwichtig
7         sehrwichtig           unwichtig
8             wichtig             wichtig
9             wichtig           unwichtig
10        sehrwichtig         sehrwichtig
11        sehrwichtig             wichtig
12        sehrwichtig           unwichtig
13            wichtig         sehrwichtig
14        sehrwichtig             wichtig""

df &lt;- read.table(textConnection(my.data))

df.labels &lt;- unique(as.character(apply(df,2,as.character)))
tallies &lt;- apply(df,2,function(x)table(x)[df.labels])
tallies[is.na(tallies)] &lt;- 0
rownames(tallies) &lt;- df.labels
</code></pre>

<p>For brevity's sake, you could combine the last 3 lines:</p>

<pre><code>tallies &lt;- apply(df,2,function(x){y &lt;- table(x)[df.labels];
                                   names(y) &lt;- df.labels; y[is.na(y)] &lt;- 0; y})
</code></pre>

<p>The output is:</p>

<pre><code>&gt; tallies
            online offline
sehrwichtig     10       4
wichtig          4       4
unwichtig        0       6
</code></pre>
"
2403674,133234,2010-03-08T18:24:32Z,2403466,9,TRUE,"<p>Try this:</p>

<pre><code>p + facet_grid(fy ~ fx, labeller = label_parsed)
</code></pre>
"
2403838,170792,2010-03-08T18:51:51Z,2402885,2,FALSE,"<pre><code>#generate data
df&lt;- read.table(textConnection('
      online              offline
 sehrwichtig             wichtig
     wichtig           unwichtig
 sehrwichtig           unwichtig
 sehrwichtig         sehrwichtig
 sehrwichtig         sehrwichtig
 sehrwichtig           unwichtig
 sehrwichtig           unwichtig
     wichtig             wichtig
     wichtig           unwichtig
 sehrwichtig         sehrwichtig
 sehrwichtig             wichtig
 sehrwichtig           unwichtig
     wichtig         sehrwichtig
 sehrwichtig             wichtig'
 ),header=T)

#factor levels should be the same
levels(df$online) &lt;- levels(df$offline)

my_table &lt;- t(aaply(df,2,table))

library(graphics)
barplot2(my_table,legend = rownames(my_table), ylim = c(0, 20))
</code></pre>

<p><img src=""https://lh6.ggpht.com/_QHi-vzRmFVU/S5VHCZMfYnI/AAAAAAAACok/VehcMK7BiAk/s512/stacked-bars.png"" alt=""alt text""></p>
"
2404129,163053,2010-03-08T19:34:36Z,2404085,13,TRUE,"<p>I would try to bind another column called ""value"" and set <code>value = TRUE</code>.  </p>

<pre><code>df &lt;- data.frame(cbind(1:10, 2:11, 1:3))
colnames(df) &lt;- c(""ID"",""DATE"",""SECTOR"")
df &lt;- data.frame(df, value=TRUE)
</code></pre>

<p>Then do a reshape:</p>

<pre><code>reshape(df, idvar=c(""ID"",""DATE""), timevar=""SECTOR"", direction=""wide"")
</code></pre>

<p>The problem with using the <code>reshape</code> function is that the default for missing values is NA (in which case you will have to iterate and replace them with FALSE). </p>

<p>Otherwise you can use <code>cast</code> out of the <code>reshape</code> package (<a href=""https://stackoverflow.com/questions/1533493/reshaping-data-frame-in-r"">see this question for an example</a>), and set the default to <code>FALSE</code>.</p>

<pre><code>df.wide &lt;- cast(df, ID + DATE ~ SECTOR, fill=FALSE)
&gt; df.wide 
   ID DATE     1     2     3
1   1    2  TRUE FALSE FALSE
2   2    3 FALSE  TRUE FALSE
3   3    4 FALSE FALSE  TRUE
4   4    5  TRUE FALSE FALSE
5   5    6 FALSE  TRUE FALSE
6   6    7 FALSE FALSE  TRUE
7   7    8  TRUE FALSE FALSE
8   8    9 FALSE  TRUE FALSE
9   9   10 FALSE FALSE  TRUE
10 10   11  TRUE FALSE FALSE
</code></pre>
"
2405589,158065,2010-03-08T23:29:04Z,2404085,2,FALSE,"<p>Here's another approach using <code>xtabs</code> which may or may not be faster (if someone would try and let me know):</p>

<pre><code>df &lt;- data.frame(cbind(1:12, 2:13, 1:3))
colnames(df) &lt;- c(""ID"",""DATE"",""SECTOR"")
foo &lt;- xtabs(~ paste(ID, DATE) + SECTOR, df)
cbind(t(matrix(as.numeric(unlist(strsplit(rownames(foo), "" ""))), nrow=2)), foo)
</code></pre>
"
2405594,146704,2010-03-08T23:30:37Z,1296646,20,FALSE,"<p>if SQL comes naturally to you, sqldf handles ORDER BY as Codd intended.</p>
"
2406164,163053,2010-03-09T01:59:11Z,2405575,6,TRUE,"<p>You could try using either <code>rgl</code> (<a href=""http://www2.warwick.ac.uk/fac/sci/moac/currentstudents/peter_cock/r/3d_bar_chart/"" rel=""nofollow noreferrer"">see here</a>) or <code>3dscatterplot</code> (as <a href=""http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=116"" rel=""nofollow noreferrer"">in this example</a>).  Lattice also supports this:</p>

<pre><code>library(lattice) 
library(latticeExtra) 
?panel.3dbars
</code></pre>

<p>You can see an example of this <a href=""http://learnr.wordpress.com/2009/07/20/ggplot2-version-of-figures-in-lattice-multivariate-data-visualization-with-r-part-6/"" rel=""nofollow noreferrer"">on the Learnr blog</a>.</p>

<p>I don't believe that's technically a stacked histogram (a stacked histogram stacks the bars on top of each other).  Moreover, a different kind of histogram could be more informative: look at <a href=""http://had.co.nz/ggplot2/geom_histogram.html"" rel=""nofollow noreferrer"">the <code>ggplot2</code> the documentation here</a> for some examples.</p>

<pre><code> hist_cut &lt;- ggplot(diamonds, aes(x=price, fill=cut)) 
 hist_cut + geom_bar() # defaults to stacking 
</code></pre>

<p>Another option is to use latticing instead, with <code>facet_wrap</code> in ggplot2 (see <a href=""http://learnr.wordpress.com/2009/07/02/ggplot2-version-of-figures-in-lattice-multivariate-data-visualization-with-r-part-4/"" rel=""nofollow noreferrer"">this post as an example</a>).</p>
"
2409795,212593,2010-03-09T14:40:28Z,2409317,21,TRUE,"<ol>
<li>The binwidth is measured in seconds, so to bin per week set <code>binwidth=7*24*60*60</code>.</li>
<li>Limits can be given as a vector of 2 POSIXct objects.</li>
</ol>

<p>An example:</p>

<pre><code>y&lt;-as.POSIXct('1970/01/01')+cumsum(rnorm(100,mean=24*60*60,sd=24*60*60))
p&lt;-qplot(y,binwidth=7*24*60*60,fill=I('steelblue'),col=I('black'))
p&lt;-p+scale_x_datetime(major=""1 week"",
                      minor=""1 days"",
                      format=""%e/%m/%Y"",
                      limits=c(as.POSIXct('1970/02/01'),
                               as.POSIXct('1970/03/31')))
print(p)
</code></pre>
"
2410216,37751,2010-03-09T15:34:58Z,2409357,45,TRUE,"<p>If you use geom_text() instead of annotate() you can pass a group color to your plot:</p>

<pre><code>ggplot(data2, aes(x=time, y=value, group=type, col=type))+
geom_line()+
geom_point()+
theme_bw() +
geom_text(aes(7, .9, label=""correct color"", color=""NA*"")) +
geom_text(aes(15, .6, label=""another correct color!"", color=""MVH"")) 
</code></pre>

<p>So using annotate() it looks like this:
<a href=""http://www.cerebralmastication.com/wp-content/uploads/2010/03/before.png"">alt text http://www.cerebralmastication.com/wp-content/uploads/2010/03/before.png</a></p>

<p>then after using geom_text() it looks like this:
<a href=""http://www.cerebralmastication.com/wp-content/uploads/2010/03/after.png"">alt text http://www.cerebralmastication.com/wp-content/uploads/2010/03/after.png</a></p>
"
2410412,37751,2010-03-09T15:59:45Z,1358003,42,FALSE,"<p>I love Dirk's .ls.objects() script but I kept squinting to count characters in the size column. So I did some ugly hacks to make it present with pretty formatting for the size:</p>

<pre><code>.ls.objects &lt;- function (pos = 1, pattern, order.by,
                        decreasing=FALSE, head=FALSE, n=5) {
    napply &lt;- function(names, fn) sapply(names, function(x)
                                         fn(get(x, pos = pos)))
    names &lt;- ls(pos = pos, pattern = pattern)
    obj.class &lt;- napply(names, function(x) as.character(class(x))[1])
    obj.mode &lt;- napply(names, mode)
    obj.type &lt;- ifelse(is.na(obj.class), obj.mode, obj.class)
    obj.size &lt;- napply(names, object.size)
    obj.prettysize &lt;- sapply(obj.size, function(r) prettyNum(r, big.mark = "","") )
    obj.dim &lt;- t(napply(names, function(x)
                        as.numeric(dim(x))[1:2]))
    vec &lt;- is.na(obj.dim)[, 1] &amp; (obj.type != ""function"")
    obj.dim[vec, 1] &lt;- napply(names, length)[vec]
    out &lt;- data.frame(obj.type, obj.size,obj.prettysize, obj.dim)
    names(out) &lt;- c(""Type"", ""Size"", ""PrettySize"", ""Rows"", ""Columns"")
    if (!missing(order.by))
        out &lt;- out[order(out[[order.by]], decreasing=decreasing), ]
        out &lt;- out[c(""Type"", ""PrettySize"", ""Rows"", ""Columns"")]
        names(out) &lt;- c(""Type"", ""Size"", ""Rows"", ""Columns"")
    if (head)
        out &lt;- head(out, n)
    out
}
</code></pre>
"
2413328,133234,2010-03-09T23:11:13Z,2413163,5,TRUE,"<p>I can reproduce this under Linux: If I run R from the terminal, setting <code>options(htmlhelp=TRUE)</code> brings up help in a browser, but it fails from ESS.  However,</p>

<pre><code>help(""plot"",htmlhelp=getOption(""htmlhelp""))
</code></pre>

<p>works -- help opens in Firefox for me.  I think that's an anomalous behavior -- htmlhelp=getOption(""htmlhelp"") is a default argument for help (see ?help).  Could it be a bug?..  A possible workaround is to invoke help via <code>help(""plot"",htmlhelp=T)</code> directly, or write a wrapper like <code>myhelp &lt;- function(x) help(x,htmlhelp=TRUE)</code>.</p>

<p>EDIT: A possible solution: put this into your .emacs</p>

<pre><code>(setq inferior-ess-r-help-command ""help(\""%s\"", htmlhelp=TRUE)\n"")
</code></pre>

<p>as per <a href=""https://stat.ethz.ch/pipermail/ess-debian/2009-October/000093.html"" rel=""nofollow noreferrer"">https://stat.ethz.ch/pipermail/ess-debian/2009-October/000093.html</a>.  This refers to ESS 5.5; the current one in Ubuntu repositories is 5.4, so I can't test the fix w/o upgrading ESS.</p>

<p>EDIT2: I upgraded to the latest R/ESS as per Dirk's suggestion, and did the following, after launching R/ESS: <code>M-x customize-variable</code>  <code>inferior-ess-r-help-command</code> and replaced help_type=text with help_type=html.  That fixed things (at least for version 2.10.1/ESS 5.8).</p>
"
2414145,16632,2010-03-10T02:35:40Z,2405575,8,FALSE,"<p>One doesn't.  This is a terrible display of data because the front histograms obscure the rear histograms and the perspective makes it just about impossible to read the values off the y-axis.</p>
"
2414986,108741,2010-03-10T06:40:39Z,2414915,3,FALSE,"<p>The main thing for scaling up to large data is to avoid situations where you're reading huge datasets into memory at once. In pythonic terms this generally means using iterators to consume the dataset in manageable pieces.</p>
"
2415028,95810,2010-03-10T06:49:57Z,2414915,9,TRUE,"<p>While languages and associated technologies/frameworks are important for scaling, they tend to pale in comparison to the importance of the algorithms, data structure, and architectures.  Forget threads: the number of cores you can exploit that way is just too limited -- you want separate processes exchanging messages, so you can scale up at least to a small cluster of servers on a fast LAN (and ideally a <em>large</em> cluster as well!-).</p>

<p>Relational databases may be an exception to ""technologies pale"" -- they can really clamp you down when you're trying to scale up a few orders of magnitude. Is that your situation -- are you worried about mere dozens or at most hundreds of servers, or are you starting to think about thousands or myriads?  In the former case, you can still stretch relational technology (e.g. by horizontal and vertical sharding) to support you -- in the latter, you're at the breaking point, or well past it, and <em>must</em> start thinking in terms of key/value stores.</p>

<p>Back to algorithms -- ""data analysis"" cover a wide range... most of my work for Google over the last few years falls in that range, e.g. in cluster management software, and currently in business intelligence.  Do you need <em>deterministic</em> analysis (e.g. for accounting purposes, where you can't possibly overlook a single penny out of 8-digit figures), or can you stand some <em>non</em>-determinism?  Most ""data mining"" applications fall into the second category -- you don't need total precision and determinism, just a good estimate of the range that your results can be proven to fall within, with, say, 95% probability.</p>

<p>This is particularly crucial if you ever need to do ""real-near-time"" data analysis -- near-real-time and 100% accuracy constraints on the same computation do <strong>not</strong> a happy camper make.  But even in bulk/batch off-line data mining, if you can deliver results that are 95% guaranteed orders of magnitude faster than it would take for 99.99% (I don't know if data mining can ever <em>be</em> 100.00%!-), that may be a wonderful tradeoff.</p>

<p>The work I've been doing over the last few years has had a few requirements for ""near-real-time"" and many more requirements for off-line, ""batch"" analysis -- and only a very few cases where absolute accuracy is an absolute must. Gradually-refined sampling (when full guaranteed accuracy is not required), especially coupled with stratified sampling (designed closely with a domain expert!!!), has proven, over and over, to be a great approach; if you don't understand this terminology, and still want to scale up, beyond the terabytes, to exabytes and petabytes' worth of processing, you desperately need a good refresher course in Stats 201, or whatever course covers these concepts in your part of the woods (or on iTunes University, or the YouTube offerings in university channels, or blip.tv's, or whatever).</p>

<p>Python, R, C++, whatever, only come into play <em>after</em> you've mastered these algorithmic issues, the architectural issues that go with them (can you design a computation architecture to ""statistically survive"" the death of a couple of servers out of your myriad, recovering to within statistically significant accuracy without a lot of rework...?), and the supporting design and storage-technology choices.</p>
"
2416720,16632,2010-03-10T12:14:30Z,2402885,3,FALSE,"<p>With ggplot2, you don't need to pre-aggregate the data:</p>

<pre><code>library(ggplot2)
qplot(online, data = df, fill = offline)
qplot(offline, data = df, fill = online)
</code></pre>
"
2417768,245603,2010-03-10T14:47:48Z,2417623,15,TRUE,"<p>The idea is that for each manual annotation you have to define not only the label, but all the variables that define the panel, color, etc. The following code adds two labels in different panels.</p>

<pre><code>pl &lt;- ggplot(funny, aes(Mndr, y=Data, group=Type, col=Type))+geom_line()
      +facet_grid(.~Institution)   #your plot
nd &lt;- data.frame(Institution=c(""Q-branch"",""Some-Ville""),  #panel
                 Type=c(""Unknown"", ""Tastes good""),        #color
                 Mndr=c(7,12),                            #x-coordinate of label
                 Data= c(170,50),                         #y-coordinate of label
                 Text=c(""Label 1"", ""Label 2""))            #label text
# add labels to plot:
pl &lt;- pl + geom_text(aes(label=Text), data=nd, hjust=0, legend=FALSE)
pl
</code></pre>

<p>The <code>legend=FALSE</code> option will ensure that the small a's denoting the text are not added to the legend. You don't have to have a data frame for the labels, you could have a separate <code>geom_text</code> for each, but I find this way simpler.</p>
"
2420132,457898,2010-03-10T20:01:31Z,2394902,0,FALSE,"<p>Well, I figured out that <code>unclass</code> function can be utilized to remove classes (who would tell, aye?!):</p>

<pre><code>library(Hmisc)
# let's presuppose that variable x is gathered through spss.get() function
# and that x is factor
&gt; class(x)
[1] ""labelled"" ""factor""
&gt; foo &lt;- unclass(x)
&gt; class(foo)
[1] ""integer""
</code></pre>

<p>It's not the luckiest solution, just imagine back-converting bunch of vectors... If anyone tops this, I'll check it as an answer...</p>
"
2429177,256662,2010-03-11T22:25:21Z,2427279,0,FALSE,"<p>I used </p>

<pre><code>formu.names &lt;- all.vars(formu)
Y.name &lt;- formu.names[1]
X.name &lt;- formu.names[2]
block.name &lt;- formu.names[3]
</code></pre>

<p>In the code I wrote about doing a post-hoc for a friedman test:</p>

<p><a href=""http://www.r-statistics.com/2010/02/post-hoc-analysis-for-friedmans-test-r-code/"" rel=""nofollow noreferrer"">http://www.r-statistics.com/2010/02/post-hoc-analysis-for-friedmans-test-r-code/</a></p>

<p>But it will only work for: Y`X|block</p>

<p>I hope for a better answer others will give.</p>
"
2430200,255531,2010-03-12T02:49:33Z,2427742,5,FALSE,"<pre><code>qplot(factor(cyl), data=mtcars, geom='bar', fill=factor(gear, level=5:3))
</code></pre>
"
2430551,212593,2010-03-12T04:43:41Z,2427279,4,TRUE,"<p>Partial answer. You can subscript a formula object to get a parse tree of the formula:</p>

<pre><code>&gt; f&lt;-a~b+c|d
&gt; f[[1]]
`~`
&gt; f[[2]]
a
&gt; f[[3]]
b + c | d
&gt; f[[3]][[1]]
`|`
&gt; f[[3]][[2]]
b + c
&gt; f[[3]][[3]]
d
</code></pre>

<p>Now all you need is code to walk this tree.</p>

<p>UPDATE: Here's is an example of a function that walks the tree.</p>

<pre><code>walker&lt;-function(formu){
  if (!is(formu,""formula""))
    stop(""Want formula"")
  lhs &lt;- formu[[2]]
  formu &lt;- formu[[3]]

  if (formu[[1]]!='|')
    stop(""Want conditional part"")

  condi &lt;- formu[[3]]

  flattener &lt;- function(f) {if (length(f)&lt;3) return(f);
                            c(Recall(f[[2]]),Recall(f[[3]]))}
  vars &lt;- flattener(formu[[2]])

  list(lhs=lhs,condi=condi,vars=vars)
}

walker(y~a+b|c)
</code></pre>

<p>Also look at the documentation for <code>terms.formula</code> and <code>terms.object</code>. Looking at the code for some functions that take conditional formulas can help, for eg. the <code>lmer</code> function in <code>lme4</code> package.</p>
"
2432517,292328,2010-03-12T12:04:44Z,1395301,-1,FALSE,"<p>Put a shortcut for the R gui into your desired directory.  Right-click and look at the shortcut properties.  Delete the entry for ""Start In"" and click OK.  When you launch the R gui from this shortcut the default directory will be the folder from which you have launched.  Copy/paste this shortcut wherever you desire.</p>
"
2435053,163053,2010-03-12T18:57:06Z,2434833,8,FALSE,"<p>You can use the <code>split</code> function:</p>

<pre><code>split(letters, as.integer((seq_along(letters) - 1) / 10))
</code></pre>

<p>If you want to make this into a new function:</p>

<pre><code>slice &lt;- function(x, n) split(x, as.integer((seq_along(x) - 1) / n))
slice(letters, 10)
</code></pre>
"
2436738,16632,2010-03-13T00:31:38Z,2427742,5,TRUE,"<p><code>qplot(factor(cyl), data=mtcars, geom=""bar"", fill=factor(gear), order = -gear)</code></p>
"
2436911,143305,2010-03-13T01:35:30Z,2436502,3,FALSE,"<p>I tend to invert it, i.e. put the <code>with()</code> outside and have <code>subset()</code> work on its data:</p>

<pre><code>R&gt; data(mtcars)
R&gt; with(subset(mtcars, gear==4), lm(mpg ~ wt)) # no data arg

Call:
lm(formula = mpg ~ wt)

Coefficients:
(Intercept)           wt  
      42.49        -6.86  
</code></pre>

<p>This is also a silly example because <code>lm(mpg ~ wt, data=mtcars, subset=gear==4)</code>
obviously does the same but you get the drift.</p>
"
2436926,66549,2010-03-13T01:40:25Z,2436688,1,FALSE,"<p>This is a straightforward way to add items to an R List:</p>

<pre><code># create an empty list:
small_list = list()

# now put some objects in it:
small_list$k1 = ""v1""
small_list$k2 = ""v2""
small_list$k3 = 1:10

# retrieve them the same way:
small_list$k1
# returns ""v1""

# ""index"" notation works as well:
small_list[""k2""]
</code></pre>

<p>Or programmatically:</p>

<pre><code>kx = paste(LETTERS[1:5], 1:5, sep="""")
vx = runif(5)
lx = list()
cn = 1

for (itm in kx) { lx[itm] = vx[cn]; cn = cn + 1 }

print(length(lx))
# returns 5
</code></pre>
"
2436960,143305,2010-03-13T01:56:18Z,2436688,224,TRUE,"<p>If it's a list of string, just use the <code>c()</code> function :</p>

<pre><code>R&gt; LL &lt;- list(a=""tom"", b=""dick"")
R&gt; c(LL, c=""harry"")
$a
[1] ""tom""

$b
[1] ""dick""

$c
[1] ""harry""

R&gt; class(LL)
[1] ""list""
R&gt; 
</code></pre>

<p>That works on vectors too, so do I get the bonus points?</p>

<p><em>Edit (2015-Feb-01):</em> This post is coming up on its fifth birthday.  Some kind readers keep repeating any shortcomings with it, so by all means also see some of the comments below. One suggestion for <code>list</code> types:</p>

<pre><code>newlist &lt;- list(oldlist, list(someobj))
</code></pre>

<p>In general, R types can make it hard to have one and just one idiom for all types and uses.</p>
"
2437039,138470,2010-03-13T02:30:07Z,2436688,5,FALSE,"<p>If you pass in the list variable as a quoted string, you can reach it from within the function like:</p>

<pre><code>push &lt;- function(l, x) {
  assign(l, append(eval(as.name(l)), x), envir=parent.frame())
}
</code></pre>

<p>so:</p>

<pre><code>&gt; a &lt;- list(1,2)
&gt; a
[[1]]
[1] 1

[[2]]
[1] 2

&gt; push(""a"", 3)
&gt; a
[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 3

&gt; 
</code></pre>

<p>or for extra credit:</p>

<pre><code>&gt; v &lt;- vector()
&gt; push(""v"", 1)
&gt; v
[1] 1
&gt; push(""v"", 2)
&gt; v
[1] 1 2
&gt; 
</code></pre>
"
2437259,212593,2010-03-13T04:25:32Z,2434833,8,TRUE,"<pre><code>slice&lt;-function(x,n) {
    N&lt;-length(x);
    lapply(seq(1,N,n),function(i) x[i:min(i+n-1,N)])
}
</code></pre>
"
2437301,212593,2010-03-13T04:51:22Z,2436129,1,FALSE,"<p>Don't have enough experience to answer (1). But the way to avoid (2) is to use a random number generator meant for parallel programs: look at the <code>rlecuyer</code> package and the <code>clusterSetupRNG</code> function in <code>snow</code>.</p>
"
2437576,133234,2010-03-13T07:04:24Z,2436735,6,TRUE,"<p>So, what is your question? :)</p>

<p>What you've described can be reproduced by <code>M-x list-colors-display</code> (or Control-click with middle mouse button, then select display colors from the pop-up menu) -- and it's an Emacs feature, nothing to do with R or ESS.  Is that what you are looking for?</p>
"
2437619,216064,2010-03-13T07:20:09Z,2437312,4,TRUE,"<p>Does</p>

<pre><code>par(mar=c(5,2,4,2))+0.1
</code></pre>

<p>help?</p>
"
2438093,457898,2010-03-13T11:06:40Z,2351204,0,FALSE,"<p>@Hadley, I've checked your response since it's quite straightforward and easy for bookkeeping (besides the fact it's more general-purpose-solution). However, here's my  not-so-long script that does the thing and requires only <code>base</code> package (which is trivial since I install <code>plyr</code> and <code>reshape</code> just after installing R). Now, here's the source:</p>

<pre><code>dfsub &lt;- function(dt, lst, fun) {
        # check whether dt is `data.frame`
        stopifnot (is.data.frame(dt))
        # convert data.frame factors to numeric
        dt &lt;- as.data.frame(lapply(dt, as.numeric))
        # check if vectors in lst are ""whole"" / integer
        # vector elements should be column indexes
        is.wholenumber &lt;- function(x, tol = .Machine$double.eps^0.5)  abs(x - round(x)) &lt; tol
        # fall if any non-integers in list
        idx &lt;- rapply(lst, is.wholenumber)
        stopifnot(idx)
        # check for list length
        stopifnot(ncol(dt) == length(idx))
        # subset the data
        subs &lt;- list()
        for (i in 1:length(lst)) {
                # apply function on each part, by row
                subs[[i]] &lt;- apply(dt[ , lst[[i]]], 1, fun)
        }
        names(subs) &lt;- names(lst)
        # convert to data.frame
        subs &lt;- as.data.frame(subs)
        # guess what =)
        return(subs)
}
</code></pre>
"
2438745,255531,2010-03-13T14:54:07Z,2438486,4,TRUE,"<p>lyx? <a href=""http://www.lyx.org/"" rel=""nofollow noreferrer"">http://www.lyx.org/</a></p>

<p>On Ubuntu: </p>

<pre><code>sudo apt-get install lyx
</code></pre>

<p>From the lyx page:</p>

<blockquote>
  <p>LyX combines the power and flexibility
  of TeX/LaTeX with the ease of use of a
  graphical interface. This results in
  world-class support for creation of
  mathematical content (via a fully
  integrated equation editor) and
  structured documents like academic
  articles, theses, and books.</p>
</blockquote>
"
2440346,168747,2010-03-13T22:50:32Z,2436502,5,TRUE,"<p>Using <code>parent.frame()</code>:</p>

<pre><code># sample data:
set.seed(2436502)
dfrm &lt;- data.frame(x1 = rnorm(100), x2 = rnorm(100), g1 = sample(letters, 100, TRUE))

# how to use it:
with(subset(dfrm, x1&lt;0), {
    str(parent.frame(2)$data)
    ""Hello!""
})

# 'data.frame':   47 obs. of  3 variables:
#  $ x1: num  -0.836 -0.343 -0.341 -1.044 -0.665 ...
#  $ x2: num  0.362 0.727 0.62 -0.178 -1.538 ...
#  $ g1: Factor w/ 26 levels ""a"",""b"",""c"",""d"",..: 11 4 15 19 8 13 22 15 15 23 ...
</code></pre>

<h3>How the magic works</h3>

<p>Using <code>ls()</code> you can inspect <code>parent.frames</code>:</p>

<pre><code>with(subset(dfrm, x1&lt;0), {
    print(ls())
    print(ls(parent.frame(1)))
    print(ls(parent.frame(2)))
    print(ls(parent.frame(3)))
})
# [1] ""g1"" ""x1"" ""x2""
# [1] ""enclos"" ""envir""  ""expr""  
# [1] ""data"" ""expr""
# [1] ""dfrm""
</code></pre>

<p>As you can see:</p>

<ul>
<li><code>parent.frame(3)</code> is base environment (in this case),</li>
<li><code>parent.frame(2)</code> is environment of <code>subset</code> function</li>
<li><code>parent.frame(1)</code> is environment of <code>{</code> function (see <code>?Paren</code>)</li>
</ul>
"
2441229,163053,2010-03-14T04:45:14Z,2441136,6,FALSE,"<p>Use a <a HREF=""http://cran.r-project.org/web/views/TimeSeries.html"" rel=""nofollow noreferrer"">time series package.</a> The xts package has functions designed specifically to do that.  Or look at the aggregate and rollapply functions in the zoo package. </p>

<p>The rmetrics ebook has a useful discussion, including a performance comparison of the various packages:  <a href=""https://www.rmetrics.org/files/freepdf/TimeSeriesFAQ.pdf"" rel=""nofollow noreferrer"">https://www.rmetrics.org/files/freepdf/TimeSeriesFAQ.pdf</a></p>

<p><em>Edit</em>: Look at <a href=""https://stackoverflow.com/questions/2134972/r-how-to-split-a-chron-date-time-object-in-zoo-for-aggregation/2135272#2135272"">my answer to this question</a>.  Basically you need to truncate every timestamp into a specific interval and then do the aggregation using those new truncated timestamps as your grouping vector.</p>
"
2441374,6148,2010-03-14T06:17:55Z,2436735,2,FALSE,"<p>If/when you find yourself in that situation, where something happened in Emacs, but you don't know how.  You can ask Emacs what the last 300 keys you pressed were with <kbd>C-h l</kbd> (which is bound to <a href=""http://www.gnu.org/software/emacs/manual/html_node/emacs/Misc-Help.html"" rel=""nofollow noreferrer""><code>'view-lossage</code></a>, and that might give you enough information as to what you did.</p>
"
2441496,133234,2010-03-14T07:30:19Z,2441136,0,FALSE,"<p>This is an interesting question; with the proliferation of the various time series packages and methods, there ought to be an approach for binning irregular time series other than by brute force that the OP suggests.  Here is one ""high-level"" way to get the intervals that you can then use for <code>aggregate</code> et al, using a version of <code>cut</code> defined for <code>chron</code> objects.</p>

<pre><code>require(chron)
require(timeSeries)

my.times &lt;- ""
2010-01-13 03:02:38 UTC
2010-01-13 03:08:14 UTC
2010-01-13 03:14:52 UTC
2010-01-13 03:20:42 UTC
2010-01-13 03:22:19 UTC
""

time.df &lt;- read.delim(textConnection(my.times),header=FALSE,sep=""\n"",strip.white=FALSE)
time.seq &lt;- seq(trunc(timeDate(time.df[1,1]),units=""hours""),by=15*60,length=nrow(time.df))
intervals &lt;- as.numeric(cut(as.chron(as.character(time.df$V1)),breaks=as.chron(as.character(time.seq))))
</code></pre>

<p>You get</p>

<pre><code>intervals  
[1] 1 1 1 2 2
</code></pre>

<p>which you can now append to the data frame and aggregate.</p>

<p>The coersion acrobatics above (from character to timeDate to character to chron) is a little unfortunate, so if there are cleaner solutions for binning irregular time data using xts or any of the other timeSeries packages, I'd love to hear about them as well!..</p>

<p>I am also curious to know what would be the most efficient approach for binning large high-frequency irregular time series, e.g. creating 1-minute volume bars on tick data for a very liquid stock.</p>
"
2441538,217595,2010-03-14T07:51:44Z,2438486,1,FALSE,"<p>If you want to produce Latex with a simpler markup you could use the ASCII package that has a Sweave driver that can be used with reSTructured text, which can then be converted to Latex.  Although I would only use it if you want to be able to convert the same doc also to html or odf. In any case it is a good idea to learn the basic Latex.</p>
"
2441650,133234,2010-03-14T08:52:45Z,2441562,9,TRUE,"<p>Your code almost works -- but remember that R creates copies of the objects that you modify (i.e. pass-by-value semantics).  So you need to explicitly assign the new string to colnames, like so:</p>

<pre><code>dataA &lt;- dataB &lt;- data.frame(matrix(1:20,ncol=5))
names(dataA) &lt;- c(""foo"",""code"",""lp15"",""bar"",""lh15"")
names(dataB) &lt;- c(""a"",""code"",""lp50"",""ls50"",""foo"")
dataList &lt;- list(dataA, dataB)
f &lt;- function(i, xList) {
  colnames(xList[[i]]) &lt;- gsub(pattern=c(""foo|bar""), replacement=c(""baz""), x=colnames(xList[[i]]))
  xList[[i]]
}
dataList &lt;- lapply(seq(dataList), f, xList=dataList)
</code></pre>

<p>The new list will have data frames with the replaced names.  In terms of replacing both foo and bar, just use an alternate pattern in the regex in gsub (""foo|bar"").</p>

<p>Note, by the way, that you don't have to do this by indexing into your list -- just use a function that operates on the elements of your list directly:</p>

<pre><code>f &lt;- function(df) {
  colnames(df) &lt;- gsub(pattern=c(""foo|bar""), replacement=c(""baz""), x=colnames(df))
  df
}
dataList &lt;- lapply(dataList, f)
</code></pre>
"
2441839,216064,2010-03-14T10:33:24Z,2438486,1,FALSE,"<p>The online text processor <a href=""http://www.zoho.com"" rel=""nofollow noreferrer"">zoho</a> allows export to latex. Maybe this can be helpful to learn latex, but I do not know how to integrate Sweave/R in this. (I did not work with zoho, by the way).</p>
"
2441931,168747,2010-03-14T11:12:29Z,2441136,6,TRUE,"<p>Standard functions to split vectors are <code>cut</code> and <code>findInterval</code>:</p>

<pre><code>v &lt;- as.POSIXct(c(
  ""2010-01-13 03:02:38 UTC"",
  ""2010-01-13 03:08:14 UTC"",
  ""2010-01-13 03:14:52 UTC"",
  ""2010-01-13 03:20:42 UTC"",
  ""2010-01-13 03:22:19 UTC""
))

# Your function return list:
interv(v, as.POSIXlt(""2010-01-13 03:00:00 UTC""), 900)
# [[1]]
# [1] ""2010-01-13 03:00:00""
# [[2]]
# [1] ""2010-01-13 03:00:00""
# [[3]]
# [1] ""2010-01-13 03:00:00""
# [[4]]
# [1] ""2010-01-13 03:15:00 CET""
# [[5]]
# [1] ""2010-01-13 03:15:00 CET""

# cut returns factor, you must provide proper breaks:
cut(v, as.POSIXlt(""2010-01-13 03:00:00 UTC"")+0:2*900)
# [1] 2010-01-13 03:00:00 2010-01-13 03:00:00 2010-01-13 03:00:00
# [4] 2010-01-13 03:15:00 2010-01-13 03:15:00
# Levels: 2010-01-13 03:00:00 2010-01-13 03:15:00

# findInterval returns vector of interval id (breaks like in cut)
findInterval(v, as.POSIXlt(""2010-01-13 03:00:00 UTC"")+0:2*900)
# [1] 1 1 1 2 2
</code></pre>

<p>For the record: <code>cut</code> has a method for <code>POSIXt</code> type, but unfortunately there is no way to provide <code>start</code> argument, effect is:</p>

<pre><code>cut(v,""15 min"")
# [1] 2010-01-13 03:02:00 2010-01-13 03:02:00 2010-01-13 03:02:00
# [4] 2010-01-13 03:17:00 2010-01-13 03:17:00
# Levels: 2010-01-13 03:02:00 2010-01-13 03:17:00
</code></pre>

<p>As you see it's start at 03:02:00. You could mess with labels of output factor (convert labels to time, round somehow and convert back to character).</p>
"
2443443,291831,2010-03-14T19:29:19Z,2443127,1,FALSE,"<p>Interesting problem and agree that R is cool, but somehow i find R to be a bit cumbersome in this respect. I seem to prefer to get the data in intermediate plain text form first in order to be able to verify that the data is correct in every step...  If the data is ready in its final form or for uploading your data somewhere RCurl is very useful.</p>

<p>Simplest in my opinion would be to (on linux/unix/mac/or in cygwin) just mirror the entire <a href=""http://gtrnadb.ucsc.edu/"" rel=""nofollow noreferrer"">http://gtrnadb.ucsc.edu/</a> site (using wget) and take the files named <em>/</em>-structs.html, sed or awk the data you would like and format it for reading into R.</p>

<p>I'm sure there would be lots of other ways also. </p>
"
2443773,143305,2010-03-14T21:01:22Z,2443556,4,FALSE,"<p>Are you by chance looking for the <a href=""http://cran.r-project.org/package=lme4"" rel=""nofollow noreferrer"">lme4</a> package which is focussed on linear mixed-effects (ie. fixed versus random) modelling?</p>
"
2444750,168137,2010-03-15T02:28:02Z,2444686,2,FALSE,"<p>If you don't want to <code>cbind</code> p1...p10. </p>

<p>Then try:</p>

<p><code>data.frame(p, fpartnumber, ftest, finspector)</code></p>

<p>and: </p>

<p><code>str(p)</code></p>

<p>Using:</p>

<pre><code>p = c(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10) 
</code></pre>

<p>instead</p>
"
2444771,144537,2010-03-15T02:35:21Z,2443127,16,TRUE,"<p>Tal,</p>

<p>You could use R and the <code>XML</code> package to do this, but (damn) that is some poorly formed HTML you are trying to parse.  In fact, in most cases your would want to be using the <code>readHTMLTable()</code> function, <a href=""https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package"">which is covered in this previous thread</a>.</p>

<p>Given this ugly HTML, however, we will have to use the <code>RCurl</code> package to pull the raw HTML and create some custom functions to parse it.  This problem has two components:</p>

<ol>
<li>Get all of the genome URLS from the base webpage (<a href=""http://gtrnadb.ucsc.edu/"" rel=""nofollow noreferrer"">http://gtrnadb.ucsc.edu/</a>) using the <code>getURLContent()</code> function in the <code>RCurl</code>package and some regex magic :-)</li>
<li>Then take that list of URLS and scrape the data you are looking for, and then stick it into a <code>data.frame</code>.</li>
</ol>

<p>So, here goes...</p>

<pre><code>library(RCurl)

### 1) First task is to get all of the web links we will need ##
base_url&lt;-""http://gtrnadb.ucsc.edu/""
base_html&lt;-getURLContent(base_url)[[1]]
links&lt;-strsplit(base_html,""a href="")[[1]]

get_data_url&lt;-function(s) {
    u_split1&lt;-strsplit(s,""/"")[[1]][1]
    u_split2&lt;-strsplit(u_split1,'\\""')[[1]][2]
    ifelse(grep(""[[:upper:]]"",u_split2)==1 &amp; length(strsplit(u_split2,""#"")[[1]])&lt;2,return(u_split2),return(NA))
}

# Extract only those element that are relevant
genomes&lt;-unlist(lapply(links,get_data_url))
genomes&lt;-genomes[which(is.na(genomes)==FALSE)]

### 2) Now, scrape the genome data from all of those URLS ###

# This requires two complementary functions that are designed specifically
# for the UCSC website. The first parses the data from a -structs.html page
# and the second collects that data in to a multi-dimensional list
parse_genomes&lt;-function(g) {
    g_split1&lt;-strsplit(g,""\n"")[[1]]
    g_split1&lt;-g_split1[2:5]
    # Pull all of the data and stick it in a list
    g_split2&lt;-strsplit(g_split1[1],""\t"")[[1]]
    ID&lt;-g_split2[1]                             # Sequence ID
    LEN&lt;-strsplit(g_split2[2],"": "")[[1]][2]     # Length
    g_split3&lt;-strsplit(g_split1[2],""\t"")[[1]]
    TYPE&lt;-strsplit(g_split3[1],"": "")[[1]][2]    # Type
    AC&lt;-strsplit(g_split3[2],"": "")[[1]][2]      # Anticodon
    SEQ&lt;-strsplit(g_split1[3],"": "")[[1]][2]     # ID
    STR&lt;-strsplit(g_split1[4],"": "")[[1]][2]     # String
    return(c(ID,LEN,TYPE,AC,SEQ,STR))
}

# This will be a high dimensional list with all of the data, you can then manipulate as you like
get_structs&lt;-function(u) {
    struct_url&lt;-paste(base_url,u,""/"",u,""-structs.html"",sep="""")
    raw_data&lt;-getURLContent(struct_url)
    s_split1&lt;-strsplit(raw_data,""&lt;PRE&gt;"")[[1]]
    all_data&lt;-s_split1[seq(3,length(s_split1))]
    data_list&lt;-lapply(all_data,parse_genomes)
    for (d in 1:length(data_list)) {data_list[[d]]&lt;-append(data_list[[d]],u)}
    return(data_list)
}

# Collect data, manipulate, and create data frame (with slight cleaning)
genomes_list&lt;-lapply(genomes[1:2],get_structs) # Limit to the first two genomes (Bdist &amp; Spurp), a full scrape will take a LONG time
genomes_rows&lt;-unlist(genomes_list,recursive=FALSE) # The recursive=FALSE saves a lot of work, now we can just do a straigh forward manipulation
genome_data&lt;-t(sapply(genomes_rows,rbind))
colnames(genome_data)&lt;-c(""ID"",""LEN"",""TYPE"",""AC"",""SEQ"",""STR"",""NAME"")
genome_data&lt;-as.data.frame(genome_data)
genome_data&lt;-subset(genome_data,ID!=""&lt;/PRE&gt;"")   # Some malformed web pages produce bad rows, but we can remove them

head(genome_data)
</code></pre>

<p>The resulting data frame contains seven columns related to each genome entry: ID, length, type, sequence, string, and name.  The name column contains the base genome, which was my best guess for data organization.  Here it what it looks like:</p>

<pre><code>head(genome_data)
                                   ID   LEN TYPE                           AC                                                                       SEQ
1     Scaffold17302.trna1 (1426-1498) 73 bp  Ala     AGC at 34-36 (1459-1461) AGGGAGCTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGtACCGGGATCGATGCCCGGGTTTTCCA
2   Scaffold20851.trna5 (43038-43110) 73 bp  Ala   AGC at 34-36 (43071-43073) AGGGAGCTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGtACCGGGATCGATGCCCGGGTTCTCCA
3   Scaffold20851.trna8 (45975-46047) 73 bp  Ala   AGC at 34-36 (46008-46010) TGGGAGCTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGtACCGGGATCGATGCCCGGGTTCTCCA
4     Scaffold17302.trna2 (2514-2586) 73 bp  Ala     AGC at 34-36 (2547-2549) GGGGAGCTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGtACAGGGATCGATGCCCGGGTTCTCCA
5 Scaffold51754.trna5 (253637-253565) 73 bp  Ala AGC at 34-36 (253604-253602) CGGGGGCTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGtACCGGGATCGATGCCCGGGTCCTCCA
6     Scaffold17302.trna4 (6027-6099) 73 bp  Ala     AGC at 34-36 (6060-6062) GGGGAGCTAGCTCAGATGGTAGAGCGCTCGCTTAGCATGCGAGAGGtACCGGGATCGATGCCCGAGTTCTCCA
                                                                        STR  NAME
1 .&gt;&gt;&gt;&gt;&gt;&gt;..&gt;&gt;&gt;&gt;........&lt;&lt;&lt;&lt;.&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;.....&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;.. Spurp
2 .&gt;&gt;&gt;&gt;&gt;&gt;..&gt;&gt;&gt;&gt;........&lt;&lt;&lt;&lt;.&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;.....&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;.. Spurp
3 .&gt;&gt;&gt;&gt;&gt;&gt;..&gt;&gt;&gt;&gt;........&lt;&lt;&lt;&lt;.&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;.....&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;.. Spurp
4 &gt;&gt;&gt;&gt;&gt;&gt;&gt;..&gt;&gt;&gt;&gt;........&lt;&lt;&lt;&lt;.&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;.....&gt;.&gt;&gt;&gt;.......&lt;&lt;&lt;.&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;. Spurp
5 .&gt;&gt;&gt;&gt;&gt;&gt;..&gt;&gt;&gt;&gt;........&lt;&lt;&lt;&lt;.&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;.....&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;.. Spurp
6 &gt;&gt;&gt;&gt;&gt;&gt;&gt;..&gt;&gt;&gt;&gt;........&lt;&lt;&lt;&lt;.&gt;&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;&lt;......&gt;&gt;&gt;&gt;.......&lt;&lt;&lt;&lt;.&lt;&lt;&lt;&lt;&lt;&lt;&lt;. Spurp
</code></pre>

<p>I hope this helps, and thanks for the fun little Sunday afternoon R challenge!</p>
"
2445058,133234,2010-03-15T04:17:14Z,2444973,6,TRUE,"<p>To answer your first question: you could simply reorder factor levels so that they are no longer alphabetical, like so:</p>

<pre><code>spark$Alarm&lt;-factor(spark$Alarm, levels(spark$Alarm)[c(1,4,2,3)])
</code></pre>

<p>For the second question, you could write your own labeller function so associate Alarms and Ranks, something like</p>

<pre><code>lbl.fn &lt;- function(variable, value) {  paste(spark$Rank[which(as.character(spark$Alarm)==as.character(value))],as.character(value)) }
s + facet_grid(Alarm ~ ., labeller=""lbl.fn"") + geom_line()
</code></pre>
"
2447474,190597,2010-03-15T13:38:19Z,2447454,33,TRUE,"<p>You need to add </p>

<pre><code>import rpy2.robjects.numpy2ri
rpy2.robjects.numpy2ri.activate()
</code></pre>

<p>See <a href=""http://rpy.sourceforge.net/rpy2/doc-2.2/html/numpy.html"" rel=""noreferrer"">http://rpy.sourceforge.net/rpy2/doc-2.2/html/numpy.html</a>:</p>

<blockquote>
  <p>That import alone is sufficient to
  switch an automatic conversion of
  numpy objects into rpy2 objects.</p>
  
  <p>Why make this an optional import,
  while it could have been included in
  the function py2ri() (as done in the
  original patch submitted for that
  function) ?</p>
  
  <p>Although both are valid and reasonable
  options, the design decision was taken
  in order to decouple rpy2 from numpy
  the most, and do not assume that
  having numpy installed automatically
  meant that a programmer wanted to use
  it.</p>
</blockquote>

<p><strong>edit:</strong>
With the rpy2 series 2.2.x, the import alone is no longer sufficient. The conversion needs to be explicitly activated.</p>
"
2448285,209467,2010-03-15T15:31:21Z,2443095,5,TRUE,"<p>In the source code of the <strong>sos</strong> package, findFn.R, line 80, I found the mistake</p>

<pre><code>  if (substr(string, 1, 1) != ""{"")
    string &lt;- gsub("" "", ""+"", string)
</code></pre>

<p>This ""if"" is wrong, with an != instead of ==, and therefore the space doesn't get translated into a +. The quick solution would be to use the ""+"" syntax yourself</p>

<p>so:</p>

<pre><code>&gt; findFn(""{randomization+test}"")
found 19 matches
</code></pre>
"
2449162,163053,2010-03-15T17:34:54Z,2449083,3,FALSE,"<p>There's no need to use a loop here.  </p>

<p>Try this:</p>

<pre><code>gcCount &lt;-  function(line, st, sp){
  chars = strsplit(as.character(line),"""")[[1]][st:sp]
  length(which(tolower(chars) == ""g"" | tolower(chars) == ""c""))
}
</code></pre>
"
2449175,58681,2010-03-15T17:37:32Z,2449083,6,FALSE,"<p>A one liner: </p>

<pre><code>table(strsplit(toupper(a), '')[[1]])
</code></pre>
"
2449294,169947,2010-03-15T17:56:48Z,2436688,5,FALSE,"<p>You want something like this maybe?</p>

<pre><code>&gt; push &lt;- function(l, x) {
   lst &lt;- get(l, parent.frame())
   lst[length(lst)+1] &lt;- x
   assign(l, lst, envir=parent.frame())
 }
&gt; a &lt;- list(1,2)
&gt; push('a', 6)
&gt; a
[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 6
</code></pre>

<p>It's not a very polite function (assigning to <code>parent.frame()</code> is kind of rude) but IIUYC it's what you're asking for.</p>
"
2449441,169947,2010-03-15T18:20:40Z,2449083,14,TRUE,"<p>Better to not split at all, just count the matches:</p>

<pre><code>gcCount2 &lt;-  function(line, st, sp){
  sum(gregexpr('[GCgc]', substr(line, st, sp))[[1]] &gt; 0)
}
</code></pre>

<p>That's an order of magnitude faster.</p>

<p>A small C function that just iterates over the characters would be yet another order of magnitude faster.</p>
"
2449667,245603,2010-03-15T18:56:46Z,2449226,4,TRUE,"<p>The <code>boot</code> library is convenient for bootstrap and permutation tests, but it will not perform exact tests (which is OK most of the time). The <code>coin</code> library implements exact randomization tests as well.</p>
"
2450649,294322,2010-03-15T21:32:27Z,2443127,1,FALSE,"<p>Just tried it using Mozenda (<a href=""http://www.mozenda.com"" rel=""nofollow noreferrer"">http://www.mozenda.com</a>).  After roughly 10 minutes and I had an agent that could scrape the data as you describe.  You may be able to get all of this data just using their free trial.  Coding is fun, if you have time, but it looks like you may already have a solution coded for you.  Nice job Drew.</p>
"
2451424,240358,2010-03-16T00:34:47Z,2451411,0,FALSE,"<p>You asked specifically for ""A through H, then 0-9 or 10-12"". Call the exception ""InvalidInputException"" or any similarly named object- ""Not Valid"" ""Input"" ""Exception""</p>

<pre><code>/^[A-H]([0-9]|(1[0-2]))$/
</code></pre>

<p>In Pseudocode: </p>

<pre><code>validateData(String data)
  if not data.match(""/^[A-H]([0-9]|(1[0-2]))$/"")
    throw InvalidInputException
</code></pre>
"
2451552,133234,2010-03-16T01:14:05Z,2451411,2,TRUE,"<p>In R code, using David's regex:  [edited to reflect Marek's suggestion]</p>

<pre><code>validate.input &lt;- function(x){
  match &lt;- grepl(""^[A-Ha-h]([0-9]|(1[0-2]))$"",x,perl=TRUE)
  ## as Marek points out, instead of testing the length of the vector
  ## returned by grep() (which will return the index of the match and integer(0) 
  ## if there are no matches), we can use grepl()
  if(!match) stop(""invalid input"")
  list(well_row=substr(x,1,1), well_col=as.integer(substr(x,2,nchar(x))))
}
</code></pre>

<p>This simply produces an error.  If you want finer control over error handling, look up the documentation for <code>tryCatch</code>, here's a primitive usage example (instead of getting an error as before we'll return NA):</p>

<pre><code>validate.and.catch.error &lt;- function(x){
  tryCatch(validate.input(x), error=function(e) NA)
}
</code></pre>

<p>Finally, note that you can use <code>substr</code> to extract your letters and numbers instead of doing strsplit.</p>
"
2452864,89482,2010-03-16T08:13:35Z,2449083,4,FALSE,"<p>I don't know that it's any faster, but you might want to look at the R package seqinR - <a href=""http://pbil.univ-lyon1.fr/software/seqinr/home.php?lang=eng"" rel=""nofollow noreferrer"">http://pbil.univ-lyon1.fr/software/seqinr/home.php?lang=eng</a>.  It is an excellent, general bioinformatics package with many methods for sequence analysis.  It's in CRAN (which seems to be down as I write this).</p>

<p>GC content would be:</p>

<pre><code>mysequence &lt;- s2c(""agtctggggggccccttttaagtagatagatagctagtcgta"")
    GC(mysequence)  # 0.4761905
</code></pre>

<p>That's from a string, you can also read in a fasta file using ""read.fasta()"".</p>
"
2453619,144157,2010-03-16T10:41:24Z,2453326,138,TRUE,"<p>Use the <code>partial</code> argument of <code>sort()</code>. For the second highest value:</p>

<pre><code>n &lt;- length(x)
sort(x,partial=n-1)[n-1]
</code></pre>
"
2453860,134830,2010-03-16T11:22:08Z,2453462,2,FALSE,"<p>You can simplify the task by using run length encoding.</p>

<p>First, convert <code>Str</code> to be a vector of individual characters, then call <code>rle</code>.</p>

<pre><code>split_Str &lt;- strsplit(Str, """")[[1]]
rle_Str &lt;- rle(split_Str)

Run Length Encoding
  lengths: int [1:14] 7 2 4 8 4 1 5 7 5 5 ...
  values : chr [1:14] ""&gt;"" ""."" ""&gt;"" ""."" ""&lt;"" ""."" ""&gt;"" ""."" ""&lt;"" ""."" ""&gt;"" ""."" ""&lt;"" "".""
</code></pre>

<p>Now you just need to parse <code>rle_Str$values</code>, which is perhaps simpler.  For instance, an inner stem will always look like <code>""&gt;"" ""."" ""&lt;""</code>.</p>

<p>I think the main thing that you need to think about is the structure of the data.  Does a <code>"".""</code> always have to come between <code>""&gt;""</code> and <code>""&lt;""</code>, or is it optional?  Can you have a <code>"".""</code> at the start? Do you need to be able to generalise to stems within stems within stems, or even more complex structures?</p>

<p>Once you have this solved, contructing your list output should be straightforward.</p>

<p>Also, don't worry about using loops, they are in the language because they are useful.  Get the thing working first, then worry about speed optimisations (if you really have to) afterwards.</p>
"
2454018,143591,2010-03-16T11:49:31Z,2453326,38,FALSE,"<p>Slightly slower alternative, just for the records:</p>

<pre><code>x &lt;- c(12.45,34,4,0,-234,45.6,4)
max( x[x!=max(x)] )
min( x[x!=min(x)] )
</code></pre>
"
2456890,295025,2010-03-16T18:05:41Z,2403466,3,FALSE,"<p>posting this here since it's related:</p>

<p>If you want the name of the variable itself as well as the levels/values of the variable to be evaluated as an expression (i.e. rendered as if they were latex), try this:</p>

<pre><code>label_parseall &lt;- function(variable, value) {
    plyr::llply(value, function(x) parse(text = paste(variable, 
        x, sep = ""=="")))
}
</code></pre>

<p>Example:</p>

<pre><code>data &lt;- data.frame(x = runif(10), y = runif(10), 
    gamma = sample(c(""gamma[0]"", ""gamma[1]""), 10, rep = T))
ggplot(data, aes(x, y)) + geom_point() + facet_grid(~gamma, 
    labeller = label_parselabel) 
</code></pre>

<p><img src=""https://i.stack.imgur.com/Nlfp4.png"" alt=""enter image description here""></p>

<p>image at <a href=""http://img709.imageshack.us/img709/1168/parseall.png"" rel=""nofollow noreferrer"">http://img709.imageshack.us/img709/1168/parseall.png</a></p>
"
2456941,170792,2010-03-16T18:13:16Z,2456864,16,TRUE,"<pre><code>LETTERS

""A"" ""B"" ""C"" ""D"" ""E"" ""F"" ""G"" ""H"" ""I"" ""J"" ""K"" ""L"" ""M"" ""N"" ""O"" ""P"" ""Q"" ""R"" ""S"" ""T"" ""U"" ""V"" ""W"" ""X"" ""Y"" ""Z""
</code></pre>
"
2457005,165787,2010-03-16T18:20:57Z,2456864,10,FALSE,"<pre><code>&gt; LETTERS
 [1] ""A"" ""B"" ""C"" ""D"" ""E"" ""F"" ""G"" ""H"" ""I"" ""J"" ""K"" ""L"" ""M"" ""N"" ""O"" ""P"" ""Q"" ""R"" ""S"" ""T"" ""U"" ""V"" ""W"" ""X""
[25] ""Y"" ""Z""
&gt; letters
 [1] ""a"" ""b"" ""c"" ""d"" ""e"" ""f"" ""g"" ""h"" ""i"" ""j"" ""k"" ""l"" ""m"" ""n"" ""o"" ""p"" ""q"" ""r"" ""s"" ""t"" ""u"" ""v"" ""w"" ""x""
[25] ""y"" ""z""
&gt; LETTERS[5:10]
[1] ""E"" ""F"" ""G"" ""H"" ""I"" ""J""
&gt; 
</code></pre>
"
2457212,143305,2010-03-16T18:54:14Z,2457129,20,TRUE,"<p>The help page actually hints at a difference: </p>

<pre><code>Value:

     ‘as.POSIXct’ and ‘as.POSIXlt’ return an object of the appropriate
     class.  If ‘tz’ was specified, ‘as.POSIXlt’ will give an
     appropriate ‘""tzone""’ attribute.
</code></pre>

<p>This stuff is finicky -- I think there is an implicit TZ conversion happening for <code>as.POSIXct</code>.  Consider that </p>

<pre><code>R&gt; print(as.numeric(as.POSIXct(as.POSIXlt(1268736919, 
                               origin=""1970-01-01""))), digits=10)
[1] 1268736919
R&gt; print(as.numeric(as.POSIXct(1268736919, origin=""1970-01-01"")), digits=10)
[1] 1268758519
</code></pre>

<p>the second one (using <code>as.POSIXct</code>) does not return the original input.  Unfortunately, Brian D. Ripley seems to be the only human having all the details here.</p>

<p>Lastly, you can't do it without the origin. But you could define wrappers that use the epoch as origin (as here) or use 2000-01-01 or ...  Just keep it consistent.</p>
"
2457401,23056,2010-03-16T19:20:18Z,2457112,2,TRUE,"<p>You're missing a semi-colon when specifying the <code>HashMap</code> return type.</p>

<pre><code>.jcall(javaObj, ""Ljava/util/HashMap;"", ""getDbInfoMap"")
</code></pre>

<p>See <a href=""http://java.sun.com/docs/books/jni/html/types.html#70113"" rel=""nofollow noreferrer"">JNI Types Field Descriptors</a> and <a href=""http://www.rforge.net/rJava/"" rel=""nofollow noreferrer"">examples of .jcall</a>.</p>
"
2457908,172261,2010-03-16T20:34:00Z,2457871,4,FALSE,"<p>Try <code>grep</code>:</p>

<pre><code>R&gt; grep(""G"", LETTERS)
[1] 7
</code></pre>
"
2458016,163053,2010-03-16T20:47:52Z,2457871,7,TRUE,"<p>Or <code>which</code>:</p>

<pre><code>which(LETTERS==""G"")
</code></pre>

<p>The <code>which</code> function is designed specifically for this purpose:</p>

<blockquote>
  <p>Give the 'TRUE' indices of a logical
  object, allowing for array indices.</p>
</blockquote>

<p>The <code>which</code> function can also return the index for logical <code>TRUE</code> values in a matrix by setting the arr.ind argument to TRUE (this is very useful).</p>

<pre><code>&gt; which(matrix(LETTERS, nrow=5)==""G"")
[1] 7
&gt; which(matrix(LETTERS, nrow=5)==""G"", arr.ind=TRUE)
     row col
[1,]   2   2
</code></pre>

<p>You may also want to read <a href=""http://userprimary.net/posts/2010/03/14/r-internals-quick-tour/"" rel=""noreferrer"">this recent blog post from Seth Falcon</a> where he talks about optimizing it in C.</p>
"
2458180,163053,2010-03-16T21:12:00Z,2458013,20,FALSE,"<p><code>fix</code> is the best way that I know of doing this, although you can also use <code>edit</code> and re-assign it:</p>

<pre><code>foo &lt;- edit(foo)
</code></pre>

<p>This is what <code>fix</code> does internally.  You might want to do this if you wanted to re-assign your changes to a different name.</p>
"
2458358,66549,2010-03-16T21:40:09Z,2458013,7,FALSE,"<p>You can use the '<strong>body</strong>' function. This function will return the body of function:</p>

<pre><code>fnx = function(a, b) { return(a^2 + 7*a + 9)}
body(fnx)
# returns the body of the function
</code></pre>

<p>So a good way to 'edit' a function is to use 'body' on the left-hand side of an assignment statement:</p>

<pre><code>body(fnx) = expression({a^2 + 11*a + 4})
</code></pre>
"
2458377,258334,2010-03-16T21:43:08Z,2458013,18,TRUE,"<p>Or take a look at the debugging function <code>trace()</code>. It is probably not exactly what you are looking for but it lets you play around with the changes and it has the nice feature that you can always go back to your original function with <code>untrace()</code>.
<code>trace()</code> is part of the <code>base</code> package and comes with a nice and thorough help page. </p>

<p>Start by calling <code>as.list (body(foo))</code> to see all the lines of your code.</p>

<pre><code>as.list(body(foo))
[[1]]
`{`

[[2]]
line1 &lt;- x

[[3]]
line2 &lt;- 0

[[4]]
line3 &lt;- line1 + line2

[[5]]
return(line3)
</code></pre>

<p>Then you simply define what to add to your function and where to place it by defining the arguments in <code>trace()</code>.</p>

<pre><code>trace (foo, quote(line2 &lt;- 2), at=4)
foo (2)
[1] 4
</code></pre>

<p>I said in the beginning that <code>trace()</code> might not be exactly what you are looking for since you didn't really change your third line of code and instead simply reassigned the value to the object <code>line2</code> in the following, inserted line of code. It gets clearer if you print out the code of your now traced function</p>

<pre><code>body (foo)
{
    line1 &lt;- x
    line2 &lt;- 0
    {
        .doTrace(line2 &lt;- 2, ""step 4"")
        line3 &lt;- line1 + line2
    }
    return(line3)
}
</code></pre>
"
2460943,255531,2010-03-17T09:14:02Z,2456696,6,TRUE,"<p>Here is another way, dropping NAs when sum the vectors:</p>

<pre><code>df &lt;- data.frame(vector1, vector2, vector3, vector4)
rowSums(df, na.rm=T)
</code></pre>
"
2460964,168747,2010-03-17T09:16:33Z,2457871,6,FALSE,"<p>Just for the notice: I think you wanted <code>match(""G"",LETTERS)</code> which gives you <code>7</code>.<br>
Benefits of this solution over <code>grep</code> or <code>which</code> is that you could use it on vector of letters:</p>

<pre><code>match(c(""S"",""T"",""A"",""C"",""K"",""O"",""V"",""E"",""R"",""F"",""L"",""O"",""W""), LETTERS)
# gives:
# [1] 19 20  1  3 11 15 22  5 18  6 12 15 23
</code></pre>
"
2461867,457898,2010-03-17T11:42:54Z,2456696,1,FALSE,"<p>Actually it's not as easy as it may seem. I reckon you want to get rid of NA's and replace them with 0 (zeros). Yet another solution is:</p>

<pre><code># create dummy variables
set.seed(1234)
x &lt;- round(rnorm(10, 15, 3.2))
y &lt;- round(runif(10, 12, 27))
z &lt;- round(rpois(n = 10, lambda = 5))
# create some NA's
x[c(2,3)] &lt;- NA
y[c(1,3,7)] &lt;- NA
z[c(3,6,10)] &lt;- NA
</code></pre>

<p>And now, if you do:</p>

<pre><code>x + y + z  # the result is:
[1] NA NA NA 20 31 41 NA 39 37 25
</code></pre>

<p>So run:</p>

<pre><code>x[is.na(x)] &lt;- 0
y[is.na(y)] &lt;- 0
z[is.na(z)] &lt;- 0
</code></pre>

<p>hence:</p>

<pre><code>x + y + z  # yields:
[1] 16 21  0 25 34 41 16 42 48 25
</code></pre>

<p>But, frankly, I recommend that you stick with <b>@xiechao</b>'s solution!
It's quite easy and straightforward!</p>
"
2463201,143305,2010-03-17T14:50:33Z,2462708,3,FALSE,"<p>Couple of guesses:</p>

<ul>
<li>you are running vim-tiny (which doesn't have syntax highlighting) instead of vim-full or vim-gnome</li>
<li>you do not have 'syntax on' in ~/.vimrc</li>
<li>you have a permissions problem on the snippet </li>
</ul>
"
2463527,158065,2010-03-17T15:26:14Z,2456696,0,FALSE,"<pre><code>do.call(""+"", list(vector1, vector2, vector3, vector4))
</code></pre>
"
2463718,143305,2010-03-17T15:50:43Z,2463437,4,FALSE,"<p>I think you can't do much better than the <a href=""http://cran.r-project.org/package=inline"" rel=""nofollow noreferrer""><strong>inline</strong></a> package (which supports C, C++ and Fortran):</p>

<pre><code>library(inline)
fun &lt;- cfunction(signature(x=""ANY""), 
                 body='printf(""Hello, world\\n""); return R_NilValue;')
res &lt;- fun(NULL)
</code></pre>

<p>which will print 'Hello, World' for you.  And you don't even know where / how / when the compiler and linker are invoked. [ The R_NilValue is R's NULL version of a SEXP and the <code>.Call()</code> signature used here requires that you return a SEXP -- see the 'Writing R Extensions' manual which you can't really avoid here. ]</p>

<p>You will then take such code and wrap it in a package.  We had great success with using 
<a href=""http://cran.r-project.org/package=inline"" rel=""nofollow noreferrer""><strong>inline</strong></a> for the 
<a href=""http://cran.r-project.org/package=Rcpp"" rel=""nofollow noreferrer""><strong>Rcpp</strong></a> unit tests (over 200 and counting now) and some of the examples.</p>

<p>Oh, and this <a href=""http://cran.r-project.org/package=inline"" rel=""nofollow noreferrer""><strong>inline</strong></a> example will work on any OS. Even Windoze provided you have the R package building tool chain installed, in the PATH etc pp.</p>

<p><strong><em>Edit:</em></strong> I misread the question.  What you want is essentially what the <a href=""http://dirk.eddelbuettel.com/code/littler.html"" rel=""nofollow noreferrer""><strong>littler</strong></a> front-end does (using pure C) and what the <a href=""http://cran.r-project.org/package=RInside"" rel=""nofollow noreferrer""><strong>RInside</strong></a> classes factored-out for C++.  </p>

<p>Jeff and I never bothered with porting <a href=""http://dirk.eddelbuettel.com/code/littler.html"" rel=""nofollow noreferrer""><strong>littler</strong></a> to Windoze, but <a href=""http://cran.r-project.org/package=RInside"" rel=""nofollow noreferrer""><strong>RInside</strong></a> did work there in most-recent release. So you should be able to poke around the build recipes and create a C-only variant of <a href=""http://cran.r-project.org/package=RInside"" rel=""nofollow noreferrer""><strong>RInside</strong></a> so that you can feed expression to an embedded R process.  I suspect that you still want something like <a href=""http://cran.r-project.org/package=Rcpp"" rel=""nofollow noreferrer""><strong>Rcpp</strong></a> for the clue as it gets tedious otherwise. </p>

<p><strong><em>Edit 2:</em></strong> And as Shane mentions, there are indeed a few examples in the R sources in tests/Embedding/ along with a Makefile.win.  Maybe that is the simplest start if you're willing to learn about R internals.</p>
"
2463996,163053,2010-03-17T16:25:48Z,2463437,10,TRUE,"<p>You want to call R from C?  </p>

<p>Look at <a href=""http://cran.r-project.org/doc/manuals/R-exts.html#Embedding-R-under-Unix_002dalikes"" rel=""nofollow noreferrer"">section 8.1 in the Writing R Extensions</a> manual. You should also look into the ""tests"" directory (download the source package extract it and you'll have the tests directory).  A similar question was previously asked on R-Help and <a href=""http://n4.nabble.com/Qs-on-calling-R-from-C-td923995.html"" rel=""nofollow noreferrer"">here was the example</a>:</p>

<pre><code>#include &lt;Rinternals.h&gt; 
#include &lt;Rembedded.h&gt; 

SEXP hello() { 
  return mkString(""Hello, world!\n""); 
} 

int main(int argc, char **argv) { 
  SEXP x; 
  Rf_initEmbeddedR(argc, argv); 
  x = hello(); 
  return x == NULL;             /* i.e. 0 on success */ 
} 
</code></pre>

<p>The simple example from the R manual is like so:</p>

<pre><code> #include &lt;Rembedded.h&gt;

 int main(int ac, char **av)
 {
     /* do some setup */
     Rf_initEmbeddedR(argc, argv);
     /* do some more setup */

     /* submit some code to R, which is done interactively via
         run_Rmainloop();

         A possible substitute for a pseudo-console is

         R_ReplDLLinit();
         while(R_ReplDLLdo1() &gt; 0) {
           add user actions here if desired
         }
      */
     Rf_endEmbeddedR(0);
     /* final tidying up after R is shutdown */
     return 0;
 }
</code></pre>

<p>Incidentally, you might want to consider using <strong>Rinside</strong> instead: Dirk provides <a href=""http://dirk.eddelbuettel.com/code/rinside.html"" rel=""nofollow noreferrer"">a nice ""hello world"" example</a> on the project homepage.</p>

<p>In you're interested in calling C from R, here's my original answer:</p>

<p>This isn't exactly ""hello world"", but here are some good resources:</p>

<ul>
<li>Jay Emerson recently gave a talk on R package development at the New York useR group, and he provided some very nice examples of using C from within R.  Have a look at <a href=""http://www.stat.yale.edu/~jay/Rmeetup/Rmeetup.pdf"" rel=""nofollow noreferrer"">the paper from this discussion on his website</a>, starting on page 9.  All the related source code is here: <a href=""http://www.stat.yale.edu/~jay/Rmeetup/MyToolkitWithC/"" rel=""nofollow noreferrer"">http://www.stat.yale.edu/~jay/Rmeetup/MyToolkitWithC/</a>.  </li>
<li>The course taught at Harvard by Gopi Goswami in 2005: <a href=""http://www.stat.harvard.edu/ccr2005/"" rel=""nofollow noreferrer"">C-C++-R (in Statistics)</a>.  This includes extensive examples and source code.  </li>
</ul>
"
2464034,249085,2010-03-17T16:31:43Z,2462708,5,FALSE,"<p>Is the file type getting detected?</p>

<pre><code>:set ft?
</code></pre>

<p>That should print out something like <code>filetype=r</code> in your case.  If it's not set, try:</p>

<pre><code>:set ft=r
</code></pre>

<p>Still nothing?  Try:</p>

<pre><code>:syntax on
</code></pre>

<p>I place all downloaded plugins in <code>~/.vim/plugins/</code>.  You shouldn't have to place any plugins in the system folders.</p>

<p>In your <code>.vimrc</code>:</p>

<pre><code>filetype plugin on
syntax on
</code></pre>
"
2464479,165384,2010-03-17T17:37:41Z,2463437,9,FALSE,"<p>Here you go. It's the main function, but you should be able to adapt it to a more general purpose function. This example builds an R expression from C calls and also from a C string. You're on your own for the compiling on windows, but I've provided compile steps on linux:</p>

<pre><code> /* simple.c */
 #include &lt;Rinternals.h&gt;
 #include &lt;Rembedded.h&gt;
 #include &lt;R_ext/Parse.h&gt;
 int
 main(int argc, char *argv[])
 {
    char *localArgs[] = {""R"", ""--no-save"",""--silent""};
    SEXP e, tmp, ret;
    ParseStatus status;
    int i;

    Rf_initEmbeddedR(3, localArgs);

    /* EXAMPLE #1 */

    /* Create the R expressions ""rnorm(10)"" with the R API.*/
    PROTECT(e = allocVector(LANGSXP, 2));
    tmp = findFun(install(""rnorm""), R_GlobalEnv);
    SETCAR(e, tmp);
    SETCADR(e, ScalarInteger(10));

    /* Call it, and store the result in ret */
    PROTECT(ret = R_tryEval(e, R_GlobalEnv, NULL));

    /* Print out ret */
    printf(""EXAMPLE #1 Output: "");
    for (i=0; i&lt;length(ret); i++){
        printf(""%f "",REAL(ret)[i]);
    }
    printf(""\n"");

    UNPROTECT(2);


    /* EXAMPLE 2*/

    /* Parse and eval the R expression ""rnorm(10)"" from a string */
    PROTECT(tmp = mkString(""rnorm(10)""));
    PROTECT(e = R_ParseVector(tmp, -1, &amp;status, R_NilValue));
    PROTECT(ret = R_tryEval(VECTOR_ELT(e,0), R_GlobalEnv, NULL));

    /* And print. */
    printf(""EXAMPLE #2 Output: "");
    for (i=0; i&lt;length(ret); i++){
        printf(""%f "",REAL(ret)[i]);
    }
    printf(""\n"");

    UNPROTECT(3);
    Rf_endEmbeddedR(0);
    return(0);
 }
</code></pre>

<p>Compile steps:</p>

<pre><code>$ gcc -I/usr/share/R/include/ -c -ggdb simple.c
$ gcc -o simple simple.o  -L/usr/lib/R/lib -lR
$ LD_LIBRARY_PATH=/usr/lib/R/lib R_HOME=/usr/lib/R ./simple
EXAMPLE #1 Output: 0.164351 -0.052308 -1.102335 -0.924609 -0.649887 0.605908 0.130604 0.243198 -2.489826 1.353731
EXAMPLE #2 Output: -1.532387 -1.126142 -0.330926 0.672688 -1.150783 -0.848974 1.617413 -0.086969 -1.334659 -0.313699
</code></pre>
"
2464813,37213,2010-03-17T18:23:26Z,2464721,1,FALSE,"<p>You're assuming that R is looking at the windows path, but the code is telling you that it's not.  So check your assumption: R is getting the path somewhere else.</p>

<p>If I open up a command shell on my Windows machine and type ""java -version"" I get this: </p>

<pre><code>C:\&gt;java -version
java version ""1.6.0_13""
Java(TM) SE Runtime Environment (build 1.6.0_13-b03)
Java HotSpot(TM) Client VM (build 11.3-b02, mixed mode, sharing)
</code></pre>

<p>If I check the PATH on my machine, I get (edited for clarity): </p>

<pre><code>C:\&gt;set path
Path=;C:\JDKs\jdk1.6.0_13\bin;
</code></pre>

<p>If I open up R version 2.8.1 and run system(""java -version"") I get this: </p>

<pre><code>&gt; system(""java -version"")
java version ""1.6.0_15""
Java(TM) SE Runtime Environment (build 1.6.0_15-b03)
Java HotSpot(TM) Client VM (build 14.1-b02, mixed mode, sharing)
&gt;
</code></pre>

<p>So, like I said, R is not using my path to find java.exe.  It's using something else. </p>
"
2464859,294738,2010-03-17T18:29:44Z,2464721,1,FALSE,"<p>You may also need to check the registry, R may have its own setting. You can also start regedit and do a search on the path to binary that it is starting.</p>
"
2467098,163053,2010-03-18T01:40:27Z,2467019,1,TRUE,"<p>There are many ways to do this (as with everything in R).  I always recommend using a time series when working with time series data.  </p>

<p>The <code>zoo</code> package is probably the most popular time series package (although you can also look at others such as xts, timeSeries, its, fts):</p>

<pre><code>library(zoo)
z &lt;- zoo(data.frame(a=1:50, b=3:52), as.Date(1:50))
rollapply(z, 30, cor, by.column=F, align = ""right"")
</code></pre>

<p>You may also find the <code>chart.RollingCorrelation</code> function in the <code>PerformanceAnalytics</code> package useful.</p>
"
2467499,133234,2010-03-18T04:18:01Z,2467329,4,TRUE,"<p>In terms of introspection: R allows you to easily examine and operate on language objects.<br>
For more details, see <a href=""http://cran.r-project.org/doc/manuals/R-lang.html"" rel=""nofollow noreferrer"">R Language Definition</a>, particularly sections 2 and 6. For instance, in your case, summary(rdnM)$call is a ""call"" object.  You can retrieve pieces of it by indexing, but you can't construct another call object by assigning to indices like you are trying to do.  You'd have to construct a new call.</p>

<p>In your case you are constructing an updated call to lm() out of an existing call. If you want to reuse the formula on different data, you would extract the formula from the call object via <code>formula(foo$call)</code>, like so:</p>

<pre><code> foo &lt;- lm(formula = y ~ x1, data = data.frame(y=rnorm(10),x1=rnorm(10)))
 bar &lt;- lm(formula(foo$call), data = data.frame(y=rnorm(10),x1=rnorm(10)))
</code></pre>

<p>On the other hand, if you are trying to update the formula, you could use <code>update()</code>:</p>

<pre><code>baz &lt;- update(bar, . ~ . - 1)
baz$call
##&gt;lm(formula = y ~ x1 - 1, data = data.frame(y = rnorm(10), x1 = rnorm(10)))
</code></pre>
"
2468603,168747,2010-03-18T09:20:12Z,2464721,2,TRUE,"<p>You problem depends on 64/32 bit versions.<br>
You run 32-bit R, which use 32-bit command prompt and find 32-bit java. If you use 64-bit R then it runs 64-bit command promt and proper java.</p>

<p>You could check it by run 32-bit command promt (following <a href=""http://www.tipandtrick.net/2008/how-to-open-and-run-32-bit-command-prompt-in-64-bit-x64-windows/"" rel=""nofollow noreferrer"" title=""How to Open and Run 32-bit Command Prompt in 64-bit (x64) Windows"">this post</a>):</p>

<ol>
<li>Click Start.</li>
<li>Type <code>%windir%\SysWoW64\cmd.exe</code> in Start Search box.</li>
<li>Press Enter.</li>
<li>Type <code>java -version</code></li>
</ol>

<p>In my system it fails because I don't have 32-bit java. With standard cmd.exe I get proper path.</p>

<p>For possible solution there are two ways. Install 32-bit R and 32-bit Java or 64-bit R (which is <a href=""http://cran.at.r-project.org/bin/windows64/base/"" rel=""nofollow noreferrer"" title=""R 2.11.0 64-bit"">officially supported from 2.11 version</a>) and 64-bit Java. On my system (64-bit Windows 7) I've got both sets, so on 32-bit combination I get:</p>

<pre><code>&gt; system(""java -version"")
java version ""1.6.0_20""
Java(TM) SE Runtime Environment (build 1.6.0_20-b02)
Java HotSpot(TM) Client VM (build 16.3-b01, mixed mode, sharing)
</code></pre>

<p>And on 64-bit:</p>

<pre><code>&gt; system(""java -version"")
java version ""1.6.0_18""
Java(TM) SE Runtime Environment (build 1.6.0_18-b07)
Java HotSpot(TM) 64-Bit Server VM (build 16.0-b13, mixed mode)
</code></pre>

<p>On 64-bit version you could call 32-bit Java using 32-bit cmd:</p>

<pre><code>shell(
    ""java -version"",
    shell = file.path(Sys.getenv(""windir""),""SysWoW64/cmd.exe"")
)
java version ""1.6.0_20""
Java(TM) SE Runtime Environment (build 1.6.0_20-b02)
Java HotSpot(TM) Client VM (build 16.3-b01, mixed mode, sharing)
</code></pre>

<p>About Shane's comment I think the question is how R get path to 32-bit cmd. Because I can't find a way to call 64-bit cmd on 32-bit R.</p>
"
2470277,16363,2010-03-18T13:54:44Z,2470248,273,TRUE,"<pre><code>fileConn&lt;-file(""output.txt"")
writeLines(c(""Hello"",""World""), fileConn)
close(fileConn)
</code></pre>
"
2470676,168747,2010-03-18T14:44:29Z,2470295,2,TRUE,"<p>I don't think there is a straight way, but you could create two logical vectors telling you if next/previous element is 1 greatest/least. E.g.:</p>

<pre><code>data.frame(
  a,
  is_first = c(TRUE,diff(a)!=1),
  is_last = c(diff(a)!=1,TRUE)
)
# Gives you:
   a is_first is_last
1  1     TRUE   FALSE
2  2    FALSE   FALSE
3  3    FALSE    TRUE
4  6     TRUE   FALSE
5  7    FALSE   FALSE
6  8    FALSE   FALSE
7  9    FALSE   FALSE
8 10    FALSE    TRUE
9 20     TRUE    TRUE
</code></pre>

<p>So ranges are:</p>

<pre><code>cbind(a[c(TRUE,diff(a)!=1)], a[c(diff(a)!=1,TRUE)])
[1,]    1    3
[2,]    6   10
[3,]   20   20
</code></pre>
"
2470686,143305,2010-03-18T14:46:14Z,2470654,4,TRUE,"<p>Just test it by using <code>set.seed()</code>:</p>

<pre><code>R&gt; set.seed(42); rchisq(2, 1:2, 1:2)              # base case
[1] 8.676 1.653
R&gt; set.seed(42); rchisq(1, 1, 1); rchisq(1, 2, 2) # matches
[1] 8.676
[1] 1.653
R&gt; set.seed(42); rchisq(1, 1, 1); rchisq(1, 1, 1) # does not match
[1] 8.676
[1] 0.5874
</code></pre>

<p>so it looks like you get N draws using degrees of freedom and non-centrality from the corresponding value in the supplied vector.  </p>

<p>Viewed another way, recycling as actually the rules as scalar values for <code>df</code> and <code>ncp</code> get recycled to vector length which makes some sense.</p>
"
2470985,170792,2010-03-18T15:23:18Z,2470295,1,FALSE,"<p>I did this (not so elegant I admit) in case you want all the numbers of each sequence in a list</p>

<pre><code>a &lt;- c(1,2,3,6,7,8,9,10,20)

z &lt;- c(1,which(c(1,diff(a))!=1))

g &lt;- lapply(seq(1:length(z)),function(i) {
if (i &lt; length(z)) a[z[i] : (z[i+1] - 1)] 
else a[z[i] : length(a)] 
})


[[1]]
[1] 1 2 3

[[2]]
[1]  6  7  8  9 10

[[3]]
[1] 20
</code></pre>

<p>Then you can get a 2D array with something like this</p>

<pre><code>sapply(g,function(x) c(x[1],x[length(x)]))

     [,1] [,2] [,3]
[1,]    1    6   20
[2,]    3   10   20
</code></pre>
"
2471243,163053,2010-03-18T15:52:07Z,2471188,14,TRUE,"<p>Here's a cleaner solution:</p>

<pre><code>as.list(data.frame(t(foo)))
</code></pre>

<p>That takes advantage of the fact that a data frame is really just a list of equal length vectors (while a matrix is really a vector that is displayed with columns and rows...you can see this by calling foo[5], for instance).</p>

<p>You could also do this, although it isn't much of an improvement:</p>

<pre><code>lapply(1:nrow(foo), function(i) foo[i,])
</code></pre>
"
2471451,170792,2010-03-18T16:14:15Z,2471075,5,TRUE,"<p>I would try passing parameter <code>fileEncoding</code> to read.table with a value of <code>iso8859-8</code>. </p>

<p>Use <code>iconvlist()</code> to get an alphabetical list of the supported encodings. As I saw <a href=""http://finzi.psych.upenn.edu/R/library/ISOcodes/html/ISO_8859.html"" rel=""noreferrer"">here</a> Hebrew must be part 8 of ISO 8859.</p>
"
2471806,457898,2010-03-18T16:57:22Z,2470248,115,FALSE,"<p>Actually you <strong>can</strong> do it with <code>sink()</code>:</p>

<pre><code>sink(""outfile.txt"")
cat(""hello"")
cat(""\n"")
cat(""world"")
sink()
</code></pre>

<p>hence do:</p>

<pre><code>file.show(""outfile.txt"")
# hello
# world
</code></pre>
"
2471981,227406,2010-03-18T17:23:32Z,1107605,2,FALSE,"<p>As of version 5.9, ESS will recognize scripts using the Rscript or litter interpreter; see <a href=""http://ess.r-project.org/Manual/ess.html#New-features"" rel=""nofollow noreferrer"">http://ess.r-project.org/Manual/ess.html#New-features</a>.</p>

<p>For reference, to assign a mode to an interpreter, use the interpreter-mode-alist variable. For example,</p>

<pre><code>(add-to-list 'interpreter-mode-alist '(""Rscript"" . r-mode))
</code></pre>

<p>Any file with a first line like ""#!/usr/bin/Rscript"" will be recognized as r-mode. </p>
"
2472134,163053,2010-03-18T17:42:42Z,2472107,6,FALSE,"<p>A quick google can lead you to a lot of good resources:</p>

<ul>
<li><a href=""http://www.stat.uni-muenchen.de/~leisch/Sweave/"" rel=""nofollow noreferrer"">http://www.stat.uni-muenchen.de/~leisch/Sweave/</a></li>
<li><a href=""http://www.stat.umn.edu/~charlie/Sweave/"" rel=""nofollow noreferrer"">http://www.stat.umn.edu/~charlie/Sweave/</a></li>
<li><a href=""http://jeromyanglim.blogspot.com/2010/02/getting-started-with-sweave-r-latex.html"" rel=""nofollow noreferrer"">http://jeromyanglim.blogspot.com/2010/02/getting-started-with-sweave-r-latex.html</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Sweave"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Sweave</a></li>
</ul>

<p>See my answer to this question: <a href=""https://stackoverflow.com/questions/1890215/getting-r-plots-into-latex"">Getting R plots into LaTeX?</a></p>

<p>I personally recommend using either LyX or Eclipse (with StatET and TeXlipse) for this, although there are many options for editing LaTeX out there.  </p>

<p><em>Edit:</em></p>

<p>If your script takes a long time to run, then you can also look at <a href=""http://cran.r-project.org/web/packages/cacheSweave/index.html"" rel=""nofollow noreferrer"">the cacheSweave package</a>.</p>
"
2472512,212593,2010-03-18T18:51:02Z,2470295,0,FALSE,"<pre><code>&gt; a &lt;- c(1,2,3,6,7,8,9,10,20)
&gt; N&lt;-length(a)
&gt; k&lt;-2:(N-1)
&gt; z&lt;-(a[k-1]+1)!=a[k] | (a[k+1]-1)!=a[k]
&gt; c(a[1],a[k][z],a[N])
[1]  1  3  6 10 20
</code></pre>
"
2472901,16632,2010-03-18T19:48:59Z,2471188,5,FALSE,"<pre><code>library(plyr)
alply(foo, 1)
</code></pre>
"
2472916,16632,2010-03-18T19:51:15Z,2472107,6,TRUE,"<p>This seems a very unusual request in my opinion.  I can understand a referee expecting you to provide reproducible code, but requiring a specific format is over-the-top.  I would respond by providing your code.  Sweave is <em>not</em> standard practice for academic journals.</p>
"
2473050,190597,2010-03-18T20:12:08Z,2472958,2,TRUE,"<p>Perhaps use the <code>collapse</code> option:</p>

<pre><code>foo = list('bee','bar','baz')
paste(foo,collapse='|')
</code></pre>

<p>yields</p>

<pre><code>""bee|bar|baz""
</code></pre>
"
2473126,170792,2010-03-18T20:25:00Z,2472958,1,FALSE,"<pre><code>paste(rep(foo,4),collapse='|')

[1] ""blah|blah|blah|blah""
</code></pre>
"
2473398,163053,2010-03-18T21:18:07Z,2473336,0,FALSE,"<p>Not totally following your question.  Are you looking for a switch statement?  Have a look at this example:</p>

<pre><code>ccc &lt;- c(""b"",""QQ"",""a"",""A"",""bb"")
for(ch in ccc)
     cat(ch,"":"",switch(EXPR = ch, a=1,     b=2:3), ""\n"")
</code></pre>
"
2473402,169947,2010-03-18T21:18:26Z,2472958,2,FALSE,"<p>The problem is actually the first time you call <code>paste(paste_all_together,...)</code> - it essentially pastes the empty string to <code>""blah""</code>, putting a <code>|</code> between them.</p>

<p>There are already 2 answers here that are better than what I'm about to suggest, but to fix your example with minimal surgery would look something like this:</p>

<pre><code>foo &lt;- ""blah""
all_together &lt;- character(0)
for (n in 1:4) {
    all_together &lt;- c(all_together, foo)
}
paste(all_together, collapse=""|"")
</code></pre>
"
2473714,143305,2010-03-18T22:14:32Z,2473659,7,TRUE,"<p>The <a href=""http://cran.r-project.org/package=plyr"" rel=""nofollow noreferrer""><strong>plyr</strong></a> package is popular for this, but the base functions <code>by()</code> and <code>aggregate()</code> will also help. </p>

<pre><code>&gt; ddply(my_df, ""read_time"", function(X) data.frame(OD=mean(X$OD),stdev=sd(X$OD)))
   read_time      OD   stdev
1 2010-02-15 0.15000 0.07071
2 2010-02-16 0.23333 0.15275
3 2010-02-17 0.50000      NA
</code></pre>

<p>You can add the missing bit to return 0 instead of NA for the last std.dev. </p>

<p>Also, you don't need the quotes (on the variables) you had in the data.frame construction.</p>
"
2473743,279497,2010-03-18T22:20:44Z,2473336,1,TRUE,"<p>Not totally sure, but I would suggest to use something like:</p>

<pre><code>f &lt;- approxfun(0:100,c(rep(1,40),rep(2,20),rep(3,41)),method=""c"")
fac &lt;- f(foo)
tapply(foo,fac,FUN,...)
</code></pre>

<p>where you can use any function FUN.</p>
"
2474755,142477,2010-03-19T03:10:23Z,2473659,1,FALSE,"<p>This illustrates how you could use <code>aggregate</code> to get the mean and standard deviation by your <code>read_time</code>.</p>

<pre><code>&gt;aggregate(my_df$OD, by=list(my_df$read_time), function(x) mean(x))

     Group.1         x
1 2010-02-15 0.1500000
2 2010-02-16 0.2333333
3 2010-02-17 0.5000000


&gt;aggregate(my_df$OD, by=list(my_df$read_time), function(x) sd(x))
     Group.1          x
1 2010-02-15 0.07071068
2 2010-02-16 0.15275252
3 2010-02-17         NA
</code></pre>
"
2474819,143305,2010-03-19T03:30:57Z,2474808,3,FALSE,"<p>Try the <code>ecdf()</code> function in base R --- which uses <code>plot.stepfun()</code> if memory serves --- or the <code>Ecdf()</code> function in <a href=""http://cran.r-project.org/package=Hmisc"" rel=""nofollow noreferrer""><strong>Hmisc</strong></a> by Frank Harrell.  Here is an example from <code>help(Ecdf)</code> that uses a grouping variable to show two ecdfs in one plot:</p>

<pre><code> # Example showing how to draw multiple ECDFs from paired data
 pre.test &lt;- rnorm(100,50,10)
 post.test &lt;- rnorm(100,55,10)
 x &lt;- c(pre.test, post.test)
 g &lt;- c(rep('Pre',length(pre.test)),rep('Post',length(post.test)))
 Ecdf(x, group=g, xlab='Test Results', label.curves=list(keys=1:2))
</code></pre>
"
2475719,279497,2010-03-19T07:54:34Z,2475511,1,FALSE,"<p>Supposing you have only one vector <code>x</code>, then you can do the following:</p>

<pre><code>f &lt;- ecdf(x)
plot(1-f(x),x)
</code></pre>
"
2475753,255531,2010-03-19T08:02:32Z,2475511,2,FALSE,"<pre><code>df &lt;- data.frame(x, g)
df$y &lt;- apply(df, 1, function(v){nrow(subset(df, g == v[2] &amp; x &gt;= v[1]))})
library(ggplot2)
qplot(x, y, data=df, geom='line', colour=g)
</code></pre>
"
2476134,134830,2010-03-19T09:26:13Z,2472599,2,TRUE,"<p>There are a few possibilities.</p>

<ol>
<li><p>The problem is with the R code.  To test this, run the code in R itself and see if you can replicate the issue.</p></li>
<li><p>The problem is with Rpy.  Run the program in a debugger and see what exactly you are passing from Python to R and from R to Python.  At a guess, you have some kind of variable mismatch that only occurs under some conditions.</p></li>
<li><p>The problem is with the environment.  Does some other software need to be loaded for the progam to run?  Does it work once then fail when you run it again?  (Perhaps you aren't closing a connection to a file?)  Does it fail on other people's machines or just yours?</p></li>
<li><p>The problem is with the data.  Are you randomly generating anything?  That would explain the intermittent nature of the problem.  </p></li>
</ol>

<p>If you're still stuck, read some <a href=""http://www.software-carpentry.org/3_0/debugging.html"" rel=""nofollow noreferrer"">tips on debugging</a>.</p>
"
2476834,168747,2010-03-19T11:34:23Z,2475511,3,TRUE,"<p>Using Musa suggestion:</p>

<pre><code>pre.ecdf &lt;- ecdf(pre.test)
post.ecdf &lt;- ecdf(post.test)

r &lt;- range(pre.test,post.test)
curve(1-pre.ecdf(x), from=r[1], to=r[2], col=""red"", xlim=r)
curve(1-post.ecdf(x), from=r[1], to=r[2], col=""blue"", add=TRUE)
</code></pre>

<p><img src=""https://imgur.com/M3L50.png"" alt=""Proportions""></p>

<p>You could set some parameters like title, legend, etc.</p>

<p>If you want frequency instead of proportion simple solution will be:</p>

<pre><code>pre.ecdf &lt;- ecdf(pre.test)
post.ecdf &lt;- ecdf(post.test)

rx &lt;- range(pre.test,post.test)
ry &lt;- max(length(pre.test),length(post.test))
curve(length(pre.test)*(1-pre.ecdf(x)), from=rx[1], to=rx[2], col=""red"", xlim=rx, ylim=c(0,ry))
curve(length(post.test)*(1-post.ecdf(x)), from=rx[1], to=rx[2], col=""blue"", add=TRUE)
</code></pre>

<p><img src=""https://imgur.com/G11NU.png"" alt=""Frequencies""></p>
"
2476924,143305,2010-03-19T11:50:55Z,2475511,5,FALSE,"<p>The more general <code>Ecdf</code> function from the <a href=""http://cran.r-project.org/package=Hmisc"" rel=""nofollow noreferrer""><strong>Hmisc</strong></a> has a <code>what=</code> option for that:</p>

<blockquote>
  <p>Arguments:</p>

<pre><code>   x: a numeric vector, data frame, or Trellis/Lattice formula

what: The default is ‘""F""’ which results in plotting the fraction
      of values &lt;= x.  Set to ‘""1-F""’ to plot the fraction &gt; x or
      ‘""f""’ to plot the cumulative frequency of values &lt;= x.
</code></pre>
</blockquote>

<p>So with that we can modify the answer <a href=""https://stackoverflow.com/questions/2474808/howto-plot-two-cumulative-frequency-graph-together"">from your earlier question</a> and add <code>what=""1-F""</code>:</p>

<pre><code> # Example showing how to draw multiple ECDFs from paired data
 pre.test &lt;- rnorm(100,50,10)
 post.test &lt;- rnorm(100,55,10)
 x &lt;- c(pre.test, post.test)
 g &lt;- c(rep('Pre',length(pre.test)),rep('Post',length(post.test)))
 Ecdf(x, group=g, what=""1-F"", xlab='Test Results', label.curves=list(keys=1:2))
</code></pre>
"
2477664,163053,2010-03-19T13:35:15Z,2477398,1,FALSE,"<p>Did you install the library properly first?  You might want to try using the <code>lib.loc</code> parameter.</p>

<pre><code>library(""Media"", lib.loc = ""c:/Users/Albert/Documents"")
</code></pre>
"
2477726,163053,2010-03-19T13:42:38Z,2476946,1,TRUE,"<p>I would recommend profiling the function using <code>Rprof</code> or the <a href=""http://cran.r-project.org/web/packages/profr/"" rel=""nofollow noreferrer""><code>profr</code></a> package.  This will show you where your bottleneck is, and you then you can think about ways to either optimize the function or change the way that you're using it.</p>

<p>Your <code>paste</code> example would be much faster in part because it's vectorized.  For a more fair comparison, you can see the difference there by looping over <code>paste</code> as you are currently doing with <code>newXMLNode</code> and see the difference in timing.</p>

<p><em>Edit:</em></p>

<p>Here is the output from profiling your loop with <code>profr</code>.  </p>

<pre><code>library(profr)
xml.prof &lt;- profr(for(i in 1:N) 
    newXMLNode(""Parameter"", parent=seq, attrs=c(id=pars[i])))
plot(xml.prof)
</code></pre>

<p>There is nothing especially obvious in here about places that you can improve this.  I see that it spends a reasonable amount of time in the <code>%in%</code> function, so improving that would reduce the overall time somewhat (although you still need to iterate over this repeatedly, so it won't make a huge difference).  The best solution would be to rewrite <code>newXMLNode</code> as a vectorized function so you can skip the <code>for</code> loop entirely.
<a href=""http://i42.tinypic.com/5x26gw.jpg"" rel=""nofollow noreferrer"">alt text http://i42.tinypic.com/5x26gw.jpg</a></p>
"
2478353,163053,2010-03-19T15:09:19Z,2478259,2,TRUE,"<p>With base functions, you can do something like this:</p>

<pre><code>tapply(m[,2], m[,1], `[`)        # outputs an array
by(m, m[,1], function(m) m[,2])  # outputs a by object, which is a list
</code></pre>

<p>You could use <code>plyr</code>:</p>

<pre><code>dlply(m, 1, function(m) m[,2])   # outputs a list
dlply(m, 1, `[`, 2)              # another way to do it...
</code></pre>
"
2478624,168747,2010-03-19T15:45:03Z,2478352,94,TRUE,"<p>Citing <code>?write.table</code>, section <strong>CSV files</strong>:</p>

<blockquote>
  <p>By default there is no column name for
  a column of row names. If <code>col.names =
  NA</code> and <code>row.names = TRUE</code> a blank
  column name is added, which is the
  convention used for CSV files to be
  read by spreadsheets.</p>
</blockquote>

<p>So you must do</p>

<pre><code>write.table(a, 'a.txt', col.names=NA)
</code></pre>

<p>and you get</p>

<pre><code>"""" ""A"" ""B"" ""C""
""A"" 1 4 7
""B"" 2 5 8
""C"" 3 6 9
</code></pre>
"
2479072,170792,2010-03-19T16:53:19Z,2478272,1,FALSE,"<p>Unfortunately I don't know of a function like this. If there isn't one already, it would be an interesting task to construct a function that returns a matrix with all pairwise treatment comparisons. A contrast is considered significant if the following inequality is satisfied</p>

<p><a href=""http://www.statsdirect.com/help/image/stat0199_wmf.gif"" rel=""nofollow noreferrer"">alt text http://www.statsdirect.com/help/image/stat0199_wmf.gif</a></p>

<p>where T is the Kruskal-Wallis test statistic for k samples, S^2 is the denominator of the T statistic, N is the total number (all ni) and Ri is the sum of the ranks (from all samples pooled) for the ith sample, and t is a quantile from the Student t distribution on N-k degrees of freedom. </p>

<p>I know I didn't help much :)<br>
I am also waiting for a better answer </p>
"
2479179,115738,2010-03-19T17:09:32Z,2479059,1,TRUE,"<p>You're using Linux, so depending on how well R understands unicode, you could map one of your spare keyboard keys to the <a href=""http://en.wikipedia.org/wiki/Compose_key"" rel=""nofollow noreferrer"">Compose Key</a> and then just type it out. To get a —, press Compose and then the normal - key two or three times (depending on your system's mappings). Note that when using the Compose key, you don't hold it down - just press the keys in sequence.</p>

<p>Exactly how you'd enable that varies, but in Ubuntu, System->Preferences->Keyboard, Layout tab, Layout Options button, and select something appropriate for the ""Compose key position"" item. I usually use the Menu key.</p>

<p>Edit: My mistake, you wanted an en-dash, not an em-dash. Then en-dash (–) is Compose dash dash period, rather than Compose dash dash dash.</p>
"
2479804,245603,2010-03-19T18:45:36Z,2479689,7,FALSE,"<p>The <code>Hmisc</code> package has the <code>summary.formula</code> function that can do something along the lines you want. It is very flexible, so look at the help page for examples, but here is an application to your problem:</p>

<pre><code>library(Hmisc)
dd &lt;- data.frame(Q1=sample(1:3, 20, replace=T), Q2=sample(1:3, 20, replace=T), 
                 Q3=sample(1:3, 20, replace=T))  #fake data
summary(~Q1+Q2+Q3, data=dd, fun=table)
</code></pre>

<p>This gives the following result:  </p>

<pre><code> Descriptive Statistics  (N=20)

 +------+-------+
 |      |       |
 +------+-------+
 |Q1 : 1|25% (5)|
 +------+-------+
 |    2 |45% (9)|
 +------+-------+
 |    3 |30% (6)|
 +------+-------+
 |Q2 : 1|30% (6)|
 +------+-------+
 |    2 |35% (7)|
 +------+-------+
 |    3 |35% (7)|
 +------+-------+
 |Q3 : 1|35% (7)|
 +------+-------+
 |    2 |30% (6)|
 +------+-------+
 |    3 |35% (7)|
 +------+-------+
</code></pre>

<p>The possible values are given in rows, because it has the flexibility of different sets of values for different variables. You might be able to play with the function parameters (like <code>method</code> and <code>fun</code>) to get the other direction.</p>
"
2480048,245603,2010-03-19T19:27:23Z,2474808,1,FALSE,"<p>Just for the record, here is how you get multiple lines in the same plot ""by hand"":</p>

<pre><code>plot(cumfreq1, ylab=""CumFreq"",xlab=""Loglik Ratio"", type=""l"") 
          # or type=""b"" for lines and points
lines(cumfreq2, col=""red"") 
</code></pre>
"
2480152,170792,2010-03-19T19:48:40Z,2474808,6,TRUE,"<pre><code>data &lt;- read.table(""http://dpaste.com/173536/plain/"", header = FALSE)

sample1 &lt;- unlist(apply(as.matrix(data),1,function(x) rep(x[1],x[2])))
sample2 &lt;- unlist(apply(as.matrix(data),1,function(x) rep(x[1],x[3])))

plot(ecdf(sample1), verticals=TRUE, do.p=FALSE,
main=""ECDF plot for both samples"", xlab=""Scores"", 
ylab=""Cumulative Percent"",lty=""dashed"")

lines(ecdf(sample2), verticals=TRUE, do.p=FALSE,
col.h=""red"", col.v=""red"",lty=""dotted"")

legend(100,.8,c(""Sample 1"",""Sample 2""),
col=c(""black"",""red""),lty=c(""dashed"",""dotted""))
</code></pre>
"
2480461,205054,2010-03-19T20:41:14Z,2479689,0,FALSE,"<p>You could use a custom function to use <code>rbind()</code> on several tables, something like this:</p>

<pre><code>multitab &lt;- function(...){
   tabs&lt;-list(...)
   tablist&lt;-lapply(tabs,table)
   bigtab&lt;-t(sapply(tablist,rbind))
   bigtab } 
</code></pre>
"
2480635,235984,2010-03-19T21:15:26Z,2473659,3,FALSE,"<p>You can try the package data.table. If you know MySQL it should be very easy for you to get all the functions, otherwise the basics are good enough too ;-)</p>

<pre><code>my_dfdt&lt;-data.table(my_df)
mean&lt;-my_dfdt[,mean(OD), by=""read_time""]
sd&lt;-  ..  
</code></pre>

<p>you can also join both in one line or to cbind at the end, your call of style</p>

<p>Another advantage: it is extremely fast, if you have large samples. Very fast...see documentation why.</p>
"
2481045,16632,2010-03-19T22:56:11Z,2478272,1,FALSE,"<p>I would have thought you'd be able to do the following:</p>

<pre><code>data(diamonds, package = ""ggplot2"")

library(coin)
library(multcomp)

kt &lt;- kruskal_test(price ~ clarity, data = diamonds)
glht(kt, mcp(clarity = ""Tukey""))
</code></pre>

<p>But it seems like <code>multcomp</code> doesn't support <code>coin</code> objects (yet?).</p>
"
2481153,119759,2010-03-19T23:27:02Z,2479689,1,FALSE,"<p>just check <a href=""https://stackoverflow.com/users/16632/hadley"">Hadley Wickham's</a> <a href=""http://had.co.nz/reshape/"" rel=""nofollow noreferrer"">reshape package</a>.
AFAIS, you need <code>cast</code> function from the package.</p>
"
2482190,133234,2010-03-20T06:27:45Z,2482125,12,TRUE,"<p><code>format()</code> will take a vector argument, so 
<code>format(df$datetime,""%Y-%m-%dT%H:%M:%S"")</code> should do what you need.</p>

<p>When you use sapply, your objects are coerced to numeric, and so the wrong format method is being invoked.  You could coerce them back to POSIXct by using <code>sapply(df$datetime, function(x) format(as.POSIXct(x, origin=""1970-01-01""),""%Y-%m-%dT%H:%M:%S""))</code>, but unless you have a special reason to use <code>apply</code>, just use the method above</p>
"
2482280,138470,2010-03-20T07:19:10Z,2479059,2,FALSE,"<p>In this example, you can use the <code>expression()</code> function to get en dashes rendered properly:</p>

<pre><code>axis(1, 
     at=c(0:2), 
     labels=c(expression(0-10), 
              expression(11-30), 
              expression(31-70)))
</code></pre>
"
2483544,1855677,2010-03-20T15:15:12Z,2464680,1,FALSE,"<p>The ""chi-square test"" is usually generated as the sum of squared individual cell deviations from the ""expected"" = products of row and column sums divided by the total sum. As such, one can compare the individual cell contributions to the sum to the critical value of a chi-square with 1 d.f. It is a fairly simple task to modify the chisq.test() code to return the cell chi-squares. I just added:</p>

<pre><code>cell.chisq = (x - E)^2/E,
</code></pre>

<p>to the structure call at the end. They won't get print()-ed, but you can assign the result to an object and use:</p>

<pre><code> obj$cell.chisq
</code></pre>
"
2483836,1855677,2010-03-20T16:39:14Z,2478272,16,TRUE,"<p>One other approach besides kruskal::agricolae mentioned by Marek, is the Nemenyi-Damico-Wolfe-Dunn test implemented in the help page for oneway_test in the coin package that uses multcomp. Using hadley's setup and reducing the B= value for the approximate() function so it finishes in finite time:</p>

<pre><code>#updated translation of help page implementation of NDWD
NDWD &lt;- 
    independence_test(dv ~ iv, data = sum_codings1, distribution = approximate(B = 10000), 
                          ytrafo = function(data) trafo(data, numeric_trafo = rank_trafo), 
                          xtrafo = mcp_trafo(iv = ""Tukey""))


    ### global p-value
    print(pvalue(NDWD))

    ### sites (I = II) != (III = IV) at alpha = 0.01 (page 244)
    print(pvalue(NDWD, method = ""single-step""))
</code></pre>

<p>More stable results on that larger dataset may require increasing the B value and increasing the user's patience.</p>

<p>Jan: 2012: There was recently a posting on R-help claiming unexpected results from this method so I forwarded that email to the maintainer. Mark Difford said he had confirmed the problems and offered an alternate tests with the nparcomp package:  <a href=""https://stat.ethz.ch/pipermail/r-help/2012-January/300100.html"" rel=""nofollow noreferrer"">https://stat.ethz.ch/pipermail/r-help/2012-January/300100.html</a></p>

<p>There were also in the same week a couple of other suggestions on rhelp for post-hoc contrasts to KW tests:
<a href=""https://stat.ethz.ch/pipermail/r-help/2012-January/300316.html"" rel=""nofollow noreferrer""> kruskalmc suggested by Mario Garrido Escudero </a>  and
 <code>rms::polr</code> followed by <code>rms::contrasts</code> suggested by Frank Harrell <a href=""https://stat.ethz.ch/pipermail/r-help/2012-January/300329.html"" rel=""nofollow noreferrer"">https://stat.ethz.ch/pipermail/r-help/2012-January/300329.html</a></p>

<p>Nov 2015: Agree with toto_tico that help page code of coin package has been changed in the intervening years. The <code>?independence_test</code> help page now offers a multivariate-KW test and the <code>?oneway_test</code> help page has replace its earlier implementation with the code above usng the <code>independence_test</code> function.</p>
"
2485519,1855677,2010-03-21T01:45:10Z,2480984,2,FALSE,"<p>You should send the question to Martin Maechler, the author of much of the  Matrix package. There is a cBind function, but it does not at the moment recognize rownames, only dimensions, at least as far as I can tell (even when increasing the deparse.level argument to 2).</p>
"
2485556,1855677,2010-03-21T02:00:30Z,2458013,30,FALSE,"<pre><code>&gt; body(foo)[[3]] &lt;- substitute(line2 &lt;- 2)
&gt; foo
function (x) 
{
    line1 &lt;- x
    line2 &lt;- 2
    line3 &lt;- line1 + line2
    return(line3)
}
</code></pre>

<p>(The ""{"" is body(foo)[[1]] and each line is a successive element of the list.)</p>
"
2485810,158065,2010-03-21T03:35:21Z,2485639,10,TRUE,"<p>Your frames are right I think, it's just that <code>rm</code> is trying to remove <code>a</code> itself instead of evaluating <code>a</code> to get the quoted name of the variable to remove.  Use the <code>list</code> parameter instead:</p>

<pre><code>rm(list=a,envir=sys.frame(-1))
</code></pre>
"
2486159,224671,2010-03-21T06:14:03Z,2486130,2,FALSE,"<pre><code>as.character(round(p.mean(variable)))
</code></pre>

<p>? </p>
"
2486288,NA,2010-03-21T07:03:12Z,2486130,3,FALSE,"<p>Try 'format':</p>

<pre><code>options(digits=1)
</code></pre>

<p>and then:</p>

<pre><code>\Sexpr{format(p.mean(variable))}
</code></pre>

<p>You can also use the 'nsmall' argument:</p>

<pre><code>options(digits=3)
...
\Sexpr{format(sqrt(2)-0.41,nsmall=2)}
</code></pre>

<p>And you can also try 'sprintf' if you're familiar with C.</p>
"
2486383,457898,2010-03-21T08:11:37Z,2486130,4,TRUE,"<p>@KennyTM got it. @David, you should avoid <code>options(digits = 1)</code> since it will affect all of your calculations (it will suppress decimals in each output afterwards). So use <code>round()</code> function after applying the <code>weighted.mean()</code>. Something like this: </p>

<pre><code>\Sexpr{round(p.mean(x))}
</code></pre>

<p>And <strong>do not</strong> use <code>print()</code>, but <code>return()</code>. Here's why:</p>

<pre><code>&gt; set.seed(1234)
&gt; x &lt;- rnorm(100)
&gt; foo &lt;- function(x) {
   res &lt;- mean(x) + 5
   print(res)
}
&gt; foo(x)
[1] 5
&gt; foo(x) + 10
[1] 5
[1] 15
</code></pre>

<p>Use <code>return()</code> or just type the resulting variable in the last line:</p>

<pre><code>&gt; bar &lt;- function(x) {
   res &lt;- mean(x) + 5
   return(res)
}
&gt; bar(x) + 10
[1] 15
</code></pre>

<p>So, rewrite your function, and be sure to use <code>as.character()</code>... you have all the bits, now just put it all together.</p>

<p>P.S.</p>

<p>I'm not sure how you function works... I've never used weighed mean in my analysis. The bit that's puzzling me the most is <code>weight=weight</code>. Wouldn't it be nicer to put one more argument in function? Frankly, I'm still amazed by the fact that your function is giving you right result... probably because you have <code>weight</code> variable defined prior to function definition. <strong>[EDIT]</strong> You <strong>will not</strong> get the weighted mean with your function if you don't have <code>weight</code> function predefined, but ""regular"" mean!</p>

<p>I hope this one helped you!</p>

<p>Kind regards,
Aleksandar</p>
"
2488138,143305,2010-03-21T18:16:33Z,2487922,3,FALSE,"<p>How about this:</p>

<pre><code>R&gt; x[x!=y]
[1] 2 1 1 1 3
Warning message:
In x != y : longer object length is not a multiple of shorter object length
R&gt;
</code></pre>

<p>This is difficult problem, I think, as you are mixing values and positions.  The easier solution relies on one of the 'set' functions in R:</p>

<pre><code>R&gt; setdiff(x,y)
[1] 2 3
</code></pre>

<p>but that uses only values and not position.</p>

<p>The problem with the answer I gave you is the implicit use of recycling and the warning it triggered:  as your <code>x</code> is longer than your <code>y</code>, the first few values of <code>y</code> get reused.  But recycling is considered ""clean"" on when the longer vector has an integer-multiple length of the length of the shorter vector. But that is not the case here, and hence I am not sure we can solve your problem all that cleanly.</p>
"
2488285,158065,2010-03-21T18:53:28Z,2487922,3,FALSE,"<p>If I understand the problem, you can use <code>table</code> to compute the difference in the number of elements in each set and then create a vector based on the difference of those counts (note that this won't necessarily give you the order you gave in your question). </p>

<pre><code>&gt; diffs &lt;- table(x) - table(factor(y, levels=levels(factor(x))))
&gt; rep(as.numeric(names(diffs)), ifelse(diffs &lt; 0, 0, diffs))
[1] 1 1 2 3
</code></pre>
"
2488872,163053,2010-03-21T21:51:11Z,2488853,6,TRUE,"<pre><code>apply(df, 1, paste, collapse="""")
</code></pre>
"
2489166,168747,2010-03-21T23:09:41Z,2488853,5,FALSE,"<pre><code>with(df, paste(a, b, sep=""""))
</code></pre>

<p>And this should be faster than <code>apply</code>.</p>

<p><strong><em>About timing</em></strong></p>

<p>For 10000 rows we get:</p>

<pre><code>df &lt;- data.frame(
    a = sample(c(""x"",""y""), 10000, replace=TRUE),
    b = sample(1L:4L, 10000, replace=TRUE)
)

N = 100
mean(replicate(N, system.time( with(df, paste(a, b, sep="""")) )[""elapsed""]), trim=0.05)
# 0.005778
mean(replicate(N, system.time( apply(df, 1, paste, collapse="""") )[""elapsed""]), trim=0.05)
# 0.09611
</code></pre>

<p>So increase in speed is visible for few thousands.<br>
It's because Shane's solution call <code>paste</code> for each row separately. So there is <code>nrow(df)</code> calls of <code>paste</code>, in my solution is one call.</p>
"
2490975,279497,2010-03-22T09:12:06Z,2490860,1,FALSE,"<p>Here a more general solution (you can replace <code>X</code> by any vector containing unique entries):</p>

<pre><code>X&lt;-1:n
B&lt;-apply(A,2,function(x,ref) ref[!ref%in%x],ref=X)
B&lt;-do.call(cbind,B)
</code></pre>

<p>Whereas in your previous question x and y were not sets, provided that the columns of A are proper sets, the above code should work.</p>
"
2491345,279497,2010-03-22T10:18:47Z,2487922,8,TRUE,"<p>Here a solution using <code>pmatch</code> (this gives the ""complement"" as you require):</p>

<pre><code>x &lt;- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,2,1,1,1,3)
y &lt;- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1)
res &lt;- x[is.na(pmatch(x,y))]
</code></pre>

<p>From <code>pmatch</code> documentation:</p>

<p>""If duplicates.ok is FALSE, values of table  once matched are excluded from the search for subsequent matches.""</p>
"
2491434,170792,2010-03-22T10:35:01Z,2490860,4,TRUE,"<p>using Musa's <a href=""https://stackoverflow.com/questions/2487922/r-how-can-i-get-the-complement-of-vector-y-in-vector-x/2491345#2491345"">idea</a></p>

<pre><code>B &lt;- apply(A,2,function(z) x[is.na(pmatch(x,z))])
</code></pre>

<p>as regards the first example:</p>

<pre><code>B &lt;- apply(A,2,function(z) (1:n)[is.na(pmatch((1:n),z))])
</code></pre>
"
2492998,170792,2010-03-22T14:34:59Z,2492947,9,FALSE,"<p>Check <a href=""http://bm2.genes.nig.ac.jp/RGM2/R_current/library/PerformanceAnalytics/man/chart.Boxplot.html"">chart.Boxplot</a> from package <code>PerformanceAnalytics</code>. It lets you define the symbol to use for the mean of the distribution.</p>

<p><a href=""http://bm2.genes.nig.ac.jp/RGM2/R_current/library/PerformanceAnalytics/man/images/big_chart.Boxplot_001.png"">alt text http://bm2.genes.nig.ac.jp/RGM2/R_current/library/PerformanceAnalytics/man/images/big_chart.Boxplot_001.png</a></p>
"
2493010,269476,2010-03-22T14:36:19Z,2492947,33,TRUE,"<pre><code>abline(h=mean(x))
</code></pre>

<p>for a horizontal line (use v instead of h for vertical if you orient your boxplot horizontally), or</p>

<pre><code>points(mean(x))
</code></pre>

<p>for a point. Use the parameter <code>pch</code> to change the symbol. You may want to colour them to improve visibility too.</p>

<p>Note that these are called after you have drawn the boxplot.</p>

<p>If you are using the formula interface, you would have to construct the vector of means. For example, taking the first example from <code>?boxplot</code>:</p>

<pre><code>boxplot(count ~ spray, data = InsectSprays, col = ""lightgray"")
means &lt;- tapply(InsectSprays$count,InsectSprays$spray,mean)
points(means,col=""red"",pch=18)
</code></pre>

<p>If your data contains missing values, you might want to replace the last argument of the <code>tapply</code> function with <code>function(x) mean(x,na.rm=T)</code></p>
"
2494087,299077,2010-03-22T16:47:26Z,2479689,6,FALSE,"<p>Modifying a previous example</p>

<pre><code>library(Hmisc)
library(plyr)
dd &lt;- data.frame(q1=sample(1:3, 20, replace=T),
 q2=sample(1:3, 20, replace=T), 
 q3=sample(1:3, 20, replace=T))  #fake data

cross &lt;- ldply(describe(dd), function(x) x$values[1,])[-1]

rownames(cross) &lt;- c(""Q1. Likes it"",""Q2. Recommends it"",""Q3. Used it"")
names(cross) &lt;- c(""1 (very Often)"",""2 (Rarely)"",""3 (Never)"")
</code></pre>

<p>Now cross looks like this</p>

<pre><code>&gt; cross
                  1 (very Often) 2 (Rarely) 3 (Never)
Q1. Likes it                   4         10         6
Q2. Recommends it              7          9         4
Q3. Used it                    6          4        10
</code></pre>
"
2494278,269476,2010-03-22T17:16:45Z,2479689,0,FALSE,"<p><code>xtabs</code> has a formula interface that can take some practice to get used to, but this can be done. If you have the data in a dataframe <code>df</code> and your variables are called <code>ques</code> and <code>resp</code>, you can use:</p>

<pre><code>xtabs(~ques+resp,data=df)
</code></pre>

<p>For example:</p>

<pre><code>&gt; t1 &lt;- rep(c(""A"",""B"",""C""),5)
&gt; t2 &lt;- rpois(15,4)
&gt; df &lt;- data.frame(ques=t1,resp=t2)
&gt; xtabs(~ques+resp,data=df)
     resp
names 2 3 4 5 6 7 9
    A 1 0 2 1 0 0 1
    B 1 0 0 2 1 1 0
    C 1 2 0 1 0 1 0
</code></pre>
"
2495638,163053,2010-03-22T20:51:20Z,2494676,1,TRUE,"<p>One alternative option you could consider is using Google spreadsheets.  You can then access the data from either <a href=""http://www.omegahat.org/RGoogleDocs/"" rel=""nofollow noreferrer"">RGoogleDocs</a> or <a href=""http://r-forge.r-project.org/projects/rgoogledata/"" rel=""nofollow noreferrer"">RGoogleData</a>.</p>
"
2497218,163053,2010-03-23T02:34:34Z,2497111,5,FALSE,"<p>Because <code>apply</code> changes the data type in your function:</p>

<pre><code>&gt; apply(df, 1, class)
[1] ""character"" ""character"" ""character"" ""character""

&gt; apply(df, 1, colnames)
NULL
</code></pre>

<p>Since there are no column names, you can't reference the values with the <code>$</code> operator.</p>

<p>From the <code>apply</code> documentation:</p>

<blockquote>
  <p>If X is not an array but has a
  dimension attribute, apply attempts to
  coerce it to an array via as.matrix if
  it is two-dimensional (e.g., data
  frames) or via as.array.</p>
</blockquote>
"
2497221,255531,2010-03-23T02:35:25Z,2497111,9,FALSE,"<p>because <code>data.frame.in</code> is not a data.frame:</p>

<pre><code>apply(df, 1, function(v){print(class(v))})
</code></pre>

<p>but you can access the named elements with:</p>

<pre><code>test_fun_2 &lt;- function (data.frame_in) {
+     print (data.frame_in['a'])}
</code></pre>
"
2497353,16632,2010-03-23T03:10:40Z,2497111,19,TRUE,"<p>You could use <code>adply</code> from the plyr package instead:</p>

<pre><code>library(plyr)
adply(df, 1, function (data.frame_in) print(data.frame_in$a))
</code></pre>
"
2498459,172261,2010-03-23T08:35:20Z,2498285,7,FALSE,"<p>Roxygen is used in Hadley's <a href=""http://cran.r-project.org/web/packages/stringr/index.html"" rel=""nofollow noreferrer"">stringr</a> (see also this previous SO question: <a href=""https://stackoverflow.com/questions/1734470/r-documentation-with-roxygen"">R documentation with Roxygen</a>), <a href=""http://cran.r-project.org/web/packages/mutatr/index.html"" rel=""nofollow noreferrer"">mutatr</a> and <a href=""http://cran.r-project.org/web/packages/testthat/index.html"" rel=""nofollow noreferrer"">testthat</a> packages.</p>

<p>But <a href=""http://cran.r-project.org/web/packages/testthat/index.html"" rel=""nofollow noreferrer"">testthat</a> is used for testing in the mentioned packages instead of RUnit.</p>
"
2498975,143305,2010-03-23T10:13:19Z,2498285,6,TRUE,"<p>If you look at the <a href=""http://cran.r-project.org/package=RUnit"" rel=""nofollow noreferrer"">RUnit page at CRAN</a> you see the list of of packages that have a Depends:, Imports: or Suggests: on it. Maybe try one of those?  The list includes <a href=""http://cran.r-project.org/package=plyr"" rel=""nofollow noreferrer"">plyr</a> and a bunch of Rmetrics packages.</p>

<p>Likewise, the <a href=""http://cran.r-project.org/package=roxygen"" rel=""nofollow noreferrer"">roxygen page at CRAN</a> can be looked at but it only lists a single package.</p>
"
2500926,225089,2010-03-23T15:02:32Z,2500896,1,FALSE,"<p>I don't know R but in general you need to make a http request to the tinyurl-url. You should get back a 301 response with the actual url.</p>
"
2501446,300080,2010-03-23T15:57:21Z,2500896,0,FALSE,"<pre><code>library(RCurl)

decode.short.url &lt;- function(u) {
  x &lt;- try( getURL(u, header = TRUE, nobody = TRUE, followlocation = FALSE) )
  if(class(x) == 'try-error') {
    return(u)
  } else {
    x &lt;- strsplit(x, ""Location: "")[[1]][2]
    return(strsplit(x, ""\r"")[[1]][1])
  }
}


( u &lt;- c(""http://tinyurl.com/adcd"", ""http://tinyurl.com/fnqsh"") )
( sapply(u, decode.short.url) )
</code></pre>
"
2501480,457898,2010-03-23T16:00:21Z,2501369,0,FALSE,"<pre><code>sudo apt-get install libxml2-dev
</code></pre>

<p>it should work after that...</p>
"
2501502,143305,2010-03-23T16:03:22Z,2501369,2,TRUE,"<p>Better still:   <code>sudo apt-get install r-cran-xml</code></p>
"
2501867,300123,2010-03-23T16:50:32Z,2500896,16,TRUE,"<p>Below is a quick and dirty solution, but should get the job done:</p>

<pre><code>library(RCurl)

decode.short.url &lt;- function(u) {
  x &lt;- try( getURL(u, header = TRUE, nobody = TRUE, followlocation = FALSE) )
  if(class(x) == 'try-error') {
    return(u)
  } else {
    x &lt;- strsplit(x, ""Location: "")[[1]][2]
    return(strsplit(x, ""\r"")[[1]][1])
  }
}
</code></pre>

<p>The variable 'u' below contains one shortend url, and one regular url.</p>

<pre><code>u &lt;- c(""http://tinyurl.com/adcd"", ""http://www.google.com"") 
</code></pre>

<p>You can then get the expanded results by doing the following.</p>

<pre><code> sapply(u, decode.short.url) 
</code></pre>

<p>The above should work for most services which shorten the URL, not just tinyURL. I think. </p>

<p>HTH</p>

<p>Tony Breyal</p>
"
2501913,143305,2010-03-23T16:56:38Z,2501895,27,TRUE,"<p>Look at  <code>help(sink)</code> to do that.  On Unix I'd do</p>

<pre><code>sink(""/dev/null"")    # now suppresses
....                 # do stuff
sink()               # to undo prior suppression, back to normal now
</code></pre>

<p>and the Windows equivalent (with a tip-of-the-hat to Johannes) is</p>

<pre><code>sink(""NUL"")
....
sink()
</code></pre>
"
2504929,190597,2010-03-24T02:14:33Z,2504827,17,TRUE,"<p>How about <code>cut</code>:</p>

<pre><code>binned.x=cut(x, breaks=c(-1:9,Inf), labels=c(as.character(0:9),'10+'))
</code></pre>

<p>Which yields:</p>

<pre><code> [1] 0   1   3   4   2   4   2   5   10+ 10+ 10+ 2   10+ 2   10+ 3   4   2  
Levels: 0 1 2 3 4 5 6 7 8 9 10+
</code></pre>
"
2505163,255531,2010-03-24T03:33:51Z,2502485,6,TRUE,"<pre><code># your data
df &lt;- data.frame(gp = rep(LETTERS[1:5], each =8), y = sample(1:4,40,replace=TRUE))
# calculate offsets
df &lt;- ddply(df, .(y, gp), transform, offset = (1:length(gp)-1)/20)
qplot(gp, y, data=df) + stat_identity(aes(as.numeric(gp)+offset)) + theme_bw() 
</code></pre>
"
2506707,212593,2010-03-24T10:04:39Z,2502485,8,FALSE,"<p>You can use <code>position_dodge</code>.</p>

<pre><code>df &lt;- data.frame(gp = rep(LETTERS[1:5], each =8), 
                 y = sample(1:4,40,replace=TRUE))
qplot(gp,y,data=df,order=y,position=position_dodge(width=0.5))
</code></pre>

<p><a href=""http://img100.imageshack.us/img100/8760/dodgel.png"">alt text http://img100.imageshack.us/img100/8760/dodgel.png</a></p>
"
2508382,212593,2010-03-24T14:19:24Z,2492947,6,FALSE,"<p>With <code>ggplot2</code>:</p>

<pre><code>p&lt;-qplot(spray,count,data=InsectSprays,geom='boxplot')
p&lt;-p+stat_summary(fun.y=mean,shape=1,col='red',geom='point')
print(p)
</code></pre>
"
2508424,212593,2010-03-24T14:24:44Z,2490860,2,FALSE,"<p>Use the <code>setdiff</code> function:</p>

<pre><code>N &lt;- 5
m &lt;- 2    
A &lt;- combn(N,m)
B &lt;- apply(A,2,function(S) setdiff(1:N,S))
</code></pre>

<p>MODIFIED: The above works only when the vectors have unique values. For the second example, we write a replacement for <code>setdiff</code> that can handle duplicate values. We use <code>rle</code> to count the number of occurence of each element in the two sets, subtract the counts, then invert the RLE:</p>

<pre><code>diffdup &lt;- function(x,y){
  rx &lt;- do.call(data.frame,rle(sort(x)))
  ry &lt;- do.call(data.frame,rle(sort(y)))
  m &lt;- merge(rx,ry,by='values',all.x=TRUE)
  m$lengths.y[is.na(m$lengths.y)] &lt;- 0
  rz &lt;- list(values=m$values,lengths=m$lengths.x-m$lengths.y)
  inverse.rle(rz)
}

x&lt;-c(0,1,0,2,0,1) ; k&lt;- 4
A &lt;- combn(x,k)
B &lt;- apply(A,2,function(z) diffdup(x,z))
</code></pre>
"
2508702,168747,2010-03-24T14:58:27Z,2504827,7,FALSE,"<p>You question is inconsistent.<br>
In description <code>10</code> belongs to ""10+"" group, but in code <code>10</code> is separated level.
If <code>10</code> <strong>should</strong> be in the ""10+"" group then you code should be</p>

<pre><code>as.factor(ifelse(x &gt;= 10,""10+"",x))
</code></pre>

<p>In this case you could truncate data to 10 (if you don't want a factor):</p>

<pre><code>pmin(x, 10)
# [1]  0  1  3  4  2  4  2  5 10 10 10  2 10  2 10  3  4  2 10
</code></pre>
"
2510322,269476,2010-03-24T18:15:51Z,2504827,2,FALSE,"<pre><code>x[x&gt;=10]&lt;-""10+""
</code></pre>

<p>This will give you a vector of strings. You can use <code>as.numeric(x)</code> to convert back to numbers (""10+"" become <code>NA</code>), or <code>as.factor(x)</code> to get your result above.</p>

<p>Note that this will modify the original vector itself, so you may want to copy to another vector and work on that.</p>
"
2512032,242673,2010-03-24T22:56:18Z,2508897,1,FALSE,"<p>Are you sure your dependent variable is as factor?
I think the error is because your dependent variable is not a factor.
Try str(data96), or summary(data96) to see if trade962a has levels and how many levels.
If not, then, just set as a factor. For example: data96$trade962a.f = as.factor(data96$trade962a)</p>

<p>Finally, if you wanna run a logistic regression, you should use 'model=""logit""', not ""mlogit"". Mlogit is for multinomial logit. Thus, if your variable is binary, using mlogit may cause the cited error. If so, just use model=""logit"".</p>

<p>If none of the above solve your problem, try to plot your dependent variable to see if the data is all right.</p>

<p>regards,
Manoel</p>
"
2512354,142651,2010-03-25T00:10:06Z,2486130,1,FALSE,"<p>the sprintf() function is great for formatting numerical output.</p>
"
2513196,220347,2010-03-25T05:08:47Z,2513142,7,TRUE,"<p>Admittedly my R knowledge is sparse and this is drycoded, but something like the following should work:</p>

<pre><code>foo &lt;- function(x) {
    y &lt;- x-2
    if (y==0) {return(NULL)} # return NULL then check for it
    z &lt;- y + 100
    z
}

for (i in 1:3) {
    j &lt;- foo(i)
    if(is.null(j)) {break}
    print(j)
}
</code></pre>

<p><i>Edit: updated null check for posterity</i></p>
"
2513197,8707,2010-03-25T05:09:26Z,2513142,0,FALSE,"<p>I have no clue how <code>r</code> works but I found the question interesting because I could lookup a new language's syntax so excuse my answer if it is totally wrong :)</p>

<pre><code>foo &lt;- function(x) { 
    y &lt;- x-2 
    if (y!=0) z &lt;- NULL else z &lt;- y + 100 
    z 
}


for (i in 1:3)
{ 
    a &lt;- foo(i)
    if (a == NULL) {next}
    print(a)
}
</code></pre>
"
2513438,133234,2010-03-25T06:15:15Z,2513142,2,FALSE,"<p>An alternative way is to throw an error and catch it with <code>try</code>, like so:</p>

<pre><code>foo &lt;- function(x) {
    y &lt;- x-2
    if (y==0) {stop(""y==0"")} 
    z &lt;- y + 100
    z
}

try(for (i in 0:5) {
       print(foo(i))
}, silent=TRUE)

## or use tryCatch:
for (i in 0:5) {
   bar &lt;- tryCatch(foo(i),error=function(e) NA)
   if(is.na(bar)){ break } else { print(bar) }
}
</code></pre>
"
2513647,160314,2010-03-25T07:07:41Z,2513142,4,FALSE,"<p>As a matter of coding practice, don't do this. Having a function that can only be used inside a particular loop is not a great idea. As a matter of educational interest, you can evaluate the 'break' in the parent environment.</p>

<pre><code>foo &lt;- function(x) {
    y &lt;- x-2
    if (y==0) {eval.parent(parse(text=""break""),1)} 
    z &lt;- y + 100
    z
}



for (i in 0:3) {
    print(foo(i))
}
</code></pre>
"
2515455,165787,2010-03-25T12:37:59Z,2513142,4,FALSE,"<p>Are we allowed to be a little more creative? Could you recast your problem to take advantage of the following approach, where the operation is based on vectors?</p>

<pre><code>x &lt;- 1:3  
y &lt;- x[x-2 &lt; 0] - 2 + 100 # I'm leaving the ""- 2"" separate to highlight the parallel to your code  
y  
</code></pre>

<p>If, however, a deeper form underlies the question and we need to follow this pattern for now, perhaps tweak it just a bit...</p>

<pre><code>foo &lt;- function(x) {
  y &lt;- x - 2
  if (y != 0) {
    z &lt;- y + 100
    z
  } # else implicitly return value is NULL
}

for (i in 1:3) {
  if (is.numeric(result &lt;- foo(i))) {
    print(result)
  } else {
    break 
  }
}
</code></pre>
"
2516119,172261,2010-03-25T14:00:22Z,2515985,4,TRUE,"<p>Maybe <code>grid.text()</code>:</p>

<pre><code>grid.text(""label"", x = unit(xpos, ""npc""), y = unit(ypos, ""npc""))
</code></pre>

<p>with appropriate values for <code>xpos</code> and <code>ypos</code>.</p>
"
2516476,163053,2010-03-25T14:48:47Z,2516400,-1,FALSE,"<p>After some further thought, I think that this should work:</p>

<pre><code>path.expand(""~"")
</code></pre>

<p>That will give the home directory, which should have write access.</p>
"
2516929,134830,2010-03-25T15:37:17Z,2516400,10,TRUE,"<p>Yes, there is: <a href=""https://www.rdocumentation.org/packages/base/topics/tempdir"" rel=""nofollow noreferrer""><code>tempdir</code></a>.</p>

<p>This will return a session specific directory within the user's temp directory.  (So it gives the same value every time you call it within a specific R session.  Shut R and restart, and it will give you a different directory.)</p>

<p><a href=""https://www.rdocumentation.org/packages/pathological/topics/temp_dir"" rel=""nofollow noreferrer""><code>pathological::temp_dir</code></a> provides a more user friendly wrapper.</p>
"
2518457,170792,2010-03-25T18:52:17Z,2517868,1,FALSE,"<p>I think I have made it much more complicated...
System.time didn't help me in performance evaluation in such a small dataset.  </p>

<pre><code>windows &lt;- numeric(6)

mylist = vector(""list"")
mylist[[1]] = c(1,20)
mylist[[2]] = c(120,320)


library(plyr)

l_ply(mylist, function(x) {
sapply((floor(x[1]/100)+1) : (floor(x[2]/100)+1), function(z){
    eval.parent(parse(text=paste(""windows["",z,""] &lt;- "", 
        min(z*100, x[2]) - max((z-1)*100, x[1]) + 1,sep="""")),sys.nframe())
    })          
})

print(windows)
</code></pre>

<p><strong>EDIT</strong></p>

<p>A modification to eliminate <code>eval</code></p>

<pre><code>g &lt;- llply(mylist, function(x) {
ldply((floor(x[1]/100)+1) : (floor(x[2]/100)+1), function(z){
        t(matrix(c(z,min(z*100, x[2]) - max((z-1)*100, x[1]) + 1),nrow=2))
    })          
})

for(i in 1:length(g)){
    windows[unlist(g[[i]][1])] &lt;- unlist(g[[i]][2])
}
</code></pre>
"
2518629,245603,2010-03-25T19:17:05Z,2517868,0,FALSE,"<p>I don't have a bright idea, but you can get rid of the inner loop, and speed up things a bit. Notice that if a window falls fully wihtin a mylist interval, then you just have to add 100 to the corresponding <code>windows</code> element. So only the <code>st</code>-th and <code>sp</code>-th windows need special handling.</p>

<pre><code>  windows &lt;- numeric(100)
  for(i in 1:length(mylist)){ 
    win &lt;- mylist[[i]]         # for cleaner code
    st &lt;- floor(win[1]/100)+1 
    sp &lt;- floor(win[2]/100)+1 
    # start and stop are within the same window
    if (sp == st){
      windows[st] &lt;- windows[st] + (win[2]%%100) - (win[1]%%100) +1 
    }
    # start and stop are in separate windows - take care of edges
    if (sp &gt; st){
      windows[st] &lt;- windows[st] + 100 - (win[1]%%100) + 1
      windows[sp] &lt;- windows[sp] + (win[2]%%100)
    }
    # windows completely inside win
    if (sp &gt; st+1){
      windows[(st+1):(sp-1)] &lt;- windows[(st+1):(sp-1)] + 100
    }       
  }
</code></pre>

<p>I generated a bigger list:</p>

<pre><code>  cuts &lt;- sort(sample(1:10000, 70))  # random interval endpoints
  mylist &lt;- split(cuts, gl(35,2))
</code></pre>

<p>and got 1.08 sec for 1000 replicates of this version versus 1.72 sec for 1000 replicates for the original. With real data the speed-up will depend on whether the intervals in <code>mylist</code> tend to be much longer than 100 or not.</p>

<p>By the way, one could rewrite the inside loop as a separate function, and then <code>lapply</code> it over <code>mylist</code>, but that does not make it work faster.</p>
"
2519083,158065,2010-03-25T20:24:25Z,2517868,4,FALSE,"<p>So I'm not entirely sure why the third and fourth windows aren't 100 and 20 because that would make more sense to me.  Here's a one liner for that behavior:</p>

<pre><code>Reduce('+', lapply(mylist, function(x) hist(x[1]:x[2], breaks = (0:6) * 100, plot = F)$counts)) 
</code></pre>

<p>Note that you need to specify the upper bound in <code>breaks</code>, but it shouldn't be hard to make another pass to get it if you don't know it in advance.</p>
"
2519092,83761,2010-03-25T20:25:44Z,2517868,6,TRUE,"<p>The ""Right"" thing to do is to use the bioconductor <code>IRanges</code> package, which uses an IntervalTree data structure to represent these ranges.</p>

<p>Having both of your objects in their own <code>IRanges</code> objects, you would then use the <code>findOverlaps</code> function to win.</p>

<p>Get it here:</p>

<p><a href=""http://www.bioconductor.org/packages/release/bioc/html/IRanges.html"" rel=""noreferrer"">http://www.bioconductor.org/packages/release/bioc/html/IRanges.html</a></p>

<p>By the by, the internals of the package are written in C, so its super fast.</p>

<p>EDIT</p>

<p>On second thought, it's not as much of a slam-dunk as I'm suggesting (a one liner), but you should definitely start using this library if you're working at all with genomic intervals (or other types) ... you'll likely need to do some set operations and stuff. Sorry, don't have time to provide the exact answer, though.</p>

<p>I just thought it's important to point this library out to you.</p>
"
2520972,83761,2010-03-26T03:20:30Z,2520780,17,TRUE,"<p>Assuming there is only one object saved in <code>saved.file.rda</code>, about:</p>

<pre><code>bar &lt;- load('saved.file.rda')
the.object &lt;- get(bar)
</code></pre>

<p>or just:</p>

<pre><code>bar &lt;- get(load('saved.file.rda'))
</code></pre>

<p>If you want to be ""neat"" and not pollute your global workspace with the stuff you loaded (and forgot the name of), you can load your object into an environment, and specify that environment in you call to <code>get</code>.</p>

<p>Maybe:</p>

<pre><code>temp.space &lt;- new.env()
bar &lt;- load('saved.file.rda', temp.space)
the.object &lt;- get(bar, temp.space)
rm(temp.space)
...
</code></pre>
"
2520976,66549,2010-03-26T03:22:21Z,2520780,3,FALSE,"<p>well, i do know a function that eliminates the need to do that (i.e., find the name of the object in the R binary file you just loaded)--in other words, you can use this technique to load R binary files instead of 'load':</p>

<pre><code>file_path = ""/User/dy/my_R_data/a_data_set.RData""
attach(file_path, pos=2, name=choose_a_name, warn.conflict=T)
</code></pre>

<ul>
<li><p>'warn.conflicts=T' is the default
option</p></li>
<li><p>'pos=2' is also the default; ""2""
refers to the position in your search
path. For instance, position 1 is
"".GlobalEnv."" To get the entire array
of search paths, use <strong>search</strong>().  So
you would access the search path for
the new object by search()[2]</p></li>
<li><p>use 'detach' to remove the object</p></li>
</ul>
"
2521782,160314,2010-03-26T08:01:45Z,2517868,4,FALSE,"<p>Okay, so I wasted WAY too much time on this, and still only got a factor of 3 speed-up. Can anyone beat this?</p>

<p>The code:</p>

<pre><code>my &lt;- do.call(rbind,mylist)
myFloor &lt;- floor(my/100)
myRem &lt;- my%%100
#Add intervals, over counting interval endpoints
counts &lt;- table(do.call(c,apply(myFloor,1,function(r) r[1]:r[2])))
windows[as.numeric(names(counts))+1] &lt;- counts*101

#subtract off lower and upper endpoints
lowerUncovered &lt;- tapply(myRem[,1],myFloor[,1],sum)
windows[as.numeric(names(lowerUncovered))+1]  &lt;-  windows[as.numeric(names(lowerUncovered))+1]  - lowerUncovered
upperUncovered &lt;- tapply(myRem[,2],myFloor[,2],function(x) 100*length(x) - sum(x))
windows[as.numeric(names(upperUncovered))+1]  &lt;-  windows[as.numeric(names(upperUncovered))+1] - upperUncovered
</code></pre>

<p>The test:</p>

<pre><code>mylist = vector(""list"")
for(i in 1:20000){
    d &lt;- round(runif(1,,500))
    mylist[[i]] &lt;- c(d,d+round(runif(1,,700)))
}

windows &lt;- numeric(200)


new_code &lt;-function(){
    my &lt;- do.call(rbind,mylist)
    myFloor &lt;- floor(my/100)
    myRem &lt;- my%%100
    counts &lt;- table(do.call(c,apply(myFloor,1,function(r) r[1]:r[2])))
    windows[as.numeric(names(counts))+1] &lt;- counts*101

    lowerUncovered &lt;- tapply(myRem[,1],myFloor[,1],sum)
    windows[as.numeric(names(lowerUncovered))+1]  &lt;-  windows[as.numeric(names(lowerUncovered))+1]  - lowerUncovered

    upperUncovered &lt;- tapply(myRem[,2],myFloor[,2],function(x) 100*length(x) - sum(x))
    windows[as.numeric(names(upperUncovered))+1]  &lt;-  windows[as.numeric(names(upperUncovered))+1] - upperUncovered

    #print(windows)
}


#old code
old_code &lt;- function(){
    for(i in 1:length(mylist)){
        st &lt;- floor(mylist[[i]][1]/100)+1
        sp &lt;- floor(mylist[[i]][2]/100)+1
        for(j in st:sp){       
            b &lt;- max((j-1)*100, mylist[[i]][1])
            e &lt;- min(j*100, mylist[[i]][2])
            windows[j] &lt;- windows[j] + e - b + 1
        }
    }
    #print(windows)
}

system.time(old_code())
system.time(new_code())
</code></pre>

<p>The result:</p>

<pre><code>&gt; system.time(old_code())
   user  system elapsed 
  2.403   0.021   2.183 
&gt; system.time(new_code())
   user  system elapsed 
  0.739   0.033   0.588 
</code></pre>

<p>Very frustrating that the system time is basically 0, but the observed time is so great. I bet if you did go down to C you would get a 50-100X speed-up.</p>
"
2522347,168747,2010-03-26T10:04:27Z,2520780,5,FALSE,"<p>As you can read in <code>?load</code> you can load data to specified environment. Then you could use <code>get</code> and <code>ls</code> to get what you want:</p>

<pre><code>tmp_env &lt;- new.env()
load('saved.file.rda', tmp_env)
get(ls(tmp_env), envir=tmp_env) # it returns only first object in environment
# [1] ""a""
</code></pre>
"
2523208,133234,2010-03-26T12:34:07Z,2522396,9,TRUE,"<p>The results are not surprising.  For distributions with large kurtosis, expected variance of the sample variance is roughly mu4/N, where mu4 is the 4th moment of the distribution.  For a lognormal, mu4 exponentially depends on the parameter sigma^2, meaning that for large enough values of sigma, your sample variance will be all over the place relative to the true variance.  This is precisely what you have observed.  In your example, mu4/N ~ (coeffOfVar^8)/N ~ 50^8/1e6 ~ 4e7.</p>

<p>For derivation of the expected variance of sample var. see <a href=""http://mathworld.wolfram.com/SampleVarianceDistribution.html"" rel=""noreferrer"">http://mathworld.wolfram.com/SampleVarianceDistribution.html</a>.  Below is some code to illustrate the ideas in a more exact manner.  Note the large value of both the variance of the sample variance and its theoretical expected value, even for coeffOfVar = 5.</p>

<pre><code>exp.var.of.samp.var &lt;- function(n,mu2,mu4){
  (n-1)*((n-1)*mu4-(n-3)*mu2^2)/n^3
}
mu2.lnorm &lt;- function(mu,sigma){
  (exp(sigma^2)-1)*exp(2*mu+sigma^2)
}
mu4.lnorm &lt;- function(mu,sigma){
  mu2.lnorm(mu,sigma)^2*(exp(4*sigma^2)+2*exp(3*sigma^2)+3*exp(2*sigma^2)-3)
}
exp.var.lnorm.var &lt;- function(n,mu,sigma){
  exp.var.of.samp.var(n,mu2.lnorm(mu,sigma),mu4.lnorm(mu,sigma))
}
exp.var.norm.var &lt;- function(n,mu,sigma){
  exp.var.of.samp.var(n,sigma^2,3*sigma^4)
}    

coeffOfVar &lt;- 5 
mean &lt;- 2
sigma &lt;- sqrt(log(coeffOfVar^2 + 1)) # gives sigma=1.805020
mu &lt;- log(mean) - sigma^2 / 2 # mu=-0.935901
n &lt;- 1e4
m &lt;- 1e4
## Get variance of sample variance for lognormal distribution:
var.trial &lt;- replicate(m,var(rlnorm(n, mu, sigma)))
cat(""samp. variance (mean of"",m,""trials):"",mean(var.trial),""\n"")
cat(""theor. variance:"",mu2.lnorm(mu,sigma),""\n"")
cat(""variance of the sample var:"",var(var.trial),""\n"")
cat(""expected variance of the sample var:"",exp.var.lnorm.var(n,mu,sigma),""\n"")
&gt; samp. variance (mean of 10000 trials): 105.7192 
&gt; theor. variance: 100 
&gt; variance of the sample var: 350997.7 
&gt; expected variance of the sample var: 494053.2 
## Do this with normal distribution:
var.trial &lt;- replicate(m,var(rnorm(n, mu, sigma)))
cat(""samp. variance (mean of"",m,""trials):"",mean(var.trial),""\n"")
cat(""theor. variance:"",sigma^2,""\n"")
cat(""variance of the sample var:"",var(var.trial),""\n"")
cat(""expected variance of the sample var:"",exp.var.norm.var(n,mu,sigma),""\n"")
&gt; samp. variance (mean of 10000 trials): 3.257944 
&gt; theor. variance: 3.258097 
&gt; variance of the sample var: 0.002166131 
&gt; expected variance of the sample var: 0.002122826 
</code></pre>
"
2526597,163053,2010-03-26T20:40:44Z,2526305,2,FALSE,"<p>My two cents:</p>

<p>I find working with JRI to be straight-forward.  There are several examples provided with JRI that give a good demonstration of how to use it.</p>

<p>In general, you shouldn't have to make any major changes to your script because you can simply pass in an entire script as an expression with the <code>eval()</code> function and then handle the return value.  The main advantage as I see it is that you're then handling the R process from within your Java code, so you can with exceptions properly without having to make any system calls.  JRI also provides some R data type equivalents in Java such as <code>RVector</code>.</p>
"
2527798,16632,2010-03-27T01:31:15Z,2527713,6,FALSE,"<p>This is pretty much exactly what the <code>plyr</code> package is designed to make easier.  However it's unlikely that it will make things much faster - most of the time is probably spent doing the statistics.</p>
"
2527886,66549,2010-03-27T02:21:07Z,2527713,2,FALSE,"<p>You have already suggested vectorizing and avoiding making unnecessary copies of intermediate results, so you are certainly on the right track. Let me caution you not to do what i did and just <em>assume</em> that vectorizing will <em>always</em> give you a performance boost (like it does in other languages, e.g., Python + NumPy, MATLAB).</p>

<p>An example:</p>

<pre><code># small function to time the results:
time_this = function(...) {
  start.time = Sys.time(); eval(..., sys.frame(sys.parent(sys.parent()))); 
  end.time = Sys.time(); print(end.time - start.time)
}

# data for testing: a 10000 x 1000 matrix of random doubles
a = matrix(rnorm(1e7, mean=5, sd=2), nrow=10000)

# two versions doing the same thing: calculating the mean for each row
# in the matrix
x = time_this( for (i in 1:nrow(a)){ mean( a[i,] ) } )
y = time_this( apply(X=a, MARGIN=1, FUN=mean) )

print(x)    # returns =&gt; 0.5312099
print(y)    # returns =&gt; 0.661242
</code></pre>

<p>The 'apply' version is actually <strong>slower</strong> than the 'for' version. (According to the <em>Inferno</em> author, if you are doing this you are not vectorizing, you are 'loop hiding'.)</p>

<p>But where you can get a performance boost is by using <strong>built-ins</strong>.  Below, i've timed the same operation as the two above, just using the built-in function, 'rowMeans':</p>

<pre><code>z = time_this(rowMeans(a))
print(z)    # returns =&gt; 0.03679609
</code></pre>

<p>An order of magnitude improvement versus the 'for' loop (and the vectorized version).</p>

<p>The other members of the apply family are not just wrappers over a native 'for' loop.</p>

<pre><code>a = abs(floor(10*rnorm(1e6)))

time_this(sapply(a, sqrt))
# returns =&gt; 6.64 secs

time_this(for (i in 1:length(a)){ sqrt(a[i])})
# returns =&gt; 1.33 secs
</code></pre>

<p>'sapply' is about <strong>5x</strong> slower compared with a 'for' loop.</p>

<p>Finally, w/r/t vectorized versus 'for' loops, i don't think i ever use a loop if i can use a vectorized function--the latter is usually less keystrokes and and it's a more natural way (for me) to code, which is a different kind of performance boost, i suppose.</p>
"
2529173,119759,2010-03-27T11:59:42Z,2527713,3,FALSE,"<p>Besides <code>plyr</code>, you can try to use <a href=""http://cran.r-project.org/web/packages/foreach/index.html"" rel=""nofollow noreferrer""><code>foreach</code></a> package to exclude explicit loop counter, but I don't know if it will give you any performance benefits.</p>

<p><code>Foreach</code>, neverless, gives you a quite simple interface to parallel chunk processing if you have multicore workstation (with <code>doMC</code>/<code>multicore</code> packages) (check <a href=""http://cran.r-project.org/web/packages/doMC/vignettes/gettingstartedMC.pdf"" rel=""nofollow noreferrer"">Getting Started with doMC and foreach</a> for details), if you exclude parallel processing only because it is not very easy to understand for students. If it is not the only reason, <code>plyr</code> is very good solution IMHO.</p>
"
2530202,235984,2010-03-27T17:23:30Z,2527713,2,FALSE,"<p>Personally, I find plyr not very easy to understand. I prefer data.table which is also faster. For instance you want to do the standard deviation of colum my_column for each ID.</p>

<pre><code>dt &lt;- datab.table[df] # one time operation...changing format of df to table
result.sd &lt;- dt[,sd(my_column),by=""ID""] # result with each ID and SD in second column 
</code></pre>

<p>Three statements of this kind and a cbind at the end - that is all you need.
You can also use dt do some action for only one ID without a subset command in an new syntax:</p>

<pre><code>result.sd.oneiD&lt;- dt[ID=""oneID"",sd(my_column)]  
</code></pre>

<p>The first statment refers to rows (i), the second to columns (j).</p>

<p>If find it easier to read then player and it is more flexible, as you can also do sub domains within a ""subset""...
The documentation describes that it uses SQL-like methods. For instance, the by is pretty much ""group by"" in SQL. Well, if you know SQL, you can probably do much more, but it is not necessary to make use of the package.
Finally, it is extremely fast, as each operation is not only parallel, but also data.table grabs the data needed for calculation. Subset, however, maintain the levels of the whole matrix and drag it trough the memory.</p>
"
2531417,232707,2010-03-28T00:07:07Z,2531372,55,TRUE,"<p>From <a href=""http://ess.r-project.org/Manual/ess.html"" rel=""noreferrer"">ESS's manual</a> (look under ""Changes/New Features in 5.2.0""):</p>

<blockquote>
  <p>ESS[S]: Pressing underscore (""_"") once inserts "" &lt;- "" (as before); pressing underscore twice inserts a literal underscore. To stop this smart behaviour, add ""(ess-toggle-underscore nil)"" to your .emacs after ess-site has been loaded</p>
</blockquote>
"
2531598,258755,2010-03-28T01:33:44Z,2531402,1,FALSE,"<p>Could you calculate the +/- standard deviation values from the data and add a fitted curve of them to the plot? </p>
"
2531706,302970,2010-03-28T02:36:24Z,2531489,4,FALSE,"<p>I don't know enough about poisson and quasi-poisson distributions to answer your question in the depth asked for (i.e. an exact equation that will transform the variables into the residuals using the model), but if any of the confusion is due to what residual types are being used and why the two commands give a different answer, this could help:</p>

<p>resid() defaults to a ""deviance"" type in R. However, glm() assigns different residuals to the $residuals vector.</p>

<p>If you're using the quasi-poisson family, glm() will assign residuals of the working type, whereas, resid() gives the deviance type as default.</p>

<p>To try this out, you can use:</p>

<blockquote>
  <p>resid(glm,type=""working"")  </p>
</blockquote>

<p>and</p>

<blockquote>
  <p>glm$residuals  </p>
</blockquote>

<p>and that should give you the same answer (at least, it did on a sample dataset I used).</p>

<p>According to R, working residuals are: ""the residuals in the final iteration of the IWLS fit""</p>

<p>If you look up the book: ""Generalized Linear models and extensions"" (by Hardin and Hilbe) on googlebooks, you can access section 4.5 which explains the various types of residuals.</p>
"
2532018,160314,2010-03-28T05:39:19Z,2531489,16,TRUE,"<p>Calling resid(model) will default to the deviance residuals, whereas model$resid will give you the working residuals. Because of the link function, there is no single definition of what a model residual is. There are the deviance, working, partial, Pearson, and response residuals. Because these only rely on the mean structure (not the variance), the residuals for the quasipoisson and poisson have the same form. You can take a look at the <code>residuals.glm</code> function for details, but here is an example:</p>

<pre><code>counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
glm.D93 &lt;- glm(counts ~ outcome + treatment, family=quasipoisson())
glm.D93$resid


#working
resid(glm.D93,type=""working"")
(counts - glm.D93$fitted.values)/exp(glm.D93$linear)

#deviance
resid(glm.D93,type=""dev"")
fit &lt;- exp(glm.D93$linear)
poisson.dev &lt;- function (y, mu) 
    sqrt(2 * (y * log(ifelse(y == 0, 1, y/mu)) - (y - mu)))
poisson.dev(counts,fit) * ifelse(counts &gt; fit,1,-1)

#response
resid(glm.D93,type=""resp"")
counts - fit

#pearson
resid(glm.D93,type=""pear"")
(counts - fit)/sqrt(fit)
</code></pre>
"
2532944,16632,2010-03-28T12:55:06Z,2521941,3,TRUE,"<p>It is not currently possible.  Hopefully after this summer.</p>
"
2536070,144157,2010-03-29T06:10:37Z,2535740,11,FALSE,"<p>Both <code>stl()</code> and <code>decompose()</code> are for <strong>seasonal</strong> decomposition, so you must have a seasonal component. If you just want to estimate a trend, then any nonparametric smoothing method will do the job. For example:</p>

<pre><code>fit &lt;- loess(crude.data$Price ~ crude.data$Time)
plot(cbind(observed=crude.data$Price,trend=fit$fitted,random=fit$residuals),main="""")
</code></pre>
"
2536149,170792,2010-03-29T06:36:00Z,2535234,9,FALSE,"<p>Check these functions <a href=""http://finzi.psych.upenn.edu/R/library/lsa/html/cosine.html"" rel=""nofollow noreferrer"">lsa::cosine()</a>, <a href=""http://finzi.psych.upenn.edu/R/library/clv/html/dot_product.html"" rel=""nofollow noreferrer"">clv::dot_product()</a> and <a href=""http://finzi.psych.upenn.edu/R/library/arules/html/dissimilarity.html"" rel=""nofollow noreferrer"">arules::dissimilarity()</a></p>
"
2537779,95048,2010-03-29T12:26:10Z,2531402,1,FALSE,"<p>Have a look at my question ""<a href=""https://stackoverflow.com/questions/2370648/modify-lm-or-loess-function-to-use-it-within-ggplot2s-geom-smooth"">modify lm or loess function..</a>""</p>

<p>I am not sure I followed your question very well, but maybe a:</p>

<pre><code>+ stat_smooth(method=yourfunction)
</code></pre>

<p>will work, provided that you define your function as <a href=""https://stackoverflow.com/questions/2370648/modify-lm-or-loess-function-to-use-it-within-ggplot2s-geom-smooth"">described here</a>.</p>
"
2540172,66549,2010-03-29T18:05:32Z,2535234,51,TRUE,"<p>These sort of questions come up all the time (for me--and as evidenced by the <em>r</em>-tagged SO question list--others as well):</p>

<p><em>is there a built-in function that does x?</em> and if so,</p>

<p><em>where can i find it among the +2000 R Packages in CRAN?</em></p>

<p>short answer: give the <strong><em>sos package</em></strong> a try when these sort of questions come up</p>

<p>One of the earlier answers gave <em>cosine</em> along with a link to its help page. This is probably exactly what the OP wants. When you look at the linked-to page you see that this function is in the <em>lsa</em> package. </p>

<p>But <strong><em>how would you find this function if you didn't already know which Package to look for it in?</em></strong></p>

<p>you can always try the standard R help functions ("">"" below just means the R command line):</p>

<pre><code>&gt; ?&lt;some_name&gt;

&gt; ??&lt;some_name&gt;

&gt; *apropos*&lt;some_name&gt;
</code></pre>

<p>if these fail, then install &amp; load the <strong><em>sos</em></strong> package, then</p>

<pre><code>***findFn***
</code></pre>

<p><em>findFn</em> is also aliased to ""???"", though i don't often use that because i don't think you can pass in arguments other than the function name</p>

<p>for the question here, try this:</p>

<pre><code>&gt; library(sos)

&gt; findFn(""cosine"", maxPages=2, sortby=""MaxScore"")
</code></pre>

<p>The additional arguments passed in (""maxPages=2"" and ""sortby=""MaxScore"") just limits the number of results returned, and specifies how the results are ranked, respectively--ie, ""find a function named 'cosine' or that has the term 'cosine' in the function description, only return two pages of results, and order them by descending relevance score""</p>

<p>The <strong><em>findFn</em></strong> call above returns a data frame with nine columns and the results as rows--rendered as HTML. </p>

<p>Scanning the last column, <em>Description and Link</em>, item (row) 21 you find:</p>

<p><strong>Cosine Measures (Matrices)</strong> </p>

<p>this text is also a link; clicking on it takes you to the help page for that function in the Package which contains that function--in other words</p>

<p><em>using <strong>findFn</strong>, you can pretty quickly find the function you want <strong>even though</strong> you have no idea which Package it's in</em></p>
"
2540248,163053,2010-03-29T18:18:53Z,2540232,5,FALSE,"<p>You can combine those statements into a clause:</p>

<pre><code>{ v1 &lt;- readline(""Number of rows?: ""); v2 &lt;- readline(""Number of columns?: "") }
</code></pre>

<p>Or generally, make them into a function:</p>

<pre><code>readlines &lt;- function(...) {
   lapply(list(...), readline)
}
readlines(""Number of rows?: "", ""Number of columns?: "")
</code></pre>
"
2540573,66549,2010-03-29T19:10:29Z,2540129,38,FALSE,"<p>The <em>Lattice</em> Package often (but not always) ignores the <em>par</em> command, so i just avoid using it when plotting w/ <em>Lattice</em>. </p>

<p>To place multiple lattice plots on a single page:</p>

<ul>
<li><p><strong><em>create</em></strong> (but don't plot) the lattice/trellis <strong><em>plot objects</em></strong>, then</p></li>
<li><p>call <strong><em>print</em></strong> once for each plot</p></li>
<li><p>for each <em>print</em> call, pass in arguments for (i) the <em>plot</em>; (ii)
<strong><em>more</em></strong>, set to <em>TRUE</em>, and which is only passed in for the initial call to <em>print</em>, and (iii) <strong><em>pos</em></strong>, which gives the position of each plot on the page specified as x-y coordinate pairs for the plot's lower left-hand corner and upper right-hand
corner, respectively--ie, a vector with four numbers.</p></li>
</ul>

<p>much easier to show than to tell:</p>

<pre><code>data(AirPassengers)     # a dataset supplied with base R
AP = AirPassengers      # re-bind to save some typing

# split the AP data set into two pieces 
# so that we have unique data for each of the two plots
w1 = window(AP, start=c(1949, 1), end=c(1952, 1))
w2 = window(AP, start=c(1952, 1), end=c(1960, 12))

px1 = xyplot(w1)
px2 = xyplot(w2)

# arrange the two plots vertically
print(px1, position=c(0, .6, 1, 1), more=TRUE)
print(px2, position=c(0, 0, 1, .4))
</code></pre>
"
2540574,170792,2010-03-29T19:10:33Z,2540232,4,FALSE,"<p>You may find useful the <a href=""http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/editboxes.html"" rel=""nofollow noreferrer"">tkentry</a> function in package tcltk (for more examples see <a href=""http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/"" rel=""nofollow noreferrer"">here</a>). There is also a <a href=""http://finzi.psych.upenn.edu/R/library/svDialogs/html/guiDlg.html"" rel=""nofollow noreferrer"">guiDlg</a> function in package svDialogs </p>

<pre><code>library(svDialogs)
display(guiDlg(""SciViews-R"", ""My first dialog box with SciViews-R""))
</code></pre>

<p>Check <a href=""http://www.sciviews.org/SciViews-R/"" rel=""nofollow noreferrer"">this page</a> for more..</p>
"
2542668,180626,2010-03-30T03:19:12Z,2531402,4,TRUE,"<h3>Apology</h3>

<p>Folks, I want to apologize for my ignorance. Hadley is absolutely right, and the answer was right in front of me all along. As I suspected, my question was born of statistical, rather than programmatic ignorance.</p>

<h3>We get the 68% Confidence Interval for Free</h3>

<p><code>geom_smooth()</code> defaults to <code>loess</code> smoothing, <em>and</em> it superimposes the +1sd and -1sd lines as part of the deal. That's what Hadley meant when he said ""Isn't that just a 68% confidence interval?"" I just completely forgot that's what the 68% interval <em>is</em>, and kept searching for something that I already knew how to do. It didn't help that I'd actually turned the confidence intervals <em>off</em> in my code by specifying <code>geom_smooth(se = FALSE)</code>.</p>

<h3>What my Sample Code Should Have Looked Like</h3>

<pre><code># First, I'll make a simple linear model and get its diagnostic stats.
library(ggplot2)
data(cars)
mod &lt;- fortify(lm(speed ~ dist, data = cars))
attach(mod)
str(mod)

# Now I want to make sure the residuals are homoscedastic.
# By default, geom_smooth is loess and includes the 68% standard error bands.
qplot (x = dist, y = .resid, data = mod) + 
geom_abline(slope = 0, intercept = 0) +
geom_smooth() 
</code></pre>

<h3>What I've Learned</h3>

<p>Hadley implemented a really beautiful and simple way to get what I'd wanted all along. But because I was focused on loess <em>lines</em>, I lost sight of the fact that the 68% confidence interval was bounded by the very lines I needed. Sorry for the trouble, everyone.</p>
"
2544402,278198,2010-03-30T10:25:07Z,1898815,4,FALSE,"<p>R does not treat the tzone attribute of POSIXct objects consistently.  zoo has nothing to do with this.  You can come up with examples like this even without using zoo at all.  c and d in the original code actually are equal except for the tzone attribute.</p>

<p>If you remove the tzone attribute then c and d will be equal:</p>

<pre><code>attr(X.date, ""tzone"") &lt;- NULL  ##### add after defining X.date
</code></pre>

<p>In the case of zoo since it supports just about any time class you could consider using a different time class if you do not need time zones.  Time zones just complicate things for no good reason if you do not need them.  In the example here we could have used Date class, for example.</p>

<p>Read R News 4/1 for more.</p>
"
2545288,66549,2010-03-30T12:57:00Z,2545228,4,FALSE,"<pre><code>c(df$x, df$y)
# returns: 26 21 20 34 29 28
</code></pre>

<p>if the particular order is important then:</p>

<pre><code>M = as.matrix(df)
c(m[1,], c[2,], c[3,])
# returns 26 34 21 29 20 28 
</code></pre>

<p>Or more generally:</p>

<pre><code>m = as.matrix(df)
q = c()
for (i in seq(1:nrow(m))){
  q = c(q, m[i,])
}

# returns 26 34 21 29 20 28
</code></pre>
"
2545548,279497,2010-03-30T13:31:28Z,2545228,84,TRUE,"<p>You can try <code>as.vector(t(test))</code>.Please note that, if you want to have by columns you should use <code>as.vector(as.matrix(test))</code> (<code>t</code> implicitly converts to a matrix). </p>
"
2545782,147320,2010-03-30T14:02:59Z,2545668,1,FALSE,"<p>I have started using an Open Source product called <a href=""http://www.talend.com/products-data-integration/talend-open-studio.php"" rel=""nofollow noreferrer"">Talend Open Studio</a> to doing these kind of extract/transform/load tasks. It is a GUI-based code-generation tool that outputs to portable Perl or Java, and comes with gazillions of connections to database and file types.</p>

<p>It would require a learning curve; it is not entirely intuitive to do some of the more complex tasks. However, I suspect that configuring it to read your XML and output to XLS would be fairly fast and easy.</p>
"
2545877,278198,2010-03-30T14:13:32Z,2134972,2,FALSE,"<p>chron has a trunc.times method so we can do this:</p>

<pre><code>library(zoo)
library(chron)
z &lt;- zoo(1:5, 
   chron(c(""02/27/92"", ""02/27/92"", ""01/14/92"", ""02/28/92"", ""02/01/92""),
   c(""23:03:20"", ""23:29:56"", ""01:03:30"", ""18:21:03"", ""16:56:26"")))
aggregate(z, function(x) trunc(x, ""00:15:00""), mean)
</code></pre>
"
2546227,245603,2010-03-30T15:00:27Z,2545879,25,TRUE,"<p>What I usually do is to run <code>sapply</code> on the row numbers <code>1:nrow(test)</code> instead of <code>test</code>, and use <code>test[i,]</code> inside the function:</p>

<pre><code>t(sapply(1:nrow(test), function(i) test[i,]^(1/i)))
</code></pre>

<p>I am not sure this is really efficient, though.</p>
"
2546307,245603,2010-03-30T15:10:54Z,2546016,0,FALSE,"<p>How about using <code>fill=..count..</code> or <code>fill=I(..count..&gt;9)</code> right after <code>y=..density..</code>? You have to tinker with the legend title and labels a bit, but it gets the coloring right.</p>

<p>EDIT:<br>
It seems I misunderstood your question a bit. If you want to define color based on the x-coordinate, you can use the <code>..x..</code> automatic variable similarly.</p>
"
2546589,166686,2010-03-30T15:47:49Z,2545668,5,TRUE,"<p>You seem to need more help with XML concepts, in general, than with specific R packages and snippets to deal with XML files (although this may come later ;-) ). Also, you may find it preferable to convert the input file to some more palatable format <em>before</em> using it within R, Stata or other statistical tools.</p>

<p>For illustration purposes, I'm reproducing below the first <code>&lt;incident&gt;</code> record from the source mentioned in the question. We can assume that other incidents will have a similar structure.
By looking at the DTD file, we could assert whether the root contains other nodes (""records"") than <code>&lt;incidents&gt;</code> and whether these incidents have exactly the same structure (or if for example some incident types may have say an extra, say, <code>&lt;LocalWeatherConditions&gt;</code> node, or if, say, the <code>&lt;facilityList&gt;</code> node is optional). For the purpose of this discussion it's OK to assume that all incidents records have the same general structure.</p>

<p>Your request of a ""<em>spreadsheet where one row represents a single terrorist incident, and no info from the xml should be missing</em>"" may be hard to achieve because of cardinality issues. This is a fancy way to say that some sub-elements of the incident records may be repeated.  For example most of the nodes which name ends with ""List"" can typically contain more than one sub-record (BTW this ""List in the name"" thing is not an XML rule, merely a convention the custodians of this particular database are using).  For example, there could be multiple <code>&lt;CityStateProvince&gt;</code> records, each with its own values the for City and StateProvince, or there could be multiple ` records, each with its long list of values.<br>
It is possible to ""flatten out"" the data, into a single row.  The general process is one of ""denormalization"", whereby the single row includes columns with numbered labels:</p>

<pre><code>  ..., City1, StateProv1, City2, StateProv2, City3, StateProv3 ... (btw where do we stop?)
</code></pre>

<p>Furthermore, aside from leading to wide records which possibly exceed the (absolute or practical) limits of the underlying language, this format is very cumbursome with regards to aggregating and performing statistics at large: Say you wish to get counts by StateProv: you now need to instruct the program to ""look"" into all possible locations where this info is found: ""StateProv1"", ""StateProv2""...</p>

<p>An alternative format, more suitable for statistical treatment, is to export to multiple ""spreadsheets"". whereby a main spreadsheet contains one row per incident <em>for all the non repeatable</em> properties of the incident record, and additional spreadsheets contain the ""sub-records"" that may repeat.  These sub-records should include a ""key"" which can be used to relate to the underlying record in the main spreadsheet (probably the ICN, here), and they may also include duplicated info from the main spreadsheet, for example bringing in the IncidateDate, the Assanination flag etc.  The purpose of this denormalization [of another kind] is to possibly make these extra spreadsheet self sufficient for some of the targeted analysis.</p>

<p><strong>Where to go from there?</strong></p>

<ul>
<li>You need to define the precise format for the speadsheet(s) to be produced from the XML input.<br>
You'll likely agree with the fact that the numbered-labels approach is impractical and hence you'll need to look at the input data and see how you wish to split it (again with ability to replicate data).</li>
<li>You can use R for example this <a href=""http://watson.nci.nih.gov/cran_mirror/index.html"" rel=""noreferrer"">XML Package</a> to parse the input into R variables (table, lists, vectors...)</li>
<li>Alternatively, you can (I think should), use an external program, to perform this export of the XML input into tabular form (CSV format and the like), which is more readily ingested by R.<br>
Although I use the XML package mentioned, for small files (and mostly for output purposes), I fear it may be inefficient, bug prone (you lack the ability to inspect, easily, the  effective input, as can be done with a text file), and generally clumsy.</li>
</ul>

<p>Luckily, you can soon get over this conversion/import job, and focus on the stats at hand!</p>

<p>A few final pointers:</p>

<ul>
<li><p>Even if you do not readily understand the DTD language, take a look at the XTD file, in particular the many <code>&lt;xs:enumeration ...&gt;</code> lists, which comprise the bulk of the file, as these will supply you the factor (in R lingo) values.  Of course R can infer these as well, from the data, but you can use the info from the enumerations for cross referencing purposes (to confirm that the data was <em>a priori</em> loaded properly etc.)<br></p></li>
<li><p>It is probably ok to infer the schema from <em>several</em> record samples (people unfamiliar with XML can more readily understand XML data than XSD files).  To be sure however one needs to read the XSD file.</p></li>
</ul>

<hr>

<pre><code>&lt;IncidentList xmlns=""http://wits.nctc.gov"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://wits.nctc.gov WITS.XSD""&gt;

&lt;Incident&gt;
   &lt;ICN&gt;200458431&lt;/ICN&gt;
   &lt;Subject&gt;10 civilians killed, at least 45 wounded by suspected GAM in Peureulak, Indonesia&lt;/Subject&gt;
   &lt;Summary&gt;On 1 January 2004, in Peureulak, Aceh Province, Indonesia, a bomb exploded at a concert, killing ten civilians, wounding 45 others, and causing major damage to the stage area.  Many of the victims were Indonesian teenagers.  Police blamed the Free Aceh Movement (GAM), although the GAM denied responsibility.  No other group claimed responsibility.&lt;/Summary&gt;
   &lt;IncidentDate&gt;01/01/2004&lt;/IncidentDate&gt;
   &lt;ApproximateDate&gt;No&lt;/ApproximateDate&gt;
   &lt;MultipleDays&gt;No&lt;/MultipleDays&gt;
   &lt;EventTypeList&gt;
      &lt;EventType&gt;Bombing&lt;/EventType&gt;
   &lt;/EventTypeList&gt;
   &lt;Assassination&gt;No&lt;/Assassination&gt;
   &lt;Suicide&gt;No&lt;/Suicide&gt;
   &lt;WeaponTypeList&gt;
       &lt;WeaponType&gt;Explosive&lt;/WeaponType&gt;
   &lt;/WeaponTypeList&gt;
   &lt;IED&gt;No&lt;/IED&gt;
   &lt;Location&gt;
      &lt;Region&gt;East Asia-Pacific&lt;/Region&gt;
      &lt;Country&gt;Indonesia&lt;/Country&gt;
      &lt;CityStateProvinceList&gt;
         &lt;CityStateProvince&gt;
            &lt;City&gt;Peureulak&lt;/City&gt;
            &lt;StateProvince&gt;Aceh&lt;/StateProvince&gt;
         &lt;/CityStateProvince&gt;
      &lt;/CityStateProvinceList&gt;
   &lt;/Location&gt;
   &lt;VictimList&gt;
      &lt;Victim&gt;
      &lt;VictimType&gt;Civilian&lt;/VictimType&gt;
      &lt;Combatant&gt;No&lt;/Combatant&gt;
      &lt;Nationality&gt;Indonesia&lt;/Nationality&gt;
      &lt;DefiningCharacteristicList&gt;
         &lt;DefiningCharacteristic&gt;None&lt;/DefiningCharacteristic&gt;
      &lt;/DefiningCharacteristicList&gt;
      &lt;TargetedCharacteristicList&gt;
         &lt;TargetedCharacteristic&gt;Unknown&lt;/TargetedCharacteristic&gt;
      &lt;/TargetedCharacteristicList&gt;
      &lt;Indicator&gt;Targeted&lt;/Indicator&gt;
      &lt;Child&gt;No&lt;/Child&gt;
      &lt;DeadCount&gt;10&lt;/DeadCount&gt;
      &lt;WoundedCount&gt;45&lt;/WoundedCount&gt;
      &lt;HostageCount&gt;0&lt;/HostageCount&gt;
      &lt;/Victim&gt;
   &lt;/VictimList&gt;
   &lt;FacilityList&gt;
      &lt;Facility&gt;
         &lt;FacilityType&gt;Public Place/Retail&lt;/FacilityType&gt;
         &lt;Combatant&gt;No&lt;/Combatant&gt;
         &lt;Nationality&gt;Indonesia&lt;/Nationality&gt;
         &lt;DefiningCharacteristicList&gt;
         &lt;DefiningCharacteristic&gt;None&lt;/DefiningCharacteristic&gt;
         &lt;/DefiningCharacteristicList&gt;
         &lt;TargetedCharacteristicList&gt;
         &lt;TargetedCharacteristic&gt;Unknown&lt;/TargetedCharacteristic&gt;
         &lt;/TargetedCharacteristicList&gt;
         &lt;Indicator&gt;Targeted&lt;/Indicator&gt;
         &lt;Damage&gt;Light&lt;/Damage&gt;
         &lt;Quantity&gt;1&lt;/Quantity&gt;
      &lt;/Facility&gt;
   &lt;/FacilityList&gt;
   &lt;PerpetratorList&gt;
      &lt;Perpetrator&gt;
         &lt;Nationality&gt;Indonesia&lt;/Nationality&gt;
         &lt;Characteristic&gt;Secular/Political/Anarchist&lt;/Characteristic&gt;
      &lt;/Perpetrator&gt;
   &lt;/PerpetratorList&gt;
&lt;/Incident&gt;
[...]
&lt;/IncidentList&gt;
</code></pre>
"
2547164,457898,2010-03-30T17:17:17Z,2546016,0,FALSE,"<p>What about <code>scale_manual</code>? Here's <a href=""http://had.co.nz/ggplot2/scale_manual.html"" rel=""nofollow noreferrer"">link</a> to Hadley's site. I've used this function to set an appropriate fill colour for a boxplot. Not sure if it'll work with histogram, though...</p>
"
2547551,142879,2010-03-30T18:19:29Z,2547402,34,FALSE,"<p>found this on the r mailing list, hope it's helpful. It is also what I was thinking anyways. You'll want to table() the data, sort and then pick the first name. It's hackish but should work.</p>

<pre><code>names(sort(-table(x)))[1]
</code></pre>
"
2547607,166686,2010-03-30T18:25:50Z,2547402,2,FALSE,"<p>R has so many add-on packages that some of them may well provide the [statistical] mode of a numeric list/series/vector.</p>

<p>However the standard library of R itself doesn't seem to have such a built-in method!  One way to work around this is to use some construct like the following (and to turn this to a function if you use often...):</p>

<pre><code>mySamples &lt;- c(19, 4, 5, 7, 29, 19, 29, 13, 25, 19)
tabSmpl&lt;-tabulate(mySamples)
SmplMode&lt;-which(tabSmpl== max(tabSmpl))
if(sum(tabSmpl == max(tabSmpl))&gt;1) SmplMode&lt;-NA
&gt; SmplMode
[1] 19
</code></pre>

<p>For bigger sample list, one should consider using a temporary variable for the max(tabSmpl) value  (I don't know that R would automatically optimize this)</p>

<p>Reference: see ""How about median and mode?"" in this <a href=""http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_dstat.html"" rel=""nofollow noreferrer""><strong>KickStarting R lesson</strong></a><br>
This seems to confirm that (at least as of the writing of this lesson) there isn't a mode function in R  (well...  mode() as you found out is used for asserting the type of variables).</p>
"
2547766,183988,2010-03-30T18:47:15Z,1735540,4,FALSE,"<p>With a simple example:</p>

<pre><code> &gt; data
    PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8     PC9    PC10 
0.29056 0.23833 0.11003 0.05549 0.04678 0.03788 0.02770 0.02323 0.02211 0.01925 
</code></pre>

<p><code>barplot(data)</code> does things correctly</p>

<p>the ggplot equivalent ""should be"": <code>qplot(x=names(data), y=data, geom='bar')</code></p>

<p>But that incorrectly reorders/sorts the bars alphabetically... because that's how <code>levels(factor(names(data)))</code> would be ordered.</p>

<p>Solution: <code>qplot(x=factor(names(data), levels=names(data)), y=data, geom='bar')</code></p>

<p>Phew!</p>
"
2547918,170792,2010-03-30T19:05:46Z,2547402,51,FALSE,"<p>There is package <code>modeest</code> which provide estimators of the mode of univariate unimodal (and sometimes multimodal) data and values of the modes of usual probability distributions.</p>

<pre><code>mySamples &lt;- c(19, 4, 5, 7, 29, 19, 29, 13, 25, 19)

library(modeest)
mlv(mySamples, method = ""mfv"")

Mode (most likely value): 19 
Bickel's modal skewness: -0.1 
Call: mlv.default(x = mySamples, method = ""mfv"")
</code></pre>

<p>For more information see <a href=""http://finzi.psych.upenn.edu/R/library/modeest/html/mlv.html"" rel=""noreferrer"">this page</a></p>
"
2547919,172261,2010-03-30T19:05:54Z,2547306,21,TRUE,"<p>The <em>grid</em>-based graphics functions in lattice and ggplot2 create a graph object, but do not display it. The <code>print()</code> method for the graph object produces the actual display, i.e.,</p>

<pre><code>print(qplot(x, y))
</code></pre>

<p>solves the problem.</p>

<p>See <a href=""http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-lattice_002ftrellis-graphics-not-work_003f"" rel=""noreferrer"">R FAQ 7.22</a>.</p>
"
2548085,16632,2010-03-30T19:31:34Z,2546016,11,TRUE,"<p>Hopefully this should get you started:</p>

<pre><code>ggplot(data, aes(status, fill = ..x..))+
  geom_histogram(binwidth = 1) + 
  scale_fill_gradient(low = ""black"", high = ""white"")

ggplot(data, aes(status, fill = ..x.. &gt; 9))+
  geom_histogram(binwidth = 1) + 
  scale_fill_grey()
</code></pre>
"
2548416,279497,2010-03-30T20:21:29Z,2547402,10,FALSE,"<p>Here, another solution:</p>

<pre><code>freq &lt;- tapply(mySamples,mySamples,length)
#or freq &lt;- table(mySamples)
as.numeric(names(freq)[which.max(freq)])
</code></pre>
"
2548871,166686,2010-03-30T21:27:05Z,2548815,178,TRUE,"<p>Use </p>

<pre><code>&gt; basename(""C:/some_dir/a.ext"")
[1] ""a.ext""
&gt; dirname(""C:/some_dir/a.ext"")
[1] ""C:/some_dir""
</code></pre>
"
2548889,74658,2010-03-30T21:30:30Z,2547447,6,FALSE,"<p>here is a few things I've found from Google.</p>

<p>This topic was discussed on the mailing list <a href=""https://stat.ethz.ch/pipermail/r-devel/2009-October/055057.html"" rel=""noreferrer"">here</a> which mentioned the packages above.  It seems RUnit and svunit will execute the same test code, but Runit also has tracking and code coverage functions.</p>

<p>A comparison of the two is also on the R wiki at <a href=""http://rwiki.sciviews.org/doku.php?id=developers:runit"" rel=""noreferrer"">http://rwiki.sciviews.org/doku.php?id=developers:runit</a></p>
"
2552083,134830,2010-03-31T10:30:47Z,2547447,28,TRUE,"<p>Unit testing seems to be more or less a solved problem, so all three packages will likely be adequate for your needs.  There are subtle differences between them though:</p>

<p><code>RUnit</code> is based on xunit, and as such is easy to understand if you've used any versions of it from other languages.</p>

<p><code>svUnit</code> uses the same tests as RUnit, but includes a GUI to help with interactive use.</p>

<p><code>testthat</code> isn't compatible with either, but includes much the same features and can check to only execute tests on files that haven't changed, which is useful for testing large projects.</p>

<hr>

<p><em>Several years later...</em></p>

<p><a href=""https://cran.r-project.org/web/packages/RUnit"" rel=""nofollow noreferrer""><code>RUnit</code></a> and <a href=""https://cran.r-project.org/web/packages/svUnit"" rel=""nofollow noreferrer""><code>svUnit</code></a> still don't have checks for messages and warnings, nor test caching, and don't seem to be under much development, so <a href=""https://cran.r-project.org/web/packages/testthat/"" rel=""nofollow noreferrer""><code>testthat</code></a> should be your first choice for new projects.  You can convert <code>RUnit</code> tests to <code>testthat</code> tests using my <a href=""https://cran.r-project.org/web/packages/runittotestthat/"" rel=""nofollow noreferrer""><code>runittotestthat</code></a> package.</p>
"
2552751,216064,2010-03-31T12:15:13Z,2551921,0,FALSE,"<p>If you are not restricted to ggplot2, you could use ?text from base graphics or ?boxed.labels from the plotrix package.</p>
"
2553017,170792,2010-03-31T12:56:04Z,2551921,4,FALSE,"<p>A hard way to do it. I'm sure there are better approaches.</p>

<pre><code>ggplot(mtcars,aes(factor(cyl))) + 
geom_bar() + 
geom_text(aes(y=sapply(cyl,function(x) 1+table(cyl)[names(table(cyl))==x]),
label=sapply(cyl,function(x) table(cyl)[names(table(cyl))==x])))
</code></pre>
"
2553204,245603,2010-03-31T13:19:20Z,2553108,2,FALSE,"<p>How about testing for missingness:</p>

<pre><code>ifelse(is.na(df$status), df$conc, as.character(df$status))
</code></pre>
"
2553324,245603,2010-03-31T13:35:37Z,2551921,16,TRUE,"<p><code>geom_text</code> is tha analog of <code>text</code> from base graphics:</p>

<pre><code>p + geom_bar() + stat_bin(aes(label=..count..), vjust=0, 
                          geom=""text"", position=""identity"")
</code></pre>

<p>If you want to adjust the y-position of the labels, you can use the <code>y=</code> aesthetic within <code>stat_bin</code>: for example, <code>y=..count..+1</code> will put the label one unit above the bar.</p>

<p>The above also works if you use <code>geom_text</code> and <code>stat=""bin""</code> inside.</p>
"
2553430,168747,2010-03-31T13:47:41Z,2553108,3,FALSE,"<p>You must explicit test for <code>NA</code> so you can use:</p>

<pre><code>ifelse(df$status==""NR"" | is.na(df$status),""NR"",df$conc) # gives you NR for NA
</code></pre>

<p>or</p>

<pre><code>ifelse(df$status==""NR"" &amp; !is.na(df$status),""NR"",df$conc) # gives you df$conc for NA
</code></pre>
"
2554227,306035,2010-03-31T15:27:11Z,2545879,2,FALSE,"<p>Actually, in the case of a matrix, you don't even need ""apply"". Just:</p>

<p>test^(1/row(test))</p>

<p>does what you want, I think. I think the ""row"" function is the thing you are looking for.</p>
"
2554412,143319,2010-03-31T15:53:45Z,2553108,4,TRUE,"<p>Use <code>%in%</code> instead of <code>==</code> :</p>

<p><code>ifelse(df$status %in% ""NR"",""NR"", df$conc)</code></p>

<p>Side-by-side comparison of the two methods:</p>

<pre><code>data.frame(df, ph = ifelse(df$status==""NR"",""NR"",df$conc), mp = ifelse(df$status %in% ""NR"",""NR"",df$conc))
</code></pre>

<p>Check out <code>?match</code> for more information - I'm not sure I could explain it well.</p>
"
2558063,163809,2010-04-01T04:06:45Z,2557863,2,FALSE,"<p>Have you tried the function <code>cor</code>?  There is a method you can set to <code>""kendall""</code> (also options for <code>""pearson""</code> and<code>""spearman""</code> if needed), not sure if that covers all the standard errors you are looking for but it should get you started.</p>
"
2558326,306589,2010-04-01T05:31:05Z,2545879,1,FALSE,"<p><code>cbind()</code>ing the row numbers seems a pretty straightforward approach. For a matrix (or a data frame) the following should work:</p>

<pre><code>apply( cbind(1:(dim(test)[1]), test), 1, function(x) plot(x[-1], main=x[1]) )
</code></pre>

<p>or whatever you want to plot.</p>
"
2558354,212593,2010-04-01T05:40:29Z,2558191,13,TRUE,"<p>Use <code>aggregate</code> to summarize across a factor:</p>

<pre><code>&gt; df&lt;-read.table(textConnection('
+ egg 1 20
+ egg 2 30
+ jap 3 50
+ jap 1 60'))
&gt; aggregate(df$V3,list(df$V1),mean)
  Group.1  x
1     egg 25
2     jap 55
</code></pre>

<p>For more flexibility look at the <code>tapply</code> function and the <code>plyr</code> package.</p>

<p>In <code>ggplot2</code> use <code>stat_summary</code> to summarize</p>

<pre><code>qplot(V1,V3,data=df,stat=""summary"",fun.y=mean,geom='bar',width=0.4)
</code></pre>
"
2558452,160314,2010-04-01T06:07:25Z,2557863,4,FALSE,"<p>Just to expand of Stedy's answer...  <code>cor(x,y,method=""kendall"")</code> will give you the correlation, <code>cor.test(x,y,method=""kendall"")</code> will give you a p-value and CI.</p>

<p>Also, take a look at the Kendall package, which provides a function which claims a better approximation.</p>

<pre><code>&gt; library(Kendall)
&gt; Kendall(x,y)
</code></pre>

<p>There is also the cor.matrix function in the Deducer package for nice printing:</p>

<pre><code>&gt; library(Deducer)
&gt; cor.matrix(variables=d(mpg,hp,wt),,
+ data=mtcars,
+ test=cor.test,
+ method='kendall',
+ alternative=""two.sided"",exact=F)

                          Kendall's rank correlation tau                          

           mpg     hp      wt     
mpg    cor 1       -0.7428 -0.7278
         N 32      32      32     
    stat**         -5.871  -5.798 
   p-value         0.0000  0.0000 
----------                        
 hp    cor -0.7428 1       0.6113 
         N 32      32      32     
    stat** -5.871          4.845  
   p-value 0.0000          0.0000 
----------                        
 wt    cor -0.7278 0.6113  1      
         N 32      32      32     
    stat** -5.798  4.845          
   p-value 0.0000  0.0000         
----------                        
    ** z
    HA: two.sided 
</code></pre>
"
2560050,457898,2010-04-01T11:42:25Z,2557863,0,FALSE,"<p>There's a routine for Kendall's coefficient in <code>psych</code> package with <code>corr.test(x, method = ""kendall"")</code>. This function can be applied on data.frame, and also displays <em>p-values</em> for each pair of variables. I guess it displays <em>tau-a</em> coefficient. Only downside is that it's actually a wrapper for <code>cor()</code> function.</p>

<p>Wikipedia has <a href=""http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient"" rel=""nofollow noreferrer"">good reference</a> on Kendall's coefficient, and check <a href=""http://www.wessa.net/rwasp_kendall.wasp"" rel=""nofollow noreferrer"">this link</a> out. Try <code>sos</code> package and <code>findFn()</code> function. I got bunch of stuff when querying <code>""tau a""</code> and <code>tau b</code>, but both ended with no luck. And search results seem to merge to <code>Kendall</code> package, as <strong>@Ian</strong> suggested.</p>
"
2560507,269476,2010-04-01T13:05:40Z,2560431,3,FALSE,"<p>Do you have the 2 lists in b held as separate variables?</p>

<p>If so you can use:</p>

<pre><code>x&lt;-data.frame(rbind(b1,b2))
rownames(x)&lt;-a
</code></pre>
"
2560623,163053,2010-04-01T13:23:11Z,2560431,6,FALSE,"<p>There are many ways to do this kind of thing.  Your first problem is that your ""b"" object is not a matrix.  You need to define it as one with rows and columns (or by using rbind).</p>

<p>You can create the data frames and then combine them (this is better than working with a matrix to begin with, because it will maintain each objects type as numeric or character, etc., while a matrix would lose that):</p>

<pre><code>x1 &lt;- data.frame(X=c(""a"",""b""))
x2 &lt;- data.frame(rbind(c(1,2,3), c(4,5,6)))
data.frame(x1, x2)
  X X1 X2 X3
1  a  1  2  3
2  b  4  5  6
</code></pre>

<p>If x1 is for rownames, then you should follow James's example:</p>

<pre><code>x2 &lt;- data.frame(rbind(c(1,2,3), c(4,5,6)))
rownames(x2) &lt;- c(""a"",""b"")
</code></pre>
"
2560685,299077,2010-04-01T13:34:09Z,2560431,2,FALSE,"<p>Another way</p>

<pre><code>a = c(""a"",""b"")
b = list(c(1,2,3), c(4,5,6))

library(plyr)
df &lt;- ldply(b)
rownames(df) &lt;- a
</code></pre>
"
2562615,306879,2010-04-01T18:18:15Z,2560431,0,FALSE,"<p>Thank you a lot !
Before reading your response, I tried another method using :</p>

<pre><code>for (x in niveaux) {
    assign(x,pi)
    assign(paste(x,""moy""),""moy"")
    }
# Que j'utilise ensuite avec qq chose du genre :
assign(x,append(get(x),1))
</code></pre>

<p>But it's more complicated!</p>

<p>Olivier</p>

<p>PS : 'a' can be a rowname or a column, the aim is to export data.frame as a CSV file. </p>
"
2563683,163053,2010-04-01T21:03:10Z,2563511,9,TRUE,"<p>Well, you could just update them with the <code>update.packages()</code> function.  </p>

<p>You could use <code>installed.packages()</code> and <code>available.packages()</code> to find any differences.  Just merge the two results together on the name, and then look for version differences.</p>

<pre><code>i &lt;- installed.packages()
a &lt;- available.packages()
ia &lt;- merge(i, a, by=""Package"")[,c(""Package"", ""Version.x"", ""Version.y"")]
ia[as.character(ia$Version.x) != as.character(ia$Version.y),]
</code></pre>
"
2564029,216064,2010-04-01T22:14:54Z,2563824,22,FALSE,"<p>You could use rbind:</p>

<pre><code>d &lt;- data.frame()
for (i in 1:20) {d &lt;- rbind(d,c(i+i, i*i, i/1))}
</code></pre>
"
2564041,163053,2010-04-01T22:17:32Z,2563824,4,FALSE,"<p>For loop have side-effects, so the usual way of doing this is to create an empty dataframe before the loop and then add to it on each iteration. You can instantiate it to the correct size and then assign your values to the i'th row on each iteration, or else add to it and reassign the whole thing using rbind().</p>

<p>The former approach will have better performance for large datasets.  </p>
"
2564276,233293,2010-04-01T23:33:35Z,2564258,408,TRUE,"<p><code>lines()</code> or <code>points()</code> will add to the existing graph, but will not create a new window. So you'd need to do</p>

<pre><code>plot(x,y1,type=""l"",col=""red"")
lines(x,y2,col=""green"")
</code></pre>
"
2564280,215542,2010-04-01T23:34:06Z,2564258,14,FALSE,"<p>If you are using base graphics (i.e. not lattice/ grid graphics), then you can mimic MATLAB's hold on feature by using the points/lines/polygons functions to add additional details to your plots without starting a new plot. In the case of a multiplot layout, you can use <code>par(mfg=...)</code> to pick which plot you add things to.</p>
"
2564421,133234,2010-04-02T00:16:02Z,2564275,6,TRUE,"<p>That's not strange -- your pl.f doesn't take <code>i</code> as a parameter.  In fact, if you don't define <code>i</code>, you can't even run your code.  I think you want something like</p>

<pre><code>pl.f &lt;- function(i)
   ggplot(diamonds.tf, aes(x=diamonds.tf[,i]))+
            geom_bar()+xlab(names(diamonds.tf[i]))

for (i in 1:ncol(diamonds.tf)) {
  p &lt;- pl.f(i)
  ggsave(paste(""plot.f"",i,"".png"",sep=""""), plot=p, height=3.5, width=5.5)
}
</code></pre>
"
2565380,158065,2010-04-02T06:44:46Z,2564765,17,TRUE,"<p>The output of <code>by</code> is essentially just a list.  If you want to combine those vectors, you can use <code>do.call(rbind, BT_by)</code> (or <code>cbind</code> depending on what shape you actually want).</p>
"
2566909,203692,2010-04-02T13:41:01Z,2566766,2,FALSE,"<p>(if I understand correctly) You could use ddply:</p>

<pre><code>ff &lt;- data.frame(f1=c(""a"", ""b"", ""b"", ""b"", ""b"", ""b"", ""b""), f2=c(""p"", ""p"", ""p"", ""q"", ""q"", ""q"", ""q""), f3=c(""x"",""x"",""x"",""x"",""y"", ""y"", ""y""), val=c(1:7)) 

ddply(ff, .(f1), numcolwise(sum))
ddply(ff, .(f2), numcolwise(sum))
ddply(ff, .(f3), numcolwise(sum))
</code></pre>
"
2566916,245603,2010-04-02T13:41:53Z,2566766,3,FALSE,"<p>The general approach is to use the <code>apply</code> function, but specifically for totals the <code>margin.table</code> function might be more convenient:</p>

<pre><code>#create 3 factors
a &lt;- gl(2,4, length=20)
b &lt;- gl(3,2, length=20)
d &lt;- gl(4,2, length=20)
# table
tt &lt;- xtabs(~a+b+d)

# marginal sums
margin.table(tt, 1)
apply(tt, 1, sum)  #same answer

#multi-way margins
margin.table(tt, 1:2)
apply(tt, 1:2, sum)  #same answer
</code></pre>
"
2567473,269476,2010-04-02T15:27:46Z,2566766,0,FALSE,"<p>Comments aren't working above. Thanks for the answers, but they didn't do what I was expecting - individual totals in each subgrouping.</p>

<p>After a little digging around, I found that the xtabs output in this case is a 3 dimensional array, and wrote the following function to achieve my desired result (note its incomplete, but works for column totals so far):</p>

<pre><code>xtabTotals &lt;- function(tabs,margin=1)
# takes a 3 dimensional xtabs array and performs margin total on each sub table
# only doing column margins so far
{
    out &lt;- array(0,dim(tabs)+c(1,0,0))
    dnout &lt;- dimnames(tabs)
    dnout[[1]] &lt;- c(dnout[[1]],""Total"")
    dimnames(out) &lt;- dnout

    for (i in 1:dim(tabs)[3])
    {
        out[,,i] &lt;- rbind(tabs[,,i],colSums(tabs[,,i]))
    }
    out
}
</code></pre>
"
2567869,66549,2010-04-02T16:41:58Z,2557863,35,TRUE,"<p>There are <em>three</em> <strong>Kendal tau statistics</strong> (<em>tau-a</em>, <em>tau-b</em>, and <em>tau-c</em>).</p>

<p>They are <strong>not</strong> interchangeable, and none of the answers posted so far deal with the last two, which is the subject of the OP's question.</p>

<p>I was unable to find functions to calculate tau-b or tau-c, either in the R <em>Standard Library</em> (<em>stat et al</em>.) or in any of the Packages available on CRAN or other repositories. I used the excellent R Package <strong>sos</strong> to search, so i believe results returned were reasonably thorough.</p>

<p>So that's the short answer to the OP's Question: <strong>no built-in or Package function for tau-b or tau-c</strong>.</p>

<p>But it's easy to roll your own.</p>

<p>Writing R functions for the Kendall statistics is just a matter of 
translating these equations into code:</p>

<pre><code>Kendall_tau_a = (P - Q) / (n*(n-1)/2)

Kendall_tau_b = (P - Q) / ( (P + Q + Y0)*(P + Q + X0) )^0.5 

Kendall_tau_c = (P-Q)*( (2*m)/n^2*(m-1) )
</code></pre>

<p><strong>tau-a:</strong> equal to concordant minus discordant pairs, divided by a factor to account for total number of pairs (sample size).</p>

<p><strong>tau-b:</strong> explicit accounting for <em>ties</em>--ie, both members of the data pair have the same value; this value is equal to concordant minus discordant pairs divided by a <em>term representing the geometric mean between the number of pairs not tied</em> on x (X0) and the number not tied on y (Y0).</p>

<p><strong>tau-c:</strong> <em>larger-table variant</em> also optimized for non-square tables; equal to concordant minus discordant pairs multiplied by a factor that adjusts for table size).</p>

<pre><code># number of concordant pairs 
P = function(t) {   
  r_ndx = row(t)
  c_ndx = col(t)
  sum(t * mapply(function(r, c){sum(t[(r_ndx &gt; r) &amp; (c_ndx &gt; c)])},
    r = r_ndx, c = c_ndx))}

# number of discordant pairs
Q = function(t) {
  r_ndx = row(t)
  c_ndx = col(t)
  sum(t * mapply( function(r, c){
      sum(t[(r_ndx &gt; r) &amp; (c_ndx &lt; c)])
  },
    r = r_ndx, c = c_ndx) )
}

# sample size (total number of pairs)
n = n = sum(t)

# the lesser of number of rows or columns
m = min(dim(t))
</code></pre>

<p>So these four parameters are all you need to calculate <em>tau-a</em>, <em>tau-b</em>, and <em>tau-c</em>:</p>

<ul>
<li><p><strong>P</strong></p></li>
<li><p><strong>Q</strong></p></li>
<li><p><strong>m</strong></p></li>
<li><p><strong>n</strong></p></li>
</ul>

<p>(plus <strong>XO</strong> &amp; <strong>Y0</strong> for <em>tau-b</em>)</p>

<hr>

<p>For instance, the code for <strong>tau-c</strong> is:</p>

<pre><code>kendall_tau_c = function(t){
    t = as.matrix(t) 
    m = min(dim(t))
    n = sum(t)
    ks_tauc = (m*2 * (P(t)-Q(t))) / ((n^2)*(m-1))
}
</code></pre>

<p>So how are Kendall's tau statistics <em>related to</em> the other statistical tests used in categorical data analysis?</p>

<p>All three Kendall tau statistics, along with Goodman's and Kruskal's <em>gamma</em> are for <strong>correlation of ordinal and binary data</strong>. (The Kendall tau statistics are more sophisticated alternatives to the gamma statistic (just P-Q).)</p>

<p>And so Kendalls's <em>tau</em> and the <em>gamma</em> are counterparts to the simple <em>chi-square</em> and <em>Fisher's exact tests</em>, both of which are (as far as i know) suitable only for <em>nominal data</em>. </p>

<p><strong>example:</strong></p>

<pre><code>cpa_group = c(4, 2, 4, 3, 2, 2, 3, 2, 1, 5, 5, 1)
revenue_per_customer_group = c(3, 3, 1, 3, 4, 4, 4, 3, 5, 3, 2, 2)
weight = c(1, 3, 3, 2, 2, 4, 0, 4, 3, 0, 1, 1)

dfx = data.frame(CPA=cpa_group, LCV=revenue_per_customer_group, freq=weight)

# reshape data frame so 1 row for each event 
# (prediate step to create contingency table)
dfx2 = data.frame( lapply(dfx, function(x){rep(x, dfx$freq)}))

t = xtabs(~ revenue + cpa, dfx)

kc = kendall_tau_c(t)

# returns -.35
</code></pre>
"
2567915,160314,2010-04-02T16:52:14Z,2566766,4,FALSE,"<p>If you are not tied to xtabs, the Deducer package has some nice functions for contingency tables:</p>

<pre><code>&gt; a &lt;- gl(2,4, length=20)
&gt; b &lt;- gl(3,2, length=20)
&gt; d &lt;- rnorm(20)&gt;0
&gt; dat &lt;- data.frame(a,b,d)
&gt; tables&lt;-contingency.tables(
+ row.vars=a,
+ col.vars=b,
+ stratum.var=d,data=dat)
&gt; tables
================================================================================

               ==================================================               
                      ========== Table: a by b ==========                      

                       | -- Stratum = FALSE --
                       | b 
                     a |        1  |        2  |        3  | Row Total | 
-----------------------|-----------|-----------|-----------|-----------|
           1  Count    |        2  |        2  |        1  |        5  | 
              Row %    |   40.000% |   40.000% |   20.000% |   55.556% | 
              Column % |   40.000% |  100.000% |   50.000% |           | 
              Total %  |   22.222% |   22.222% |   11.111% |           | 
-----------------------|-----------|-----------|-----------|-----------|
           2  Count    |        3  |        0  |        1  |        4  | 
              Row %    |   75.000% |    0.000% |   25.000% |   44.444% | 
              Column % |   60.000% |    0.000% |   50.000% |           | 
              Total %  |   33.333% |    0.000% |   11.111% |           | 
-----------------------|-----------|-----------|-----------|-----------|
          Column Total |        5  |        2  |        2  |        9  | 
              Column % |   55.556% |   22.222% |   22.222% |           | 

                       | -- Stratum = TRUE --
                       | b 
                     a |        1  |        2  |        3  | Row Total | 
-----------------------|-----------|-----------|-----------|-----------|
           1  Count    |        2  |        2  |        3  |        7  | 
              Row %    |   28.571% |   28.571% |   42.857% |   63.636% | 
              Column % |   66.667% |   50.000% |   75.000% |           | 
              Total %  |   18.182% |   18.182% |   27.273% |           | 
-----------------------|-----------|-----------|-----------|-----------|
           2  Count    |        1  |        2  |        1  |        4  | 
              Row %    |   25.000% |   50.000% |   25.000% |   36.364% | 
              Column % |   33.333% |   50.000% |   25.000% |           | 
              Total %  |    9.091% |   18.182% |    9.091% |           | 
-----------------------|-----------|-----------|-----------|-----------|
          Column Total |        3  |        4  |        4  |       11  | 
              Column % |   27.273% |   36.364% |   36.364% |           | 


================================================================================
</code></pre>
"
2568363,158065,2010-04-02T18:20:34Z,2568234,12,TRUE,"<pre><code>require(ggplot2)
qplot(trans.factor, casp6)
</code></pre>
"
2568877,163053,2010-04-02T19:59:51Z,2568840,15,FALSE,"<p>Use</p>

<pre><code>df[df$id %in% v,]
</code></pre>
"
2568882,66549,2010-04-02T20:00:55Z,2568840,11,TRUE,"<p>This should do what you want:</p>

<pre><code>ndx = which(df$id %in% v)
df[ndx,]
</code></pre>
"
2569204,457898,2010-04-02T21:05:08Z,2568234,4,FALSE,"<p>You can do it with <code>ggplot2</code>, using <code>facets</code>. When I read <em>""I want to create a plot where the data points are grouped as defined by the factor""</em>, the first thing that came to my mind was <code>facets</code>.</p>

<p>But in this particular case, faster alternative should be:</p>

<pre><code>plot(as.numeric(trans.factor), casp6)
</code></pre>

<p>And you can play with plot options afterwards (<code>type</code>, <code>fg</code>, <code>bg</code>...), but I recommend sticking with <code>ggplot2</code>, since it has much cleaner code, great functionality, you can avoid overplotting... etc. etc.</p>

<p>Learn how to deal with factors. You got barplot when evaluating <code>plot(trans.factor, casp6)</code> 'cause <code>trans.factor</code> was class of <code>factor</code> (ironically, you even named it in such manor)... and <code>trans.factor</code>, as such, was declared <strong>before</strong> a continuous (numeric) variable within <code>plot()</code> function... hence <code>plot()</code> ""feels"" the need to subset data and draw boxplot based on each part (if you declare continuous variable first, you'll get an ordinary graph, right?). <code>ggplot2</code>, on the other hand, interprets factor in a different way... as <em>""an ordinary""</em>, numeric variable (this stands for syntax provided by <em>Jonathan Chang</em>, you must specify <code>geom</code> when doing something more complex in <code>ggplot2</code>).</p>

<p>But, let's presuppose that you have one continuous variable and a factor, and you want to apply histogram on each part of continuous variable, defined by factor levels. This is where the things become complicated with base graph capabilities.</p>

<pre><code># create dummy data
&gt; set.seed(23)
&gt; x &lt;- rnorm(200, 23, 2.3)
&gt; g &lt;- factor(round(runif(200, 1, 4)))
</code></pre>

<p>By using base graphs (<code>package:graphics</code>):</p>

<pre><code>par(mfrow = c(1, 4))
tapply(x, g, hist)
</code></pre>

<p>ggplot2 way:</p>

<pre><code>qplot(x, facets = . ~ g)
</code></pre>

<p>Try to do this with <code>graphics</code> in one line of code (semicolons and custom functions are considered cheating!):</p>

<pre><code>qplot(x, log(x), facets = . ~ g)
</code></pre>

<p>Let's hope that I haven't bored you to death, but helped you!</p>

<p>Kind regards,<br>
<em>aL3xa</em></p>
"
2570241,143305,2010-04-03T04:39:05Z,2564141,1,TRUE,"<p>What happens when you do this (and ti doesn't require <code>su</code> or <code>sudo</code>):</p>

<pre><code>R&gt; repos &lt;- ""http://cran.r-project.org""
R&gt; AP &lt;- available.packages(contrib.url(repos))
R&gt; AP[ which(AP[,1]==""Matrix""), 1:3]
      Package       Version      Priority 
     ""Matrix"" ""0.999375-38"" ""recommended"" 
R&gt; 
</code></pre>

<p>The only time I ran into issue similar to the one you are reporting was when my R version was out of sync with how the repo is organized (ie too old).</p>
"
2572339,143305,2010-04-03T19:11:17Z,2572001,4,TRUE,"<p>Your question is essentially <em>just</em> a complicated indexing question.  I have a solution here though there may be simpler ones.    I loaded your examples data into <code>DF</code>:</p>

<p>First, this gets us the best row index (easy using <code>which.min()</code>) :</p>

<pre><code>R&gt; bind &lt;- which.min(DF[,""fitness.mean""])  # index of best row
</code></pre>

<p>Next, we <code>apply()</code> a row-wise comparison (over the subset of columns we care about, here index simply by position 5 to 7). </p>

<p>We use a comparison function <code>cmpfun</code> to compare the current row <code>r</code> to the best row (indexed by <code>bind</code>) and use <code>all()</code> to get rows where all elements correspond. [ We need <code>drop=FALSE</code> here to make it comparable on both sides, else <code>as.numeric()</code> helps. ] </p>

<pre><code>R&gt; cmpfun &lt;- function(r) all(r == DF[bind,5:7,drop=FALSE])  # compare to row bind
</code></pre>

<p>This we simply <code>apply</code> this row-wise:</p>

<pre><code>R&gt; brows &lt;- apply(DF[,5:7], 1, cmpfun)
</code></pre>

<p>And these are the rows we wanted:</p>

<pre><code>R&gt; DF[brows, ]
  eval.num eval.count fitness fitness.mean green.h.0 green.v.0 offset.0
1        1          1    1500         1500       100       120       40
2        2          2    1000         1250       100       120       40
3        3          3    1250         1250       100       120       40
4        4          4    1000         1188       100       120       40
R&gt; 
</code></pre>

<p>It did not matter that we use three columns for comparison -- all that mattered is that we had an indexing expression (here <code>5:7</code>) for the columns we wanted.</p>
"
2572359,143305,2010-04-03T19:18:36Z,2572330,4,FALSE,"<p>You may benefit from reading an <em>Introduction to R</em>, especially on matrices, data.frames and indexing.  Your <code>a</code> is a column of a data.frame, your <code>x</code> is a scalar. The comparison you have there does not work. </p>

<p>Maybe you meant </p>

<pre><code>R&gt; DF$a == min(c(1,2,3))
[1]  TRUE FALSE FALSE
R&gt; DF[,""a""] == min(c(1,2,3))
[1]  TRUE FALSE FALSE
R&gt; 
</code></pre>

<p>which tells you that the first row fits but not the other too. Wrapping this in <code>which()</code> gives you indices instead.</p>
"
2572785,158065,2010-04-03T21:52:10Z,2572559,8,FALSE,"<p>Ok, let's first do it in the easy case where you just have one column. </p>

<pre><code>&gt; data &lt;- rep(sample(1000, 5),
              sample(5, 5))
&gt; head(data)
[1] 435 435 435 278 278 278
</code></pre>

<p>Then you can just use rle to figure out the contiguous sequences:</p>

<pre><code>&gt; sequence(rle(data)$lengths)
[1] 1 2 3 1 2 3 4 5 1 2 3 4 1 2 1
</code></pre>

<p>Or altogether:</p>

<pre><code>&gt; head(cbind(data, sequence(rle(data)$lengths)))
[1,]  435 1
[2,]  435 2
[3,]  435 3
[4,]  278 1
[5,]  278 2
[6,]  278 3
</code></pre>

<p>For your case with multiple columns, there are probably a bunch of ways of applying this solution.  Easiest might be to just <code>paste</code> the columns you care about together to form a single vector.</p>
"
2572906,308375,2010-04-03T22:36:29Z,2572559,1,FALSE,"<p>Okay I used the answer I had on another question and worked out a loop that I think will work.  This is what I'm going to use:</p>

<pre><code>cmpfun2 &lt;- function(r) {
    count &lt;- 0
    if (r[1] &gt; 1)
    {
        for (row in 1:(r[1]-1))
        {
            if(all(r[27:51] == DF[row,27:51,drop=FALSE]))  # compare to row bind
            {
                count &lt;- count + 1
            }
        }
    }
    return (count)
}
brows &lt;- apply(DF[], 1, cmpfun2)
print(brows)
</code></pre>

<p>Please comment if I made a mistake and this won't work, but I think I've figured it out.  Thanks!</p>
"
2573204,66549,2010-04-04T01:02:10Z,2573132,3,FALSE,"<p>I don't believe there are any available options for Python 3.1.1.</p>

<p>The current status of R-Python bindings:</p>

<p>At the moment, three options: RPy, RPy2, and <a href=""http://www.omegahat.org/RSPython/"" rel=""nofollow noreferrer"">RSPython</a>.</p>

<p>RPy and RPy2 were developed and are maintained by the same team of developers; RPy2 is a substantial rewrite of RPy (which in turn is based on Omega hat's RSPython). </p>

<p>RPy is still actively maintained.</p>

<p>RSPython is still available but i believe is no longer actively developed. I looked a couple of months ago and the latest version i could find (0.7-1) was released in October 2006.</p>

<p>The most current stable version as well as the dev version (2.1 rc) of RPy2 is optimized for R version 2.10 (current stable version) and Python version <strong>2.6</strong> (ie, those are the versions used in development). I am not aware of any announcement by the RPy2 developers to support Python versions 3.x.</p>
"
2576897,66549,2010-04-05T04:16:30Z,2576876,12,TRUE,"<pre><code>data(AirPassengers)   # already in your R installation, via package ""datasets""
AP = AirPassengers    
class(AP)
# returns ""ts""

AP1 = as.numeric(AP)
# returns ""numeric""

# another way to do it
AP1 = unclass(AP)
</code></pre>

<p>AP1 is a vector with the <em>same</em> <strong><em>values</em></strong> and length as AP. The class is now numeric instead of ts, which means, in part that the indices are no longer some sort of date-time object but just ordinary sequential integers.  </p>

<p>So w/r/t the specific question in the OP, either of the two snippets above will ""<em>convert [a ts object] to a plain old vector</em>""</p>

<p>If you need to do the same thing with the <em>indices</em> rather than, or in addition to, the values--ie, from Date objects to numeric, you can do that like so:</p>

<pre><code>fnx = function(num_days_since_origin, origin=""1970-01-01"") {
  as.Date(num_days_since_origin, origin=""1970-01-01"")
}

a = as.Date(""1985-06-11"")
a2 = as.numeric(a)
# returns: 5640
a3 = fnx(5640)
# returns: ""1985-06-11"" (a date object)
</code></pre>
"
2577809,216064,2010-04-05T10:05:35Z,2577636,3,FALSE,"<p>You could take care of the latest directory used yourself. For instance, you could edit your .Rprofile and add a function .Last that stores the latest directory in a file and a function .First that reads that file und sets the working directory. Something similar to</p>

<pre><code>.Last &lt;- function() cat(getwd(), file=""~/.Rlastdir"")
.First &lt;- function() setwd(readLines(""~/.Rlastdir""))
</code></pre>
"
2578741,169947,2010-04-05T14:01:45Z,2572330,2,TRUE,"<p>I think this is what you're looking for:</p>

<pre><code>&gt; x &lt;- min(DF$a)
&gt; DF[DF$a == x,]
  a b c d
1 1 2 3 4
</code></pre>

<p>An easier way (avoiding the 'x' variable) would be this:</p>

<pre><code>&gt; DF[which.min(DF$a),]
  a b c d
1 1 2 3 4
</code></pre>

<p>or this:</p>

<pre><code>&gt; subset(DF, a==min(a))
  a b c d
1 1 2 3 4
</code></pre>
"
2580113,172261,2010-04-05T18:16:18Z,2579995,58,TRUE,"<p>Try the <code>cex</code> argument:</p>

<p><code>?par</code></p>

<ul>
<li><code>cex</code><br>
A numerical value giving the
amount by which plotting text and
symbols should be magnified relative
to the default. Note that some
graphics functions such as
plot.default have an argument of this
name which multiplies this graphical
parameter, and some functions such as
points accept a vector of values
which are recycled. Other uses will
take just the first value if a vector
of length greater than one is
supplied.</li>
</ul>
"
2580209,66549,2010-04-05T18:28:31Z,2579995,79,FALSE,"<p><strong>pch=20</strong> returns a symbol sized <em>between</em> ""."" and 19. </p>

<p>It's a <em>filled</em> symbol (which is probably what you want).</p>

<p>Aside from that, even the base graphics system in R allows a user fine-grained control over symbol size, color, and shape. E.g.,</p>

<pre><code>dfx = data.frame(ev1=1:10, ev2=sample(10:99, 10), ev3=10:1)

with(dfx, symbols(x=ev1, y=ev2, circles=ev3, inches=1/3,
                  ann=F, bg=""steelblue2"", fg=NULL))
</code></pre>

<p><a href=""https://i.stack.imgur.com/6kFx8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6kFx8.png"" alt=""Graph example""></a></p>
"
2580235,457898,2010-04-05T18:31:47Z,2579995,16,FALSE,"<p>As <strong><em>rcs</em></strong> stated, <code>cex</code> will do the job in base graphics package. I reckon that you're not willing to do your graph in <code>ggplot2</code> but if you do, there's a <code>size</code> aesthetic attribute, that you can easily control (<code>ggplot2</code> has user-friendly function arguments: instead of typing <code>cex</code> (character expansion), in <code>ggplot2</code> you can type e.g. <code>size = 2</code> and you'll get 2mm point).</p>

<p>Here's the example:</p>

<pre><code>### base graphics ###
plot(mpg ~ hp, data = mtcars, pch = 16, cex = .9)

### ggplot2 ###
# with qplot()
qplot(mpg, hp, data = mtcars, size = I(2))
# or with ggplot() + geom_point()
ggplot(mtcars, aes(mpg, hp), size = 2) + geom_point()
# or another solution:
ggplot(mtcars, aes(mpg, hp)) + geom_point(size = 2)
</code></pre>
"
2580626,457898,2010-04-05T19:32:59Z,2578961,1,FALSE,"<p>About percentages insted of <code>..count..</code> , try:</p>

<pre><code>ggplot(mtcars, aes(factor(cyl), prop.table(..count..) * 100)) + geom_bar()
</code></pre>

<p>but since it's not a good idea to shove a function into the <code>aes()</code>, you can write custom function to create percentages out of <code>..count..</code> , round it to <code>n</code> decimals etc.</p>

<p>You labeled this post with <code>plyr</code>, but I don't see any <code>plyr</code> in action here, and I bet that one <code>ddply()</code> can do the job. Online <code>plyr</code> documentation should suffice.</p>
"
2580660,144537,2010-04-05T19:37:44Z,2578961,1,FALSE,"<p>If I am understanding you correctly, to fix the axis labeling problem make the following change:</p>

<pre><code># p&lt;-ggplot(Interest, aes(Interest2, ..count..))
p&lt;-ggplot(Interest, aes(Interest2, ..density..))
</code></pre>

<p>As for the second one, I think you would be better off working with the <a href=""http://had.co.nz/reshape/"" rel=""nofollow noreferrer"">reshape package</a>.  You can use it to aggregate data into groups very easily.</p>

<p>In reference to aL3xa's comment below...</p>

<pre><code>library(ggplot2)
r&lt;-rnorm(1000)
d&lt;-as.data.frame(cbind(r,1:1000))
ggplot(d,aes(r,..density..))+geom_bar()
</code></pre>

<p>Returns...</p>

<p><a href=""http://www.drewconway.com/zia/wp-content/uploads/2010/04/density.png"" rel=""nofollow noreferrer"">alt text http://www.drewconway.com/zia/wp-content/uploads/2010/04/density.png</a></p>

<p>The bins are now densities...</p>
"
2581827,309600,2010-04-05T23:48:05Z,2568234,3,FALSE,"<p>You may be able to get close to what you want using lattice graphics by doing:</p>

<pre><code>library(lattice)    
xyplot(casp6 ~ trans.factor, 
       scales = list(x = list(at = 1:4, labels = levels(trans.factor))))
</code></pre>
"
2582028,235349,2010-04-06T00:51:02Z,2581698,5,TRUE,"<p>You can easily achieve this by using melt (in the reshape package). Here is the code you add after you define the data frame.</p>

<pre><code>id1 = c(""month_num"",""founded_month"", ""founded_year"",""month_abb"",""short_year"",""label"");   
m_summary3 = melt(m_summary2, id = id1);
p = ggplot(m_summary3, aes(x = month_num, y = value, group = variable, colour = variable));
c1 = rgb(0/255, 172/255, 0/255);
c2 = rgb(18/255, 111/255, 150/255);
x_scale = scale_x_continuous(""Month"", breaks = m_summary2$month_num, labels = m_summary2$label);
y_scale = scale_y_continuous(""# Startups Founded"")

p + geom_line() + scale_colour_manual(values = c(c1,c2)) + x_scale + y_scale;
</code></pre>

<p>Ramnath</p>
"
2582174,99492,2010-04-06T01:39:24Z,2582150,3,FALSE,"<p>Easiest way is probably to wrap it in a call to cmd.exe:</p>

<pre><code>cmd.exe /C ""Rterm.exe --quiet --slave --vanilla &lt; `""C:\some_script.R`""""
</code></pre>
"
2582209,235349,2010-04-06T01:48:16Z,2581698,5,FALSE,"<p>Here is a way to manually annotate your plot. I have assumed that you save the plot that you have printed as p2. So you need to add this code to what you already have.</p>

<pre><code> x1 = max(m_summary2$month_num)-3;
 y1 = m_summary2$count[x1];
 y2 = m_summary2$proj[x1];
 a1 = annotate(""text"", x = x1, y = y1, label = ""Current"", vjust = -2, hjust = 0.2, colour = c1);
 a2 = annotate(""text"", x = x1, y = y2, label = ""Projection"", vjust = -2, hjust = 0.2, colour = c2);       
 p2 + a1 + a2;
</code></pre>

<p>Let me know if this works!</p>
"
2582252,143305,2010-04-06T02:03:18Z,2582150,11,TRUE,"<p>You should probably look <code>Rscript</code> instead of redirection -- this would become</p>

<pre><code>Rscript.exe C:\someScript.R
</code></pre>

<p>where you can add the usual options.</p>
"
2584990,168747,2010-04-06T13:11:07Z,2584806,1,TRUE,"<p>You can create <code>temp_nom</code> in two ways (at least):</p>

<pre><code># strsplit create list so you can sapply on it
sapply(strsplit(names(df),""_""), ""["", 2)

# using regular expressions:
sub("".+_|[^_]+"", """", names(df))
</code></pre>

<p>And for assigment you could convert <code>temp_nom</code> to numeric (in other case it mess with column types)</p>

<pre><code>df[nrow(df)+1,] &lt;- as.numeric(temp_nom)
</code></pre>

<p>Of course you can do it in one line:</p>

<pre><code>df[nrow(df)+1,] &lt;- as.numeric(sapply(strsplit(names(df),""_""), ""["", 2))
# or
df[nrow(df)+1,] &lt;- as.numeric(sub("".+_|[^_]+"", """", names(df)))
</code></pre>
"
2585634,163053,2010-04-06T14:35:46Z,2585583,2,TRUE,"<p>I think that you will need an apply-type function.  This will work:</p>

<pre><code>df[is.na(df$level),""level""] &lt;- 0
df$level &lt;- sapply(df$level, function(x) paste(rep(""*"",x),collapse=""""))
</code></pre>

<p>You would be better using <code>sapply</code> than <code>lapply</code> in this instance since it returns a vector instead of a list.</p>

<p>From the help for rep:</p>

<blockquote>
  <p>If 'times' consists of a single
  integer, the result consists of
       the whole input repeated this many times.  If 'times' is a vector
       of the same length as 'x' (after replication by 'each'), the
       result consists of 'x[1]' repeated 'times[1]' times, 'x[2]'
       repeated 'times[2]' times and so on.</p>
</blockquote>

<p>One problem with using <code>rep</code> with a vector for the times parameter is that it just returns a vector and it discards instances when times=0.  You can see this with this command: <code>rep(rep(""*"", nrow(df)), times=df$level)</code>.</p>
"
2586193,168747,2010-04-06T15:44:04Z,2585583,3,FALSE,"<p>You could create stars vector as </p>

<pre><code>vstars &lt;- sapply(1L:nlevels(df$reason), function(i) paste(rep(""*"",i),collapse=""""))
vstars
# [1] ""*""  ""**""
</code></pre>

<p>And then indexing it with <code>df$reason</code> (which works because its a factor):</p>

<pre><code>vstars[df$reason]
# [1] NA   NA   ""**"" NA   NA   NA   ""**"" ""*""  NA   NA
</code></pre>

<p>For large <code>data.frame</code> should be much faster then <code>paste</code> in each row.</p>
"
2587687,162832,2010-04-06T19:31:18Z,2578961,1,FALSE,"<p>Your first question: Would this help?</p>

<pre><code>geom_bar(aes(y=..count../sum(..count..)))
</code></pre>

<p>Your second question; could you use reorder to sort the bars? Something like </p>

<pre><code>aes(reorder(Interest, Value, mean), Value)
</code></pre>

<p>(just back from a seven hour drive - am tired - but I guess it should work)</p>
"
2588179,NA,2010-04-06T20:45:20Z,2588130,0,FALSE,"<p>What do you want to do to the B expression?  Do you want to dynamically add behavior?  Then there is a decorator pattern in your problem.  Want to optionally add behavior?  Proxy.  Need to swap out one behavior for another under certain circumstances?  Strategy.</p>

<p>You are far better off relying on design patterns - which work and make you more effective regardless of the language you use - than you are trying to use some language-specific feature  that let's you mutate the behavior of a strategy.</p>
"
2588478,168747,2010-04-06T21:29:17Z,2588130,2,TRUE,"<p>Have you check <code>substitute</code>? I don't know it satisfies you needs but you could use fact that it returns hidden <code>list</code> structure which you can modify as below</p>

<pre><code>test.func &lt;- function(a, b) {
    f &lt;- substitute(b)
    f[[""val1""]] &lt;- a
    eval(f)
}

test.func(a=""a"", b=paste(1, 2))
# ""1 2 a""
</code></pre>
"
2589367,163053,2010-04-07T01:01:01Z,2589275,38,TRUE,"<p>Use a <code>tryCatch</code> expression around the function that can throw the error message:</p>

<pre><code>testFunction &lt;- function (date_in) {
  return(tryCatch(as.Date(date_in), error=function(e) NULL))
}
</code></pre>

<p>The nice thing about the <code>tryCatch</code> function is that you can decide what to do in the case of an error (in this case, return <code>NULL</code>).</p>

<pre><code>&gt; lapply(dates2, testFunction)
[[1]]
[1] ""2010-04-06""

[[2]]
NULL

[[3]]
[1] ""2010-04-08""
</code></pre>
"
2589369,143305,2010-04-07T01:01:13Z,2589275,6,FALSE,"<p>One could try to keep it simple rather than to make it complicated:</p>

<ul>
<li>Use the vectorised date parsing</li>
</ul>

<blockquote>
<pre><code>R&gt; as.Date( c(""2010-04-06"", ""foo"", ""2010-04-08"") )
[1] ""2010-04-06"" NA           ""2010-04-08""
</code></pre>
</blockquote>

<p>You can trivially wrap <code>na.omit()</code> or whatever around it.  Or find the index of NAs and extract accordingly from the initial vector, or use the complement of the NAs to find the parsed dates, or, or, or.  It is all here already.</p>

<ul>
<li><p>You can make your <code>testFunction()</code> do something. Use the test there -- if the returned (parsed) date is NA, do something.</p></li>
<li><p>Add a <code>tryCatch()</code> block or a <code>try()</code> to your date parsing.</p></li>
</ul>

<p>The whole things is a little odd as you go from a one-type data structure (vector of chars) to something else, but you can't easily mix types unless you keep them in a <code>list</code> type. So maybe you need to rethink this.</p>
"
2589385,166686,2010-04-07T01:03:17Z,2589275,0,FALSE,"<p>Assuming the <code>testFunction()</code> is not trivial and/or that one cannot alter it, it can be wrapped in a function of your own, with a tryCatch() block.  For example:</p>

<pre><code>&gt; FaultTolerantTestFunction &lt;- function(date_in) {
+    tryCatch({ret &lt;- testFunction(date_in);}, error = function(e) {ret &lt;&lt;- NA});
+    ret
+ }
&gt; FaultTolerantTestFunction('bozo')
[1] NA
&gt; FaultTolerantTestFunction('2010-03-21')
[1] ""2010-03-21""
</code></pre>
"
2590833,133234,2010-04-07T08:00:45Z,2590043,5,TRUE,"<p>The simplest/shortest way is to <code>apply</code> <code>assign</code> over rows:</p>

<pre><code>mDF &lt;- read.table(textConnection(""
Param1 w.IL.L
1   AuZgFw    0.5
2   AuZfFw      2
3   AuZgVw   74.3
4   AuZfVw  20.52
5   AuTgIL   80.9
6   AuTfIL  193.3
7   AuCgFL    0.2
""),header=T,stringsAsFactors=F)
invisible(apply(mDF,1,function(x)assign(x[[1]],as.numeric(x[[2]]),envir = .GlobalEnv)))
</code></pre>

<p>This involves converting the second column of the data frame to and from a string.  <code>invisible</code> is there only to suppress the output of <code>apply</code>.<br>
EDIT: You can also use <code>mapply</code> to avoid coersion to/from strings:</p>

<p><code>invisible(mapply(function(x,y)assign(x,y,envir=.GlobalEnv),mDF$Param1,mDF$w.IL.L))</code></p>
"
2592681,163053,2010-04-07T13:16:34Z,2591795,4,TRUE,"<p>As Marek says, there doesn't appear to be a version for R 2.10: <a href=""http://www.omegahat.org/R/bin/windows/contrib/2.10/"" rel=""nofollow noreferrer"">http://www.omegahat.org/R/bin/windows/contrib/2.10/</a>.  This command works for me in R 2.9, but not in R 2.10:</p>

<pre><code>install.packages(""RGoogleDocs"", repos = ""http://www.omegahat.org/R"", type=""source"")
</code></pre>
"
2593445,163053,2010-04-07T14:53:04Z,2593412,6,TRUE,"<p>You could look at <a href=""https://stackoverflow.com/questions/2209258/merge-several-data-frames-into-one-data-frame-with-a-loop/2209371#2209371"">this related question</a>.  You can create the file names easily with a paste command:</p>

<pre><code>file.names &lt;- paste(sprintf(""%02d"",1:10), ""_data.csv"", sep="""")
</code></pre>

<p>Once you have your file names (whether by creating them or by reading them from the directory as in the other question), you can import them quickly with an lapply:</p>

<pre><code>import.list &lt;- lapply(file.names, read.csv)
</code></pre>

<p>Lastly, to combine the list into one dataframe, the easiest approach is to use the <code>reshape</code> function below:</p>

<pre><code>library(reshape)
data &lt;- merge_recurse(import.list)
</code></pre>
"
2593604,143305,2010-04-07T15:12:09Z,2593412,4,FALSE,"<p>It is also very easy to read the content of a directory including use of regular expressions to skip focus on certain names only, e.g.</p>

<pre><code>filestoread &lt;- list.files(someDir, pattern=""\\.csv$"", full.names=TRUE)
</code></pre>

<p>returns all (fully-formed, including full path) files in the given directory <code>someDir</code> that end on "".csv"".  You can get fancier with better regular expressions which are documented in many places.</p>

<p>Once you have your list of files, it is straightforward to read them all using <code>apply</code> or <code>lapply</code> or a loop.</p>
"
2593713,143305,2010-04-07T15:26:27Z,2593643,2,FALSE,"<p>Sure, just use <code>cex</code>:</p>

<pre><code>set.seed(42)
DF &lt;- data.frame(x=1:10, y=rnorm(10)*10, z=runif(10)*3) 
with(DF, plot(x, y, cex=z))
</code></pre>

<p>which gives you varying circle sizes. Color can simply be a fourth dimension.</p>
"
2593999,66549,2010-04-07T16:07:12Z,2593643,3,TRUE,"<p>You can use '<strong>symbols</strong>' (analogous to the methods 'lines', 'abline' et al.)</p>

<p>This method will give you fine-grained control over both symbols size and color in a single line of code.</p>

<p>Using 'symbols' you can set the symbol size, color, and shape. Shape and size are set by passing in a vector for the size of each symbol and binding it to either 'circles', 'squares', 'rectangles', or 'stars', e.g., 'stars' = c(4, 3, 5, 1). Color is set with 'bg' and/or 'fg'.</p>

<pre><code>symbols( x, y, circles = circle_radii, inches=1/3, bg=""blue"", fg=NULL) 
</code></pre>

<p>If i understand the second part of your question, you want to be reasonably sure that the function you use to scale the symbols in your plot does so in a meaningful way. The 'symbols' function scales (for instance) the <strong>radii</strong> of circles based on values in a 'z' variable (or data.frame column, etc.) In the line below, I set the max symbol size (radius) as 1/3 inches--every symbol except for the largest has a radius some fraction smaller, scaled by the ratio of the value of that dat point over the largest value. than that one in proportion to  Is this a good choice? I don't know--it seems to me that diameter or particularly circumference might be better. In any event, that's a trivial change. In sum, 'symbols' with 'circles' passed in will scale the radii of the symbols in proportion to the 'z' coordinate--probably best suited for continuous variables. I would use color ('bg') for discrete variables/factors.</p>

<p>One way to use 'symbols' is to call your plot function and pass in type='n' which creates the plot object but suppresses drawing the symbols so that you can draw them with the 'symbols' function next.</p>

<p>I would not recommend 'cex' for this purpose. 'cex' is a scaling factor for both text size and symbols size, but which of those two plot elements it affects depends on when you pass it in--if you set it via 'par' then it acts on most of the text appearing on the plot; if you set it within the 'plot' function then it affects symbols size.</p>
"
2597078,163053,2010-04-08T01:58:07Z,2597062,4,TRUE,"<p>You can just reference the columns and re-assign them:</p>

<pre><code> x &lt;- x[,c(2,3,1)]
</code></pre>

<p>Here's a working example:</p>

<pre><code>&gt; data(sample_matrix)
&gt; x &lt;- head(as.xts(sample_matrix, descr='my new xts object'))[,c(1,2,3)]
&gt; x
               Open     High      Low
2007-01-02 50.03978 50.11778 49.95041
2007-01-03 50.23050 50.42188 50.23050
2007-01-04 50.42096 50.42096 50.26414
2007-01-05 50.37347 50.37347 50.22103
2007-01-06 50.24433 50.24433 50.11121
2007-01-07 50.13211 50.21561 49.99185
&gt; x &lt;- x[,c(2,3,1)]
&gt; x
               High      Low     Open
2007-01-02 50.11778 49.95041 50.03978
2007-01-03 50.42188 50.23050 50.23050
2007-01-04 50.42096 50.26414 50.42096
2007-01-05 50.37347 50.22103 50.37347
2007-01-06 50.24433 50.11121 50.24433
2007-01-07 50.21561 49.99185 50.13211
</code></pre>
"
2600798,143305,2010-04-08T14:38:34Z,2600640,1,FALSE,"<p>You should find the fine R manuals of great help as you start to explore R, and its help facilities are very good too.  </p>

<p>If you start with </p>

<pre><code>  help(sqlSave)
</code></pre>

<p>you will see the <code>colNames</code> argument. Supplying a vector <code>c(""A"", ""B"")</code> would put your first data.frame column into a table column A etc.</p>
"
2601177,293843,2010-04-08T15:27:42Z,2600640,2,TRUE,"<p>I'm now doing it this way (maybe that's also what you meant):</p>

<pre><code>colnames(dat) &lt;- c(""A"", ""B"")
sqlSave(channel, dat, tablename = ""tblTest"", rownames=FALSE, append=TRUE)
</code></pre>

<p>It works for me. Thanks for your help.</p>
"
2602619,61974,2010-04-08T18:56:53Z,2602583,64,FALSE,"<p>No, but there are a few people who have written one, such as <a href=""http://www.personality-project.org/R/html/geometric.mean.html"" rel=""noreferrer"">here</a>.</p>

<p>Another possibility is to use this:</p>

<pre><code>exp(mean(log(x)))
</code></pre>
"
2603318,66549,2010-04-08T20:43:01Z,2603184,45,TRUE,"<p><strong>No</strong>.</p>

<p>Objects in assignment statements are immutable. R will copy the object not <em>just</em> the reference.</p>

<pre><code>&gt; v = matrix(1:12, nrow=4)
&gt; v
           [,1] [,2] [,3]
     [1,]    1    5    9
     [2,]    2    6   10
     [3,]    3    7   11
     [4,]    4    8   12
&gt; v1 = v
&gt; v1[,1]     # fetch the first column 
     [1] 1 2 3 4
</code></pre>

<p>(<em>proviso</em>: the statement above is true for R <em>primitives</em>, e.g., vectors, matrices), and also for <em>functions</em>; I cannot say for certain whether it's true for <em>all</em> R objects--just most of them, as well as the vast majority of the ones most often used.)</p>

<p>If you don't like this behavior you can opt out of it with the help from an R Package. E.g., there is an R Package called <strong>R.oo</strong> that allows you to mimic pass-by-reference behavior; R.oo is available on <a href=""http://cran.r-project.org/"" rel=""noreferrer"">CRAN</a>.</p>
"
2603568,279497,2010-04-08T21:24:21Z,2603184,15,FALSE,"<p>Pass-by-reference is possible for <code>environment</code>s. You can try to use them: basically whenever you create an object you would need to create an environment slot as well. But I think that it is cumbersome. Have a look at <a href=""http://www.stat.berkeley.edu/~paciorek/computingTips/Pointers_passing_reference_.html"" rel=""noreferrer"">Pointers and passing by reference in R</a> and <a href=""https://stat.ethz.ch/pipermail/r-devel/2009-January/051899.html"" rel=""noreferrer"">Pass by reference for S4.</a></p>
"
2609660,172261,2010-04-09T17:54:20Z,2609647,1,TRUE,"<p>You probably want the <code>%in%</code> operator</p>

<pre><code>&gt; dat &lt;- data.frame(series = c(""1a"", ""1b"", ""1e""), reading = c(0.1, 0.4, 0.6))
&gt; series_you_want &lt;- c(""1a"", ""1e"")
&gt; subset(dat, series %in% series_you_want) 
</code></pre>
"
2610610,143305,2010-04-09T20:16:43Z,2610521,5,TRUE,"<p>Just use <code>diff()</code> once switched to a time-aware data structure like <a href=""http://cran.r-project.org/package=zoo"" rel=""noreferrer"">zoo</a>: </p>

<pre><code>&gt; library(zoo)
&gt; DF &lt;- data.frame(date=Sys.time() + 0:4*3600, x = cumsum(runif(5)*10), 
                                               y=cumsum(runif(5)*20))
&gt; DF
                 date       x      y
1 2010-04-09 15:14:54  9.6282 14.709
2 2010-04-09 16:14:54 12.4041 28.665
3 2010-04-09 17:14:54 18.1643 34.244
4 2010-04-09 18:14:54 27.5785 41.028
5 2010-04-09 19:14:54 33.2779 57.020
&gt; zdf &lt;- zoo(DF[,-1], order.by=DF[,1])
&gt; diff(zdf)
                         x       y
2010-04-09 16:14:54 2.7759 13.9556
2010-04-09 17:14:54 5.7602  5.5792
2010-04-09 18:14:54 9.4142  6.7844
2010-04-09 19:14:54 5.6995 15.9919
&gt; 
</code></pre>

<p>You can easily pad the first row back, merge, ... etc -- see the excellent documentation for package <a href=""http://cran.r-project.org/package=zoo"" rel=""noreferrer"">zoo</a> for details.</p>
"
2611373,313227,2010-04-09T22:31:10Z,2349820,2,FALSE,"<p>I wrote some example code along these lines last year:</p>

<p><a href=""http://illposed.net/R4P.html"" rel=""nofollow noreferrer"">http://illposed.net/R4P.html</a></p>

<p>Best,</p>

<p>Bryan</p>
"
2613329,16632,2010-04-10T12:17:55Z,2612495,14,TRUE,"<p>Have you tried the <code>family = ""symmetric""</code> argument to <code>geom_smooth</code> (which will in turn get passed on to <code>loess</code>)?  This will make the loess smooth resistant to outliers.</p>

<p>However, looking at your data, why do you think a linear fit is not adequate?  You only have 4 x values, and there certainly doesn't seem to be strong evidence for a departure from linearity.</p>
"
2613832,143305,2010-04-10T15:11:30Z,2613420,17,TRUE,"<p>Exactly what to do with missing data -- which may be flagged as <code>NA</code> if we know it is missing -- may well differ from domain to domain.</p>

<p>To take an example related to time series, where you may want to skip, or fill, or interpolate, or interpolate differently, ... is that <em>just</em> the (very useful and popular) <a href=""http://cran.r-project.org/package=zoo"" rel=""noreferrer"">zoo</a> has all these functions related to <code>NA</code> handling:</p>

<pre><code>zoo::na.approx  zoo::na.locf    
zoo::na.spline  zoo::na.trim    
</code></pre>

<p>allowing to approximate (using different algorithms), carry-forward or backward, use spline interpolation or trim.</p>

<p>Another example would be the numerous missing imputation packages on CRAN -- often providing domain-specific solutions. [ So if you call R a DSL, what is this? ""Sub-domain specific solutions for domain specific languages"" or SDSSFDSL? Quite a mouthful :) ]</p>

<p>But for your specific question: no, I am not aware of a bit-level flag in base R that allows you to mark observations as 'to be excluded'.  I presume most R users would resort to functions like <code>na.omit()</code> et al or use the <code>na.rm=TRUE</code> option you mentioned.</p>
"
2614450,228220,2010-04-10T18:31:11Z,2612495,2,FALSE,"<p>First, I'm not sure an 'outlier' is even properly defined on such small data.  </p>

<p>Second, you'd then have to decide what you mean by ""outlier"" that is, is it one of the drugs, one of the replicates, or one of the time points?</p>

<p>As Hadley notes, there is little evidence of deviation from linearity.</p>

<p>Finally, I think part of the point of using a smoother is that it deals well with outliers, provided there is enough data.  But you have very little.</p>

<p>So, I have to ask exactly why you want to remove outliers.  That is, what are you going to do with these data (besides making nice plots)?  </p>

<p>I hope this helps</p>
"
2614592,66549,2010-04-10T19:12:02Z,2614400,1,FALSE,"<p>You use 'with' to create a localized and temporary namespace inside which you evaluate some expression. In your code above, you haven't passed in an expression.</p>

<p>For instance:</p>

<pre><code>data(iris)   # this data is in your R installation, just call 'data' and pass it in
</code></pre>

<p>Ordinarily you have to refer to variable names within a data frame like this:</p>

<pre><code>tx = tapply(iris$sepal.len, list(iris$species), mean)
</code></pre>

<p>Unless you do this:</p>

<pre><code>attach(iris)
</code></pre>

<p>The problem with using 'attach' is the likelihood of namespace clashes, so you've got to remember to call 'detach'</p>

<p>It's much cleaner to use 'with':</p>

<pre><code>tx = with( iris, tapply(sepal.len, list(species), mean) )
</code></pre>

<p>So, the call signature (informally) is:  with( data, function() )</p>
"
2614710,143377,2010-04-10T19:48:51Z,2614400,16,TRUE,"<p>You can use <code>get</code>:</p>

<pre><code>with(df, get(column.name))
</code></pre>
"
2614785,143305,2010-04-10T20:07:40Z,2614767,3,TRUE,"<p>You are making the common mistake of confusing 'access to Yahoo or Google data' with 'everything I see on Yahoo or Google Finance can be downloaded'.</p>

<p>When R functions download historical stock price data, they almost always access an interface explicitly designed for this purpose as e.g. a cgi handler providing csv files given a stock symbol and start and end date.  So this <em>easy</em> as all we need to do is form the appropriate query, hit the webserver, fetch the csv file an dparse it.</p>

<p>Now balance sheet information is (as far as I know) not available in such an interface. So you will need to 'screen scrape' and parse the html directly.  </p>

<p>It is not clear that R is the best tool for this. I am aware of some Perl modules for the purpose of getting non-time-series data off Yahoo Finance but have not used them.</p>
"
2615143,143305,2010-04-10T21:50:56Z,2615128,26,FALSE,"<p>This is documented in the 'R Installation and Administration' manual that came with your installation.</p>

<p>On my Linux box:</p>

<pre><code>R&gt; .libPaths()
[1] ""/usr/local/lib/R/site-library"" ""/usr/lib/R/site-library""      
[3] ""/usr/lib/R/library""           
R&gt; 
</code></pre>

<p>meaning that the default path is the first of these. You can override that via an argument to both <code>install.packages()</code> (from inside R) or <code>R CMD INSTALL</code> (outside R).</p>

<p>You can also override by setting the R_LIBS_USER variable. </p>
"
2615147,57458,2010-04-10T21:51:50Z,2615128,154,TRUE,"<p>The install.packages command looks through the .libPaths variable. Here's what mine defaults to on OSX:</p>

<pre><code>&gt; .libPaths()
[1] ""/Library/Frameworks/R.framework/Resources/library""
</code></pre>

<p>I don't install packages there by default, I prefer to have them installed in my home directory. In my .Rprofile, I have this line:</p>

<pre><code>.libPaths( ""/Users/tex/lib/R"" )
</code></pre>

<p>This adds the directory ""/Users/tex/lib/R"" to the front of the .libPaths variable. </p>
"
2615437,313701,2010-04-10T23:36:17Z,2614949,8,TRUE,"<p>You'll want to start here:
<a href=""http://en.wikipedia.org/wiki/Wikipedia:Database_download"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Wikipedia:Database_download</a></p>

<p>Which will take you to here:
<a href=""http://download.wikimedia.org/enwiki/20100312/"" rel=""nofollow noreferrer"">http://download.wikimedia.org/enwiki/20100312/</a></p>

<p>And the file you probably want is:</p>

<pre><code># 2010-03-17 04:33:50 done Log events to all pages.
    * This contains the log of actions performed on pages.
    * pages-logging.xml.gz 1.0 GB
</code></pre>

<p><a href=""http://download.wikimedia.org/enwiki/20100312/enwiki-20100312-pages-logging.xml.gz"" rel=""nofollow noreferrer"">http://download.wikimedia.org/enwiki/20100312/enwiki-20100312-pages-logging.xml.gz</a></p>

<p>You'll then import the xml into MySQL. Generating a histogram of users per day, week, year, etc. won't require R. You'll be able to do that with a single MySQL query. Something like:</p>

<pre><code>select DAYOFYEAR(wiki_edit_timestamp), count(*)
from page_logs
group by DAYOFYEAR(wiki_edit_timestamp)
order by DAYOFYEAR(wiki_edit_timestamp);
</code></pre>

<p>etc.</p>

<p>(I'm not sure what their actual schema is, but it'll be something like that.)</p>

<p>You'll run into issues, no doubt, but you'll learn a lot too. Good luck!</p>
"
2615572,28381,2010-04-11T00:22:15Z,2614767,4,FALSE,"<p>The question you want to ask, and get an answer to!, is where can I get free XBRL data for analysing corporate balance sheets, and is there a library for consuming such data in R?</p>

<p>XBRL (Extensible Business Reporting Language - <a href=""http://en.wikipedia.org/wiki/XBRL"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/XBRL</a>) is a standard for marking up accounting statments (income statements, balance sheets, profit &amp; loss statements) in XML format such that they can easily be parsed by computer and put into a spreadsheet.</p>

<p>As far as I know, a lot of corporate regulators (e.g. the SEC in the US, ASIC in Australia) are encouraging the companies under their jurisdiction to report using such a format, or running pilots, but I don't believe it has been mandated at this point. If you limited your investment universe (I am assuming you want this data in electronic format for investment purposes) to firms that have made their quarterly reports freely available in XBRL form, I expect you will have a pretty short list of firms to invest in!</p>

<p>Bloomberg, Reuters et al all have pricey feeds for obtaining corporate fundamental data. There may also be someone out there running a tidy business publishing balance sheets in XBRL format. Cheaper, but still paid for, are XIgnite's <a href=""http://www.xignite.com/xFundamentals.asmx"" rel=""nofollow noreferrer"">xFundamentals</a> and <a href=""http://www.xignite.com/xGlobalFundamentals.asmx"" rel=""nofollow noreferrer"">xGlobalFundamentals</a> web services, but you aren't getting full balance sheet data from them.</p>
"
2615987,190597,2010-04-11T03:51:15Z,2615907,7,FALSE,"<p>To calculate the differences, perhaps you could use</p>

<pre><code>outer(df$Score,df$Score,""-"")
</code></pre>
"
2616029,66549,2010-04-11T04:15:06Z,2615907,3,FALSE,"<pre><code>colmx = matrix(rep(df[,2], 10), ncol=10, byrow=F)
rowmx = matrix(rep(df[,2], 10), ncol=10, byrow=T)
delta = colmx - rowmx
</code></pre>
"
2616928,279497,2010-04-11T12:05:20Z,2615907,4,TRUE,"<p>Here another solution using <code>combn</code>:</p>

<pre><code>df &lt;- data.frame(ID=1:10,Score=4*10:1)
cm &lt;- combn(df$ID,2)
delta &lt;- df$Score[cm[1,]]-df$Score[cm[2,]]
</code></pre>

<p>or more directly</p>

<pre><code>df &lt;- data.frame(ID=1:10,Score=4*10:1)
delta &lt;- combn(df$ID,2,function(x) df$Score[x[1]]-df$Score[x[2]])
</code></pre>
"
2616929,216064,2010-04-11T12:06:31Z,2614949,5,FALSE,"<p>You could</p>

<ul>
<li>work with the <a href=""http://en.wikipedia.org/wiki/Wikipedia:Database_download"" rel=""nofollow noreferrer"">wikipedia database dumps</a>, as already mentioned</li>
<li>work with the <a href=""https://stackoverflow.com/questions/627594/is-there-a-wikipedia-api"">live mediawiki API</a>, see <a href=""http://rosettacode.org/wiki/Rosetta_Code/Rank_languages_by_popularity#R"" rel=""nofollow noreferrer"">this minimal example at Rosettacode</a> or <a href=""http://r-forge.r-project.org/plugins/scmsvn/viewcvs.php/pkg/wzd/R/mediawiki.R?rev=11&amp;root=wzd&amp;view=markup"" rel=""nofollow noreferrer"">my unfinished approach with a S3 class</a> or <a href=""http://r-forge.r-project.org/projects/wikirobot/"" rel=""nofollow noreferrer"">this package by Peter Konings</a></li>
<li>work with <a href=""http://wiki.dbpedia.org/Datasets"" rel=""nofollow noreferrer"">dbpedia</a>, an effort to extract knowledge from wikipedia into a knowledge base. They offer an <a href=""http://wiki.dbpedia.org/OnlineAccess"" rel=""nofollow noreferrer"">online sparql access</a> I don't know much about, and also datasets as <a href=""http://en.wikipedia.org/wiki/N-Triples"" rel=""nofollow noreferrer"">n-triples</a> for download. See <a href=""http://www.koders.com/python/fid0322478CADF5D83DC0B6FAA963681CC7A5B1FB34.aspx?s=cheese"" rel=""nofollow noreferrer"">this python script</a> which might be a starting point for an R script. This approach might be useful to access the content stored in the wikipedia (such as the infoboxes) but I am not sure if information on contributors to the wikipedia is available.</li>
</ul>
"
2617823,172261,2010-04-11T16:53:34Z,2617600,130,TRUE,"<p>First install the <a href=""http://cran.r-project.org/web/packages/rjson/"" rel=""noreferrer""><code>rjson</code></a> package:</p>

<pre><code>install.packages(""rjson"")
</code></pre>

<p>Then:</p>

<pre><code>library(""rjson"")
json_file &lt;- ""http://api.worldbank.org/country?per_page=10&amp;region=OED&amp;lendingtype=LNX&amp;format=json""
json_data &lt;- fromJSON(paste(readLines(json_file), collapse=""""))
</code></pre>

<p><strong>Update:</strong> since version 0.2.1</p>

<pre><code>json_data &lt;- fromJSON(file=json_file)
</code></pre>
"
2618016,171659,2010-04-11T17:46:55Z,2615907,3,FALSE,"<p>dist() is your friend.</p>

<pre><code>dist(df$Score)
</code></pre>

<p>You can put it as a matrix : </p>

<pre><code>as.matrix( dist(df$Score) )
</code></pre>
"
2618021,170792,2010-04-11T17:49:14Z,2617842,5,TRUE,"<p>I don't know why, but the name <code>y</code> seems to be the problem. If you change it, then it works</p>

<pre><code>r.together &lt;- read.table(textConnection(""
          reg         coef        se      myfactor
 (Intercept)  5.068608671 0.6990873 Labels
    goodTRUE  0.310575129 0.5228815 Labels
   indiaTRUE -1.196868662 0.5192330 Labels
   moneyTRUE -0.586451273 0.6011257 Labels
    maleTRUE -0.157618168 0.5332040 Labels
 (Intercept)  4.225580743 0.6010509  Bonus
    goodTRUE  1.272760149 0.4524954  Bonus
   indiaTRUE -0.829588862 0.4492838  Bonus
   moneyTRUE -0.003571476 0.5175601  Bonus
    maleTRUE  0.977011737 0.4602726  Bonus
""),header=T)

qplot(reg, coef, data = r.together, facets= .~myfactor ) + coord_flip() 
</code></pre>
"
2618061,203692,2010-04-11T17:58:09Z,2617842,2,FALSE,"<p>Could be y a reserved word?</p>

<pre><code>r.together=data.frame(
  reg=c(""(Intercept)"", ""goodTRUE"", ""indiaTRUE"", ""moneyTRUE"", ""maleTRUE"", ""(Intercept)"", ""goodTRUE"", ""indiaTRUE"", ""moneyTRUE"", ""maleTRUE""),
  coef=c(5.068608671, 0.310575129, -1.196868662, -0.586451273, -0.157618168, 4.225580743, 1.272760149, -0.829588862, -0.003571476, 0.977011737),
  se=c(0.6990873, 0.5228815, 0.519233, 0.6011257, 0.533204, 0.6010509, 0.4524954, 0.4492838, 0.5175601, 0.4602726),
  yy= c(""Labels"", ""Labels"", ""Labels"", ""Labels"", ""Labels"", ""Bonus"", ""Bonus"", ""Bonus"", ""Bonus"", ""Bonus"")
 )
</code></pre>

<p>yy seems to work</p>

<pre><code>g &lt;- qplot(reg, coef, data = r.together,facets=. ~ yy) +coord_flip()
</code></pre>
"
2618290,279497,2010-04-11T19:00:57Z,2618248,12,FALSE,"<p>You can try this: </p>

<pre><code>mn &lt;- tapply(d$score,d$template,mean)
df &lt;- data.frame(template=names(mn),mean=mn)
</code></pre>
"
2618362,216064,2010-04-11T19:18:20Z,2617600,28,FALSE,"<p>An alternative package is RJSONIO. To convert a nested list, lapply can help:</p>

<pre><code>l &lt;- fromJSON('[{""winner"":""68694999"",  ""votes"":[ 
   {""ts"":""Thu Mar 25 03:13:01 UTC 2010"", ""user"":{""name"":""Lamur"",""user_id"":""68694999""}},   
   {""ts"":""Thu Mar 25 03:13:08 UTC 2010"", ""user"":{""name"":""Lamur"",""user_id"":""68694999""}}],   
  ""lastVote"":{""timestamp"":1269486788526,""user"":
   {""name"":""Lamur"",""user_id"":""68694999""}},""startPrice"":0}]'
)
m &lt;- lapply(
    l[[1]]$votes, 
    function(x) c(x$user['name'], x$user['user_id'], x['ts'])
)
m &lt;- do.call(rbind, m)
</code></pre>

<p>gives information on the votes in your example.</p>
"
2618402,66549,2010-04-11T19:33:28Z,2618248,18,FALSE,"<p>There are a lot of different ways to transform the output from a <em>tapply</em> call into a data.frame. </p>

<p>But it's much simpler to <em>avoid</em> the call to <em>tapply</em> in the first place and substitute that with a call to a similar function that returns a <em>data frame</em> instead of a vector: </p>

<p>more specifically: </p>

<ul>
<li><p><em>tapply</em> returns a vector</p></li>
<li><p><strong><em>aggregate</em></strong> returns a data frame</p></li>
</ul>

<p>so just change your function call from <em>tapply</em> to <strong>aggregate</strong>, like so:</p>

<pre><code>data(iris)     # in 'datasets' just call 'data' and pass in 'iris' as an argument

tx = tapply(iris$sepal.len, list(iris$species), mean)
# returns: versicolor  virginica 
             5.94       6.59 

class(tx)
# returns: vector

tx = aggregate(iris$sepal.len, list(iris$species), mean)
# returns:
         Group.1    x
     1 versicolor 5.94
     2  virginica 6.59


class(tx)
# returns: data.frame
</code></pre>
"
2618462,16632,2010-04-11T19:51:30Z,2618248,6,TRUE,"<pre><code>library(plyr)
ddply(d, ""template"", summarise, mean = mean(score))
</code></pre>
"
2619398,66549,2010-04-12T01:15:44Z,2619069,3,FALSE,"<p>The plot below (which was created w/ the code just above it) shows the type of cars produced by the major car makers. </p>

<p>I mapped bar height (actually bar-segment height) to automobile class; and I mapped bar-segment color to automobile manufacturer. Hence, each of the seven x-axis labels corresponds to one level in the factor 'class'; likewise, each color of the bar segments corresponds to one level in the factor 'manufacturer' (both 'manufacturer' and 'class' are variables/columns w/in the 'mpg' dataframe. Finally, the y axis shows the number of cars in each class (bar height) by manufacturer (segment color).</p>

<pre><code>library(ggplot2)
data(mpg)     # data set provided w/ ggplot2

px = ggplot(mpg, aes(x=class, fill=manufacturer)) + geom_bar() 

print(px)
</code></pre>

<p><a href=""http://img245.imageshack.us/img245/6678/stackedbar.png"" rel=""nofollow noreferrer"">alt text http://img245.imageshack.us/img245/6678/stackedbar.png</a></p>
"
2619430,16632,2010-04-12T01:35:36Z,2619400,4,TRUE,"<p>Use a list:</p>

<pre><code>my_scales &lt;- list(log_scale, xscale, yscale, title) 
</code></pre>
"
2619439,16632,2010-04-12T01:39:39Z,2619346,2,TRUE,"<p>I think they are the same colour - it's the different surrounds that make them look different.</p>
"
2619613,163053,2010-04-12T02:47:11Z,2619543,8,FALSE,"<p>I believe that you want <code>.Machine$double.eps</code>.</p>
"
2619620,143305,2010-04-12T02:48:40Z,2619543,40,TRUE,"<p>Try <code>.Machine$double.eps</code> -- and <code>.Machine</code> which on my 32-bit Linux machine yields this:</p>

<pre><code>R&gt; .Machine
$double.eps
[1] 2.220e-16

$double.neg.eps
[1] 1.110e-16

$double.xmin
[1] 2.225e-308

$double.xmax
[1] 1.798e+308

$double.base
[1] 2

$double.digits
[1] 53

$double.rounding
[1] 5

$double.guard
[1] 0

$double.ulp.digits
[1] -52

$double.neg.ulp.digits
[1] -53

$double.exponent
[1] 11

$double.min.exp
[1] -1022

$double.max.exp
[1] 1024

$integer.max
[1] 2147483647

$sizeof.long
[1] 4

$sizeof.longlong
[1] 8

$sizeof.longdouble
[1] 12

$sizeof.pointer
[1] 4

R&gt; 
</code></pre>
"
2619639,163053,2010-04-12T02:55:45Z,2619618,16,FALSE,"<p>The <code>by</code> function returns a list, so you can do something like this:</p>

<pre><code>data.frame(do.call(""rbind"", by(x, column, mean)))
</code></pre>
"
2622189,296155,2010-04-12T12:57:53Z,2615128,7,FALSE,"<p>Thanks for the direction from the above two answerers. James Thompson's suggestion worked best for Windows users.</p>

<ol>
<li><p>Go to where your R program is installed. This is referred to as <code>R_Home</code> in the literature. Once you find it, go to the /etc subdirectory.</p>

<pre><code>C:\R\R-2.10.1\etc
</code></pre></li>
<li><p>Select the file in this folder named Rprofile.site. I open it with VIM. You will find this is a bare-bones file with less than 20 lines of code. I inserted the following inside the code:</p>

<pre><code># my custom library path
.libPaths=(""C:/R/library"")
</code></pre></li>
</ol>

<p>-the comment added to keep track of what I did to the file.</p>

<ol start=""3"">
<li>In R, typing the <code>.libPaths()</code> function yields the first target at <code>C:/R/Library</code></li>
</ol>

<p>NOTE: there is likely more than one way to achieve this, but other methods I tried didn't work for some reason.</p>
"
2624510,143305,2010-04-12T18:46:58Z,2624164,2,TRUE,"<p>That is an excellent question.  From the top of my head, start with a comparison between </p>

<ul>
<li>a serial solution (no snow), </li>
<li>a serial solution with snow (to get an idea of overhead) and </li>
<li>a parallel solution maybe controlling N to see what type of increase you get.</li>
</ul>

<p>The never-released-on-CRAN version 0.3.4 of <a href=""http://cran.r-project.org/package=snow"" rel=""nofollow noreferrer"">snow</a> also has additional plotting commands that are useful for analysis.  You can get it from <a href=""http://www.stat.uiowa.edu/~luke/R/cluster/"" rel=""nofollow noreferrer"">this directory at Luke Tierney's site</a>. </p>

<p>Real profiling, of course, is <em>hard</em> given the distributed nature.</p>
"
2624814,143305,2010-04-12T19:32:47Z,2624791,11,TRUE,"<p>You can stick a vector (a restricted structure where all components have to be of the same type) into a list (unrestricted).</p>

<p>But you cannot do the reverse.  Use lists of lists of lists ... and then use lapply et al to extract.</p>
"
2624961,66549,2010-04-12T19:57:30Z,2624791,1,FALSE,"<p>the expression 'c(theOneVector, list(tmpList))' actually didn't return a vector of length 1, it returned a list (by coersion) because the items in a vector must all be of the same mode (data type).</p>

<p>Here's what you can do to create a container in R that will hold items of different mode and whose items are easy to access:</p>

<pre><code># create the container (an R 'list')
vx = vector(mode=""list"")

# create some items having different modes to put in it
item1 = 1:5
item2 = ""another item""
item3 = 34
item4 = list(a = c(1:5), b = c(10:15))

# now fill the container 
vx$item1 = item1
vx$item2 = item2
vx$item3 = item3
vx$item4 = item4

# access the items in the container by name:
vx$item1
# returns: [1] 4 5 6
vx$item2
# returns: [1] ""another item""
</code></pre>
"
2626305,314020,2010-04-13T00:37:56Z,2626236,25,TRUE,"<p>you can do that by specifying the coord system:</p>

<pre><code>df &lt;- data.frame(age=c(10,10,20,20,25,25,25),veg=c(0,1,0,1,1,0,1))
g=ggplot(data=df,aes(x=age,y=veg))
g=g+stat_summary(fun.y=mean,geom=""point"")
g+coord_cartesian(ylim=c(0.2,1)) #do not use +ylim() here
</code></pre>
"
2626576,143305,2010-04-13T02:19:35Z,2626567,25,TRUE,"<p>Maybe <code>duplicated()</code> can help:</p>

<pre><code>R&gt; d[ !duplicated(d$x), ]
  x  y  z
1 1 10 20
3 2 12 18
4 4 13 17
R&gt; 
</code></pre>

<p><em>Edit</em>  Shucks, never mind. This picks the first in each block of repetitions, you wanted the last.  So here is another attempt using <a href=""http://cran.r-project.org/package=plyr"" rel=""noreferrer"">plyr</a>:</p>

<pre><code>R&gt; ddply(d, ""x"", function(z) tail(z,1))
  x  y  z
1 1 11 19
2 2 12 18
3 4 13 17
R&gt; 
</code></pre>

<p>Here <a href=""http://cran.r-project.org/package=plyr"" rel=""noreferrer"">plyr</a> does the hard work of finding unique subsets, looping over them and applying the supplied function -- which simply returns the last set of observations in a block <code>z</code> using <code>tail(z, 1)</code>.</p>
"
2627295,160314,2010-04-13T06:00:54Z,2626567,12,FALSE,"<p>Just to add a little to what Dirk provided... <code>duplicated</code> has a <code>fromLast</code> argument that you can use to select the last row:</p>

<pre><code>d[ !duplicated(d$x,fromLast=TRUE), ]
</code></pre>
"
2627573,66549,2010-04-13T06:56:25Z,2627431,2,FALSE,"<pre><code>dx = ""20100316 20100317 20100318""    
# if your dates already exists as individual items of mode ""character""
# then obviously, skip this step 
dx2 = unlist(strsplit(dx, split="" ""))

fnx = function(x){as.Date(x, format=""%Y%m%d"")}

dx3 = fnx(dx2)
dx3
# returns: [1] ""2010-03-16"" ""2010-03-17"" ""2010-03-18""

class(dx3)
# returns: [1] ""Date""
</code></pre>
"
2627636,170792,2010-04-13T07:08:57Z,2624688,14,TRUE,"<p>Function <code>anova</code> (or <code>summary.aov</code>) will give you the so called type I (or sequential) sum of squares. To get type III sum of squares, you can use the <a href=""http://finzi.psych.upenn.edu/R/library/car/html/Anova.html"" rel=""nofollow noreferrer"">Anova</a> function from library <code>car</code> with parameter <code>type=""III""</code>. The difference between these two approaches in unbalanced datasets (and also sample R code to produce both tables) is presented in detail <a href=""http://sites.psu.edu/stat461psbsp2013/wp-content/uploads/sites/1906/2013/03/InteractionsAndTypesOfSS.pdf"" rel=""nofollow noreferrer"">here</a>.</p>
"
2629136,216064,2010-04-13T11:36:40Z,2628680,1,FALSE,"<p>Maybe this untested fragment can be helpful:</p>

<pre><code>reader &lt;- file(""DATA.CSV"", ""r"")
lines &lt;- readLines(reader)
writer1 &lt;- textConnection(""csv1"", open = ""w"", local = TRUE)
writer2 &lt;- textConnection(""csv2"", open = ""w"", local = TRUE)
currWriter &lt;- writer1
lastLine &lt;- length(lines)
lineNumber &lt;- 4
repeat {
    if (lineNumber&gt;lastLine) break
    if (lines[lineNumber]==""********************************************************"") {
        lineNumber &lt;- lineNumber + 2 # eat two lines
        currWriter &lt;- writer2
    } else {
        writeLines(line, currWriter)
    }
    lineNumber &lt;- lineNumber + 1
}
close(reader)
close(writer1)
close(writer2)
csv1Reader &lt;- textConnection(csv1, ""r"")
csv2Reader &lt;- textConnection(csv2, ""r"")
df1 &lt;- read.csv(csv1Reader)
df2 &lt;- read.csv(csv2Reader)
close(csv1Reader)
close(csv2Reader)
</code></pre>
"
2629278,168747,2010-04-13T11:59:51Z,2628680,4,TRUE,"<p>In this case I will do something like:</p>

<pre><code># Import raw data:
data_raw &lt;- readLines(""test.txt"")

# find separation line:
id_sep &lt;- which(data_raw=="""")

# create ranges of both data sets:
data_1_range &lt;- 4:(id_sep-1)
data_2_range &lt;- (id_sep+4):length(data_raw)

# using ranges and row data import it:
data_1 &lt;- read.csv(textConnection(data_raw[data_1_range]))
data_2 &lt;- read.csv(textConnection(data_raw[data_2_range]))
</code></pre>

<p>Actually your first example set has inconsistent structure so <code>data_1</code> looks strange.</p>
"
2629356,143305,2010-04-13T12:12:30Z,2628621,5,FALSE,"<p>One place where I used <code>&lt;&lt;-</code> was in simple GUIs using tcl/tk.  Some of the initial examples have it -- as you need to make a distinction between local and global variables for statefullness.  See for example </p>

<pre><code> library(tcltk)
 demo(tkdensity)
</code></pre>

<p>which uses <code>&lt;&lt;-</code>.  Otherwise I concur with Marek :) -- a Google search can help.</p>
"
2629416,219897,2010-04-13T12:23:22Z,2628621,5,FALSE,"<pre><code>f &lt;- function(n, x0) {x &lt;- x0; replicate(n, (function(){x &lt;&lt;- x+rnorm(1)})())}
plot(f(1000,0),typ=""l"")
</code></pre>
"
2629493,163053,2010-04-13T12:34:26Z,2628621,25,FALSE,"<p>It helps to think of <code>&lt;&lt;-</code> as equivalent to <code>assign</code> (if you set the <code>inherits</code> parameter in that function to <code>TRUE</code>).  The benefit of <code>assign</code> is that it allows you to specify more parameters (e.g. the environment), so I prefer to use <code>assign</code> over <code>&lt;&lt;-</code> in most cases.  </p>

<p>Using <code>&lt;&lt;-</code> and <code>assign(x, value, inherits=TRUE)</code> means that ""enclosing environments of the supplied environment are searched until the variable 'x' is encountered.""  In other words, it will keep going through the environments in order until it finds a variable with that name, and it will assign it to that.  This can be within the scope of a function, or in the global environment.</p>

<p>In order to understand what these functions do, you need to also understand R environments (e.g. using <code>search</code>).</p>

<p>I regularly use these functions when I'm running a large simulation and I want to save intermediate results.  This allows you to create the object outside the scope of the given function or <code>apply</code> loop.  That's very helpful, especially if you have any concern about a large loop ending unexpectedly (e.g. a database disconnection), in which case you could lose everything in the process.  This would be equivalent to writing your results out to a database or file during a long running process, except that it's storing the results within the R environment instead.</p>

<p>My primary warning with this: be careful because you're now working with global variables, especially when using <code>&lt;&lt;-</code>.  That means that you can end up with situations where a function is using an object value from the environment, when you expected it to be using one that was supplied as a parameter.  This is one of the main things that functional programming tries to avoid (see <a href=""http://en.wikipedia.org/wiki/Side_effect_(computer_science)"" rel=""noreferrer""><strong>side effects</strong></a>).  I avoid this problem by assigning my values to a unique variable names (using paste with a set or unique parameters) that are never used within the function, but just used for caching and in case I need to recover later on (or do some meta-analysis on the intermediate results).</p>
"
2630222,16632,2010-04-13T14:18:18Z,2628621,128,TRUE,"<p><code>&lt;&lt;-</code> is most useful in conjunction with closures to maintain state.  Here's a section from a recent paper of mine:</p>

<p>A closure is a function written by another function. Closures are so called because they <em>enclose</em> the environment of the parent function, and can access all variables and parameters in that function. This is useful because it allows us to have two levels of parameters. One level of parameters (the parent) controls how the function works. The other level (the child) does the work. The following example shows how can use this idea to generate a family of power functions. The parent function (<code>power</code>) creates child functions (<code>square</code> and <code>cube</code>) that actually do the hard work.</p>

<pre><code>power &lt;- function(exponent) {
  function(x) x ^ exponent
}

square &lt;- power(2)
square(2) # -&gt; [1] 4
square(4) # -&gt; [1] 16

cube &lt;- power(3)
cube(2) # -&gt; [1] 8
cube(4) # -&gt; [1] 64
</code></pre>

<p>The ability to manage variables at two levels also makes it possible to maintain the state across function invocations by allowing a function to modify variables in the environment of its parent. Key to managing variables at different levels is the double arrow assignment operator  <code>&lt;&lt;-</code>. Unlike the usual single arrow assignment (<code>&lt;-</code>) that always works on the current level, the double arrow operator can modify variables in parent levels.</p>

<p>This makes it possible to maintain a counter that records how many times a function has been called, as the following example shows. Each time <code>new_counter</code> is run, it creates an environment, initialises the counter <code>i</code> in this environment, and then creates a new function.  </p>

<pre><code>new_counter &lt;- function() {
  i &lt;- 0
  function() {
    # do something useful, then ...
    i &lt;&lt;- i + 1
    i
  }
}
</code></pre>

<p>The new function is a closure, and its environment is the enclosing environment. When the closures <code>counter_one</code> and <code>counter_two</code> are run, each one modifies the counter in its enclosing environment and then returns the current count.  </p>

<pre><code>counter_one &lt;- new_counter()
counter_two &lt;- new_counter()

counter_one() # -&gt; [1] 1
counter_one() # -&gt; [1] 2
counter_two() # -&gt; [1] 1
</code></pre>
"
2630708,168747,2010-04-13T15:11:56Z,2630541,4,TRUE,"<p><code>ls(e)</code> gives you names of objects in the environment and <code>e$name_of_object</code> gives you specified object (or <code>e[[""a""]]</code>, or <code>get(""a"",e)</code>).</p>
"
2631122,143305,2010-04-13T16:03:04Z,2631057,4,TRUE,"<p>Please read the 'Introduction to R' manual that came with your installation.</p>

<p>One of your questions is simply</p>

<pre><code>  X[ 100:200 ] &lt;- 0
</code></pre>

<p>and the other operations are similar.  This is essential material, so you need to read up a little.</p>
"
2631394,245603,2010-04-13T16:40:12Z,2631141,0,TRUE,"<p>This is not a true answer, but I think this problem is more complicated than what you present.</p>

<ol>
<li><p>Missing values act somewhat strange in SAS. For comparisons, they are equivalent to negative infinity. So a missing value is smaller than any non-missing number, but not smaller than a missing number. So the <code>. &lt; var1_a&lt;=80</code> statement is written this way to <em>avoid</em> selecting missing values, and not include them. This also means that the real problem is with inoccuous looking statements such as <code>a&lt;10</code> which will evaluate to <em>TRUE</em> in SAS if <code>a</code> is missing, but not so in R.</p></li>
<li><p>On the other hand, the <code>2 &lt; a &lt; 4</code> syntax for getting the values between 2 and 4 is allowed in SAS, but not in R, so you will have to find a way to detect this and all its variations.</p></li>
<li><p>Depending how general you want to get, you have to recode the alternative ways SAS can denote comparisons as well (EQ, NE, GEQ, etc).</p></li>
</ol>

<p>So unless your set of SAS logical statements has very restricted syntax, you will have lots of trouble.</p>
"
2632557,163053,2010-04-13T19:25:40Z,2632441,7,TRUE,"<p>There is one solution on <a href=""http://rosettacode.org/wiki/Pascal&#39;s_triangle#R"" rel=""noreferrer"">Rosetta Code</a>:</p>

<pre><code>pascalTriangle &lt;- function(h) {
  for(i in 0:(h-1)) {
    s &lt;- """"
    for(k in 0:(h-i)) s &lt;- paste(s, ""  "", sep="""")
    for(j in 0:i) {
      s &lt;- paste(s, sprintf(""%3d "", choose(i, j)), sep="""")
    }
    print(s)
  }
}
</code></pre>

<p>I would store this in a list if I was developing it myself, since that is the most natural data structure to handle variable length rows.  But you really would need to clarify a use case before making that decision.  Are you intending on doing analysis on the data after it has been generated?</p>

<p><em>Edit:</em></p>

<p>Here is the Rosetta solution rewritten with less looping, and storing the results as a list:</p>

<pre><code>pascalTriangle &lt;- function(h) {
  lapply(0:h, function(i) choose(i, 0:i))
}
</code></pre>
"
2633643,144537,2010-04-13T22:28:09Z,2633595,4,TRUE,"<p>To answer your question directly, the easiest thing to do would be to use <code>summary()</code> or <code>head()</code> to display information about the data frame.  I would suggest not pasting the actual data into a SO question, but rather providing a public link to the data for the community to play with.  If you have not seen it, the <a href=""http://www.box.net/"" rel=""nofollow noreferrer"">box.net</a> service provides a lot of free space for online collaboration.</p>

<p>Finally, if the data is exhibiting odd behavior when plotted, why not provide the code you are using to do the plots and some examples plots themselves.</p>
"
2633729,158065,2010-04-13T22:50:00Z,2633595,0,FALSE,"<p><code>dump</code> works well when the data frame is not very large.</p>
"
2633773,144537,2010-04-13T22:58:42Z,2631780,6,TRUE,"<p>I do not think there is a text wrap option in <code>ggplot2</code> (I have always just inserted \n manually).  You can, however, shrink the size of the title's text by altering your code in the following way:</p>

<pre><code>title.size&lt;-10
r + geom_smooth() + opts(title = my_title,plot.title=theme_text(size=title.size))
</code></pre>

<p>In fact, you all aspects of text with the <code>theme_text</code> function.</p>
"
2633940,66549,2010-04-13T23:40:40Z,2633595,1,FALSE,"<p>This is an excellent question.</p>

<p>Here's my attempt at an answer--in the form of recommendations for asking better questions w/r/t presenting the data that accompanies the question. I've probably violated every one of these suggestions below, but at least i've got something to refer to in the future, and perhaps it's useful for others as well.</p>

<p><strong>First</strong>, i suspect that anyone who asks a question prefers an answer </p>

<ul>
<li><p>with enough abstraction so that in
the future they can solve the general
class of problems to which the
current problem belongs; and</p></li>
<li><p>with enough practical guidance
(usually this means actual R code) to
actually solve the problem that's
just in front of us.</p></li>
</ul>

<p>Again: abstraction in your question (usually) results in abstraction in the answer, which means a more useful answer but also increases the likelihood that you'll actually get an acceptable answer--it's unlikely that the community has seen that exact data set before; it's far more likely that someone here will recognize a pattern. But the pattern can be obscured by too much data. </p>

<p><strong>Second</strong>, the amount of data that's needed to adequately explain a question is not really what matters--what matters is how long it takes the people attempting to answer the question to get that data into their R environment. There are data sets provided in the base R distribution that are 50,000 rows--doesn't matter because i can get the data into R in a few keystrokes. What' more, if you can refer to one of those data sets then you don't have to bother cutting and pasting stuff inside the question window. By contrast, i really try to avoid forcing people to scrape even a few lines of data off the SO page just so they can properly understand my question (except for Dirk, he does the calculations in his head.)</p>

<p><strong>Third</strong>, cutting and pasting the entire width of the data set in a question (all of the columns), unless it's absolutely required is just lazy. The data is rarely a substitute for a concise problem description. I would prefer that the OP's spend a minute or two and trim their actual data set so that they provide no more data than is actually required to illustrate the question. </p>

<p><strong>Fourth</strong>, if the data can be 'provided' by a formula or algorithm, then just provide that. E.g., if a question relates to a random walk, we don't need the data, just say ""random walk"" and nearly everyone here will be able to generate the data in a short line of code.</p>
"
2634086,163053,2010-04-14T00:14:17Z,2633595,2,FALSE,"<p>First, Drew's ideas are very good. </p>

<p>In addition, if you reduce the data and isolate the ""weird"" part, then use dput(). That's the most straight forward way to allow others to load it. Although you need to reduce your data to a reasonable amount first. </p>

<p>Otherwise post it as a csv file in a location that's accessible through http and people can read that directly with read.csv.  Although it's unreasonable to ask people to help you with a very large dataset. </p>

<p>Lastly, look at the answers to this question: <a href=""https://stackoverflow.com/questions/1434897/how-do-i-load-example-datasets-in-r/1434927"">How do I load example datasets in R?</a> </p>
"
2634672,143305,2010-04-14T03:21:06Z,2634512,3,FALSE,"<p>If you switch to using packages, you get namespaces as a side-benefit (provided you use a NAMESPACE file).  There are other advantages for using packages.</p>

<p>If you were really trying to avoid packages (which you shouldn't), then you could try assigning your variables in specific environments. </p>
"
2634688,143305,2010-04-14T03:25:07Z,2634386,0,TRUE,"<p>This is (again :-) a hard one.</p>

<p>You could try to dump snapshots on the nodes using <code>save()</code> or <code>save.image()</code>. You could then try to re-organize your code so that the nodes can resume after the last snapshot.</p>

<p>Or you could try to re-organize your workflow such that nodes 'take tickets' and return the results. That way the central node keeps tabs on everything and you can log interim results there. </p>

<p>Either way, what you desire is not available out of the box (as far as I know).</p>
"
2634953,160314,2010-04-14T04:44:57Z,2634512,5,FALSE,"<p>I would explore two possible solutions to this. </p>

<p>a) <strong>Think more in a more functional manner</strong>. Don't create any variables outside of a function. so, for example, main.R should contain one function main(), which sources in the other files, and does the work. when main returns, none of the clutter will remain. </p>

<p>b) <strong>Clean things up manually</strong>:</p>

<pre><code>#main.R
prior_variables &lt;- ls()
source('functions1.R')
source('functions2.R')

#stuff happens

rm(list = setdiff(ls(),prior_variables))`
</code></pre>
"
2635028,66549,2010-04-14T05:10:27Z,2634512,3,FALSE,"<p>Well avoiding namespace pollution, as you put it, is just a matter of diligently partitioning the namespace and keeping your global namespace uncluttered.</p>

<p>Here are the essential functions for those two kinds of tasks:</p>

<h2>Understanding/Navigating the Namespace Structure</h2>

<p>At start-up, R creates a new environment to store all objects created during that session--this is the ""global environment"".</p>

<pre><code># to get the name of that environment:
globalenv()
</code></pre>

<p>But this isn't the root environment. The root is an environment called ""the empty environment""--all environments chain back to it:</p>

<pre><code>emptyenv()
returns: &lt;environment: R_EmptyEnv&gt;

# to view all of the chained parent environments (which includes '.GlobalEnv'):
search()
</code></pre>

<h2>Creating New Environments:</h2>

<pre><code>workspace1 = new.env()

is.environment(workspace1)
returns: [1] TRUE

class(workspace1)
returns: [1] ""environment""

# add an object to this new environment:
with(workspace1, attach(what=""/Users/doug/Documents/test_obj.RData"",
     name=deparse(substitute(what)), warn.conflicts=T, pos=2))

# verify that it's there:
exists(""test_obj"", where=workspace1)
returns: [1] TRUE

# to locate the new environment (if it's not visible from your current environment)
parent.env(workspace1)
returns: &lt;environment: R_GlobalEnv&gt;

objects("".GlobalEnv"")
returns: [1] ""test_obj""
</code></pre>

<p>Coming from python, et al., this system (at first) seemed to me like a room full of carnival mirrors. The R Gurus on the other hand seem to be quite comfortable with it. I'm sure there are a number of reasons why, but my intuition is that they don't let environments persist. I notice that R beginners use 'attach', as in attach('this_dataframe'); I've noticed that experienced R users don't do that; they use 'with' instead eg, </p>

<pre><code>with(this_dataframe, tapply(etc....))
</code></pre>

<p>(I suppose they would achieve the same thing if they used 'attach' then 'detach' but 'with' is faster and you don't have to remember the second step.) In other words, namespace collisions are avoided in part by limiting the objects visible from the global namespace.</p>
"
2636182,314020,2010-04-14T09:15:38Z,2635938,2,TRUE,"<p>This would be a minimum example.
What is important is the data for geom_text. </p>

<pre><code>dat&lt;-data.frame(fa=gl(4,3),x=runif(12),y=runif(12))
q&lt;-ggplot(dat,aes(x=x,y=y))+geom_point()+facet_wrap(~fa)+
geom_text(data=data.frame(fa=gl(4,1),sig=c("""",""*"","""",""+"")),aes(x=0.5,y=0.5,label=sig))
print(q)
</code></pre>

<p>HTH.</p>
"
2636423,143476,2010-04-14T10:00:02Z,2634512,4,TRUE,"<p>The main function you want to use is <code>sys.source()</code>, which will load your functions/variables in a namespace (""environment"" in R) other than the global one. One other thing you can do in R that is fantastic is to attach namespaces to your <code>search()</code> path so that you need not reference the namespace directly. That is, if ""namespace1"" is on your search path, a function within it, say ""fun1"", need not be called as <code>namespace1.fun1()</code> as in Python, but as <code>fun1()</code>. [Method resolution order:] If there are many functions with the same name, the one in the environment that appears first in the <code>search()</code> list will be called. To call a function in a particular namespace explicitly, one of many possible syntaxes - albeit a bit ugly - is <code>get(""fun1"",""namespace1"")(...)</code> where <code>...</code> are the arguments to <code>fun1()</code>. This should also work with variables, using the syntax <code>get(""var1"",""namespace1"")</code>. I do this all the time (I usually load just functions, but the distinction between functions and variables in R is small) so I've written a few convenience functions that loads from my <code>~/.Rprofile</code>.</p>

<pre><code>  name.to.env &lt;- function(env.name)
    ## returns named environment on search() path
    pos.to.env(grep(env.name,search()))

  attach.env &lt;- function(env.name)
    ## creates and attaches environment to search path if it doesn't already exist
    if( all(regexpr(env.name,search())&lt;0) ) attach(NULL,name=env.name,pos=2)

  populate.env &lt;- function(env.name,path,...) {
    ## populates environment with functions in file or directory
    ## creates and attaches named environment to search() path 
    ##        if it doesn't already exist
    attach.env(env.name)
    if( file.info(path[1])$isdir )
      lapply(list.files(path,full.names=TRUE,...),
             sys.source,name.to.env(env.name)) else
    lapply(path,sys.source,name.to.env(env.name))
    invisible()
  }
</code></pre>

<p>Example usage:</p>

<pre><code>populate.env(""fun1"",""pathtofile/functions1.R"")
populate.env(""fun2"",""pathtofile/functions2.R"")
</code></pre>

<p>and so on, which will create two separate namespaces: ""fun1"" and ""fun2"", which are attached to the <code>search()</code> path (""fun2"" will be higher on the <code>search()</code> list in this case). This is akin to doing something like</p>

<pre><code>attach(NULL,name=""fun1"")
sys.source(""pathtofile/functions1.R"",pos.to.env(2))
</code></pre>

<p>manually for each file (""2"" is the default position on the <code>search()</code> path). The way that <code>populate.env()</code> is written, if a directory, say ""functions/"", contains many R files without conflicting function names, you can call it as</p>

<pre><code>populate.env(""myfunctions"",""functions/"")
</code></pre>

<p>to load all functions (and variables) into a single namespace. With <code>name.to.env()</code>, you can also do something like</p>

<pre><code>with(name.to.env(""fun1""), doStuff(var1))
</code></pre>

<p>or</p>

<pre><code>evalq(doStuff(var1), name.to.env(""fun1""))
</code></pre>

<p>Of course, if your project grows big and you have lots and lots of functions (and variables), writing a package is the way to go.</p>
"
2638525,235349,2010-04-14T15:08:14Z,2630333,0,TRUE,"<p>Here is a possible solution to your problem by merging your data with data from maps of select states. Is this what you were looking for?</p>

<pre><code>library(maps);
library(RColorBrewer);

# Create Dummy Data Frame to Play With

d = rbind(c('fairfield','connecticut',17),c('westchester','new york',70), c('luzerne','pennsylvania',1));
d = data.frame(d);
names(d) = c(""county"", ""state"", ""count"");
d$count = as.numeric(as.character(d$count));
d$stcon = paste(d$state, d$county, sep="","");

# Extract mapnames for States

mapnames2 = map(""county"",c(""new york"",""new jersey"", ""connecticut"", ""pennsylvania""),plot=FALSE)[4]$names;
mapnames2 = data.frame(mapnames2);
names(mapnames2) = ""stcon"";

# Merge with d

d = merge(mapnames2, d, all = T);
d$count[is.na(d$count)] = 0;


# Color bins
colors = brewer.pal(5, ""PuBu"");
d$colorBuckets = as.factor(as.numeric(cut(d$count,c(0,10,20,30,40,50,300))));

map(""county""
  ,c(""new york"",""new jersey"", ""connecticut"", ""pennsylvania"")
  ,col = colors[d$colorBuckets]
  ,fill = TRUE
  ,resolution = 0
  ,lty = 0
  ,lwd= 0.5
)
map(""state""
  ,c(""new york"",""new jersey"", ""connecticut"", ""pennsylvania"")
  ,col = ""black""
  ,fill=FALSE
  ,add=TRUE
  ,lty=1
  ,lwd=2
)

map(""county""
   ,c(""new york"",""new jersey"", ""connecticut"", ""pennsylvania"")
   ,col = ""black""
   ,fill=FALSE
   ,add=TRUE
  , lty=1
  , lwd=.5
)
title(main=""Respondent Home ZIP Codes by County"")
</code></pre>
"
2639682,143305,2010-04-14T17:45:16Z,2637594,3,TRUE,"<p>Your second box may have a broken Blas / Lapack installation. Unfortunately we cannot tell as you problem is <em>not reproducible</em>.</p>

<p>Here is another simple call to Blas / Lapack -- does this work for you?</p>

<pre><code>R&gt; crossprod(matrix(1:4, ncol=2))
     [,1] [,2]
[1,]    5   11
[2,]   11   25
R&gt; 
</code></pre>
"
2640087,245603,2010-04-14T18:42:13Z,2639430,16,TRUE,"<p>You first need to figure out the coordinates for the base of the perpendicular segments, then call the <code>segments</code> function which can take vectors of coordinates as inputs (no need for a loop).</p>

<pre><code>perp.segment.coord &lt;- function(x0, y0, lm.mod){
 #finds endpoint for a perpendicular segment from the point (x0,y0) to the line
 # defined by lm.mod as y=a+b*x
  a &lt;- coef(lm.mod)[1]  #intercept
  b &lt;- coef(lm.mod)[2]  #slope
  x1 &lt;- (x0+b*y0-a*b)/(1+b^2)
  y1 &lt;- a + b*x1
  list(x0=x0, y0=y0, x1=x1, y1=y1)
}
</code></pre>

<p>Now just call segments:</p>

<pre><code>ss &lt;- perp.segment.coord(temperature, diseasesev, severity.lm)
do.call(segments, ss)
#which is the same as:
segments(x0=ss$x0, x1=ss$x1, y0=ss$y0, y1=ss$y1)
</code></pre>

<p>Note that the results will not look perpendicular unless you ensure that the x-unit and y-unit of your plot have the same apparent length (isometric scales). You can do that by using <code>pty=""s""</code> to get a square plot and set <code>xlim</code> and <code>ylim</code> to the same range.</p>
"
2640533,275455,2010-04-14T19:50:48Z,2619618,8,TRUE,"<p>Consider using ddply in the plyr package instead of by. It handles the work of adding the column to your dataframe.</p>
"
2641235,133234,2010-04-14T21:37:11Z,2640525,8,TRUE,"<p>What you want is <code>(setq initial-major-mode 'R-mode)</code>.  Alternatively, you could just do M-x R-mode when in the scratch buffer to change the major mode.  </p>
"
2641674,163053,2010-04-14T23:09:42Z,2641653,55,TRUE,"<p>You can just use the column name directly:</p>

<pre><code>df &lt;- data.frame(A=1:10, B=2:11, C=3:12)
fun1 &lt;- function(x, column){
  max(x[,column])
}
fun1(df, ""B"")
fun1(df, c(""B"",""A""))
</code></pre>

<p>There's no need to use substitute, eval, etc.</p>

<p>You can even pass the desired function as a parameter:</p>

<pre><code>fun1 &lt;- function(x, column, fn) {
  fn(x[,column])
}
fun1(df, ""B"", max)
</code></pre>

<p>Alternatively, using <code>[[</code> also works for selecting a single column at a time:</p>

<pre><code>df &lt;- data.frame(A=1:10, B=2:11, C=3:12)
fun1 &lt;- function(x, column){
  max(x[[column]])
}
fun1(df, ""B"")
</code></pre>
"
2642201,160314,2010-04-15T01:36:43Z,2641653,16,FALSE,"<p>Personally I think that passing the column as a string is pretty ugly. I like to do something like:</p>

<pre><code>get.max &lt;- function(column,data=NULL){
    column&lt;-eval(substitute(column),data, parent.frame())
    max(column)
}
</code></pre>

<p>which will yield:</p>

<pre><code>&gt; get.max(mpg,mtcars)
[1] 33.9
&gt; get.max(c(1,2,3,4,5))
[1] 5
</code></pre>

<p>Notice how the specification of a data.frame is optional. you can even work with functions of your columns:</p>

<pre><code>&gt; get.max(1/mpg,mtcars)
[1] 0.09615385
</code></pre>
"
2643203,160314,2010-04-15T06:37:47Z,2642783,6,TRUE,"<p>First you need to create an <code>id</code> variable that specifies your groups without relying on the fact that they are consecutive. After that it is pretty straight forward.</p>

<pre><code>&gt; dat &lt;- data.frame(    TimeOffset = c(0,.1,.2,.4,.6,1.1,1.4,1.6,1.9,2.1),
+ Source=c(1,1,1,2,2,1,1,2,2,1),
+ Length=c(1500,1000,50,25,3,1500,18,2500,18,37))
&gt; dat
   TimeOffset Source Length
1         0.0      1   1500
2         0.1      1   1000
3         0.2      1     50
4         0.4      2     25
5         0.6      2      3
6         1.1      1   1500
7         1.4      1     18
8         1.6      2   2500
9         1.9      2     18
10        2.1      1     37
&gt; 
&gt; id &lt;- cumsum(c(TRUE,diff(dat$Source)!=0))
&gt; id
 [1] 1 1 1 2 2 3 3 4 4 5
&gt; 
&gt; cbind(TimeOffset=tapply(dat$TimeOffset,id,max),
+ Source=tapply(dat$Source,id,max),
+ Length=tapply(dat$Length,id,sum))
  TimeOffset Source Length
1        0.2      1   2550
2        0.6      2     28
3        1.4      1   1518
4        1.9      2   2518
5        2.1      1     37
</code></pre>
"
2643359,170792,2010-04-15T07:21:02Z,2642783,2,FALSE,"<p>I just saw and I like Ian's solution. Mine is too complicated...</p>

<pre><code>df &lt;- read.table(textConnection(""
TimeOffset Source Length 
 0         1           1500
 0.1       1           1000    
 0.2       1           50
 0.4       2           25
 0.6       2           3
 1.1       1           1500
 1.4       1           18
 1.6       2           2500
 1.9       2           18
 2.1       1           37
""),header=T)


ind &lt;- cbind(rle(df$Source)[[1]],cumsum(rle(df$Source)[[1]]))
ind2 &lt;- apply(ind,1,function(x) c(x[2]-(x[1]-1),x[2]))
ldply(apply(ind2,2,function(x) data.frame(df[x[2],1:2], Length=sum(df[x[1]:x[2],3]) ) ))

  TimeOffset Source Length
1        0.2      1   2550
2        0.6      2     28
3        1.4      1   1518
4        1.9      2   2518
5        2.1      1     37
</code></pre>
"
2643579,66549,2010-04-15T07:58:41Z,2642783,0,FALSE,"<pre><code># 'dfx' refers to the 'input' data frame in OP's Question
# use run-length encoding to get contiguous rows having the same Source value
a = rle(dfx$Source)
row_groups = a$lengths

result = matrix(rep(0,3))   
attr(result, ""dim"") = c(1,3)

fnx = function(a_df){
  c1 = max(a_df[,1])
  c2 = a_df[1,2]
  c3 = sum(a_df[,3])
  cbind(c1, c2, c3)
}


for (itm in row_groups){
  px = dfx[1:itm,]
  dfx = dfx[-(1:dim(px)[1]),]
  result = rbind(result, fnx(px))
}

result = result[-1,]
# returns:  
     c1  c2  c3
[1,] 0.2  1 2550
[2,] 0.6  2   28
[3,] 1.4  1 1518
[4,] 1.9  2 2518
[5,] 2.1  1   37
</code></pre>
"
2643636,78912,2010-04-15T08:08:20Z,1567718,1,FALSE,"<p>what about this:</p>

<pre><code>deparse(quote(foo.bar))
</code></pre>
"
2643882,78912,2010-04-15T08:51:20Z,2643719,2,TRUE,"<p>writing questions here helps thinking about solutions...  and I found <a href=""https://stackoverflow.com/questions/2642783/"" title=""Summarising grouped records in a dataframe in R"">a related question</a> so I could come up with this one:</p>

<pre><code>contiguousequal &lt;- function(data, value, position) {
  if(data[position] != value)
    return(rep(FALSE, length(data)))
  id &lt;- cumsum(c(1, as.numeric(diff(data) != 0)))
  id == id[position]
}
</code></pre>
"
2644009,279497,2010-04-15T09:12:55Z,2643939,96,TRUE,"<p>Try this:</p>

<pre><code>df &lt;- df[,colSums(is.na(df))&lt;nrow(df)]
</code></pre>
"
2645216,258334,2010-04-15T12:36:30Z,2643939,11,FALSE,"<p>Another way would be to use the <code>apply()</code> function.</p>

<p>If you have the data.frame</p>

<pre><code>df &lt;- data.frame (var1 = c(1:7,NA),
                  var2 = c(1,2,1,3,4,NA,NA,9),
                  var3 = c(NA)
                  )
</code></pre>

<p>then you can use <code>apply()</code> to see which columns fulfill your condition and so you can simply do the same subsetting as in the answer by Musa, only with an <code>apply</code> approach.</p>

<pre><code>&gt; !apply (is.na(df), 2, all)
 var1  var2  var3 
 TRUE  TRUE FALSE 

&gt; df[, !apply(is.na(df), 2, all)]
  var1 var2
1    1    1
2    2    2
3    3    1
4    4    3
5    5    4
6    6   NA
7    7   NA
8   NA    9
</code></pre>
"
2645673,317518,2010-04-15T13:32:34Z,1649503,1,FALSE,"<p>This is not an error, it's simply the 'repr' of the returned robject:</p>

<pre><code>&gt;&gt;&gt; r['pi']
&lt;RVector - Python:0x2c14bd8 / R:0x3719538&gt;
&gt;&gt;&gt; repr(r['pi'])
'&lt;RVector - Python:0x4b77908 / R:0x3719538&gt;'
&gt;&gt;&gt; str(r['pi'])
'[1] 3.141593'
&gt;&gt;&gt; print r['pi']
[1] 3.141593
</code></pre>

<p>You can get the value of 'pi' accessing it by index</p>

<pre><code>&gt;&gt;&gt; r['pi'][0]
3.1415926535897931
</code></pre>

<p>To access element of named lists (the '<em>object$attribute</em>' R syntax) I use</p>

<pre><code>&gt;&gt;&gt; l = r.list(a=r.c(1,2,3), b=r.c(4,5,6))
&gt;&gt;&gt; print l
$a
[1] 1 2 3

$b
[1] 4 5 6

&gt;&gt;&gt; print dict(zip(l.names, l))['a']
[1] 1 2 3
</code></pre>

<p>but I think there must be a better solution...</p>
"
2645941,279497,2010-04-15T14:01:56Z,2645578,2,FALSE,"<p>Try this:</p>

<pre><code>id &lt;- as.numeric(gsub(""P"","""",paste(df$Source,df$Target,sep="""")))
df$id &lt;- cumsum(c(TRUE,diff(id)!=0))
res &lt;- by(df, df$id,
          function(x) {
            len &lt;- nrow(x)
            start &lt;- x[1,1]
            end &lt;- x[len,1]
            dur &lt;- end - start
            src &lt;- x[1,2]
            trg &lt;- x[1,3]
            len &lt;- sum(x[,4])
            cont &lt;- paste(x[,5],collapse="""")
            return(c(start,end,dur,src,trg,len,cont))
          }
          )
do.call(rbind,res)
</code></pre>

<p>P.S.: You would need to convert the result to the ""correct"" format, as the end result is a matrix of strings.</p>
"
2646260,170792,2010-04-15T14:41:34Z,2645578,2,FALSE,"<p>Sticking on my (not elegant) way</p>

<pre><code>df1 &lt;- read.table(textConnection(""
Timestamp Source Target Length Content
0.1         P1       P2       5        ABCDE
0.2         P1       P2       3        HIJ
0.4         P1       P2       4        PQRS
0.5         P2       P1       2        ZY
0.9         P2       P1       4        SRQP
1.1         P1       P2       1        B
1.6         P1       P2       3        DEF
2.0         P2       P1       3        IJK
""),header=T)

df &lt;- adply(df1, 1 ,transform, newSource = 
as.numeric(paste(substr(Source, 2, 2),substr(Target, 2, 2),sep=""""))  ) 

ind &lt;- cbind(rle(df$newSource)[[1]],cumsum(rle(df$newSource)[[1]]))
ind2 &lt;- apply(ind,1,function(x) c(x[2]-(x[1]-1),x[2]))
res &lt;- ldply(apply(ind2,2,function(x) data.frame(StartTime = df[x[1],1] , 
EndTime = df[x[2],1] ,
Duration = df[x[2],1] - df[x[1],1] ,
Source = df[x[1],2] ,
Target = df[x[1],3] ,
Length=sum(df[x[1]:x[2],4]) ,
Content=paste(df[x[1]:x[2],5],collapse="""")
) ))

  StartTime EndTime Duration Source Target Length      Content
1       0.1     0.4      0.3     P1     P2     12 ABCDEHIJPQRS
2       0.5     0.9      0.4     P2     P1      6       ZYSRQP
3       1.1     1.6      0.5     P1     P2      4         BDEF
4       2.0     2.0      0.0     P2     P1      3          IJK
</code></pre>
"
2646517,163053,2010-04-15T15:09:50Z,2646402,7,TRUE,"<p>The simplest solution is to use the environment when referencing the object:</p>

<pre><code>y &lt;- new.env()
y$x &lt;- 1
f &lt;- function(env,z) {
    env$x+z
}
f(y,z=1)
</code></pre>

<p>You would need to assign <code>z</code> to your environment as well.</p>

<pre><code>y &lt;- new.env()
with(y, x &lt;- 1)
f &lt;- function(env,z) {
    assign(""z"", z, envir=env)
    with(env, x+z)
}
f(y,z=1)
</code></pre>

<p>One other option would be to <code>attach</code> your environment so that the variables can now be used directly.</p>

<pre><code>y &lt;- new.env()
with(y, x &lt;- 1)
f &lt;- function(env,z) {
    attach(env)
    y &lt;- x + z
    detach(env)
    y
}
f(y,z=1)
</code></pre>

<p>This latter solution is powerful because it means you can use any object from any attached environment within your new environment, but it also means that you need to be very careful about what has been assigned globally.</p>

<p><em>Edit</em>:</p>

<p>This is interesting, and I don't entirely understand the behavior (i.e. why <code>z</code> is not in the scope of the <code>with</code> call).  It has something to do with the creation of the environment originally that is causing it to be outside the scope of the function, because this version works:</p>

<pre><code>f &lt;- function(z) {
    y &lt;- new.env()
    with(y, x &lt;- 1)
    with(y, x+z)
}
f(y,z=1)
</code></pre>
"
2647578,16632,2010-04-15T17:28:23Z,2645578,7,TRUE,"<p>Here's another solution using plyr:</p>

<pre><code>id &lt;- with(df1, paste(Source, Target))
df1$group &lt;- cumsum(c(TRUE, id[-1] != id[-length(id)]))

library(plyr)
ddply(df1, c(""group""), summarise, 
  start = min(Timestamp),
  end = max(Timestamp),
  content = paste(Content, collapse = "", "")
)
</code></pre>
"
2647791,160314,2010-04-15T18:00:33Z,2647639,15,FALSE,"<pre><code>x &lt;- rnorm(100,10,10)
cut(x,c(-Inf,0,5,6,10,Inf))
</code></pre>
"
2647944,66549,2010-04-15T18:21:16Z,2647639,11,TRUE,"<p>Ian's answer (<em>cut</em>) is the most common way to do this, as far as i know.</p>

<p>I prefer to use <strong><em>shingle</em></strong>, from the <em>Lattice</em> Package</p>

<p>the argument that specifies the binning intervals seems a little more intuitive to me. </p>

<p>you use <em>shingle</em> like so:</p>

<pre><code># mock some data
data = sample(0:40, 200, replace=T)

a = c(0, 5);b = c(5,9);c = c(9, 19);d = c(19, 33);e = c(33, 41)

my_bins = matrix(rbind(a, b, c, d, e), ncol=2)

# returns: (the binning intervals i've set)
        [,1] [,2]
 [1,]    0    5
 [2,]    5    9
 [3,]    9   19
 [4,]   19   33
 [5,]   33   41

shx = shingle(data, intervals=my_bins)

#'shx' at the interactive prompt will give you a nice frequency table:
# Intervals:
   min max count
1   0   5    23
2   5   9    17
3   9  19    56
4  19  33    76
5  33  41    46
</code></pre>
"
2652227,143476,2010-04-16T10:38:50Z,2646402,3,FALSE,"<p>You only need to make one change to make your example work - redefine your function to use <code>substitute()</code> to 'fix' the desired values within the scope of <code>f()</code>:</p>

<pre><code>f &lt;- function(env,z) {
    eval(substitute(x+z,list(z=z)), env)
}
</code></pre>

<p>This can quickly get murky especially since you can even include assignment statements within <code>substitute()</code> (for instance, replace <code>x+z</code> with <code>y &lt;- x+z</code>, not that this is entirely relevant here) but that choice can be made by the developer...</p>

<p>Additionally, you can replace <code>list(z=z)</code> in the substitution expression above with <code>environment()</code> (e.g., <code>substitute(x+z,environment())</code>) as long as you don't have conflicting variable names between those passed to <code>f()</code> and those residing in your 'env', but you may not want to take this too far.</p>

<p><strong>Edit:</strong> Here are two other ways, the first of which is only meant to show the flexibility in manipulating environments and the second is more reasonable to actually use. </p>

<p>1) modify the enclosing environment of 'env' (but change it back to original value before exiting function):</p>

<pre><code>f &lt;- function(env,z) {
  e &lt;- environment(env)
  environment(env) &lt;- environment()
  output &lt;- with(env,x+z)
  environment(env) &lt;- e
  output
}
</code></pre>

<p>2) Force evaluation of 'z' in current environment of the function (using <code>environment()</code>) rather than letting it remain a free variable after evaluation of the expression, <code>x+z</code>, in 'env'. </p>

<pre><code>f &lt;- function(env,z) {
  with(environment(),with(env,x+z))
}
</code></pre>

<p>Depending on your desired resolution order, in case of conflicting symbol-value associations - e.g., if you have 'x' defined in both your function environment and the environment you created, 'y' (which value of 'x' do you want it to assume?) - you can  instead define the function body to be <code>with(env,with(environment(),x+z))</code>.</p>
"
2652934,16632,2010-04-16T12:36:56Z,2651497,2,TRUE,"<p>How about this?</p>

<pre><code>nadiff &lt;- function(x, ...) c(NA, diff(x, ...))
ddply(df, ""code"", colwise(nadiff, c(""var1"", ""var2"", ""var3"")))
</code></pre>
"
2653806,170792,2010-04-16T14:45:26Z,2653035,6,TRUE,"<p>I would try some of the other correspondence analysis functions available in R. In some of them the character expansion factor (<code>cex</code>) option is supported, so you can control the font size.
e.g.</p>

<pre><code>library(FactoMineR)
res&lt;-CA(smoke, ncp=5, row.sup=NULL, col.sup=NULL, graph = FALSE)
plot.CA(res, axes=c(1, 2), col.row=""red"", col.col=""blue"", label=c(""col"",""col.sup"", ""row"", ""row.sup""),cex=.7)

library(MASS)
biplot(corresp(smoke, nf = 2),cex=.7,col=c(""red"",""blue""))

library(anacor) # actually I didn't find a way to control font size here
res &lt;- anacor(smoke, scaling = c(""Benzecri"", ""Benzecri""),ndim=2) 
plot(res, plot.type = ""jointplot"", conf = NULL) 
</code></pre>

<p><strong>EDIT</strong></p>

<p>Of course you could get the coordinates from the ca resultset and generate this plot using ggplot2. Here I'm using the res object from CA.</p>

<pre><code>df &lt;- data.frame(dim1 = c(res$col$coord[,1],res$row$coord[,1]), 
dim2 = c(res$col$coord[,2],res$row$coord[,2]),
type=c(rep(1,length(res$col$coord[,1])),rep(2,length(res$row$coord[,1]))))

library(ggplot2)
qplot(dim1,dim2,data=df,colour=factor(type)) +
geom_text(aes(label=rownames(df)),size=3)
</code></pre>
"
2655063,318752,2010-04-16T17:44:21Z,2654397,13,TRUE,"<p>I am the creator of the yeroon.net/ggplot2, someone pointed me to this topic. I'll try to explain how the system currently works.</p>

<p>The application is using <a href=""http://code.google.com/apis/gdata/docs/auth/overview.html#AuthSub"" rel=""noreferrer"">AuthSub authentication</a>. The moment you sign into your Google account, a Google session is created. This session only has access to the Google documents and Google spreadsheet services that you gave permission for on the Google login page, so not to e.g. your mailbox.  </p>

<p>Once you logged in, you retrieve a session token from Google: a unique key that belongs to the session and can be used to make requests to access your Google data. The session token is stored as a cookie on your browser until you close it. Every time you make a request to yeroon.net servers, this token is added to the request.</p>

<p>Using this token, the yeroon.net servers can access your google data, e.g. to retreive a spreadsheet. The token is not stored on the server, although I understand that you have to take my word on this. Also it is not possible to find out your username or password from the session token; it can only be used to retreive data, as long as the session lives.</p>
"
2656611,160314,2010-04-16T23:32:02Z,2656529,6,FALSE,"<p>What version of R are you using? CRAN binaries are only kept up-to-date for the latest R release (i.e. 2.10.1). If you have an older version of R and have the development tools installed, you can use <code>install.packages(""ggplot2"",type=""source"")</code>.</p>
"
2656770,318964,2010-04-17T00:35:58Z,2656731,6,FALSE,"<p>You are certainly overoptimizing here. The overhead of a loop is negligible compared to the procedure of model fitting and therefore the simple answer is - use whatever way you find to be the most understandable. I'd go for the for-loop, but lapply is fine too.</p>
"
2656906,205054,2010-04-17T01:50:55Z,2656792,1,FALSE,"<p>Instead of actually altering mat, lapply just returns the altered version of mat (as a list). You just need to assign it to mat and turn it back into a matrix using <code>as.matrix()</code>.</p>
"
2656948,143476,2010-04-17T02:12:30Z,2656792,5,FALSE,"<p>One of the main advantages of higher-order functions like <code>lapply()</code> or <code>sapply()</code> is that you don't have to initialize your ""container"" (matrix in this case). </p>

<p>As Fojtasek suggests:</p>

<pre><code>as.matrix(lapply(1:10,function(i) rnorm(1,mean=i)))
</code></pre>

<p>Alternatively:</p>

<pre><code>do.call(rbind,lapply(1:10,function(i) rnorm(1,mean=i)))
</code></pre>

<p>Or, simply as a numeric vector:</p>

<pre><code>sapply(1:10,function(i) rnorm(1,mean=i))
</code></pre>

<p>If you really want to modify a variable above of the scope of your anonymous function (random number generator in this instance), use <code>&lt;&lt;-</code></p>

<pre><code>&gt; mat &lt;- matrix(0,nrow=10,ncol=1)
&gt; invisible(lapply(1:10, function(i) { mat[i,] &lt;&lt;- rnorm(1,mean=i)}))
&gt; mat
           [,1]
 [1,] 1.6780866
 [2,] 0.8591515
 [3,] 2.2693493
 [4,] 2.6093988
 [5,] 6.6216346
 [6,] 5.3469690
 [7,] 7.3558518
 [8,] 8.3354715
 [9,] 9.5993111
[10,] 7.7545249
</code></pre>

<p>See <a href=""https://stackoverflow.com/questions/2628621/how-do-you-use-scoping-assignment-in-r"">this post</a> about <code>&lt;&lt;-</code>. But in this particular example, a for-loop would just make more sense:</p>

<pre><code>mat &lt;- matrix(0,nrow=10,ncol=1)
for( i in 1:10 ) mat[i,] &lt;- rnorm(1,mean=i)
</code></pre>

<p>with the minor cost of creating a indexing variable, <code>i</code>, in the global workspace.</p>
"
2657002,163053,2010-04-17T02:40:07Z,2656792,24,TRUE,"<p>I discussed this issue in this related question: ""<a href=""https://stackoverflow.com/questions/2275896/is-rs-apply-family-more-than-syntactic-sugar"">Is R’s apply family more than syntactic sugar</a>"".  You will notice that if you look at the function signature for <code>for</code> and <code>apply</code>, they have one critical difference: a <code>for</code> loop evaluates an expression, while an <code>apply</code> loop evaluates a <strong>function</strong>.</p>

<p>If you want to alter things outside the scope of an apply function, then you need to use <code>&lt;&lt;-</code> or <code>assign</code>.  Or more to the point, use something like a <code>for</code> loop instead.  But you really need to be careful when working with things outside of a function because it can result in unexpected behavior. </p>

<p>In my opinion, one of the primary reasons to use an <code>apply</code> function is explicitly because it doesn't alter things outside of it.  This is a core concept in functional programming, wherein functions avoid having <a href=""http://en.wikipedia.org/wiki/Side_effect_(computer_science)"" rel=""nofollow noreferrer"">side effects</a>.  This is also a reason why the <code>apply</code> family of functions can be used in parallel processing (and similar functions exist in the various parallel packages such as snow).  </p>

<p>Lastly, the right way to run your code example is to also pass in the parameters to your function like so, and assigning back the output:</p>

<pre><code>mat &lt;- matrix(0,nrow=10,ncol=1)
mat &lt;- matrix(lapply(1:10, function(i, mat) { mat[i,] &lt;- rnorm(1,mean=i)}, mat=mat))
</code></pre>

<p>It is always best to be explicit about a parameter when possible (hence the <code>mat=mat</code>) rather than inferring it.</p>
"
2657461,170792,2010-04-17T06:11:07Z,2657434,5,TRUE,"<p>I am not sure  I have understood exactly what you want, but here is an approach</p>

<pre><code>x&lt;-c(1,2,3,4,5,6,7,40,1,2,3,21,4,1,23,4,43,23,4,12,3,43,5,36,3,45,12,31,3,4,23,41,23,5,53,45,3,7,6,36)

first2 &lt;- which(x==2)[1]
first40 &lt;- which(x&gt;=40)[1]

first40 - first2 - 1
&gt; 5

sec2 &lt;- which(x==2)[2]
sec40 &lt;- which(x&gt;=40)[2]

sec40 - sec2 - 1
&gt; 6
</code></pre>
"
2658747,37751,2010-04-17T14:50:01Z,2656731,1,FALSE,"<p>I do this type of thing with plyr, but I agree that it's not a processing efficency issue as much as what you are comfortable reading and writing. </p>
"
2658763,79061,2010-04-17T14:55:59Z,2658752,4,FALSE,"<p>You could use a matrix package such as <a href=""http://math.nist.gov/javanumerics/jama/"" rel=""nofollow noreferrer"">JAMA</a>.</p>
"
2659207,163053,2010-04-17T16:55:38Z,2658752,4,FALSE,"<p>There are several stackoverflow questions on using R with Java. This is simple with <strong><a href=""http://www.rforge.net/JRI/"" rel=""nofollow noreferrer"">JRI</a></strong>. See this question for an example: <a href=""https://stackoverflow.com/questions/2180235/r-from-within-java"">R from within Java</a>.  Once you have integrated your code, doing the matrix multiplication in R is trivial.  If you have two matrices, <code>a</code> and <code>b</code>, you would simply call: <code>a %*% b</code>.</p>

<p>If you want to stay in pure Java and work with a mathematics library, you can also look into using <a href=""http://acs.lbl.gov/software/colt/"" rel=""nofollow noreferrer""><strong>Colt</strong></a> (which is adapted from <a href=""http://acs.lbl.gov/software/colt/api/cern/colt/matrix/linalg/package-summary.html#Overview"" rel=""nofollow noreferrer"">JAMA</a>), although you could be better off just <a href=""http://math.nist.gov/javanumerics/jama/"" rel=""nofollow noreferrer"">using <strong>JAMA</strong> directly</a>.</p>

<p>Another option would be to use <a href=""http://incanter.org/"" rel=""nofollow noreferrer""><strong>Incanter</strong> from Clojure</a> (which provides a wrapper around <a href=""http://sites.google.com/site/piotrwendykier/software/parallelcolt"" rel=""nofollow noreferrer"">Parallel Colt</a>, amongst other things), and then call it as a Jar from Java.  It's trivial to integrate Clojure into Java, and if all you want is matrix multiplication, that will be easier for you than using R.</p>
"
2659698,171659,2010-04-17T19:28:33Z,2659609,3,TRUE,"<p>You can access column using a variable for the name by using brackets instead of $.</p>

<pre><code>for (name in names) data[name] &lt;- NA
</code></pre>

<p>But you could take a look at rbind.fill() in the reshape package (or plyr).</p>

<p>Hope this helps,
Etienne</p>
"
2660446,144537,2010-04-17T23:15:46Z,2659337,2,FALSE,"<p>I think this is more of model design question than on R specifically;  as such, I'd like to address the context of the question first then the appropriate R packages.</p>

<p>If your dependent variable is a probability, e.g., $y\in[0,1]$, a logistic regression is not data appropriate---particularly given that you are interested in predicting probabilities out of sample.  The logistic is going to be modeling the contribution of the independent variables to the probability that your dependent variable flips from a zero to a one, and since your variable is continuous and truncated you need a different specification.  </p>

<p>I think your latter intuition about mixed effects is a good one.  Since your observations are nested, i.e., <code>US &lt;-&gt; AZ &lt;-&gt; Phoenix</code>, a multi-level model, or in this case a hierarchical linear model, may be the best specification for your data.  The best R packages for this type of modeling are <code>multilevel</code> and <code>nlme</code>, and there is an excellent introduction to both <a href=""http://cran.r-project.org/doc/contrib/Bliese_Multilevel.pdf"" rel=""nofollow noreferrer"">multi-level models in R and nlme available here</a>.  You may be particularly interested in the discussion of data manipulation for multi-level modeling, which begins on page 26.</p>
"
2660464,144537,2010-04-17T23:22:41Z,2658338,1,FALSE,"<p>The internal data structures are `data.frame', a detailed introduction to the data frame can be found here.</p>

<p><a href=""http://cran.r-project.org/doc/manuals/R-intro.html#Data-frames"" rel=""nofollow noreferrer"">http://cran.r-project.org/doc/manuals/R-intro.html#Data-frames</a></p>
"
2661490,185475,2010-04-18T07:36:55Z,2661402,15,TRUE,"<p>My first approach would be to generate qq plots of the given data against the possible distributions.</p>

<pre><code>x &lt;- c(15.771062,14.741310,9.081269,11.276436,11.534672,17.980860,13.550017,13.853336,11.262280,11.049087,14.752701,4.481159,11.680758,11.451909,10.001488,11.106817,7.999088,10.591574,8.141551,12.401899,11.215275,13.358770,8.388508,11.875838,3.137448,8.675275,17.381322,12.362328,10.987731,7.600881,14.360674,5.443649,16.024247,11.247233,9.549301,9.709091,13.642511,10.892652,11.760685,11.717966,11.373979,10.543105,10.230631,9.918293,10.565087,8.891209,10.021141,9.152660,10.384917,8.739189,5.554605,8.575793,12.016232,10.862214,4.938752,14.046626,5.279255,11.907347,8.621476,7.933702,10.799049,8.567466,9.914821,7.483575,11.098477,8.033768,10.954300,8.031797,14.288100,9.813787,5.883826,7.829455,9.462013,9.176897,10.153627,4.922607,6.818439,9.480758,8.166601,12.017158,13.279630,14.464876,13.319124,12.331335,3.194438,9.866487,11.337083,8.958164,8.241395,4.289313,5.508243,4.737891,7.577698,9.626720,16.558392,10.309173,11.740863,8.761573,7.099866,10.032640)
&gt; qqnorm(x)
</code></pre>

<p>For more info see <a href=""http://www.statmethods.net/advgraphs/probability.html"" rel=""noreferrer"">link</a></p>

<p>Another possibility is based on the fitdistr function in the MASS package. Here is the different distributions ordered by their log-likelihood</p>

<pre><code>&gt; library(MASS)
&gt; fitdistr(x, 't')$loglik
[1] -252.2659
Warning message:
In log(s) : NaNs produced
&gt; fitdistr(x, 'normal')$loglik
[1] -252.2968
&gt; fitdistr(x, 'logistic')$loglik
[1] -252.2996
&gt; fitdistr(x, 'weibull')$loglik
[1] -252.3507
&gt; fitdistr(x, 'gamma')$loglik
[1] -255.9099
&gt; fitdistr(x, 'lognormal')$loglik
[1] -260.6328
&gt; fitdistr(x, 'exponential')$loglik
[1] -331.8191
Warning messages:
1: In dgamma(x, shape, scale, log) : NaNs produced
2: In dgamma(x, shape, scale, log) : NaNs produced
</code></pre>
"
2661688,170792,2010-04-18T09:10:55Z,2661402,14,FALSE,"<p>Another similar approach is using the <a href=""http://finzi.psych.upenn.edu/R/library/fitdistrplus/html/00Index.html"" rel=""noreferrer"">fitdistrplus</a> package</p>

<pre><code>library(fitdistrplus)
</code></pre>

<p>Loop through the distributions of interest and generate 'fitdist' objects. Use either ""mle"" for <code>maximum likelihood estimation</code> or ""mme"" for <code>matching moment estimation</code>, as the fitting method.</p>

<pre><code>f1&lt;-fitdist(x,""norm"",method=""mle"")
</code></pre>

<p>Use bootstrap re-sampling in order to simulate uncertainty in the parameters of the selected model</p>

<pre><code>b_best&lt;-bootdist(f_best)
print(f_best)
plot(f_best)
summary(f_best)
</code></pre>

<p>The <a href=""http://finzi.psych.upenn.edu/R/library/fitdistrplus/html/fitdist.html"" rel=""noreferrer"">fitdist</a> method allows for using custom distributions or distributions from other packages, provided that the corresponding density function <code>dname</code>, the corresponding distribution function <code>pname</code> and the corresponding quantile function <code>qname</code> have been defined (or even just the density function). </p>

<p>So if you wanted to test the log-likelihood for the inverse normal distribution:</p>

<pre><code>library(ig)
fitdist(x,""igt"",method=""mle"",start=list(mu=mean(x),lambda=1))$loglik
</code></pre>

<p>You may also find <a href=""http://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf"" rel=""noreferrer"">Fitting distributions with R</a> helpful.</p>
"
2661971,294017,2010-04-18T10:52:37Z,2573132,20,TRUE,"<p><strong>edit:</strong> Rewrite to summarize the edits that accumulated over time.</p>

<p>The current rpy2 release (2.3.x series) has full support for Python 3.3, while
no claim is made about Python 3.0, 3.1, or 3.2.
At the time of writing the next rpy2 release (under development, 2.4.x series) is only supporting Python 3.3.</p>

<p>History of Python 3 support:</p>

<ul>
<li><p>rpy2-2.1.0-dev / Python 3 branch in the repository - <em>experimental</em> support and application for a Google Summer of Code project consisting in porting rpy2 to Python 3 (under the Python umbrella)</p></li>
<li><p>application was accepted and thanks to Google's funding support for Python 3 slowly got into the main codebase (there was a fair bit of work still to be done after the GSoC - it made it for branch version_2.2.x).</p></li>
</ul>
"
2662089,168466,2010-04-18T11:41:16Z,2656731,0,TRUE,"<p>If you just want to perform straightforward multiple linear regression, then I would recommend <em>not</em> using lm().  There is lsfit(), but I'm not sure it would offer than much of a speed up (I have never performed a formal comparison).  Instead I would recommend performing the (X'X)^{-1}X'y using qr() and qrcoef().  This will allow you to perform multivariate multiple linear regression; that is, treating the response variable as a matrix instead of a vector and applying the same regression to each row of observations.  </p>

<pre><code>Z # design matrix
Y # matrix of observations (each row is a vector of observations)
## Estimation via multivariate multiple linear regression                    
beta &lt;- qr.coef(qr(Z), Y)
## Fitted values                                                             
Yhat &lt;- Z %*% beta
## Residuals                                                                 
u &lt;- Y - Yhat
</code></pre>

<p>In your example, is there a different design matrix per vector of observations?  If so, you may be able to modify Z in order to still accommodate this.</p>
"
2662171,25188,2010-04-18T12:10:11Z,2661402,3,FALSE,"<p>You could try using the Kolmogorov-Smirnov tests (<code>ks.test</code> in R).</p>

<p>If you have time-to-event data, here's software that does a <a href=""http://biostatistics.mdanderson.org/SoftwareDownload/SingleSoftware.aspx?Software_Id=66"" rel=""nofollow noreferrer"">Bayesian chi squared test</a> against a list of common distributions to report the best fit. </p>
"
2662423,319679,2010-04-18T13:48:34Z,2662364,-1,FALSE,"<p>I'm not really good a R, but maybe the arrays start at 0? (instead at 1)</p>

<pre><code>SAV[1] = 0
STO[1] = 100
</code></pre>

<p>==></p>

<pre><code>SAV[0] = 0
STO[0] = 100
</code></pre>

<p>And my second guess is regarding the if conditions in the for-loops. I would put brackets upon the whole expression, something like this.</p>

<pre><code>for (t in 2:1000) {
  if ((price[t]&gt;9.9)&amp;（price[t]&lt;10.1）&amp;(SAV[t-1]=0)) {
      STO[t] = STO [t-1]
      SAV[t] = 0
  }
}
</code></pre>

<p>I hardly think it could help, but it's worth a try ;)</p>

<hr>

<p>Edit:</p>

<p>Try to use <code>SAV[t-1]==0</code> instead of <code>SAV[t-1]=0</code> in your if condition ...</p>

<hr>

<p>Edit 2:</p>

<p>Also try to use the &amp;&amp; operator instead of the &amp; one ...</p>
"
2662446,16632,2010-04-18T13:56:52Z,2661402,8,FALSE,"<p>I find it hard to imagine a realistic situation where this would be useful.  Why not use a non-parametric tool like a kernel density estimate?</p>
"
2662452,235298,2010-04-18T13:58:22Z,2662364,0,FALSE,"<p>I think you're overwriting the vectors STO and SAV each iteration.  Hard to tell though because the price vector hasn't been declared.  Try initializing STO and SAV as vectors of the desired length, rather than 0-length vectors:</p>

<p>SAV = matrix(0,1,1000)</p>

<p>STO = matrix(0,1,1000)</p>
"
2662457,16632,2010-04-18T14:00:20Z,2659337,0,FALSE,"<p>I would suggest looking into penalised regressions like the elastic net. The elastic net is used in text mining where each column represents the present or absence of a single word, and there maybe hundreds of thousands of variables, an analogous problem to yours.  A good place to start with R would be the <code>glmnet</code> package and its accompanying JSS paper: <a href=""http://www.jstatsoft.org/v33/i01/"" rel=""nofollow noreferrer"">http://www.jstatsoft.org/v33/i01/</a>.</p>
"
2662919,170792,2010-04-18T16:14:48Z,2662364,3,TRUE,"<p>I would try something like the following. Modify it to be consistent with your program's logic</p>

<pre><code>for (t in 2:1000) {
        if ((price[t]&gt;9)&amp;(price[t]&lt;10)) {
             # values for STO,SAV when price in the interval and SAV[t-1]!=0
             if (SAV[t-1]!=0) { 
                SAV[t]=SAV[t-1]*i 
                STO[t]=0
             }
             # values for STO,SAV when price in the interval and SAV[t-1]==0
             else { 
               STO[t] = STO[t-1]
               SAV[t] = 0 
             }
        }
        # values for STO,SAV when price not in the interval
        else {   
           STO[t] = STO[t-1]
           SAV[t] = 1
        }
}
</code></pre>
"
2663175,189946,2010-04-18T17:28:06Z,2659308,0,TRUE,"<p>I've noticed that this usually corrects itself if I insert a new R chunk and then delete it. Not an elegant solution, but it works for me.</p>
"
2665518,320051,2010-04-19T06:26:25Z,2664655,1,FALSE,"<p>John, this sounds interesting, but if you provide the data and the article is formatted in sweave, wouldn't this long log file be redundant?  </p>

<p>back to your question, one package you might want to look into is <strong><a href=""http://gking.harvard.edu/zelig/"" rel=""nofollow noreferrer"">zelig</a></strong> since it ""automates the creation of replication data files so that you (or, if you wish, anyone else) can replicate the results of your analyses (hence satisfying the replication standard)"". Not what you are looking for, but the concept of replication data files might give you some other ideas.  notice that multiple journals are now using replication data files.</p>
"
2665698,172261,2010-04-19T07:05:29Z,2665532,15,TRUE,"<p>There are two packages:</p>

<ul>
<li><a href=""http://www.omegahat.net/RGoogleDocs/"" rel=""nofollow noreferrer"">RGoogleDocs</a> on Omegahat: the package allows you to get a list of the documents and details about each of them, download the contents of a document, remove a document, and upload a document, even binary files. </li>
<li><a href=""http://r-forge.r-project.org/projects/rgoogledata/"" rel=""nofollow noreferrer"">RGoogleData</a> on RForge: provides R access to Google services through the Google supported Java API. Currently the R interface only supports Google Docs and Spreadsheets.</li>
</ul>
"
2665724,320082,2010-04-19T07:12:57Z,2665532,0,FALSE,"<p>Since R itself is relatively limited when it comes to execution flow control, i suggest using an api to an high-level programming language provided by google: <a href=""http://code.google.com/apis/spreadsheets/code.html"" rel=""nofollow noreferrer"">link text</a>.
There you can pick whichever you are most familiar with.</p>

<p>I for one always use python templates to give R a little more flexibility, so that would be a good combination.</p>

<p>For the task of exporting data from R to google docs, the first thing that comes to my mind would be to save it to csv, then parse and talk to g/docs with one of the given languages.</p>
"
2666916,170792,2010-04-19T11:20:41Z,2666799,1,FALSE,"<p>I would say</p>

<pre><code>predict(cars.lo, data.frame(speed=5))
[1] 7.797353
</code></pre>
"
2667343,16632,2010-04-19T12:30:03Z,2666805,2,TRUE,"<p>No, because the models are only created when the plot is rendered. However, it's usually pretty easy to do it yourself with plyr. </p>

<p>Why do you want to convert sex to a number?  As.numeric should be enough by itself, but if you're going to do the subtraction in the model you'll need to surround it with I().</p>
"
2667364,144157,2010-04-19T12:32:54Z,2666799,2,FALSE,"<p>I would compute the predicted values at x, x+eps, x-eps, and then fit a quadratic to the results. It's not very efficient in computer time, but if you haven't got to do it very often, it is very efficient in programmer time.</p>
"
2667691,171659,2010-04-19T13:22:55Z,2667478,1,FALSE,"<p>Maybe you should use <code>c()</code>.</p>

<pre><code>a &lt;- NULL
for(i in 1:10){
  a &lt;- c(a,i)
}
print(a)
</code></pre>
"
2667697,163053,2010-04-19T13:24:09Z,2667673,76,FALSE,"<p>Using the index:</p>

<pre><code>df[1:4,]
</code></pre>

<p>Where the values in the parentheses can be interpreted as either logical, numeric, or character (matching the respective names):</p>

<pre><code>df[row.index, column.index]
</code></pre>

<p>Read help(`[`) for more detail on this subject, and also read about <a href=""http://cran.r-project.org/doc/manuals/R-intro.html#Index-matrices"" rel=""noreferrer"">index matrices</a> in the Introduction to R.</p>
"
2667843,143377,2010-04-19T13:45:59Z,2667673,97,TRUE,"<p>Use <code>head</code>:</p>

<pre><code>dnow &lt;- data.frame(x=rnorm(100), y=runif(100))
head(dnow,4) ## default is 6
</code></pre>
"
2667949,216064,2010-04-19T14:00:27Z,2667478,0,FALSE,"<p>Another option could be the function Reduce:</p>

<pre><code>a &lt;- Reduce(function(w,add) c(w,add), NULL, 1:10)
</code></pre>
"
2668158,245603,2010-04-19T14:26:01Z,2667478,4,TRUE,"<p>I think Etiennebr's answer shows you what you should do, but here is how to capture the output of <code>print</code> as you say you want: use the <code>capture.output</code> function.</p>

<pre><code>&gt; a &lt;- capture.output({for(i in 1:5) print(i)})
&gt; a
[1] ""[1] 1"" ""[1] 2"" ""[1] 3"" ""[1] 4"" ""[1] 5""
</code></pre>

<p>You can see that it captures everything exactly as printed. To avoid having all the <code>[1]</code>s, you can use <code>cat</code> instead of print: </p>

<pre><code>a &lt;- capture.output({for(i in 1:5) cat(i,""\n"")})
&gt; a
[1] ""1 "" ""2 "" ""3 "" ""4 "" ""5 ""
</code></pre>

<p>Once again, you probably don't really want to do this for your application, but there are situations when this approach is useful (eg. to hide automatically printed text that some functions insist on).</p>

<p>EDIT:
Since the output of <code>print</code> or <code>cat</code> is a string, if you capture it, it still will be a string and will have quotation marks To remove the quotes, just use <code>as.numeric</code>:</p>

<pre><code>&gt; as.numeric(a)
[1] 1 2 3 4 5
</code></pre>
"
2669034,88198,2010-04-19T16:18:06Z,2668938,3,TRUE,"<p>Here's what I get when I run bits of your code:</p>

<pre><code>&gt; res = c(3,5,8)
&gt; msg = cat('Results are: ', res, ', that is nice right?')
Results are:  3 5 8 , that is nice right?&gt; 
&gt; msg
NULL
</code></pre>

<p>The problem is that <code>cat</code> prints strings to stdout, rather than returning them as a string.  What you want is:</p>

<pre><code>&gt; res = c(3,5,8)
&gt; msg = paste('Results are: ', toString(res), ', that is nice right?', sep='')
&gt; msg
[1] ""Results are: 3, 5, 8, that is nice right?""
</code></pre>
"
2669232,163053,2010-04-19T16:48:51Z,2669137,5,FALSE,"<p>You can convert the summary output into a matrix and then bind them:</p>

<pre><code>a &lt;- 1:50
b &lt;- 3:53 
c &lt;- rnorm(500)
cbind(as.matrix(summary(a)), as.matrix(summary(b)), as.matrix(summary(c)))
</code></pre>

<p>Alternatively, you can combine them into a list and use an <code>apply</code> function (or <code>plyr</code>):</p>

<pre><code>library(plyr)
ldply(list(a, b, c), summary)
</code></pre>
"
2669804,163053,2010-04-19T18:19:12Z,2669598,3,FALSE,"<p>is there some reason that you don't want to embed it?  This is covered in this question: <a href=""https://stackoverflow.com/questions/2463437/r-from-c-simplest-possible-helloworld"">R from C — Simplest Possible Helloworld…</a></p>
"
2669874,66549,2010-04-19T18:32:01Z,2669427,7,FALSE,"<p>This is the most straightforward and reliable way i've found to to transfer a data frame from R to Python.</p>

<p>To begin with, I think exchanging the data through the R bindings is an unnecessary complication. R provides a simple method to export data, likewise, NumPy has decent methods for data import. The file format is the only common interface required here.</p>

<pre><code>data(iris)
iris$Species = unclass(iris$Species)

write.table(iris, file=""/path/to/my/file/np_iris.txt"", row.names=F, sep="","")

# now start a python session
import numpy as NP

fpath = ""/path/to/my/file/np_iris.txt""

A = NP.loadtxt(fpath, comments=""#"", delimiter="","", skiprows=1)

# print(type(A))
# returns: &lt;type 'numpy.ndarray'&gt;

print(A.shape)
# returns: (150, 5)

print(A[1:5,])
# returns: 
 [[ 4.9  3.   1.4  0.2  1. ]
  [ 4.7  3.2  1.3  0.2  1. ]
  [ 4.6  3.1  1.5  0.2  1. ]
  [ 5.   3.6  1.4  0.2  1. ]]
</code></pre>

<p>According to the Documentation (and my own experience for what it's worth) <em>loadtxt</em> is the preferred method for conventional data import.</p>

<p>You can also pass in to <em>loadtxt</em> a tuple of data types (the argument is <em>dtypes</em>), one item in the tuple for each column. Notice 'skiprows=1' to step over the column headers (for <em>loadtxt</em> rows are indexed from 1, columns from 0).</p>

<p>Finally, i converted the dataframe factor to integer (which is actually the underlying data type for factor) prior to exporting--'unclass' is probably the easiest way to do this.</p>

<p>If you have big data (ie, don't want to load the entire data file into memory but still need to access it)  <strong>NumPy's memory-mapped data structure</strong> ('memmap') is a good choice:</p>

<pre><code>from tempfile import mkdtemp
import os.path as path

filename = path.join(mkdtemp(), 'tempfile.dat')

# now create a memory-mapped file with shape and data type 
# based on original R data frame:
A = NP.memmap(fpath, dtype=""float32"", mode=""w+"", shape=(150, 5))

# methods are ' flush' (writes to disk any changes you make to the array), and 'close'
# to write data to the memmap array (acdtually an array-like memory-map to 
# the data stored on disk)
A[:] = somedata[:]
</code></pre>
"
2670971,169947,2010-04-19T21:18:23Z,2669137,6,TRUE,"<p>It looks like your data is in a data frame or matrix.  If so, you can do the following:</p>

<pre><code>&gt; df &lt;- data.frame(a=1:50, b=3:52, c=rnorm(500))
&gt; apply(df, 2, summary)
           a    b         c
Min.     1.0  3.0 -3.724000
1st Qu. 13.0 15.0 -0.733000
Median  25.5 27.5 -0.004868
Mean    25.5 27.5 -0.033950
3rd Qu. 38.0 40.0  0.580800
Max.    50.0 52.0  2.844000
</code></pre>
"
2673594,294017,2010-04-20T08:19:46Z,2669427,4,TRUE,"<p>Why going through a data.frame when 'exprs(immgen)' returns a /matrix/ and your end goal is to have your data in a matrix ?</p>

<p>Passing the matrix to numpy is straightforward (and can even be made without making a copy):
<a href=""http://rpy.sourceforge.net/rpy2/doc-2.1/html/numpy.html#from-rpy2-to-numpy"" rel=""nofollow noreferrer"">http://rpy.sourceforge.net/rpy2/doc-2.1/html/numpy.html#from-rpy2-to-numpy</a></p>

<p>This should beat in both simplicity and efficiency the suggestion of going through text representation of numerical data in flat files as a way to exchange data.</p>

<p>You seem to be working with bioconductor classes, and might be interested in the following:
<a href=""http://pypi.python.org/pypi/rpy2-bioconductor-extensions/"" rel=""nofollow noreferrer"">http://pypi.python.org/pypi/rpy2-bioconductor-extensions/</a></p>
"
2673621,168747,2010-04-20T08:26:08Z,2659609,3,FALSE,"<p>As you can see in examples to </p>

<pre><code>?`[&lt;-.data.frame`
</code></pre>

<p>there is no need to loop, you can just do</p>

<pre><code>data[names] &lt;- NA
</code></pre>

<p>Example:</p>

<pre><code>&gt; (data &lt;- data.frame(x=1:3, y=letters[1:3]))
  x y
1 1 a
2 2 b
3 3 c
&gt; data[names] &lt;- NA
&gt; data
  x y first second third
1 1 a    NA     NA    NA
2 2 b    NA     NA    NA
3 3 c    NA     NA    NA
</code></pre>
"
2675318,16363,2010-04-20T13:10:28Z,2673504,3,FALSE,"<p>You'd probably be best working with a version control system.  Many can be indexed and be made <a href=""https://stackoverflow.com/questions/493137/how-to-index-and-search-subversion-repository"">search-able</a>.  At my work, a stack of R, Eclipse, <a href=""http://www.walware.de/goto/statet"" rel=""nofollow noreferrer"">StatET</a>, Subversion and <a href=""http://subclipse.tigris.org/"" rel=""nofollow noreferrer"">Subclipse</a> works very well for us.</p>
"
2675402,163053,2010-04-20T13:21:47Z,2673504,3,TRUE,"<p>R comes with several mechanisms for searching for help, most of which naturally use CRAN.  Some examples: <a href=""http://cran.r-project.org/web/packages/sos/index.html"" rel=""nofollow noreferrer"">the sos package</a>, <a href=""http://dirk.eddelbuettel.com/cranberries/"" rel=""nofollow noreferrer"">cranberries</a>, <a href=""http://crantastic.org/"" rel=""nofollow noreferrer"">crantastic</a>, and <a href=""http://rseek.org/"" rel=""nofollow noreferrer"">rseek</a>.  In many cases, these could be adapted to use a local repository (you can find out <a href=""http://cran.r-project.org/doc/manuals/R-admin.html#Setting-up-a-package-repository"" rel=""nofollow noreferrer"">how to create a local repository</a> in the R manual, which is very easy to do).  Otherwise, if you package your scripts and submit them to CRAN, you will naturally have these available to you.  I would also highly recommend this presentation on the subject: <a href=""http://cran.r-project.org/doc/contrib/Graves+DoraiRaj-RPackageDevelopment.pdf"" rel=""nofollow noreferrer"">Creating R Packages, Using CRAN, R-Forge, And Local R Archive Networks And Subversion (SVN) Repositories</a> from Spencer Graves and Sundar Dorai-Raj.</p>

<p>These would require you to put your code in packages, and create documentation, all of which is worth doing anyway.  The package documentation turns out to be very useful for both documenting what things do, and helping your find them in the future.  You can use <code>roxygen</code> to create this documentation in-line with your code.  Also read this related question: <a href=""https://stackoverflow.com/questions/2284446/organizing-r-source-code/2284486"">Organizing R Source Code</a>.</p>

<p>Alternatively, the <code>help.search()</code> function can be very useful for searching local packages, regardless of whether you have a repository set up.</p>
"
2675532,163053,2010-04-20T13:42:37Z,2675502,10,TRUE,"<p>From the help file (you can see this with <code>help("":::"")</code>):</p>

<pre><code>The expression 'pkg::name' returns the value of the exported
     variable 'name' in package 'pkg' if the package has a name space.
     The expression 'pkg:::name' returns the value of the internal
     variable 'name' in package 'pkg' if the package has a name space.
</code></pre>

<p>In other words <code>:::</code> is used to directly access a member of a package that is internal (i.e. not exported from the NAMESPACE).</p>

<p>See this related question: <a href=""https://stackoverflow.com/questions/2165342/r-calling-a-function-from-a-namespace"">R: calling a function from a namespace</a>.</p>
"
2675560,16632,2010-04-20T13:45:28Z,2666799,0,FALSE,"<p>I think you're going to be on your own for this one.  I'd start by reading the references in <code>lowess</code>, and then read the source for the C <code>lowess</code> function.  I'd recommend starting there instead of with <code>loess</code> because it's an older and slightly simpler algorithm that you might find easier to adapt for your needs.</p>
"
2675572,168747,2010-04-20T13:46:38Z,2675517,7,TRUE,"<p>Simple one:</p>

<pre><code>rbind(data, AVERAGES=colMeans(data))
</code></pre>

<p>[Edit] If your <code>data.frame</code> contains other types than <code>numeric</code> (like <code>factor</code> or <code>character</code>) then you could use more complicated but safer method:</p>

<pre><code>rbind(data, AVERAGES=as.data.frame(lapply(data, mean)))
</code></pre>

<p>Simple example:</p>

<pre><code>data &lt;- data.frame(
 x_Date = Sys.Date()+1:3,
 x_numeric = 1:3+.1,
 x_POSIXt = Sys.time()+1:3,
 x_factor = factor(letters[1:3]),
 x_character = letters[1:3],
 x_logical = c(TRUE,FALSE,TRUE),
 x_complex = 1i+1:3,
 stringsAsFactors = FALSE,
 row.names=paste(""Row"",1:3)
)

rbind(data, AVERAGES=as.data.frame(lapply(data , mean)))
# Warning in mean.default(X[[4L]], ...) :
  # argument is not numeric or logical: returning NA
# Calls: rbind -&gt; as.data.frame -&gt; lapply -&gt; FUN -&gt; mean.default
# Warning in mean.default(X[[5L]], ...) :
  # argument is not numeric or logical: returning NA
# Calls: rbind -&gt; as.data.frame -&gt; lapply -&gt; FUN -&gt; mean.default
             # x_Date x_numeric            x_POSIXt x_factor x_character x_logical x_complex
# Row 1    2010-04-21       1.1 2010-04-20 23:30:42        a           a  1.000000      1+1i
# Row 2    2010-04-22       2.1 2010-04-20 23:30:43        b           b  0.000000      2+1i
# Row 3    2010-04-23       3.1 2010-04-20 23:30:44        c           c  1.000000      3+1i
# AVERAGES 2010-04-22       2.1 2010-04-20 23:30:43     &lt;NA&gt;        &lt;NA&gt;  0.666667      2+1i
</code></pre>

<p><code>logical</code> column is converted to numeric, and for non-numeric columns there are <code>NA</code>'s</p>
"
2676550,143305,2010-04-20T15:49:13Z,2673504,4,FALSE,"<p>The question </p>

<blockquote>
  <p>I am looking for a solution that allows me to keep a track of a multitude of R scripts 
  that I create for various projects and purposes. Some scripts are easily tracked to specific 
  projects, whereas others are ""convenience"" functions created to serve a set of tasks.</p>
</blockquote>

<p>fails to address the obvious follow-up of why the <strong><em>existing</em></strong> mechanism is not suitable:</p>

<ol>
<li>Create a local package for each project</li>
<li>Create one or more local packages for local utility functions</li>
<li>Use R's <strong><em>already existing</em></strong> mechanisms for searching, indexing, testing, cross-referencing</li>
<li>And use any revision control system of your liking, local or on the web, to host the code for 1. to 3. above.</li>
</ol>

<p>Reinventing an RDBMS schema for 1. to 3. is just wrong in my book.  But if you must, go ahead and replicate what you can already (mostly) get for free in tested and widely used code.</p>
"
2676611,139010,2010-04-20T15:56:22Z,2676554,23,TRUE,"<p>There's the <a href=""http://rss.acs.unt.edu/Rdoc/library/plotrix/html/00Index.html"" rel=""noreferrer"">plotrix</a> package with has a built-in function for this: <a href=""http://rss.acs.unt.edu/Rdoc/library/plotrix/html/std.error.html"" rel=""noreferrer"">std.error</a></p>
"
2676782,160314,2010-04-20T16:18:41Z,2676554,107,FALSE,"<p>The standard error is just the standard deviation divided by the square root of the sample size. So you can easily make your own function:</p>

<pre><code>&gt; std &lt;- function(x) sd(x)/sqrt(length(x))
&gt; std(c(1,2,3,4))
[1] 0.6454972
</code></pre>
"
2677121,245603,2010-04-20T17:07:03Z,2676926,2,TRUE,"<p>You really don't want to use <code>capture.output</code> for capturing numeric output like that. You can create a matrix in which to store the output, or use <code>apply</code> as follows:</p>

<pre><code> apply(a, 2, function(x)c(min(x), quantile(x, 0.25), mean(x), sd(x)/sqrt(length(x)))) 
</code></pre>

<p>(I did not add all the statistics you wanted.) This is more in line with the R way of programming.</p>
"
2677859,321622,2010-04-20T19:03:07Z,2676554,74,FALSE,"<p>It is probably more efficient to use var... since you actually sqrt twice in your code, once to get the sd (code for sd is in r and revealed by just typing ""sd"")...  </p>

<pre><code>se &lt;- function(x) sqrt(var(x)/length(x))
</code></pre>
"
2679289,163053,2010-04-20T23:03:26Z,2679193,89,TRUE,"<p>Use assign:</p>

<pre><code>assign(paste(""orca"",i,sep=""""), list_name[[i]])
</code></pre>
"
2679549,321622,2010-04-21T00:11:32Z,2679193,28,FALSE,"<p>It seems to me that you might be better off with a list rather than using <code>orca1</code>, <code>orca2</code>, etc, ... then it would be <code>orca[1]</code>, <code>orca[2]</code>, ...</p>

<p>Usually you're making a list of variables differentiated by nothing but a number because that number would be a convenient way to access them later.</p>

<pre><code>orca &lt;- list()
orca[1] &lt;- ""Hi""
orca[2] &lt;- 59
</code></pre>

<p>Otherwise, <code>assign</code> is just what you want.</p>
"
2680850,322000,2010-04-21T06:51:15Z,2676554,0,FALSE,"<p>more generally, for standard errors on any other parameter, you can use the boot package for bootstrap simulations (or write them on your own)</p>
"
2680870,168168,2010-04-21T06:55:49Z,2678141,10,TRUE,"<p>Try using</p>

<blockquote>
  <p>scale_x_continuous(breaks = NA)</p>
</blockquote>

<p>This would remove all the vertical gridlines as well as x-axis tickmark labels.</p>
"
2681716,168747,2010-04-21T09:33:41Z,2679193,0,FALSE,"<p>Another tricky solution is to name elements of list and <code>attach</code> it:</p>

<pre><code>list_name = list(
    head(iris),
    head(swiss),
    head(airquality)
    )

names(list_name) &lt;- paste(""orca"", seq_along(list_name), sep="""")
attach(list_name)

orca1
#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 1          5.1         3.5          1.4         0.2  setosa
# 2          4.9         3.0          1.4         0.2  setosa
# 3          4.7         3.2          1.3         0.2  setosa
# 4          4.6         3.1          1.5         0.2  setosa
# 5          5.0         3.6          1.4         0.2  setosa
# 6          5.4         3.9          1.7         0.4  setosa
</code></pre>
"
2682538,216064,2010-04-21T11:46:14Z,2682514,1,FALSE,"<p>Maybe the <a href=""http://cran.r-project.org/web/packages/mlbench/index.html"" rel=""nofollow noreferrer"">mlbench</a> package is what you are looking for? You can inspect the implementation by using print(nameOfFunction).</p>
"
2682842,168168,2010-04-21T12:33:53Z,2676926,0,FALSE,"<p>Or, alternatively</p>

<pre><code>a &lt;- data.frame(matrix(1:25,5,5))
fn &lt;- function (x)
    c(summary(x), Sd = sd(x), Var = var(x))

library(plyr)
colwise (fn) (a)
</code></pre>

<p>Returns:</p>

<pre><code>              X1        X2        X3        X4        X5
Min.    1.000000  6.000000 11.000000 16.000000 21.000000
1st Qu. 2.000000  7.000000 12.000000 17.000000 22.000000
Median  3.000000  8.000000 13.000000 18.000000 23.000000
Mean    3.000000  8.000000 13.000000 18.000000 23.000000
3rd Qu. 4.000000  9.000000 14.000000 19.000000 24.000000
Max.    5.000000 10.000000 15.000000 20.000000 25.000000
Sd      1.581139  1.581139  1.581139  1.581139  1.581139
Var     2.500000  2.500000  2.500000  2.500000  2.500000
</code></pre>
"
2682919,143305,2010-04-21T12:43:06Z,2682629,2,TRUE,"<p>AFAIK those refresh events are often owned by the window manager so this may be tricky. </p>
"
2683704,121332,2010-04-21T14:18:24Z,2676926,0,FALSE,"<p>Or using plyr:</p>

<pre><code>&gt; a&lt;=data.frame(value=1:25,set=factor(rep(1:5,each=5)))
&gt; summaryfn&lt;=function(v)data.frame(
+   q0=min(v),q25=quantile(v,0.25),q50=median(v),
+   q75=quantile(v,0.75),q100=max(v),mean=mean(v),
+   sd=sd(v)^(1/2),var=var(v))
&gt; g&lt;-ddply(a,.(set),function(x)summaryfn(x$value))
&gt; g
  set q0 q25 q50 q75 q100 mean       sd var
1   1  1   2   3   4    5    3 1.257433 2.5
2   2  6   7   8   9   10    8 1.257433 2.5
3   3 11  12  13  14   15   13 1.257433 2.5
4   4 16  17  18  19   20   18 1.257433 2.5
5   5 21  22  23  24   25   23 1.257433 2.5
&gt; t(g[-1])
         [,1]      [,2]      [,3]      [,4]      [,5]
q0   1.000000  6.000000 11.000000 16.000000 21.000000
q25  2.000000  7.000000 12.000000 17.000000 22.000000
q50  3.000000  8.000000 13.000000 18.000000 23.000000
q75  4.000000  9.000000 14.000000 19.000000 24.000000
q100 5.000000 10.000000 15.000000 20.000000 25.000000
mean 3.000000  8.000000 13.000000 18.000000 23.000000
sd   1.257433  1.257433  1.257433  1.257433  1.257433
var  2.500000  2.500000  2.500000  2.500000  2.500000
</code></pre>

<p>-Alex</p>
"
2684705,143305,2010-04-21T16:23:13Z,2684361,3,TRUE,"<p>This works just fine for me:</p>

<pre><code>\documentclass{article}
\usepackage{Sweave}
\begin{document}

&lt;&lt;label=start, echo=FALSE, include=FALSE&gt;&gt;=
startt&lt;-proc.time()[3]
@

Text and Sweave Code in here

This document was created on \today, with
\Sexpr{print(version$version.string)}.

&lt;&lt;results=hide,echo=FALSE&gt;&gt;=
Sys.sleep(2)  # instead of real work
@

More text and Sweave code in here

&lt;&lt;label=bye, include=FALSE, echo=FALSE&gt;&gt;=
endt&lt;-proc.time()[3]
elapsedtime&lt;-as.numeric(endt-startt)
@

It took approx \Sexpr{elapsedtime} seconds to process.

\end{document}
</code></pre>

<p>I had to remove the version string inside the <code>\Sexp{}</code> as I get an underscore with via <code>x86_64</code> which then upsets LaTeX.  Otherwise just fine, and you now get the elapsed time of just over the slept amount.</p>

<p>You could use either R to cache the elapsed time in a temporary file for the next run, or pass it to LaTeX as some sort of variable -- but you will not be able to use 'forward references' as the R chunks gets evaluated in turn. </p>
"
2684767,264696,2010-04-21T16:33:02Z,2684361,2,FALSE,"<p>btw you don't usually need print to evaluate variables R</p>

<pre><code>\Sexpr{version$version.string}
</code></pre>

<p>works fine as well</p>
"
2684771,163053,2010-04-21T16:33:42Z,2684715,4,TRUE,"<p>Here's some sample data:</p>

<pre><code>df &lt;- data.frame(a=c(FALSE, TRUE, FALSE), b=c(TRUE, FALSE, FALSE), c=c(FALSE, FALSE, TRUE))
</code></pre>

<p>You can use <code>apply</code> to do something like this:</p>

<pre><code>names(df)[apply(df, 1, which)]
</code></pre>

<p>Or without <code>apply</code> by using <code>which</code> directly:</p>

<pre><code>idx &lt;- which(as.matrix(df), arr.ind=T)
names(df)[idx[order(idx[,1]),""col""]]
</code></pre>
"
2684798,143305,2010-04-21T16:39:25Z,2684715,3,FALSE,"<p>Use <code>apply</code> to sweep your index through, and use that index to access the column names:</p>

<pre><code>&gt; df &lt;- data.frame(a=c(TRUE,FALSE,FALSE),b=c(FALSE,FALSE,TRUE),
+                  c=c(FALSE,TRUE,FALSE))
&gt; df
      a     b     c
1  TRUE FALSE FALSE
2 FALSE FALSE  TRUE
3 FALSE  TRUE FALSE
&gt; colnames(df)[apply(df, 1, which)]
[1] ""a"" ""c"" ""b""
&gt; 
</code></pre>
"
2685964,168168,2010-04-21T19:28:07Z,2684966,5,TRUE,"<p>As code in comments does not display nicely, so I am posting this as an answer. You could do something like this and add labels manually with <code>geom_text()</code>:</p>

<pre><code>ggplot(data, aes(x, y)) +
        geom_bar(stat = 'identity') +
        scale_x_continuous(breaks = NA) +
        opts(
                panel.grid.major = theme_line(size = 0.5, colour = '#1391FF'),
                panel.grid.minor = theme_blank(),
                panel.background = theme_blank(),
                axis.ticks = theme_blank()
        )+
        geom_text(aes(label = x, y = -.3))
</code></pre>
"
2686249,144537,2010-04-21T20:16:36Z,2685707,2,FALSE,"<p>If you move the <code>fill</code> into the <code>geom_bar</code> it should work.  As:</p>

<pre><code>ggplot(data.PE5, aes(ybands)) + geom_bar(aes(fill=factor(decide)),position=""dodge"") + facet_grid(~group_label)
</code></pre>

<p>The reason is the way <code>ggplot2</code> builds plots as a grammar (I think).</p>
"
2686607,163053,2010-04-21T21:14:22Z,2686438,5,TRUE,"<p>Could you state what you want your document to look like in the end?  Clearly it has repetitive structures in it.  In which case, Sweave may not be the best tool.  You might instead want to consider using something like <code>brew</code>.  See <a href=""http://learnr.wordpress.com/2009/09/09/brew-creating-repetitive-reports/"" rel=""nofollow noreferrer"">this blog post on the Learning R blog</a> for an example of how this works.</p>
"
2686683,143319,2010-04-21T21:25:34Z,2686438,2,FALSE,"<p>I've done exactly this before, using your second option.  I had a separate R file that would iterate through the group names, assign each to the group variable (your <code>specialty</code>), create a renamed copy of the Sweave master file (with the group's name pasted into the filename), and then Sweave that new file.  Your use of the word ""replace"" makes me hesitant - I wouldn't try any kind of regex solution (and maybe that's not what you intend).  Just assigning it in the master script (<code>specialty &lt;- specialties[i]</code>) is fine.</p>

<p>That code is trapped on my currently-dead home PC, but I might have it on a flashdrive somewhere.  If you have trouble getting this working right, let me know and I'll dig around for it.</p>

<p><code>brew</code> is probably also worth looking into, though I don't have any personal experience with it yet and therefore can't compare it to Sweave.</p>
"
2686816,144157,2010-04-21T21:48:56Z,2684479,21,TRUE,"<ol>
<li><p>No ARIMA(p,0,q) model will allow for a trend because the model is stationary. If you really want to include a trend, use ARIMA(p,1,q) with a drift term, or ARIMA(p,2,q). The fact that auto.arima() is suggesting 0 differences would usually indicate there is no clear trend.</p></li>
<li><p>The help file for arima() shows that the intercept is actually the mean. That is, the AR(1) model is (Y_t-c) = phi*(Y_{t-1} - c) + e_t rather than Y_t = c + phi*Y_{t-1} + e_t as you might expect.</p></li>
<li><p>auto.arima() uses a unit root test to determine the number of differences required. So check the results from the unit root test to see what's going on. You can always specify the required number of differences in auto.arima() if you think the unit root tests are not leading to a sensible model.  </p></li>
</ol>

<p>Here are the results from two tests for your data:</p>

<pre><code>R&gt; adf.test(x)

        Augmented Dickey-Fuller Test

data:  x 
Dickey-Fuller = -1.031, Lag order = 3, p-value = 0.9249
alternative hypothesis: stationary 

R&gt; kpss.test(x)

        KPSS Test for Level Stationarity

data:  x 
KPSS Level = 0.3491, Truncation lag parameter = 1, p-value = 0.09909
</code></pre>

<p>So the ADF says strongly non-stationary (the null hypothesis in that case) while the KPSS doesn't quite reject stationarity (the null hypothesis for that test). auto.arima() uses the latter by default. You could use auto.arima(x,test=""adf"") if you wanted the first test. In that case it suggests the model ARIMA(0,2,1) which does have a trend.</p>
"
2687142,135870,2010-04-21T23:02:25Z,2686320,24,TRUE,"<p>The best way I have found to do this is to indicate the table column as a ""fixed width"" column so that the text inside it wraps.  With the <code>xtable</code> package, this can be done with:</p>

<pre><code>align( calqc_xtable ) &lt;- c( 'l', 'p{1.5in}', rep('c',5) )
</code></pre>

<p><code>xtable</code> demands that you provide an alignment for the option ""rownames"" column- this is the initial <code>l</code> specification.  The section specification, <code>p{1.5in}</code>, is used for your first column header, which is quite long.  This limits it to a box 1.5 inches in width and the header will wrap onto multiple lines if necessary.  The remaining five columns are set centered using the <code>c</code> specifier.</p>

<p>One major problem with fixed width columns like <code>p{1.5in}</code> is that <strong>they set the text using a justified alignment</strong>.  This causes the inter-word spacing in each line to be expanded such that the line will fill up the entire 1.5 inches allotted.</p>

<p>Frankly, in most cases this produces results which I cannot describe using polite language (I'm an amateur typography nut and this sort of behavior causes facial ticks).</p>

<p>The fix is to provide a latex alignment command by prepending a <code>&gt;{}</code> field to the column specification:</p>

<pre><code>align( calqc_xtable ) &lt;- c( 'l', '&gt;{\\centering}p{1.5in}', rep('c',4) )
</code></pre>

<p>Other useful alignment commands are:</p>

<ul>
<li>\raggedright -> causes text to be <strong>left aligned</strong></li>
<li>\raggedleft  -> causes text to be <strong>right aligned</strong></li>
</ul>

<p>Remember to double backslashes to escape them in R strings.  You may also need to disable the string sanitation function that <code>xtable</code> uses by default.</p>

<p><strong>Note</strong></p>

<p>This alignment technique <strong>will fail</strong> if used on the last column of a table <strong>unless</strong> table rows are ended with <code>\tabularnewline</code> instead of <code>\\</code>, which I think is not the case with <code>xtable</code> and is not easily customizable through any user-settable option.</p>

<p>The other thing to consider is that you may not want the entire column line-wrapped to 1.5 inches and centered- just the header.  In that case, disable <code>xtable</code> string sanitization and set your header using a <code>\multicolumn</code> cell of width 1:</p>

<pre><code>names(calqc_table)[1]&lt;-""\\multicolumn{1}{&gt;{\\centering}p{1.5in}}{Identifier of the Run within the Study}""
</code></pre>
"
2687539,160314,2010-04-22T00:50:01Z,2687212,10,FALSE,"<p>Well, I thought that this would be pretty easy with ggplot2, but now I realize that I have no idea how the confidence limits for stat_smooth/geom_smooth are calculated.</p>

<p>Consider the following:</p>

<pre><code>library(ggplot2)
pred &lt;- as.data.frame(predict(severity.lm,level=0.95,interval=""confidence""))
dat &lt;- data.frame(diseasesev,temperature, 
    in_interval = diseasesev &lt;=pred$upr &amp; diseasesev &gt;=pred$lwr ,pred)
ggplot(dat,aes(y=diseasesev,x=temperature)) +
stat_smooth(method='lm')  + geom_point(aes(colour=in_interval)) +
    geom_line(aes(y=lwr),colour=I('red')) + geom_line(aes(y=upr),colour=I('red'))
</code></pre>

<p>This produces:
<a href=""http://ifellows.ucsd.edu/pmwiki/uploads/Main/strangeplot.jpg"">alt text http://ifellows.ucsd.edu/pmwiki/uploads/Main/strangeplot.jpg</a></p>

<p>I don't understand why the confidence band calculated by stat_smooth is inconsistent with the band calculated directly from predict (i.e. the red lines). Can anyone shed some light on this?</p>

<p>Edit:</p>

<p>figured it out. ggplot2 uses 1.96 * standard error to draw the intervals for all smoothing methods. </p>

<pre><code>pred &lt;- as.data.frame(predict(severity.lm,se.fit=TRUE,
        level=0.95,interval=""confidence""))
dat &lt;- data.frame(diseasesev,temperature, 
    in_interval = diseasesev &lt;=pred$fit.upr &amp; diseasesev &gt;=pred$fit.lwr ,pred)
ggplot(dat,aes(y=diseasesev,x=temperature)) +
    stat_smooth(method='lm')  + 
    geom_point(aes(colour=in_interval)) +
    geom_line(aes(y=fit.lwr),colour=I('red')) + 
    geom_line(aes(y=fit.upr),colour=I('red')) +
    geom_line(aes(y=fit.fit-1.96*se.fit),colour=I('green')) + 
    geom_line(aes(y=fit.fit+1.96*se.fit),colour=I('green'))
</code></pre>
"
2687554,135870,2010-04-22T00:55:03Z,2687212,6,TRUE,"<p>The easiest way is probably to calculate a vector of <code>TRUE/FALSE</code> values that indicate if a data point is inside of the confidence interval or not.  I'm going to reshuffle your example a little bit so that all of the calculations are completed before the plotting commands are executed- this provides a clean separation in the program logic that could be exploited if you were to package some of this into a function.</p>

<p>The first part is pretty much the same, except I replaced the additional call to <code>lm()</code> inside <code>predict()</code> with the <code>severity.lm</code> variable- there is no need to use additional computing resources to recalculate the linear model when we already have it stored:</p>

<pre><code>## Dataset from 
#  apsnet.org/education/advancedplantpath/topics/
#    RModules/doc1/04_Linear_regression.html

## Disease severity as a function of temperature

# Response variable, disease severity
diseasesev&lt;-c(1.9,3.1,3.3,4.8,5.3,6.1,6.4,7.6,9.8,12.4)

# Predictor variable, (Centigrade)
temperature&lt;-c(2,1,5,5,20,20,23,10,30,25)

## For convenience, the data may be formatted into a dataframe
severity &lt;- as.data.frame(cbind(diseasesev,temperature))

## Fit a linear model for the data and summarize the output from function lm()
severity.lm &lt;- lm(diseasesev~temperature,data=severity)

## Get datapoints predicted by best fit line and confidence bands
## at every 0.01 interval
xRange=data.frame(temperature=seq(min(temperature),max(temperature),0.01))
pred4plot &lt;- predict(
  severity.lm,
  xRange,
  level=0.95,
  interval=""confidence""
)
</code></pre>

<p>Now, we'll calculate the confidence intervals for the origional data points and run a test to see if the points are inside the interval:</p>

<pre><code>modelConfInt &lt;- predict(
  severity.lm,
  level = 0.95,
  interval = ""confidence""
)

insideInterval &lt;- modelConfInt[,'lwr'] &lt; severity[['diseasesev']] &amp;
  severity[['diseasesev']] &lt; modelConfInt[,'upr']
</code></pre>

<p>Then we'll do the plot- first a the high-level plotting function <code>plot()</code>, as you used it in your example, but we will only plot the points inside the interval.  We will then follow up with the low-level function <code>points()</code> which will plot all the points outside the interval in a different color.  Finally, <code>matplot()</code> will be used to fill in the confidence intervals as you used it. However instead of calling <code>par(new=TRUE)</code> I prefer to pass the argument <code>add=TRUE</code> to high-level functions to make them act like low level functions.</p>

<p>Using <code>par(new=TRUE)</code> is like playing a dirty trick a plotting function- which can have unforeseen consequences.  The <code>add</code> argument is provided by many functions to cause them to add information to a plot rather than redraw it- I would recommend exploiting this argument whenever possible and fall back on <code>par()</code> manipulations as a last resort.</p>

<pre><code># Take a look at the data- those points inside the interval
plot(
  diseasesev~temperature,
  data=severity[ insideInterval,],
  xlab=""Temperature"",
  ylab=""% Disease Severity"",
  pch=16,
  pty=""s"",
  xlim=c(0,30),
  ylim=c(0,30)
)
title(main=""Graph of % Disease Severity vs Temperature"")

# Add points outside the interval, color differently
points(
  diseasesev~temperature,
  pch = 16,
  col = 'red',
  data = severity[ !insideInterval,]
)

# Add regression line and confidence intervals
matplot(
  xRange,
  pred4plot,
  lty=c(1,2,2),   #vector of line types and widths
  type=""l"",       #type of plot for each column of y
  add = TRUE
)
</code></pre>
"
2687628,135870,2010-04-22T01:13:29Z,2687547,6,TRUE,"<p>Well, the jaded cry of ""moar specifics plz!"" may win in this case:</p>

<p>Check the output of <code>dput()</code> and post if possible. <code>str()</code> just summarizes the contents of an object whilst <code>dput()</code> dumps out all the gory details in a form that may be copied and pasted into another R interpreter to regenerate the object.</p>
"
2687688,16632,2010-04-22T01:33:19Z,2687547,6,FALSE,"<p>Generally, in this situation it's useful to try <code>all.equal</code> which will give you some information about why two objects are not equivalent.</p>
"
2689219,172999,2010-04-22T08:22:00Z,2682144,0,FALSE,"<p>As far as I know, there's not a ready-to-use function like that.</p>
"
2690063,17523,2010-04-22T10:45:45Z,2682144,2,FALSE,"<p>Quick and dirty approximation to my needs:</p>

<pre><code>def pair(data, labels=None):
    """""" Generate something similar to R `pair` """"""

    nVariables = data.shape[1]
    if labels is None:
        labels = ['var%d'%i for i in range(nVariables)]
    fig = pl.figure()
    for i in range(nVariables):
        for j in range(nVariables):
            nSub = i * nVariables + j + 1
            ax = fig.add_subplot(nVariables, nVariables, nSub)
            if i == j:
                ax.hist(data[:,i])
                ax.set_title(labels[i])
            else:
                ax.plot(data[:,i], data[:,j], '.k')

    return fig
</code></pre>

<p>The code above is hereby released into the public domain</p>
"
2690080,279497,2010-04-22T10:48:38Z,2689900,0,FALSE,"<p>You can use Rterm (under C:\Program Files\R\R-2.10.1\bin in Windows and R version 2.10.1). Or you can start R from the shell typing ""R"" (if the shell does not recognize the command you need to modify your path).</p>
"
2690584,170792,2010-04-22T12:08:59Z,2687212,4,FALSE,"<p>I liked the idea and tried to make a function for that. Of course it's far from being perfect. Your comments are welcome </p>

<pre><code>diseasesev&lt;-c(1.9,3.1,3.3,4.8,5.3,6.1,6.4,7.6,9.8,12.4)
# Predictor variable, (Centigrade)
temperature&lt;-c(2,1,5,5,20,20,23,10,30,25)

## For convenience, the data may be formatted into a dataframe
severity &lt;- as.data.frame(cbind(diseasesev,temperature))

## Fit a linear model for the data and summarize the output from function lm()
severity.lm &lt;- lm(diseasesev~temperature,data=severity)

# Function to plot the linear regression and overlay the confidence intervals   
ci.lines&lt;-function(model,conf= .95 ,interval = ""confidence""){
  x &lt;- model[[12]][[2]]
  y &lt;- model[[12]][[1]]
  xm&lt;-mean(x)
  n&lt;-length(x)
  ssx&lt;- sum((x - mean(x))^2)
  s.t&lt;- qt(1-(1-conf)/2,(n-2))
  xv&lt;-seq(min(x),max(x),(max(x) - min(x))/100)
  yv&lt;- coef(model)[1]+coef(model)[2]*xv

  se &lt;- switch(interval,
        confidence = summary(model)[[6]] * sqrt(1/n+(xv-xm)^2/ssx),
        prediction = summary(model)[[6]] * sqrt(1+1/n+(xv-xm)^2/ssx)
              )
  # summary(model)[[6]] = 'sigma'

  ci&lt;-s.t*se
  uyv&lt;-yv+ci
  lyv&lt;-yv-ci
  limits1 &lt;- min(c(x,y))
  limits2 &lt;- max(c(x,y))

  predictions &lt;- predict(model, level = conf, interval = interval)

  insideCI &lt;- predictions[,'lwr'] &lt; y &amp; y &lt; predictions[,'upr']

  x_name &lt;- rownames(attr(model[[11]],""factors""))[2]
  y_name &lt;- rownames(attr(model[[11]],""factors""))[1]

  plot(x[insideCI],y[insideCI],
  pch=16,pty=""s"",xlim=c(limits1,limits2),ylim=c(limits1,limits2),
  xlab=x_name,
  ylab=y_name,
  main=paste(""Graph of "", y_name, "" vs "", x_name,sep=""""))

  abline(model)

  points(x[!insideCI],y[!insideCI], pch = 16, col = 'red')

  lines(xv,uyv,lty=2,col=3)
  lines(xv,lyv,lty=2,col=3)
}
</code></pre>

<p>Use it like this:</p>

<pre><code>ci.lines(severity.lm, conf= .95 , interval = ""confidence"")
ci.lines(severity.lm, conf= .85 , interval = ""prediction"")
</code></pre>
"
2690746,323235,2010-04-22T12:32:15Z,2686437,2,FALSE,"<p>Can you reproduce the error message when you feed rpart random data of similar dimensions, rather than your real data (from input.csv)? If not, it's probably a problem with your data (formatting perhaps?). After importing your data using read.csv, check the data for format issues by looking at the output from 
str(train).</p>

<pre><code>#How to do an equivalent rpart fit one some random data of equivalent dimension
dats&lt;-data.frame(matrix(rnorm(4000*14), nrow=4000))

y&lt;-dats[,1]
x&lt;-dats[,-1]
library(rpart)
system.time(fit&lt;-rpart(y~.,x))
</code></pre>
"
2692209,155151,2010-04-22T15:33:49Z,1401904,20,FALSE,"<p>The method suggested above will not completely work if you have packages that are not from CRAN.  For example, a personal package or a package downloaded from a non-CRAN site.</p>

<p>My preferred method on Windows (upgrading 2.10.1 to 2.11.0):</p>

<ol>
<li>Install R-2.11.0</li>
<li>Copy <code>R-2.10.0/library/*</code> to <code>R-2.11.0/library/</code></li>
<li>Answer ""no"" to the prompts asking you if it is okay to overwrite.</li>
<li>Start R 2.11.0 </li>
<li>Run the R command <code>update.packages()</code></li>
</ol>
"
2693059,143305,2010-04-22T17:30:36Z,2689900,7,FALSE,"<p>What you ask for cannot be done.  R is single threaded and has a single <a href=""http://en.wikipedia.org/wiki/Read-eval-print_loop"" rel=""noreferrer""><strong>REPL aka Read-eval-print loop</strong></a> which is, say, attached to a single input as e.g. the console in the GUI, or stdin if you pipe into R.  But <strong>never</strong> two.</p>

<p>Unless you use something else as e.g. the most excellent <a href=""http://www.rforge.net/Rserve/"" rel=""noreferrer""><strong>Rserve</strong></a> which (when hosted on an OS other than Windoze) can handle multiple concurrent requests over tcp/ip.  You may however have to write your custom connection.  Examples for Java, C++ and  R exist in the Rserve documentation.</p>
"
2694676,249487,2010-04-22T21:32:33Z,2686438,5,FALSE,"<p>Here is some information that may be helpful to people who are new to brew.</p>

<p>(I learned about brew today and used it to create a book document, with a chapter for each ""specialty"".)</p>

<p>Shane's link is helpful. Another link is <a href=""http://cran.r-project.org/web/packages/brew/index.html"" rel=""noreferrer"">Brew</a>. This has downloads and a short reference manual (seven pages).</p>

<p>In at least one way, brew is nicer than Sweave:</p>

<ul>
<li>In brew, the tags are simpler, being variations of &lt;%...%>.</li>
<li>In Sweave, the tags are &lt;&lt;...>>=...@ and \Sexpr{...}.</li>
</ul>

<p>If you want to try brew, do the following in R:</p>

<pre><code>install.packages(""brew"")
library(brew)
</code></pre>

<p>Save the following brew code in a file called <code>book.brew</code>. The code prints some digits of pi, with one digit per chapter. Note that there is one loop, and that parts of it are in Latex, and parts of it are in brew tags.</p>

<pre><code>   \documentclass{book}
    \title{A book}
    \begin{document}
    \maketitle
    &lt;%# This comment will not appear in the Latex file. %&gt;
    &lt;%
    digits = c(3, 1, 4, 1, 5, 9)
    for (i in 1:length(digits))
    {
    %&gt;
    \chapter{Digit $&lt;%= i %&gt;$}
    Digit $&lt;%= i %&gt;$ of $\pi$ is $&lt;%= digits[i] %&gt;$.
    &lt;%
    }
    %&gt;
    \end{document}
</code></pre>

<p>Note: when you save the file, make the last line be a blank line, or brew will give you a warning about an unfinished line.</p>

<p>In R, type</p>

<pre><code>brew(""/your/path/to/book.brew"", ""/where/you/want/brew/to/create/book.tex"")
</code></pre>

<p>Compile the Latex file book.tex.</p>
"
2696703,135870,2010-04-23T06:40:13Z,2696341,4,FALSE,"<p>Try the <code>doSNOW</code> parallel backend- it is supported out of the box on Windows.  Use it with a snow socket cluster.</p>
"
2696792,172261,2010-04-23T07:02:13Z,2696341,3,FALSE,"<p>You could try <code>doSMP</code> from <a href=""http://www.revolution-computing.com/"" rel=""nofollow noreferrer"">REvolution Computing</a>.
For more information, see this blog posting: <a href=""http://www.r-statistics.com/2010/04/parallel-multicore-processing-with-r-on-windows/"" rel=""nofollow noreferrer"">
Parallel Multicore Processing with R (on Windows)</a></p>
"
2697085,74658,2010-04-23T08:13:35Z,2684361,2,FALSE,"<p>Dirk's answer is almost perfect, but still doesn't let you put the answer half way through the document.  I got quite frustrated thinking it should work, but realised that the code I had was opening the time file at the start of each run (and emptying it) and writing the empty result into my document, then putting the answer in the time file at the end !</p>

<p>I eventually did something similar but using R to only open and write the file at the end, which worked great !;</p>

<pre><code>\documentclass[a4paper]{article} 
\usepackage[OT1]{fontenc} 
\usepackage{longtable} 
\usepackage{geometry} 
\usepackage{Sweave} 
\geometry{left=1.25in, right=1.25in, top=1in, bottom=1in} 
\begin{document} 

&lt;&lt;label=start, echo=FALSE, include=FALSE&gt;&gt;= 
startt&lt;-proc.time()[3] 
@  
Text and Sweave Code in here 
%  
This document was created on \today, with \Sexpr{print(version$version.string)} running 
 on a \Sexpr{print(version$platform)} platform. It took approx \input{time}
 sec to process. 

More text and Sweave code in here 
&lt;&lt;label=bye, include=FALSE, echo=FALSE&gt;&gt;=  
odbcCloseAll() 
endt&lt;-proc.time()[3] 
elapsedtime&lt;-as.numeric(endt-startt) 
@  
&lt;&lt;label=elapsed, include=FALSE, echo=FALSE&gt;&gt;=
fileConn&lt;-file(""time.tex"", ""wt"") 
writeLines(as.character(elapsedtime), fileConn) 
close(fileConn) 
@ 
\end{document}
</code></pre>
"
2699222,143305,2010-04-23T14:14:44Z,2696341,4,TRUE,"<p>For completeness, here is the requested answer to Tal's comment which provides a simple and portable alternative.  The answer consists of running</p>

<pre><code> &gt; library(snow)
 &gt; help(makeCluster)
</code></pre>

<p>and running the first three lines of code from the top of the Examples: section:</p>

<pre><code>&gt; cl &lt;- makeCluster(c(""localhost"",""localhost""), type = ""SOCK"")
&gt; clusterApply(cl, 1:2, get(""+""), 3)
[[1]]
[1] 4

[[2]]
[1] 5

&gt; stopCluster(cl)
&gt; .Platform$OS.type
[1] ""windows""
&gt; 
</code></pre>

<p>Was that <em>really</em> that hard?</p>

<p>Add-on packages like <a href=""http://cran.r-project.org/package=doSNOW"" rel=""nofollow noreferrer"">doSNOW</a> and thereafter 
<a href=""http://cran.r-project.org/package=foreach"" rel=""nofollow noreferrer"">foreach</a> can make use of this in a portable way.</p>
"
2702406,256662,2010-04-23T22:50:12Z,1401904,3,FALSE,"<p>Following Dirk's suggestion, here is some R code to do it on windows:  <a href=""http://www.r-statistics.com/2010/04/changing-your-r-upgrading-strategy-and-the-r-code-to-do-it-on-windows/"" rel=""nofollow noreferrer"">How to easily upgrade R on windows XP</a></p>

<p>Update (15.04.11): I wrote another post on the subject, explaining how to deal with common issues of <a href=""http://www.r-statistics.com/2011/04/how-to-upgrade-r-on-windows-7/"" rel=""nofollow noreferrer"">upgrading R on windows 7</a></p>
"
2703744,256662,2010-04-24T08:27:57Z,2703517,0,FALSE,"<p>1) consider moving to matrix instead of data.frame - to have faster results.</p>

<p>2) Coudl you supply with some simple code to explain what you want to achieve ?</p>
"
2704698,163053,2010-04-24T13:57:59Z,2703517,1,TRUE,"<p>Well, you need to map your column names to another value, so you have to store it somehow.  I would say that a named list would be a more appropriate data structure, although at the end of the day it doesn't make a big difference.</p>

<p>Here's some sample data:</p>

<pre><code>df &lt;- data.frame(a=1:5, b=2:6)
mapping &lt;- list(a=3, b=4)
</code></pre>

<p>Here's a simple example of using the list:</p>

<pre><code>for(i in 1:ncol(df)) df[,i] &lt;- df[,i] * mapping[[colnames(df)[i]]]
</code></pre>

<p>Regarding Tal's recommendation for using a matrix: that is true so long as every value in your data frame is of the same type.  If you have mixed types, then you need to stick with a data frame.</p>
"
2705580,160314,2010-04-24T18:08:34Z,2703517,1,FALSE,"<p>You can use R's lexical scoping to define a function <code>function_maker</code> that returns your desired function <code>func</code>. The code to create the mapping vector is only called when <code>function_maker</code> is called, not when <code>func</code> is. <code>mapping</code> is also owned by <code>func</code> in that other parts of your code can't alter it.</p>

<pre><code>dat &lt;- data.frame(a=c(1,2,3),b=c(3,2,0),c=c(5,6,4))

function_maker &lt;- function(){
    mapping &lt;- c(a=4,b=2,c=5)
    function(df){
        for(i in 1:ncol(df)) df[,i] &lt;- df[,i] * mapping[[colnames(df)[i]]]
        return(df)
    }
}

func &lt;- function_maker()

func(dat)
</code></pre>
"
2707628,74658,2010-04-25T08:32:03Z,2703517,1,FALSE,"<p>Why not include the second data frame as a parameter to your function call, and then check if it was given, if not, create it manually, this way the code can work for datasets that match what you do currently, but can be changed to match new datasets.</p>

<p>Something like (sorry I'm not at my PC, so this is untested)</p>

<pre><code>macroIndex &lt;- function(obj, index) {
  if(!exists(index)) {
    index &lt;- data.frame(# contents of the default data frame here )
  }
  a &lt;- c()
  b &lt;- names(obj)
  for (i in 2:length(obj)) {
      obj[i] &lt;- obj[i] * index[which(index==b[i]), 2]
  }
  return(obj)
}
</code></pre>
"
2710467,225468,2010-04-26T00:08:41Z,2679830,4,TRUE,"<p>Google QODBC. It allows you to access Quickbooks via ODBC.  </p>

<p>However, it works through creating an interface to XML files. For large QB files it can be painfully slow if you don't properly use indices and limit the data to exactly what is necessary. It's also very quirky. Lastly, not all tables are exposed (Quickbooks fault, not theirs) such as some payroll tables.</p>

<p>What I often ended up doing for large QB files is exporting the data at night via the QODBC driver into a sql database so that ad hoc analysis could be run quickly without compromising the performance for QB users during the day. </p>

<p>Good luck!</p>
"
2710510,325617,2010-04-26T00:22:56Z,2710442,21,TRUE,"<p>Probably a bunch of ways to do it. In my .emacs.d I have</p>

<pre><code>  (setq comint-prompt-read-only t)
  (setq comint-scroll-to-bottom-on-input t)
  (setq comint-scroll-to-bottom-on-output t)
  (setq comint-move-point-for-output t)
</code></pre>

<p>You might also be interested in <a href=""http://www.kieranhealy.org/blog/archives/2009/10/12/make-shift-enter-do-a-lot-in-ess/"" rel=""noreferrer"">this code</a>, originally from Felipe Csaszar, which lets you do what you ask and a few other nice things besides. </p>
"
2712470,37213,2010-04-26T09:54:39Z,2712421,7,FALSE,"<p>I would still recommend version control for a solo act like you because having a safety net to catch mistakes can be a great thing to have.</p>

<p>I've worked as a solo Java developer, and I still use source control.  If I'm checking things in continuously I can't lose more than an hour's work if something goes wrong.  I can experiment and refactor without worrying, because if it goes awry I can always roll back to my last working version.  </p>

<p>If that's the case for you, I'd recommend using source control.  It's not hard to learn.</p>
"
2712612,325866,2010-04-26T10:18:19Z,2712421,17,FALSE,"<p>I do economics research using R and LaTeX, and I always put my work under version control. It's like having unlimited undo. Try Bazaar, it's one of the simplest to learn and use, and if you're on Windows it has a graphical user interface (TortoiseBZR).</p>

<p>Yes, there are additional benefits to version control when working with others, but even on solo projects it makes a lot of sense.</p>
"
2712801,6309,2010-04-26T10:52:23Z,2712421,4,FALSE,"<p>A version Control for solo development (of any kind) is really interesting for:</p>

<ul>
<li>exploring the history and compare the current work with past commits</li>
<li><a href=""https://stackoverflow.com/questions/2100829#2107672"">branching</a> and trying different versions for a same set of files</li>
</ul>

<p>If you do not see yourself doing one of those two basic version control features, a simple backup tool might be all you need.<br>
If you do have the need for those features, then you will get backup as well (with <strong><a href=""https://stackoverflow.com/questions/2545765/how-can-i-email-someone-a-git-repository/2545784#2545784""><code>git bundle</code></a></strong> for instance)</p>
"
2713374,325913,2010-04-26T12:25:10Z,2665499,0,FALSE,"<p>Try</p>

<p><code>unloadNamespace('ca')</code></p>

<p>then close the 'Rcmdr' window.</p>

<p>You'll need to reattach the 'ca' package if you still need to use it.</p>
"
2713430,197321,2010-04-26T12:35:03Z,2712421,4,FALSE,"<p>I also do solo scripting work, and I find that it keeps things simpler, rather than makes them more complex. Backup is integrated into the coding workflow and doesn't require a separate set of file system procedures. The time it takes to learn the basics of any version control system would definitely be time well spent.</p>
"
2713505,95048,2010-04-26T12:48:23Z,2712421,7,FALSE,"<p>You have to use a version control software, otherwise your analysis won't be perfectly reproducible.</p>

<p>If you want to publish your results somewhere, you should always be able to reconstruct the status of your scripts at the moment you have produced them. Let's say that one of the reviewer discovers an error in one of your scripts: how would you know which results are effected and which are not? </p>

<p>In this sense, a backup system is not sufficient because it is probably done only once per day, and it doesn't apply labels to the different backups, so you don't know which versions correspond to which results. And learning a vcs is simpler than what you think, if learn how to add a file and how to commit changes it is already enough.</p>
"
2714055,121332,2010-04-26T14:04:00Z,2708994,3,TRUE,"<p>Use facet_grid instead of position=""dodge""</p>

<pre><code>  ggplot(exstatus, aes(x=art, fill=art))+
  geom_bar(aes(y=..count../sum(..count..))) + 
  facet_grid(~type,scales=""free"",space=""free"")
</code></pre>

<p><a href=""http://www.imagechicken.com/uploads/1272294360054813000.png"" rel=""nofollow noreferrer"">alt text http://www.imagechicken.com/uploads/1272294360054813000.png</a></p>
"
2714614,169947,2010-04-26T15:19:24Z,2712421,9,FALSE,"<p>Right now, you probably think of your work as developing code that will do what you want it to do.  After you adopt using a revision control system, you'll think of your work as writing down your legacy in the repository, and making brilliant incremental changes to it.  It feels way better.</p>
"
2714945,158065,2010-04-26T16:06:34Z,2714851,1,FALSE,"<p>Look at the <code>xtabs</code> method in the <code>Matrix</code> package which does sparse cross-tabulation.</p>
"
2715070,160314,2010-04-26T16:25:30Z,2714851,1,FALSE,"<pre><code>library(plyr)
ddply(foo, ~ x + y, nrow,.drop=FALSE)
</code></pre>
"
2715354,183828,2010-04-26T17:09:38Z,2712421,6,FALSE,"<blockquote>
  <p>Is version control worth the effort?</p>
</blockquote>

<p>a big YES.</p>

<blockquote>
  <p>What are the main pros and cons of adopting version control?</p>
</blockquote>

<p>pros: you can track what you have done before. Especially useful for latex, as you may need an old paragraph that was deleted by you! When you computer crashes or you work on a new one, you have your data back on the fly. </p>

<p>cons: you need to do some settings. </p>

<blockquote>
  <p>What is a good strategy for getting started with version control for data analysis with R (e.g., examples, workflow ideas, software, links to guides)?</p>
</blockquote>

<p>Just start to use it. I use tortoise SVN on windows as a client tool and my department has an svn server, I put all my code and data (yes, you also put your data there!) there. </p>
"
2715569,135870,2010-04-26T17:40:50Z,2712421,77,TRUE,"<p>I feel the answer to your question is a resounding yes- the benefits of managing your files with a version control system far outweigh the costs of implementing such a system.</p>

<p>I will try to respond in detail to some of the points you raised:</p>

<blockquote>
  <ul>
  <li><strong>Backup:</strong> I have a backup system already in place.</li>
  </ul>
</blockquote>

<p>Yes, and so do I.  However, there are some questions to consider regarding the appropriateness of relying on a general purpose backup system to adequately track important and active files relating to your work. On the performance side:</p>

<ul>
<li>At what interval does your backup system take snapshots? </li>
<li>How long does it take to build a snapshot?  </li>
<li>Does it have to image your entire hard drive when taking a snapshot, or could it be easily told to just back up two files that just received critical updates?</li>
<li>Can your backup system show you, with pinpoint accuracy, what changed in your text files from one backup to the next?</li>
</ul>

<p>And most importantly:</p>

<ul>
<li>How many locations are the backups saved in?  Are they in the same physical location as your computer?</li>
<li>How easy is it to restore a given version of a single file from your backup system?</li>
</ul>

<p>For example, have a Mac and use Time Machine to backup to another hard drive in my computer.  Time Machine is great for recovering the odd file or restoring my system if things get messed up.  However it simply doesn't have what it takes to be trusted with my important work:</p>

<ul>
<li><p>When backing up, Time Machine has to image the whole hard drive which takes a considerable amount of time.  If I continue working, there is no guarantee that my file will be captured in the state that it was when I initiated the backup.  I also may reach another point I would like to save before the first backup finishes.</p></li>
<li><p>The hard drive to which my Time Machine backups are saved is located in my machine- this makes my data vulnerable to theft, fire and other disasters.</p></li>
</ul>

<p>With a version control system like Git, I can initiate a backup of specific files with no more effort that requesting a save in a text editor- and the file is imaged and stored instantaneously.  Furthermore, Git is distributed so each computer that I work at has a full copy of the repository.</p>

<p>This amounts to having my work mirrored across four different computers- nothing short of an act of god could destroy my files and data, at which point I probably wouldn't care too much anyway.</p>

<blockquote>
  <ul>
  <li><strong>Forking and rewinding:</strong> I've never felt the need to do this, but I can see how it could be useful (e.g., you are preparing multiple journal articles based on the same dataset; you are preparing a report that is updated monthly, etc)</li>
  </ul>
</blockquote>

<p>As a soloist, I don't fork that much either.  However, the time I have saved by having the option to rewind has single-handedly paid back my investment in learning a version control system many, many times.  You say you have never felt the need to do this- but has rewinding any file under your current backup system really been a painless, feasible option?</p>

<p>Sometimes the report just looked better 45 minutes, an hour or two days ago.</p>

<blockquote>
  <ul>
  <li><strong>Collaboration:</strong> Most of the time I am
  analysing data myself, thus, I
  wouldn't get the collaboration
  benefits of version control.</li>
  </ul>
</blockquote>

<p>Yes, but you would learn a tool that may prove to be indispensable if you do end up collaborating with others on a project.</p>

<blockquote>
  <ul>
  <li>Time to evaluate and learn a version control system</li>
  </ul>
</blockquote>

<p>Don't worry too much about this. Version control systems are like programming languages- they have a few key concepts that need to be learned and the rest is just syntactic sugar.  Basically, the first version control system you learn will require investing the most time- switching to another one just requires learning how the new system expresses key concepts.</p>

<p>Pick a popular system and go for it!</p>

<blockquote>
  <ul>
  <li>A possible increase in complexity over my current file management system</li>
  </ul>
</blockquote>

<p>Do you have one folder, say <code>Projects</code> that contains all the folders and files related to your data analysis activities?  If so then slapping version control on it is going to increase the complexity of your file system by exactly <code>0</code>.  If your projects are strewn about your computer- then you should centralize them before applying version control and this will end up <strong>decreasing</strong> the complexity of managing your files- that's why we have a <code>Documents</code> folder after all.</p>

<blockquote>
  <ol>
  <li>Is version control worth the effort?</li>
  </ol>
</blockquote>

<p>Yes!  It gives you a huge undo button and allows you to easily transfer work from machine to machine without worrying about things like losing your USB drive.</p>

<blockquote>
  <p>2 What are the main pros and cons of adopting version control?</p>
</blockquote>

<p>The only con I can think of is a slight increase in file size- but modern version control systems can do absolutely amazing things with compression and selective saving so this is pretty much a moot point.</p>

<blockquote>
  <p>3 What is a good strategy for getting started with version control for data analysis with R (e.g., examples, workflow ideas, software, links to guides)?</p>
</blockquote>

<p>Keep files that generate data or reports under version control, be selective.  If you are using something like <code>Sweave</code>, store your <code>.Rnw</code> files and not the <code>.tex</code> files that get produced from them.  Store raw data if it would be a pain to re-acquire.  If possible, write and store a script that acquires your data and another that cleans or modifies it rather than storing changes to raw data.</p>

<p>As for learning a version control system, I highly recommend Git and <a href=""http://www-cs-students.stanford.edu/~blynn/gitmagic/"" rel=""noreferrer"">this guide</a> to it.</p>

<p>These websites also have some nice tips and tricks related to performing specific actions with Git:</p>

<ul>
<li><p><a href=""http://www.gitready.com/"" rel=""noreferrer"">http://www.gitready.com/</a></p></li>
<li><p><a href=""http://progit.org/blog.html"" rel=""noreferrer"">http://progit.org/blog.html</a></p></li>
</ul>
"
2715902,16632,2010-04-26T18:32:16Z,2714851,4,FALSE,"<p>I have this method for fast (sparse) cross tabulation. I think there are possibilities for further optimisation, but it's been good enough for me for large data sets.  The key is the use of <code>ninteraction</code> from the <code>plyr</code> package to quickly generate a numeric id for each row.</p>

<pre><code>tab &lt;- function(df, drop = TRUE) {
  id &lt;- plyr::ninteraction(df)
  ord &lt;- order(id)

  df &lt;- df[ord, , drop = FALSE]
  id &lt;- id[ord]

  freq &lt;- rle(id)$lengths
  labels &lt;- unrowname(df[cumsum(freq), , drop = FALSE])

  data.frame(labels, freq)
}
</code></pre>
"
2716544,74658,2010-04-26T20:14:51Z,2712421,5,FALSE,"<p>I'd agree with the sentiments above and say that, Yes, version control is usefull.  </p>

<p>Advantages;</p>

<ul>
<li>keep your research recorded as well as backed up, (tagging)</li>
<li>it lets you try different ideas out and go back if they don't work (branching)</li>
<li>You can share your work with other people, and they can share their changes to it with you (I know you didn't specify this, but it's great)</li>
<li>Most version control systems make it easy to create a compressed bundle fo all the files under control at a certain point, for instance at the point you submit an article for publication, this can help when others review your articles. (you can do this manually, but why make up these processes when version control just does it)</li>
</ul>

<p>In terms of toolsets, I use <a href=""http://git-scm.com/"" rel=""noreferrer"">Git</a>, along with <a href=""http://www.walware.de/goto/statet"" rel=""noreferrer"">StatEt</a> and <a href=""http://www.eclipse.org/"" rel=""noreferrer"">Eclipse</a> which works well, although you certainly don't have to use Eclipse.  There are a few <a href=""http://www.eclipse.org/egit/"" rel=""noreferrer"">Git plugins for Eclipse</a>, but I generally use the command line options.</p>
"
2716789,165384,2010-04-26T20:53:15Z,2712121,3,TRUE,"<p>You found a bug! Just fixed it. Be sure to download version 0.3-0.</p>

<p>Best,</p>

<p>Jeff</p>
"
2717853,163053,2010-04-27T00:04:41Z,2717757,27,TRUE,"<p>Renaming an object and the colnames within it is a two step process:</p>

<pre><code>SPY &lt;- GSPC # assign the object to the new name (creates a copy)
colnames(SPY) &lt;- gsub(""GSPC"", ""SPY"", colnames(SPY)) # rename the column names
</code></pre>

<p>Otherwise, the getSymbols function allows you to <em>not</em> auto assign, in which case you could skip the first step (you will still need to rename the columns).</p>

<pre><code>SPY &lt;- getSymbols(""^GSPC"", auto.assign=FALSE)
</code></pre>

<hr>

<p><em>Comment from @backlin</em></p>

<p>R employs so called <em>lazy evaluation</em>. An effect of that is that when you ""copy"" <code>SPY &lt;- GSPC</code> you do not actually allocate new space in the memory for <code>SPY</code>. R knows the objects are the identical and only makes a new copy in the memory if one of them is modified (<em>i.e.</em> when they are no longer the identical, <em>e.g.</em> when you change the column names on the following line). So by doing</p>

<pre><code>SPY &lt;- GSPC
rm(GSPC)
colnames(SPY) &lt;- gsub(""GSPC"", ""SPY"", colnames(SPY))
</code></pre>

<p>you never really copy <code>GSPC</code> but merely give it a new name (<code>SPY</code>) and then tell R to forget the first name (<code>GSPC</code>). When you then change the column names you do not need to create a new copy of <code>SPY</code> since <code>GSPC</code> no longer exists, meaning you have truly renamed the object without creating intermediate copies.</p>
"
2719476,314020,2010-04-27T07:38:09Z,2719295,4,TRUE,"<p>you can do by</p>

<pre><code>with(InsectSprays, stats:::reorder.default(spray, count, median))
</code></pre>

<p>note that stats::reorder is a 'dispatcher' so in your example</p>

<pre><code>with(InsectSprays, stats::reorder(spray, count, median))
</code></pre>

<p>finally Hmisc::reorder.factor is called instead of stats::reorder.default.</p>
"
2719784,295025,2010-04-27T08:38:10Z,2685127,6,FALSE,"<p>You could change the parameterization so that the constraints are always satisfied. Rewrite the likelihood as a a function of ln(CV1) and ln(CV2), that way you can be sure that CV1 and CV2 remain strictly positive. </p>

<pre><code>NLLdiff_2 = function(v1, lnCV1, v2, lnCV2, st1 = (czI01 - czV01), st2 = (czI02 - czV02), st01 = czI01, st02 = czI02, tt1 = czT01, tt2 = czT02) { 
prob1 = (1 + v1 * exp(lnCV1) * tt1)^(-1/exp(lnCV1))
prob2 = ( 1 + v2 * exp(lnCV2) * tt2)^(-1/exp(lnCV2)) 
-(sum(dbinom(st1, st01, prob1, log = T)) + sum(dbinom(st2, st02, prob2, log = T)))
 }
</code></pre>
"
2723115,163053,2010-04-27T16:41:32Z,2723034,57,TRUE,"<p>It isn't clear why you want to do this without <code>sink</code>, but you can wrap any commands in the <code>invisible()</code> function and it will suppress the output.  For instance:</p>

<pre><code>1:10 # prints output
invisible(1:10) # hides it
</code></pre>

<p>Otherwise, you can always combine things into one line with a semicolon and parentheses:</p>

<pre><code>{ sink(""/dev/null""); ....; sink(); }
</code></pre>
"
2723193,245603,2010-04-27T16:51:54Z,2723034,8,FALSE,"<p>R only automatically prints the output of unassigned expressions, so just assign the result of the <code>apply</code> to a variable, and it won't get printed. </p>
"
2723841,143305,2010-04-27T18:20:02Z,2723730,3,TRUE,"<p>It is a bug that was fixed yesterday.  As per <a href=""http://developer.r-project.org/blosxom.cgi/R-devel/2010/04/24#c2010-04-24"" rel=""nofollow noreferrer"">this Daily R Devel News entry post</a>:</p>

<blockquote>
  <p>2.11.0 patched NEW FEATURES (Windows)</p>

<pre><code>* The Rgui console, pagers and editor on the 64-bit build now have a 
  title bar icon.
</code></pre>
</blockquote>

<p>So upgrading to the '2.11.0 patched' build may help.</p>
"
2724317,135870,2010-04-27T19:22:15Z,2723034,36,FALSE,"<p>Use the <code>capture.output()</code> function. It works very much like a one-off <code>sink()</code> and unlike <code>invisible()</code>, it can suppress more than just print messages. Set the file argument to <code>/dev/null</code> on UNIX or <code>NUL</code> on windows.  For example, considering Dirk's note:</p>

<pre><code>&gt; invisible(cat(""Hi\n""))
Hi

&gt; capture.output( cat(""Hi\n""), file='NUL')
&gt; 
</code></pre>
"
2726881,180892,2010-04-28T04:49:41Z,1249548,9,FALSE,"<p>Stephen Turner <a href=""http://gettinggeneticsdone.blogspot.com/2010/03/arrange-multiple-ggplot2-plots-in-same.html"" rel=""noreferrer"">posted the <code>arrange()</code> function on <i>Getting Genetics Done</i> blog</a> (see post for application instructions); however <code>grid.arrange()</code> is recommended, see comment below)</p>

<pre><code>vp.layout &lt;- function(x, y) viewport(layout.pos.row=x, layout.pos.col=y)
arrange &lt;- function(..., nrow=NULL, ncol=NULL, as.table=FALSE) {
 dots &lt;- list(...)
 n &lt;- length(dots)
 if(is.null(nrow) &amp; is.null(ncol)) { nrow = floor(n/2) ; ncol = ceiling(n/nrow)}
 if(is.null(nrow)) { nrow = ceiling(n/ncol)}
 if(is.null(ncol)) { ncol = ceiling(n/nrow)}
        ## NOTE see n2mfrow in grDevices for possible alternative
grid.newpage()
pushViewport(viewport(layout=grid.layout(nrow,ncol) ) )
 ii.p &lt;- 1
 for(ii.row in seq(1, nrow)){
 ii.table.row &lt;- ii.row 
 if(as.table) {ii.table.row &lt;- nrow - ii.table.row + 1}
  for(ii.col in seq(1, ncol)){
   ii.table &lt;- ii.p
   if(ii.p &gt; n) break
   print(dots[[ii.table]], vp=vp.layout(ii.table.row, ii.col))
   ii.p &lt;- ii.p + 1
  }
 }
}
</code></pre>
"
2727857,325913,2010-04-28T08:32:36Z,2727446,3,FALSE,"<p>How bout...</p>

<pre><code>aperm(daply(mydata, c(""SNP1""), function(df) cov(cbind(df$X1, df$X2))),perm=c(2,3,1))
</code></pre>

<p>'aperm' is to arrays as 't' is to matrices. The perm argument specifies the way the dims should change.</p>
"
2729319,16632,2010-04-28T12:21:34Z,2727446,4,TRUE,"<p><code>daply</code> makes the splitting variable the <strong>first</strong> dimension in the array.  </p>

<pre><code>a &lt;- daply(mydata, c(""SNP1""), function(df) cov(cbind(df$X1, df$X2)))
l &lt;- dlply(mydata, c(""SNP1""), function(df) cov(cbind(df$X1, df$X2)))
</code></pre>

<p>This is so that <code>a[1, , ]</code> and <code>l[[1]]</code> correspond to the same output.  As wkmor1 suggests, you can use <code>aperm</code> to rearrange the dimensions, but I'd like to know more about why the initial form doesn't suit your needs.</p>
"
2730811,163053,2010-04-28T15:20:33Z,2730490,6,TRUE,"<p>This is being caused by the underscore in this statement:</p>

<pre><code>cat(paste(""Table_"",i,sep=""""))
</code></pre>

<p>If you change it to </p>

<pre><code>cat(paste(""Table "",i,sep=""""))
</code></pre>

<p>Or</p>

<pre><code>cat(paste(""Table\\textunderscore"",i,sep=""""))
</code></pre>

<p>It runs.  Did you want those numbers as subscripts?</p>
"
2731385,275455,2010-04-28T16:34:02Z,2731299,1,FALSE,"<p>I'm not sure how to do it directly, but you could simply skip the step of assigning names in the data.frame() command. Assuming you store the result of data.frame() in a variable named foo, you can simply do:<br></p>

<p>names(foo) &lt;- cols<br></p>

<p>after the data frame is created</p>
"
2731701,143377,2010-04-28T17:21:21Z,2731299,2,FALSE,"<p>Use can use <code>structure</code>:</p>

<pre><code>cols &lt;- c(""a"",""b"")

foo &lt;- structure(list(c(1, 2 ), c(3, 3)), .Names = cols, row.names = c(NA, -2L), class = ""data.frame"")
</code></pre>

<p>I don't get why you are doing this though! </p>
"
2732540,158065,2010-04-28T19:28:57Z,2731754,2,FALSE,"<p>Because the second expression interprets <code>c1 = 100</code> as saying the argument named <code>c1</code> of the function <code>substitute</code> should have value 100. </p>
"
2732806,168747,2010-04-28T20:14:46Z,2731299,1,FALSE,"<p>There is one trick. You could mess with lists:</p>

<pre><code>cols_dummy &lt;- setNames(rep(list(""All""), 3), cols)
</code></pre>

<p>Then if you use call to list with <strong>one</strong> paren then you should get what you want</p>

<pre><code>data.frame(cols_dummy[1], calc.means(dat, cols[2:3]))
</code></pre>

<p>You could use it on-the-fly as <code>setNames(list(""All""), cols[1])</code> but I think it's less elegant.</p>

<p>Example:</p>

<pre><code>some_names &lt;- list(name_A=""Dummy 1"", name_B=""Dummy 2"") # equivalent of cols_dummy from above
data.frame(var1=rnorm(3), some_names[1])
#        var1  name_A
# 1 -1.940169 Dummy 1
# 2 -0.787107 Dummy 1
# 3 -0.235160 Dummy 1
</code></pre>
"
2733112,168747,2010-04-28T21:01:06Z,2731754,2,TRUE,"<p>As I understand help to <code>assignOps</code> operator <code>=</code> evaluates immediately. So the second expression is equivalent to <code>substitute(100,list(c1=100))</code>.<br>
But you could take it in braces and the result is</p>

<pre><code>&gt; substitute({c1=100},list(c1=100))
{
    100 = 100
}
</code></pre>
"
2733512,325913,2010-04-28T22:23:31Z,2732397,5,FALSE,"<p>Here is a potential solution.</p>

<p>Assuming this is what the lines in your file look like</p>

<p><code>1$$$$$2$$$$$3$$$$$4</code></p>

<p>The following will create a matrix with the variables stored as characters.</p>

<pre><code>do.call(rbind,strsplit(readLines('test.txt'),'$$$$$',fixed=T))
</code></pre>
"
2736459,321622,2010-04-29T10:17:17Z,2735537,1,FALSE,"<p>the character and factor question is something only you can answer.  It depends if you need to use them later as factors or characters.  It also depends whether you need to clean them up at all afterwards.  For example, if you plan to apply a number of ifelse() modifications to a factor afterwards you might as well just read it in as a character now and turn it into a factor later.  Or, if you want to specifically code the factor in some way you will likely be better off reading it in as character.</p>

<p>As an aside, the reason you use read.delim over read.table is because of the default settings therefore don't bother setting the sep to the same as the default.</p>
"
2736726,163053,2010-04-29T11:02:56Z,2736631,2,FALSE,"<p>You need to convert your dates from characters into a <code>Date</code> type with <code>as.Date()</code> (or a POSIX type if you have more information like the time of day). Then you can make comparisons with standard <a href=""http://en.wikipedia.org/wiki/Relational_operator"" rel=""nofollow noreferrer"">relational operators</a> such as &lt;= and >=. </p>

<p>You should consider using a timeseries package such as <code>zoo</code> for this. </p>

<p><em>Edit</em>:</p>

<p>Just to respond to your comment, here's an example of using dates with your existing vector:</p>

<pre><code>&gt; as.Date(names(bar)) &lt; as.Date(""2001-10-14"")
[1]  TRUE FALSE FALSE
&gt; bar[as.Date(names(bar)) &lt; as.Date(""2001-10-14"")]
1997-10-14 
         1
</code></pre>

<p>Although you really should just use a time series package.  Here's how you could do this with <code>zoo</code> (or <code>xts</code>, <code>timeSeries</code>, <code>fts</code>, etc.):  </p>

<pre><code>library(zoo)
ts &lt;- zoo(c(1, 2, 1), as.Date(c(""1997-10-14"", ""2001-10-14"", ""2007-10-14"")))
ts[index(ts) &lt; as.Date(""2001-10-14""),]
</code></pre>

<p>Since the index is now a <code>Date</code> type, you can make as many comparisons as you want.  Read the <code>zoo</code> vignette for more information.  </p>
"
2737117,168747,2010-04-29T12:09:20Z,2736631,1,FALSE,"<p>Using fact that dates are in lexical order:</p>

<pre><code>bar[names(bar) &gt; ""1995-01-01"" &amp; names(bar) &lt; ""2000-06-01""]
# 1997-10-14 
#          1 

bar[names(bar) &gt; ""2001-09-01"" &amp; names(bar) &lt; ""2007-11-04""]
# 2001-10-14 2007-10-14 
#          2          1 
</code></pre>

<p>Result is named vector (as you original <code>bar</code>, it's not a list it's named vector).</p>

<p>As Dirk states in his answer it's better to use <code>Date</code> for efficiency reasons. Without external packages you could rearrange you data and create two vectors (or two-column <code>data.frame</code>) one for dates, one for values:</p>

<pre><code>bar_dates &lt;- as.Date(c(""1997-10-14"", ""2001-10-14"", ""2007-10-14""))
bar_values &lt;- c(1,2,1)
</code></pre>

<p>then use simple indexing:</p>

<pre><code>bar_values[bar_dates &gt; as.Date(""1995-01-01"") &amp; bar_dates &lt; as.Date(""2000-06-01"")]
# [1] 1

bar_values[bar_dates &gt; as.Date(""2001-09-01"") &amp; bar_dates &lt; as.Date(""2007-11-04"")]
# [1] 2 1
</code></pre>
"
2737241,143305,2010-04-29T12:30:30Z,2736631,4,TRUE,"<p>This problem has been solved for good with the <a href=""http://cran.r-project.org/packages=xts"" rel=""nofollow noreferrer"">xts</a> package which extends functionality from the <a href=""http://cran.r-project.org/packages=zoo"" rel=""nofollow noreferrer"">zoo</a> package.</p>

<pre><code>R&gt; library(xts)
Loading required package: zoo
R&gt; bar &lt;- xts(1:3, order.by=as.Date(""2001-01-01"")+365*0:2)
R&gt; bar
           [,1]
2001-01-01    1
2002-01-01    2
2003-01-01    3
R&gt; bar[""2002::""]        ## open range with a start year
           [,1]
2002-01-01    2
2003-01-01    3
R&gt; bar[""::2002""]        ## or end year
           [,1]
2001-01-01    1
2002-01-01    2
R&gt; bar[""2002-01-01""]    ## or hits a particular date
           [,1]
2002-01-01    2
R&gt; 
</code></pre>

<p>There is a lot more here -- but the basic point is do <strong>not</strong> operate on strings masquerading as dates. </p>

<p>Use a <code>Date</code> type, or preferably even an extension package built to efficiently index on millions of dates.</p>
"
2738316,134830,2010-04-29T14:52:02Z,2735537,4,TRUE,"<blockquote>
  <p>Would you suggest ""character"" or ""factor"" for varchar fields?</p>
</blockquote>

<p>As John mentioned, this depends upon usage.  It is simple to switch between the two, so don't worry too much about it.  If the column represents a categorical variable, it should eventually be considered as a <code>factor</code>.  If you intend on mining the text (e.g. comments fields), then <code>character</code> makes more sense.</p>

<blockquote>
  <p>Is it ok to use ""character"" for datetime ones?</p>
</blockquote>

<p>It's fine for storing the dates in a data frame, but if you want them to be treated correctly for analysis purposes, you'll have to convert it to <code>Date</code> or <code>POSIXct/POSIXlt</code> form.</p>

<blockquote>
  <p>What should I do in order to be able to read a numeric field like this 540912.68999999994 exactly as is and not as 540912.69?</p>
</blockquote>

<p>Values are read in to usual double accuracy (about 15 sig figs); in this particular example, 540912.69 is the best accuracy you can achieve.  Compare</p>

<pre><code>print(540912.68999999994)             # 540912.7
print(540912.68999999994, digits=22)  # 540912.69
print(540912.6899999994)              # 540912.7
print(540912.6899999994, digits=22)   # 540912.6899999994
</code></pre>

<hr>

<p>EDIT: If you need more precision for your numbers, use the <a href=""http://www.stats.bris.ac.uk/R/web/packages/Rmpfr/index.html"" rel=""nofollow noreferrer"">Rmpfr</a> package.</p>

<hr>

<blockquote>
  <p>I would like an -as automatic as possible- creation of that colClasses vector, depending on the datatypes defined in the relevant table's schema.</p>
</blockquote>

<p>The default for <code>colClasses</code> (when you don't specify it) does a pretty good job of guessing what columns should be.  If you are doing things like using <code>01</code> as a character, then there's no way round explicitly specifying it.</p>
"
2738578,6148,2010-04-29T15:26:40Z,2738322,6,TRUE,"<p>A number of folks have written solutions enabling you to use multiple major modes at once.  See the Emacs Wiki for <a href=""http://www.emacswiki.org/emacs/MultipleModes"" rel=""nofollow noreferrer"">Multiple Modes</a>.  I personally have no experience with them and cannot recommend one over another.</p>
"
2740044,143305,2010-04-29T19:06:48Z,2740004,4,TRUE,"<p>Maybe you can simply use </p>

<pre><code>&gt; options(warn=2)
</code></pre>

<p>in the setup code to turn warnings into errors?  </p>

<p>See <code>help(options)</code> for the paragraph on this option.</p>
"
2740695,269767,2010-04-29T20:51:29Z,1389123,3,FALSE,"<p>there is now! check the cran</p>
"
2743978,321622,2010-04-30T10:55:05Z,2743466,0,FALSE,"<p>If, for your purposes, it's OK to have all devices off before hand then checking .Devices would be fine because then plotting commands do make a new device.  But then lines(), and points() would be exceptions.</p>

<p>In fact, this suggests that the question doesn't just have a true or false answer but depends on conditions.  Some functions will draw something even if there is no open device while others will draw something if there is something else drawn.  What would you want to do in that case?</p>
"
2744434,16632,2010-04-30T12:24:23Z,2743466,20,TRUE,"<pre><code>makes_plot &lt;- function(x) {
  before &lt;- .Internal(getSnapshot())
  force(x)
  after &lt;- .Internal(getSnapshot())
  !identical(before, after)
}

makes_plot(mean(1:10))
makes_plot(plot(1:10))
</code></pre>

<p>The <code>.getSnapshot</code> function was discovered by looking at the source of <code>recordPlot()</code>.</p>
"
2744572,170792,2010-04-30T12:47:58Z,2743204,5,TRUE,"<p>I'm not claiming this to be the most elegant approach, but I think it is working  </p>

<pre><code>library(plyr)

ldply(sapply(1:length(walk.sample), function(i) 
           if (length(walk.sample[[i]]) &gt; 1)
           cbind(walk.sample[[i]],session=rep(i,nrow(walk.sample[[i]])))
      ),rbind)
</code></pre>

<p><strong>EDIT</strong></p>

<p>After applying Marek's apt remarks</p>

<pre><code>do.call(rbind,lapply(1:length(walk.sample), function(i)
           if (length(walk.sample[[i]]) &gt; 1)
           cbind(walk.sample[[i]],session=i)  ))
</code></pre>
"
2745511,158065,2010-04-30T15:09:16Z,2743204,6,FALSE,"<p>I think this will work...</p>

<pre><code>lengths &lt;- sapply(walk.sample, function(x) if (is.null(nrow(x))) 0 else nrow(x))
cbind(do.call(rbind, walk.sample[lengths &gt; 1]),
      session = rep(1:length(lengths), ifelse(lengths &gt; 1, lengths, 0)))
</code></pre>
"
2748573,314020,2010-05-01T01:53:33Z,2748381,4,TRUE,"<p>I don't know if the layer objects should store all the results of computation in future, but currently the fitting is invoked on drawing and  the model is not stored in the layer objects.
As a simple workaround, you can get the same result by</p>

<pre><code> dlply(data,.(series_id),function(x)lm(od~day,data=x))
</code></pre>
"
2748879,325913,2010-05-01T04:26:22Z,2748725,28,TRUE,"<p>The following packages all have a function to calculate a weighted median: 'aroma.light', 'isotone', 'limma', 'cwhmisc', 'ergm', 'laeken', 'matrixStats, 'PSCBS', and 'bigvis' (on github).</p>

<p>To find them I used the invaluable findFn() in the 'sos' package which is an extension for R's inbuilt help.</p>

<pre><code>findFn('weighted median')
</code></pre>

<p>Or,</p>

<p><code>???'weighted median'</code></p>

<p>as ??? is a shortcut in the same way <code>?some.function</code> is for <code>help(some.function)</code></p>
"
2749404,273125,2010-05-01T09:18:56Z,2749390,5,FALSE,"<p>I would highly recommend to use Emacs with <a href=""http://ess.r-project.org/"" rel=""noreferrer"">ESS (Emacs speaks Statistic)</a></p>
"
2749455,325913,2010-05-01T09:40:03Z,2749390,6,FALSE,"<pre><code>savehistory('file')
myhistory &lt;- scan('file','character')
</code></pre>

<p>Might do the trick.</p>
"
2749565,170792,2010-05-01T10:26:41Z,2749390,31,TRUE,"<pre><code>history(Inf)
</code></pre>

<p>opens a window similar to that in MATLAB. </p>
"
2751201,172261,2010-05-01T18:58:38Z,2751065,90,TRUE,"<p>You can modify <code>strip.text.x</code> (or <code>strip.text.y</code>) using <code>theme_text()</code>, for instance</p>

<pre><code>qplot(hwy, cty, data = mpg) + 
      facet_grid(. ~ manufacturer) + 
      opts(strip.text.x = theme_text(size = 8, colour = ""red"", angle = 90))
</code></pre>

<p><strong>Update:</strong> for ggplot2 version > 0.9.1</p>

<pre><code>qplot(hwy, cty, data = mpg) + 
      facet_grid(. ~ manufacturer) + 
      theme(strip.text.x = element_text(size = 8, colour = ""red"", angle = 90))
</code></pre>
"
2752541,330679,2010-05-02T05:33:59Z,2661402,8,FALSE,"<p>(Answer edited to add additional explanation)</p>

<ol>
<li><p>You can't really find ""the"" distribution; the actual distribution from which data are drawn can nearly always* be guaranteed not to be in any ""laundry list"" provided by any such software. At best you can find ""a"" distribution (more likely several), one that is an adequate description. Even if you find a great fit there are always an infinity of distributions that are arbitrarily close by. Real data tends to be drawn from heterogeneous mixtures of distributions that themselves don't necessarily have simple functional form.</p>

<p>* an example where you might hope to is where you know the data were actually <em>generated</em> from exactly one distribution on a list, but such situations are extremely rare.</p></li>
<li><p>I don't think just comparing likelihoods is necessarily going to make sense, since some distributions have more parameters than others. AIC might make more sense, except that ...</p></li>
<li><p>Attempting to identify a ""best fitting"" distribution from a list of candidates will tend to produce overfitting, and unless the effect of such model selection is accounted for properly will lead to overconfidence (a model that looks great but doesn't actually fit the data not in your sample). There are such possibilities in R (the package <code>fitdistrplus</code> comes to mind), but as a common practice I would advise against the idea. If you must do it, use holdout samples or cross-validation to obtain models with better generalization error.</p></li>
</ol>
"
2752551,225468,2010-05-02T05:39:23Z,2749390,4,FALSE,"<p>Eclipse with Stat-ET plugin will get you command history window and object browser.</p>
"
2754685,123600,2010-05-02T19:33:51Z,2754469,6,FALSE,"<p><em>Argh</em>, you found the solution whilst I was writing it up for you. Here's a simple example that I came up with:</p>

<pre><code>run = function()
{
    # The probability transition matrix
    trans = matrix(c(1/3,1/3,1/3,
                0,2/3,1/3,
                2/3,0,1/3), ncol=3, byrow=TRUE);

    # The state that we're starting in
    state = ceiling(3 * runif(1, 0, 1));
    cat(""Starting state:"", state, ""\n"");

    # Make twenty steps through the markov chain
    for (i in 1:20)
    {
        p = 0;
        u = runif(1, 0, 1);

        cat(""&gt; Dist:"", paste(round(c(trans[state,]), 2)), ""\n"");
        cat(""&gt; Prob:"", u, ""\n"");

        newState = state;
        for (j in 1:ncol(trans))
        {
            p = p + trans[state, j];
            if (p &gt;= u)
            {
                newState = j;
                break;
            }
        }

        cat(""*"", state, ""-&gt;"", newState, ""\n"");
        state = newState;
    }
}

run();
</code></pre>

<p>Note that your probability transition matrix doesn't sum to 1 in each row, which it should do. My example has a slightly altered probability transition matrix which adheres to this rule.</p>
"
2755440,142879,2010-05-02T23:29:35Z,2754658,1,FALSE,"<p>I'm not entirely sure what you're aiming for. Do you want a line or bars. You should check out <a href=""http://docs.ggplot2.org/current/geom_bar.html"" rel=""nofollow noreferrer"">geom_bar</a> for filled bars. Something like:</p>

<pre><code>p &lt;- ggplot(data, aes(x = time, y = count))
p + geom_bar(stat = ""identity"")
</code></pre>

<p>If you want a line filled in underneath then you should look at <a href=""http://docs.ggplot2.org/current/geom_ribbon.html"" rel=""nofollow noreferrer"">geom_area</a> which I haven't personally used but it appears the construct will be almost the same.</p>

<pre><code>p &lt;- ggplot(data, aes(x = time, y = count))
p + geom_area()
</code></pre>

<p>Hope that helps. Give some more info and we can probably be more helpful.</p>

<p>Actually i would throw on an index, just the row of the data and use that as x, and then use</p>

<pre><code>p &lt;- ggplot(data, aes(x = index, y = count))
p + geom_bar(stat = ""identity"") + scale_x_continuous(""Intervals"", 
breaks = index, labels = intervals)
</code></pre>
"
2755744,16632,2010-05-03T01:34:52Z,2754658,1,FALSE,"<p><code>ggplot(quake.data, aes(interval, fill=tweet.type, group = 1)) + geom_density()</code></p>

<p>But I don't think this is a meaningful graphic.</p>
"
2756073,206328,2010-05-03T04:14:42Z,2751035,1,TRUE,"<p>That is still a very large matrix, you would need a parallel computer :-). 300000x300000 > 2^32 (int is likely to be 32 bits, even on 64-bit machine) .</p>

<p>See also this <a href=""http://tech.slashdot.org/story/10/05/02/2038255/MATLAB-Cant-Manipulate-64-Bit-Integers"" rel=""nofollow noreferrer"">link</a></p>
"
2757001,314020,2010-05-03T08:56:31Z,2756929,9,TRUE,"<p>probably walk.df is a subset of the factor variable with 3 levels.
say,</p>

<pre><code>a&lt;-factor(1:3)
b&lt;-a[1:2]
</code></pre>

<p>then b has 3 levels.</p>

<p>A easy way to drop extra level is:</p>

<pre><code>b&lt;-a[1:2, drop=T]
</code></pre>

<p>or if you cannot access the original variable,</p>

<pre><code>b&lt;-factor(b)
</code></pre>
"
2757253,212593,2010-05-03T09:56:23Z,2756886,3,FALSE,"<p>Don't know with <code>ggplot</code>, but with base graphics you can use <code>identify</code>:</p>

<pre><code>plot(length,coverage,type='p')
identify(length,coverage)
</code></pre>

<p>Now you can use your mouse to click on points and R will show which observation they correspond to. Clicking a mouse button other than the first ends the process and <code>identify</code> returns the observation numbers as its value.</p>
"
2757773,143305,2010-05-03T11:45:49Z,2757657,0,FALSE,"<p>In principle, why not?  It is the same as for other platforms:</p>

<ul>
<li>GL hardware support from your card and drivers is required</li>
<li>proper libraries have to be available</li>
</ul>

<p>You just pointed to the code archive at R-Forge which does not yet do win64 builds.  It may be better to </p>

<ul>
<li>check Brian Ripley's site and / or </li>
<li>check with the package maintainers</li>
</ul>

<p>and that last step is generally the recommended method anyway. </p>
"
2758343,331444,2010-05-03T13:27:55Z,2757657,0,TRUE,"<p>It definitely is. R 2.11.0 just came out and includes support for 64-bit builds on Windows. There is at least one hard-to-install packages that are otherwise supported on 32-bit AFAIK, but rgl installs simply with install.packages. </p>

<p>The R for Windows download page on CRAN includes a link to the 64-bit build, in 2.12.0 this is planned to be integrated with a single installer. </p>
"
2758429,143591,2010-05-03T13:40:35Z,2756886,7,TRUE,"<p>The grid analogue (the ggplot2 package as well as the Lattice package are based on grid graphics) of locator() is grid.locator().</p>

<p>Thanks to <a href=""http://www.amazon.co.uk/Lattice-Multivariate-Data-Visualization-Use/dp/0387759689/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1272893925&amp;sr=8-1"" rel=""noreferrer"">Deepayan Sarkar Lattice Book</a> !</p>
"
2759000,245603,2010-05-03T15:05:30Z,2758559,1,TRUE,"<p>In addition to the margins, you need to eliminate axes and the space for them, and turn off the auto-extending of the axis limits:</p>

<pre><code>par(xaxs=""i"", yaxs=""i"")  # 'internal' axis style - no extending
par(xaxt=""n"", yaxt=""n"")  # remove axes
par(mgp=c(0,0,0))        # remove room for title and axis labels
par(mar=c(0,0,0,0))      # remove margins
symbols(0,0, circles=1, bg=2, fg=NA, inches=FALSE, bty=""n"", 
        xlim=c(-1,1), ylim=c(-1,1)) #ensure limits match the size of the circle
</code></pre>

<p>The <code>fg=NA</code> part removes the foreground of the symbol which is the border of the circle. Hopefully this looks more like what you had in mind.</p>
"
2759202,16632,2010-05-03T15:37:15Z,2754658,8,TRUE,"<p>Perhaps you want:</p>

<pre><code>geom_area(aes(y = ..count..), stat = ""bin"")
</code></pre>
"
2759435,169947,2010-05-03T16:11:30Z,2749390,3,FALSE,"<p>The OS X GUI for R has a very nice command history mechanism built in.  It works well with multi-line commands (e.g. long function definitions), and you can browse through the history in a pane on the side of the window.</p>
"
2759436,269476,2010-05-03T16:11:36Z,2759394,25,TRUE,"<pre><code>subset(data,!duplicated(data$ID))
</code></pre>

<p>Should do the trick</p>
"
2759451,163053,2010-05-03T16:13:05Z,2759394,3,FALSE,"<p>If you want to keep one row for each ID, but there is different data on each row, then you need to decide on some logic to discard the additional rows.  For instance:</p>

<pre><code>df &lt;- data.frame(ID=c(1, 2, 2, 3), time=1:4, OS=""Linux"")
df
  ID time    OS
1  1    1 Linux
2  2    2 Linux
3  2    3 Linux
4  3    4 Linux
</code></pre>

<p>Now I will keep the maximum time value and the last OS value:</p>

<pre><code>library(plyr)
unique(ddply(df, .(ID), function(x) data.frame(ID=x[,""ID""], time=max(x$time), OS=tail(x$OS,1))))
  ID time    OS
1  1    1 Linux
2  2    3 Linux
4  3    4 Linux
</code></pre>
"
2759668,118402,2010-05-03T16:49:11Z,2755716,3,TRUE,"<p>This looks like the exactly type of model you could easily code up in <a href=""http://www.mrc-bsu.cam.ac.uk/bugs/"" rel=""nofollow noreferrer"">Bugs</a> or <a href=""http://www-fis.iarc.fr/~martyn/software/jags/"" rel=""nofollow noreferrer"">Jags</a>.  Bugs/Jags is probably the most flexible approach to estimating custom models in R.  You can easily move between R and Jags using <a href=""http://cran.r-project.org/web/packages/R2jags/index.html"" rel=""nofollow noreferrer"">R2Jags</a>.</p>

<p>If you are new to Bayesian models, it may take a bit to get up to speed.</p>
"
2760731,143305,2010-05-03T19:43:43Z,2760702,2,FALSE,"<p>You probably want to see what the <a href=""http://cran.r-project.org/web/views/Spatial.html"" rel=""nofollow noreferrer""><strong>CRAN Task View for Statial Data Analysis</strong></a> has to offer -- there are a number of suitable packages.</p>
"
2760769,457898,2010-05-03T19:50:21Z,2756929,0,FALSE,"<p>You can assign several factor levels to a factor that contains two levels:</p>

<pre><code> &gt; set.seed(1234)
 &gt; x &lt;- round(runif(10, 1, 2))
 &gt; x
  [1] 1 2 2 2 2 2 1 1 2 2
 &gt; y &lt;- factor(x)
 &gt; levels(y)
 [1] ""1"" ""2""
 &gt; levels(y) &lt;- c(""1"", ""2"", ""3"")
 &gt; y
  [1] 1 2 2 2 2 2 1 1 2 2
 Levels: 1 2 3
</code></pre>

<p>or even no levels at all:</p>

<pre><code> &gt; p &lt;- NA
 &gt; q &lt;- factor(p)
 &gt; levels(q)
 character(0)
 &gt; levels(q) &lt;- c(""1"", ""2"", ""3"")
 &gt; q
 [1] &lt;NA&gt;
 Levels: 1 2 3
</code></pre>
"
2761022,170792,2010-05-03T20:33:33Z,2760702,4,FALSE,"<p>You can get the distance matrix using the function <a href=""http://finzi.psych.upenn.edu/R/library/stats/html/dist.html"" rel=""nofollow noreferrer"">dist</a>. This function computes the distances between the <strong>rows</strong> of a data matrix, so I transposed your <em>points</em> array</p>

<pre><code>dist(t(points),method = ""euclidean"")
</code></pre>

<p>Another similar function to compute the distance matrix is <a href=""http://finzi.psych.upenn.edu/R/library/amap/html/dist.html"" rel=""nofollow noreferrer"">Dist</a> from package <code>amap</code>, which provides even more distance measures : (""euclidean"", ""maximum"", ""manhattan"", ""canberra"", ""binary"", ""pearson"", ""correlation"", ""spearman"", ""kendall"")</p>
"
2761134,172261,2010-05-03T20:55:24Z,2760898,16,TRUE,"<pre><code>p &lt;- ggplot(data=DATA, aes(x=TIME, y=Y, group=ID)) +
            geom_line(aes(colour=GROUP)) +
            geom_smooth(aes(group=GROUP))
</code></pre>

<p><a href=""http://img143.imageshack.us/img143/7678/geomsmooth.png"">geom_smooth plot http://img143.imageshack.us/img143/7678/geomsmooth.png</a></p>
"
2761469,330679,2010-05-03T21:51:35Z,2760702,1,FALSE,"<p>I'd suggest working with your matrix transposed, or you'll probably end up calling the function t() more than you otherwise would.</p>

<p>Aside from that, this is probably the data structure you want. You could do it with a data frame of course, but I think you're better off not doing so in this situation.</p>
"
2761565,16632,2010-05-03T22:11:41Z,2760702,4,TRUE,"<p>Calculating the Euclidean distance between two sets of points stored like this is easy:</p>

<pre><code>sqrt(colSums((points1 - points2)^2))
</code></pre>

<p>Although I'd second the recommendation to store dimensions in the columns.  In that case the code becomes:</p>

<pre><code>sqrt(rowSums((points1 - points2)^2))
</code></pre>
"
2766019,168747,2010-05-04T14:13:41Z,2765374,7,TRUE,"<p>You could use <code>cumsum</code> to compute necessary sums from direct formulas for variance/sd to vectorized operations on matrix:</p>

<pre><code>cumsd_mod &lt;- function(mat) {
    cum_var &lt;- function(x) {
        ind_na &lt;- !is.na(x)
        nn &lt;- cumsum(ind_na)
        x[!ind_na] &lt;- 0
        cumsum(x^2) / (nn-1) - (cumsum(x))^2/(nn-1)/nn
    }
    v &lt;- sqrt(apply(mat,2,cum_var))
    v[is.na(mat) | is.infinite(v)] &lt;- NA
    v
}
</code></pre>

<p>just for comparison:</p>

<pre><code>set.seed(2765374)
X &lt;- matrix(rnorm(1000),100,10)
X[cbind(1:10,1:10)] &lt;- NA # to have some NA's

all.equal(cumsd(X),cumsd_mod(X))
# [1] TRUE
</code></pre>

<p>And about timing:</p>

<pre><code>X &lt;- matrix(rnorm(100000),1000,100)
system.time(cumsd(X))
# user  system elapsed 
# 7.94    0.00    7.97 
system.time(cumsd_mod(X))
# user  system elapsed 
# 0.03    0.00    0.03 
</code></pre>
"
2766577,170792,2010-05-04T15:19:34Z,2765374,1,FALSE,"<p>Another try (Marek's is faster)</p>

<pre><code>cumsd2 &lt;- function(y) {
n &lt;- nrow(y)
apply(y,2,function(i) {
    Xmeans &lt;- lapply(1:n,function(z) rep(sum(i[1:z])/z,z))
    Xs &lt;- sapply(1:n, function(z) i[1:z])
    sapply(2:n,function(z) sqrt(sum((Xs[[z]]-Xmeans[[z]])^2,na.rm = T)/(z-1)))
})
}
</code></pre>
"
2767248,163053,2010-05-04T16:51:18Z,2767219,1,FALSE,"<p>The replace function expects a vector and you're supplying a data.frame.  </p>

<p>You should really try to use <code>NA</code> and <code>NULL</code> instead of the character values that you're currently using.  Otherwise you won't be able to take advantage of all of R's functionality to handle missing values.  </p>

<p><em>Edit</em></p>

<p>You could use an apply function, or do something like this:</p>

<pre><code>foo &lt;- data.frame(day= c(1, 3, 5, 7), od = c(0.1, NA, 0.4, 0.8))
idx &lt;- which(is.na(foo), arr.ind=TRUE)
foo[idx[1], idx[2]] &lt;- ""NULL""
</code></pre>

<p>You cannot assign a real <code>NULL</code> value in this case, because it has length zero.  It is important to understand the difference between <code>NA</code> and <code>NULL</code>, so I recommend that you read <code>?NA</code> and <code>?NULL</code>.</p>
"
2767490,331444,2010-05-04T17:26:16Z,2767219,19,TRUE,"<p>NULL really means ""nothing"", not ""missing"" so it cannot take the place of an actual value - for missing R uses NA. </p>

<p>You can use the replacement method of is.na to directly update the selected elements, this will work with a logical result. (Using which for indices will only work with is.na, direct use of [ invokes list access, which is the cause of your error). </p>

<pre><code>foo &lt;- data.frame(""day""= c(1, 3, 5, 7), ""od"" = c(0.1, ""#N/A"", 0.4, 0.8)) 
NAs &lt;- foo == ""#N/A""

## by replace method
is.na(foo)[NAs] &lt;- TRUE

 ## or directly
 foo[NAs] &lt;- NA
</code></pre>

<p>But, you are already dealing with strings (actually a factor by default) in your od column by forced coercion when it was created with c(), and you might need to treat columns individually. Any numeric column will never have a match on the string ""#N/A"", for example.</p>
"
2769618,163053,2010-05-04T23:21:43Z,2769510,9,FALSE,"<p>You could create this as a separate operator or overwrite the original >= function (probably not a good idea) if you want to use this approach frequently:</p>

<pre><code># using a tolerance
epsilon &lt;- 1e-10 # set this as a global setting
`%&gt;=%` &lt;- function(x, y) (x + epsilon &gt; y)

# as a new operator with the original approach
`%&gt;=%` &lt;- function(x, y) (all.equal(x, y)==TRUE | (x &gt; y))

# overwriting R's version (not advised)
`&gt;=` &lt;- function(x, y) (isTRUE(all.equal(x, y)) | (x &gt; y))

&gt; (a-b) &gt;= 0.5
[1] TRUE
&gt; c(1,3,5) &gt;= 2:4
[1] FALSE FALSE  TRUE
</code></pre>
"
2769784,123600,2010-05-05T00:12:42Z,2769510,5,FALSE,"<p>For completeness' sake, I'll point out that, in certain situations, you could simply round to a few decimal places (and this is kind of a lame solution by comparison to the better solution previously posted.)</p>

<pre><code>round(0.58 - 0.08, 2) == 0.5
</code></pre>
"
2769822,321622,2010-05-05T00:25:11Z,2769510,26,TRUE,"<p>I've never been a fan of <code>all.equal</code> for such things.  It seems to me the tolerance works in mysterious ways sometimes.  Why not just check for something greater than a tolerance less than 0.05</p>

<pre><code>tol = 1e-5

(a-b) &gt;= (0.05-tol)
</code></pre>

<p>In general, without rounding and with just conventional logic I find straight logic better than all.equal</p>

<p>If <code>x == y</code> then <code>x-y == 0</code>.  Perhaps <code>x-y</code> is not exactly 0 so for such cases I use</p>

<pre><code>abs(x-y) &lt;= tol
</code></pre>

<p>You have to set tolerance anyway for <code>all.equal</code> and this is more compact and straightforward than <code>all.equal</code>.</p>
"
2769830,144157,2010-05-05T00:25:49Z,2769510,3,FALSE,"<p>Choose some tolerance level:</p>

<pre><code>epsilon &lt;- 1e-10
</code></pre>

<p>Then use</p>

<pre><code>(a-b+epsilon) &gt;= 0.5
</code></pre>
"
2770698,325913,2010-05-05T04:52:46Z,2764760,3,FALSE,"<p>You can store any R code as an expression with 'expression()' and then evaluate it with 'eval()'.
e.g.</p>

<pre><code>p &lt;- expression(qplot(data = mtcars, x = factor(cyl), geom = ""bar"", fill = factor(cyl)) + 
     scale_fill_manual(name = ""Cylinders"", value = c(""firebrick3"", ""gold2"", ""chartreuse3"")) + 
     stat_bin(aes(label = ..count..), vjust = -0.2, geom = ""text"", position = ""identity"") + 
     xlab(""# of cylinders"") + ylab(""Frequency"") + 
     opts(title = ""Barplot: # of cylinders""))
</code></pre>

<p>then</p>

<p><code>eval(p)</code></p>

<p>will produce the plot but the original code is still stored in the variable 'p' as an expression.</p>

<p>so</p>

<p><code>p</code></p>

<p>produces</p>

<pre><code>expression(qplot(data = mtcars, x = factor(cyl), geom = ""bar"", 
    fill = factor(cyl)) + scale_fill_manual(name = ""Cylinders"", 
    value = c(""firebrick3"", ""gold2"", ""chartreuse3"")) + stat_bin(aes(label = ..count..), 
    vjust = -0.2, geom = ""text"", position = ""identity"") + xlab(""# of cylinders"") + 
    ylab(""Frequency"") + opts(title = ""Barplot: # of cylinders""))
</code></pre>

<p>which is what we started with.</p>

<p>'eval()' can also evaluate a character string as an expression if parsed as text with parse(), e.g.</p>

<p><code>eval(parse(text='f(arg=value)')</code></p>
"
2773211,16632,2010-05-05T12:40:27Z,2768300,0,FALSE,"<p>Do you have any perceptual justification for this change? If the weights are going to vary by star it's going to be very hard to interpret the plot.</p>

<p>(But it should be trivial to implement - instead of using equally distributed angles, use weights: <code>angles &lt;- weights / sum(weights) * 2 * pi</code>)</p>
"
2773704,16632,2010-05-05T13:42:34Z,2764760,3,TRUE,"<p>It's not currently possible to go from a ggplot2 object to the code that (might have) created it.  </p>
"
2773741,256662,2010-05-05T13:47:30Z,2768300,0,TRUE,"<p>I found out how to do it.</p>

<p>For future reference, here is the code:</p>

<pre><code># functions we'll need...
add.num.before.and.after &lt;- function(vec, num = NULL)
{
    # this will add a number before and after every number in a vector.
    # the deafult adds the number which is one more then the length of the vector 
        # assuming that later we will add a zero column to a data.frame and will use that column to add the zero columns...
    if(is.null(num)) num &lt;- rep(length(vec) +1, length(vec))
    if(length(num)==1) num &lt;- rep(num, length(vec))

    #x &lt;- as.list(vec)
    list.num.x.num &lt;- sapply(seq_along(vec) , function(i) c(num[i], vec[i], num[i]),  simplify = F)
    num.x.num &lt;- unlist(list.num.x.num)

    return(num.x.num)
}

add.0.columns.to.DF &lt;- function(DF, zero.column.name = "" "")
{
    # this function gets a data frame
    # and returns a data.frame with extra two columns (of zeros) before and after every column

    zero.column &lt;- rep(0, dim(DF)[1])   # the column of zeros
    column.seq &lt;- seq_len(dim(DF)[2])   # the column ID for the original data.frame

    DF.new.order &lt;- add.num.before.and.after(column.seq)    # add the last column id before and after every element in the column id vector

    DF.and.zero &lt;- cbind(DF, zero.column)   # making a new data.frame with a zero column at the end

    new.DF &lt;- DF.and.zero[,DF.new.order]    # moving the zero column (and replicating it) before and after every column in the data.frame

    # renaming the zero columns to be "" ""
    columns.to.erase.names &lt;- ! (colnames(new.DF) %in% colnames(DF))        
    colnames(new.DF)[columns.to.erase.names] &lt;- zero.column.name

    return(new.DF)
}


angles.by.weight &lt;-  function(angles,  weights = NULL)
{

    angles &lt;- angles[-1]    # remove the 0 from ""angles""
    angles &lt;- c(angles, 2*pi) # add last slice angle
    number.of.slices = length(angles)
    if(is.null(weights)) weights &lt;- rep(.6, number.of.slices)   # Just for the example

    slice.angle &lt;- diff(angles)[1]

    #new.angles &lt;- rep(0, 3*length(angles))
    new.angles &lt;- numeric()

    for(i in seq_along(angles))
    {
        weighted.slice.angle &lt;- slice.angle*weights[i]
        half.leftover.weighted.slice.angle &lt;- slice.angle* ((1-weights[i])/2)

        angle1 &lt;- angles[i] - (weighted.slice.angle + half.leftover.weighted.slice.angle)
        angle2 &lt;- angles[i] - half.leftover.weighted.slice.angle
        angle3 &lt;- angles[i]

        new.angles &lt;- c(new.angles,
                        angle1,angle2,angle3)                       
    }

    new.angles.length &lt;- length(new.angles)
    new.angles &lt;- c(0, new.angles[-new.angles.length])

    return(new.angles)
}

# The updated stars function
stars2 &lt;-
    function (x, full = TRUE, scale = TRUE, radius = TRUE, labels =
            dimnames(x)[[1L]], 
                locations = NULL, nrow = NULL, ncol = NULL, len = 1, key.loc = NULL, 
                key.labels = dimnames(x)[[2L]], key.xpd = TRUE, xlim = NULL, 
                ylim = NULL, flip.labels = NULL, draw.segments = FALSE, col.segments = 1L:n.seg, 
                col.stars = NA, axes = FALSE, frame.plot = axes, main = NULL, 
                sub = NULL, xlab = """", ylab = """", cex = 0.8, lwd = 0.25, 
                lty = par(""lty""), xpd = FALSE, mar = pmin(par(""mar""), 1.1 + 
                    c(2 * axes + (xlab != """"), 2 * axes + (ylab != """"), 1, 
            #            0)), add = FALSE, plot = TRUE, ...) 
                        0)), add = FALSE, plot = TRUE, col.radius = NA, polygon = TRUE, 
                        key.len = len,
                        segment.weights = NULL, 
                        ...)
{
    if (is.data.frame(x)) 
        x &lt;- data.matrix(x)
    else if (!is.matrix(x)) 
        stop(""'x' must be a matrix or a data frame"")
    if (!is.numeric(x)) 
        stop(""data in 'x' must be numeric"")


    # this code was moved here so that the angles will be proparly created...
    n.seg &lt;- ncol(x) # this will be changed to the ncol of the new x - in a few rows...
    # creates the angles
    angles &lt;- if (full) 
        seq.int(0, 2 * pi, length.out = n.seg + 1)[-(n.seg + 1)]
    else if (draw.segments) 
        seq.int(0, pi, length.out = n.seg + 1)[-(n.seg + 1)]
    else seq.int(0, pi, length.out = n.seg)
    if (length(angles) != n.seg) 
        stop(""length of 'angles' must equal 'ncol(x)'"")

    # changing to allow weighted segments
    angles &lt;- angles.by.weight(angles, segment.weights)
    #angles &lt;- angles.by.weight.2(angles)   # try2
    # try3 
    # weights &lt;- sample(c(.3,.9), length(angles)-1, replace = T)
    # angles &lt;- weights / sum(weights) * 2 * pi
    # angles &lt;- c(0,angles )




    # changing to allow weighted segments
     col.segments &lt;- add.num.before.and.after(col.segments, ""white"") # for colors
     x &lt;- add.0.columns.to.DF(x)







    n.loc &lt;- nrow(x)
    n.seg &lt;- ncol(x)
    if (is.null(locations)) {
        if (is.null(nrow)) 
            nrow &lt;- ceiling(if (!is.numeric(ncol)) sqrt(n.loc) else n.loc/ncol)
        if (is.null(ncol)) 
            ncol &lt;- ceiling(n.loc/nrow)
        if (nrow * ncol &lt; n.loc) 
            stop(""nrow * ncol &lt;  number of observations"")
        ff &lt;- if (!is.null(labels)) 
            2.3
        else 2.1
        locations &lt;- expand.grid(ff * 1L:ncol, ff * nrow:1)[1L:n.loc, 
            ]
        if (!is.null(labels) &amp;&amp; (missing(flip.labels) ||
!is.logical(flip.labels))) 
            flip.labels &lt;- ncol * mean(nchar(labels, type = ""c"")) &gt; 
                30
    }
    else {
        if (is.numeric(locations) &amp;&amp; length(locations) == 2) {
            locations &lt;- cbind(rep.int(locations[1L], n.loc), 
                rep.int(locations[2L], n.loc))
            if (!missing(labels) &amp;&amp; n.loc &gt; 1) 
                warning(""labels do not make sense for a single location"")
            else labels &lt;- NULL
        }
        else {
            if (is.data.frame(locations)) 
                locations &lt;- data.matrix(locations)
            if (!is.matrix(locations) || ncol(locations) != 2) 
                stop(""'locations' must be a 2-column matrix."")
            if (n.loc != nrow(locations)) 
                stop(""number of rows of 'locations' and 'x' must be equal."")
        }
        if (missing(flip.labels) || !is.logical(flip.labels)) 
            flip.labels &lt;- FALSE
    }
    xloc &lt;- locations[, 1]
    yloc &lt;- locations[, 2]

    # Here we created the angles, but I moved it to the beginning of the code


    if (scale) {
        x &lt;- apply(x, 2L, function(x) (x - min(x, na.rm = TRUE))/diff(range(x, 
            na.rm = TRUE)))
    }
    x[is.na(x)] &lt;- 0
    mx &lt;- max(x &lt;- x * len)
    if (is.null(xlim)) 
        xlim &lt;- range(xloc) + c(-mx, mx)
    if (is.null(ylim)) 
        ylim &lt;- range(yloc) + c(-mx, mx)
    deg &lt;- pi/180
    op &lt;- par(mar = mar, xpd = xpd)
    on.exit(par(op))
    if (plot &amp;&amp; !add) 
        plot(0, type = ""n"", ..., xlim = xlim, ylim = ylim, main = main, 
            sub = sub, xlab = xlab, ylab = ylab, asp = 1, axes = axes)
    if (!plot) 
        return(locations)
    s.x &lt;- xloc + x * rep.int(cos(angles), rep.int(n.loc, n.seg))
    s.y &lt;- yloc + x * rep.int(sin(angles), rep.int(n.loc, n.seg))
    if (draw.segments) {
        aangl &lt;- c(angles, if (full) 2 * pi else pi)
        for (i in 1L:n.loc) {
            px &lt;- py &lt;- numeric()
            for (j in 1L:n.seg) {
                k &lt;- seq.int(from = aangl[j], to = aangl[j + 
                  1], by = 1 * deg)
                px &lt;- c(px, xloc[i], s.x[i, j], x[i, j] * cos(k) + 
                  xloc[i], NA)
                py &lt;- c(py, yloc[i], s.y[i, j], x[i, j] * sin(k) + 
                  yloc[i], NA)
            }
            polygon3(px, py, col = col.segments, lwd = lwd, lty = lty)
        }
    }
    else {
        for (i in 1L:n.loc) {
#            polygon3(s.x[i, ], s.y[i, ], lwd = lwd, lty = lty, 
#                col = col.stars[i])
            if (polygon)
                polygon3(s.x[i, ], s.y[i, ], lwd = lwd, lty = lty, 
                  col = col.stars[i])
            if (radius) 
                segments(rep.int(xloc[i], n.seg), rep.int(yloc[i], 
#                  n.seg), s.x[i, ], s.y[i, ], lwd = lwd, lty = lty)
                  n.seg), s.x[i, ], s.y[i, ], lwd = lwd, lty = lty, col =
col.radius)
        }
    }
    if (!is.null(labels)) {
        y.off &lt;- mx * (if (full) 
            1
        else 0.1)
        if (flip.labels) 
            y.off &lt;- y.off + cex * par(""cxy"")[2L] * ((1L:n.loc)%%2 - 
                if (full) 
                  0.4
                else 0)
        text(xloc, yloc - y.off, labels, cex = cex, adj = c(0.5, 
            1))
    }
    if (!is.null(key.loc)) {
        par(xpd = key.xpd)
        key.x &lt;- key.len * cos(angles) + key.loc[1L]
        key.y &lt;- key.len * sin(angles) + key.loc[2L]
        if (draw.segments) {
            px &lt;- py &lt;- numeric()
            for (j in 1L:n.seg) {
                k &lt;- seq.int(from = aangl[j], to = aangl[j + 
                  1], by = 1 * deg)
                px &lt;- c(px, key.loc[1L], key.x[j], key.len * cos(k) + 
                  key.loc[1L], NA)
                py &lt;- c(py, key.loc[2L], key.y[j], key.len * sin(k) + 
                  key.loc[2L], NA)
            }
            polygon3(px, py, col = col.segments, lwd = lwd, lty = lty)
        }
        else {
#            polygon3(key.x, key.y, lwd = lwd, lty = lty)
            if (polygon)
                polygon3(key.x, key.y, lwd = lwd, lty = lty)
            if (radius) 
                segments(rep.int(key.loc[1L], n.seg), rep.int(key.loc[2L], 
#                  n.seg), key.x, key.y, lwd = lwd, lty = lty)
                  n.seg), key.x, key.y, lwd = lwd, lty = lty, col = col.radius)
        }


        lab.angl &lt;- angles + if (draw.segments) 
            (angles[2L] - angles[1L])/2
        else 0
        label.x &lt;- 1.1 * key.len * cos(lab.angl) + key.loc[1L]
        label.y &lt;- 1.1 * key.len * sin(lab.angl) + key.loc[2L]
        for (k in 1L:n.seg) {
            text.adj &lt;- c(if (lab.angl[k] &lt; 90 * deg || lab.angl[k] &gt; 
                270 * deg) 0 else if (lab.angl[k] &gt; 90 * deg &amp;&amp; 
                lab.angl[k] &lt; 270 * deg) 1 else 0.5, if (lab.angl[k] &lt;= 
                90 * deg) (1 - lab.angl[k]/(90 * deg))/2 else if (lab.angl[k] &lt;=
                270 * deg) (lab.angl[k] - 90 * deg)/(180 * deg) else 1 - 
                (lab.angl[k] - 270 * deg)/(180 * deg))
            text(label.x[k], label.y[k], labels = key.labels[k], 
                cex = cex, adj = text.adj)
        }
    }
    if (frame.plot) 
        box(...)
    invisible(locations)
}
</code></pre>

<p>Here is an example of running this:</p>

<pre><code>#require(debug)
# mtrace(stars2)
stars(mtcars[1:3, 1:8],
        draw.segments = T,
        main = ""Motor Trend Cars : stars(*, full = F)"", full = T, col.segments = 1:2)

stars2(mtcars[1:3, 1:8],
        draw.segments = T,
        main = ""Motor Trend Cars : stars(*, full = F)"", full = T, col.segments = 0:3,
        segment.weights = c(.2,.2,1,1,.4,.4,.6,.9))
</code></pre>

<p>(I'll probably publish this with explanation on <a href=""http://www.r-statistics.com"" rel=""nofollow noreferrer"">my blog</a> sometime soon...)</p>
"
2776985,333892,2010-05-05T21:39:33Z,2769510,1,FALSE,"<p>But, if your using tolerances anyway, why do you care that a-b == .5 (in fact) doesn't get evaluated?  If you are using tolerances anyway you are saying I don't care about the end points exactly.</p>

<p>Here is what is true 
if( (a-b) >= .5)
if( (a-b) &lt; .5)</p>

<p>one of those should always evaluate true on every pair of doubles.  Any code that uses one implicitly defines a no operation on the other one, at least.  If your using tolerances to get actual .5 included in the first but your problem is defined on a continuous domain you arn't accomplishing much. In most problems involving continuous values in the underlying problem there will be very little point to that, since values arbitrarily over .5 will always evaluate as they should.  Values arbitrarily close to .5 will go to the ""wrong"" flow control, but in continuous problems where you are using appropriate precision that doesn't matter.  </p>

<p>The only time that tolerances make sense is when you are dealing with problems of the type
if( (a-b) == c)
if( (a-b) != c)</p>

<p>Here no amount of ""appropriate precision"" can help you. The reason is that you have to be prepared that the second will always evaluate to true unless you set the bits of a-b at a very low level by hand, when in fact you probably want the first to sometimes be true.</p>
"
2789902,161808,2010-05-07T15:57:30Z,2664655,1,TRUE,"<p>We made an attempt at this with our recent JASA article: <a href=""http://hdl.handle.net/1902.1/12174"" rel=""nofollow noreferrer"">http://hdl.handle.net/1902.1/12174</a>. You should be able to ""make"" the whole paper. One thing to notice about our reproduction archive: we packaged versions of the R packages that we used. It turned out that as people improve their packages, sometimes they change defaults --- which would break our build. Perhaps in the future one might distribute an entire virtual machine including the R binary which would be called [recall how round(x,digits=) lost its arguments and became positional from version of R to the next -- making round(digits=,x) provide nonsense results without warning?]. </p>

<p>Anyway, this is our first attempt at such a complex document. I have a smaller version here <a href=""http://hdl.handle.net/1902.1/13376"" rel=""nofollow noreferrer"">http://hdl.handle.net/1902.1/13376</a> which does not use make.</p>
"
2800116,161808,2010-05-10T03:17:37Z,1974930,0,TRUE,"<p>I've had luck with abind() (from library(abind)). For example, say I have a simulation function returning matrices and I'd like a big array, I could use abind(,along=.5) to get the list of matrices to be bound into an array with a new dimension added to it. So you might like something like:</p>

<pre><code>myfun&lt;-function(arr){abind(arr,along=.5)}

foreach(1:n.sims,.combine=myfun) .... 
</code></pre>
"
2807061,301877,2010-05-10T23:13:02Z,2712421,21,FALSE,"<p>I worked for nine years in an analytics shop, and introduced the idea of version control for our analysis projects to that shop.  I'm a big believer in version control, obviously.  I would make the following points, however.</p>

<ol>
<li>Version control may not be appropriate if you are doing analysis for possible use in court.  It doesn't sound like this applies to you, but it would have made our clients very nervous to know that every version of every script that we had ever produced was potentially discoverable.  We used version control for code modules that were reused in multiple engagements, but did not use version control for engagement-specific code, for that reason.</li>
<li>We found the biggest benefit to version control came from storing canned modules of code that were re-used across multiple projects.  For example, you might have a particular favorite way of processing certain Census PUMS extracts.  Organize this code into a directory and put it into your VCS.  You can then check it out into each new project every time you need it. It may even be useful to create specific branches of certain code for certain project, if you are doing special processing of a particular common dataset for that project.  Then, when you are done with that project, decide how much of your special code to merge back to the main branch.</li>
<li>Don't put processed data into version control.  Only code.  Our goal was always to have a complete set of scripts so that we could delete all of our internally processed data, push a button, and have every number for the report regenerated from scratch.  That's the only way to be sure that you don't have old bugs living on mysteriously in your data.</li>
<li>To make sure that your results are really completely reproducible, it isn't sufficient just to keep your code in a VCS.  It is critical to keep careful track of which version of which modules were used to create any particular deliverable.</li>
<li>As for software, I had good luck with Subversion.  It is easy to set up and administer.  I recognize the appeal of the new-fangled distributed VCSs, like git and mercurial, but I'm not sure there are any strong advantages if you are working by yourself.  On the other hand, I don't know of any negatives to using them, either--I just haven't worked with them in an analysis environment.</li>
</ol>
"
2825679,134830,2010-05-13T09:31:06Z,2682629,4,FALSE,"<p>I spoke to John Verzani, creator of the gWidgets* packages, and the answer is incredibly simple (though not entirely intuitive).  You access the contents of list-type widgets  with <code>widget_name[]</code>.</p>

<pre><code>library(gWidgets)
library(gWidgetstcltk)

get_list_content &lt;- function() ls(envir = globalenv())  # or whatever

win &lt;- gwindow()
grp &lt;- ggroup(container = win)
ddl &lt;- gdroplist(get_list_content(), container = grp)
refresh &lt;- gimage(""refresh"", 
  dirname   = ""stock"",
  container = grp,
  handler   = function(h, ...) ddl[] &lt;- get_list_content()   
)
</code></pre>

<p>Note that there are some restrictions: radio button lists must remain the same length. </p>

<pre><code>win &lt;- gwindow()
rb &lt;- gradio(1:10, cont = win)
rb[] &lt;- 2:11     # OK
rb[] &lt;- 1:5      # Throws an error; can't change length.
</code></pre>
"
2835988,216064,2010-05-14T16:52:09Z,2658338,1,TRUE,"<p>Maybe if you inspect the <a href=""http://rpy.sourceforge.net/rpy2.html"" rel=""nofollow noreferrer"">rpy2</a> package, you can learn something about how data structures are represented (and can be accessed).</p>
"
2847990,342903,2010-05-17T09:44:25Z,2614949,2,FALSE,"<p>Try <a href=""http://meta.wikimedia.org/wiki/WikiXRay"" rel=""nofollow noreferrer"" title=""Project WikiXRay"">WikiXRay</a> (Python/R) and <a href=""https://www.zotero.org/groups/cpov"" rel=""nofollow noreferrer"" title=""Zotero"">zotero</a>.</p>
"
2925283,347165,2010-05-27T21:55:43Z,713878,2,FALSE,"<p><a href=""http://mahout.apache.org/"" rel=""nofollow noreferrer"">Apache Mahout</a> is an open-source framework built on map-reduce (i.e. it works for really really big matrices). Note that for a lot of matrix stuff the question isn't ""whats the big-o runtime"" but rather ""how parallelizable is it?"" Mahout <a href=""https://cwiki.apache.org/MAHOUT/dimensionalreduction.html"" rel=""nofollow noreferrer"">says</a> they use Lanczos, which can essentially be run in parallel on as many processors as you care to give it.</p>
"
2926401,352530,2010-05-28T02:57:23Z,2767219,12,FALSE,"<p>Why not </p>

<pre><code>x$col[is.na(x$col)]&lt;-value
</code></pre>

<p>?<br>
You wont have to change your dataframe</p>
"
2973472,446813,2010-06-04T11:01:50Z,1429907,4,FALSE,"<p>""make"" is great because (1) you can use it for all your work in any language (unlike, say, Sweave and Brew), (2) it is very powerful (enough to build all the software on your machine), and (3) it avoids repeating work.  This last point is important to me because a lot of the work is slow; when I latex a file, I like to see the result in a few seconds, not the hour it would take to recreate the figures.</p>
"
2988631,134830,2010-06-07T10:19:01Z,2458013,14,FALSE,"<p><code>fixInNamespace</code> is like <code>fix</code>, for functions in a package (including those that haven't been exported).</p>
"
2993445,183988,2010-06-07T21:51:16Z,2551921,0,FALSE,"<p>When wanting to add different info the following works:</p>

<pre><code>ggplot(mydata, aes(x=clusterSize, y=occurence)) +
geom_bar() + geom_text(aes(x=clusterSize, y=occurence, label = mydata$otherinfo))
</code></pre>
"
2994546,457898,2010-06-08T03:03:10Z,2613420,14,FALSE,"<p>It's a good practice to look at the data, hence infer about the type of missing values: is it MCAR (missing complete and random), MAR (missing at random) or MNAR (missing not at random)? Based on these three types, you can study the underlying structure of missing values and conclude whether imputation is at all applicable (you're lucky if it's not MNAR, 'cause, in that case, missing values are considered non-ignorable, and are related to some unknown underlying influence, factor, process, variable... whatever).</p>

<p>Chapter 3. in <strong><em>""Interactive and Dynamic Graphics for Data Analyst with R and GGobi""</em></strong> by <em>Di Cook</em> and <em>Deborah Swayne</em> is great reference regarding this topic.</p>

<p>You'll see <code>norm</code> package in action in this chapter, but <code>Hmisc</code> package has data imputation routines. See also <code>Amelia</code>, <code>cat</code> (for categorical missings imputation), <code>mi</code>, <code>mitools</code>, <code>VIM</code>, <code>vmv</code> (for missing data visualisation).</p>

<p>Honestly, I still don't quite understand is your question about statistics, or about R missing data imputation capabilities? I reckon that I've provided good references on second one, and about the first one: you can replace your NA's either with central tendency (mean, median, or similar), hence reduce the variability, or with random constant ""pulled out"" of observed (recorded) cases, or you can apply regression analysis with variable that contains NA's as criteria, and other variables as predictors, then assign residuals to NA's... it's an elegant way to deal with NA's, but quite often it would not go easy on your CPU (I have Celeron on 1.1GHz, so I have to be gentle).</p>

<p>This is an optimization problem... there's no definite answer, you should decide what/why are you sticking with some method. But it's always good practice to look at the data! =)
Be sure to check Cook &amp; Swayne - it's an excellent, skilfully written guide. <strong><em>""Linear Models with R""</em></strong> by <em>Faraway</em> also contains a chapter about missing values.</p>

<p>So there.</p>

<p>Good luck! =)</p>
"
3002661,361972,2010-06-09T01:55:24Z,2614767,1,FALSE,"<p>Taking the last two comments into consideration, you may be able to acquire corporate financial statements economically using EdgardOnline.  It isn't free, but is less expensive than Bloomberg and Reuters.  Another thing to consider is financial reporting normalization/standardized. Just because two companies are in the same industry and sell similar products does not necessarily mean that if you laid the two companies' income statements or balance sheets side by side, that reporting items would necessarily line up.  Compustat has normalized/standardized financial reports. </p>
"
3034932,278198,2010-06-14T04:20:15Z,2192316,2,FALSE,"<p>Using strapply in the gsubfn package.  strapply is like apply in that the args are object, modifier and function except that the object is a vector of strings (rather than an array) and the modifier is a regular expression (rather than a margin):</p>

<pre><code>library(gsubfn)
x &lt;- c(""xy13"", ""ab 12 cd 34 xy"")
strapply(x, ""\\d+"", as.numeric)
# list(13, c(12, 34))
</code></pre>

<p>This says to match one or more digits (\d+) in each component of x passing each match through as.numeric.  It returns a list whose components are vectors of matches of respective components of x.   Looking the at output we see that the first component of x has one match which is 13 and the second component of x has two matches which are 12 and 34.  See <a href=""http://gsubfn.googlecode.com"" rel=""nofollow noreferrer"">http://gsubfn.googlecode.com</a> for more info.</p>
"
3096636,162832,2010-06-22T19:55:45Z,1716600,1,FALSE,"<p>I just had this problem. It seams that</p>

<pre><code>theme_set(theme_bw(base_size=9))
</code></pre>

<p>results in the error reported. But</p>

<pre><code>base_size &lt;- 9
theme_set(theme_bw(base_size=base_size))
</code></pre>

<p>works.</p>

<p>I googled and found the example at the <a href=""http://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/"" rel=""nofollow noreferrer"">learnr blog</a></p>

<p>I dont know what the first example does not work though?</p>
"
3104650,370756,2010-06-23T19:01:17Z,2436129,3,FALSE,"<p><code>Snow</code> and <code>multicore</code> varies in one significant way -- <code>multicore</code> <strong>forks</strong> a new process, so it is using the same memory as the main process. This means that if you use <code>snow</code> you need to distribute (physically send and store in children' space) the data you want to process, but if you use <code>multicore</code> children will be just able to access the main process's copy of the data -- so it saves transfer and memory use.</p>
"
3104934,370756,2010-06-23T19:39:51Z,2187910,1,FALSE,"<p>I assume that you mean that you have vectors <code>y</code> and <code>x</code> and you try do fit a function <code>y(x)=Alog(x)</code>.<br>
First of all, fitting log is a bad idea, because it doesn't behave well. Luckily we have <code>x(y)=exp(y/A)</code>, so we can fit an exponential function which is much more convenient. We can do it using nonlinear least squares:  </p>

<pre><code> nls(x~exp(y/A),start=list(A=1.),algorithm=""port"")
</code></pre>

<p>where <code>start</code> is an initial guess for <code>A</code>. This approach is a numerical optimization, so it may fail.<br>
The more stable way is to transform it to a linear function, <code>log(x(y))=y/A</code> and fit a straight line using <code>lm</code>:</p>

<pre><code>lm(log(x)~y)
</code></pre>
"
3133374,378117,2010-06-28T14:46:19Z,2754469,8,TRUE,"<p>A while back I wrote a set of functions for simulation and estimation of Discrete Markov Chain probability matrices: <a href=""http://www.feferraz.net/files/lista/DTMC.R"" rel=""noreferrer"">http://www.feferraz.net/files/lista/DTMC.R</a>.</p>

<p>Relevant code for what you're asking:</p>

<pre><code>simula &lt;- function(trans,N) {
        transita &lt;- function(char,trans) {
                sample(colnames(trans),1,prob=trans[char,])
        }

 sim &lt;- character(N)
 sim[1] &lt;- sample(colnames(trans),1)
 for (i in 2:N) {
  sim[i] &lt;- transita(sim[i-1],trans)
 }

 sim
}

#example
#Obs: works for N &gt;= 2 only. For higher order matrices just define an
#appropriate mattrans
mattrans &lt;- matrix(c(0.97,0.03,0.01,0.99),ncol=2,byrow=TRUE)
colnames(mattrans) &lt;- c('0','1')
row.names(mattrans) &lt;- c('0','1')
instancia &lt;- simula(mattrans,255) # simulates 255 steps in the process
</code></pre>
"
3153516,355014,2010-06-30T21:22:38Z,2161052,5,FALSE,"<p>Late to this game, but I created a <a href=""http://probabilitynotes.wordpress.com/2010/06/27/plot-multiple-time-series-using-the-flowinkblotriverribbonvolcanohourglassareawhatchamacallit-plots-blue-whale-catch-per-country-w-ggplot2/"">stacked ""blot"" chart using ggplot2 and another set of data</a>. This uses geom_polygon after the data has been smoothed out.</p>

<pre><code># data: Masaaki Ishida (luna@pos.to)
# http://luna.pos.to/whale/sta.html

head(blue, 2)
##      Season Norway U.K. Japan Panama Denmark Germany U.S.A. Netherlands
## ## [1,]   1931      0 6050     0      0       0       0      0           0
## ## [2,]   1932  10128 8496     0      0       0       0      0           0
## ##      U.S.S.R. South.Africa TOTAL
## ## [1,]        0            0  6050
## ## [2,]        0            0 18624

hourglass.plot &lt;- function(df) {
  stack.df &lt;- df[,-1]
  stack.df &lt;- stack.df[,sort(colnames(stack.df))]
  stack.df &lt;- apply(stack.df, 1, cumsum)
  stack.df &lt;- apply(stack.df, 1, function(x) sapply(x, cumsum))
  stack.df &lt;- t(apply(stack.df, 1, function(x) x - mean(x)))
  # use this for actual data
  ##  coords.df &lt;- data.frame(x = rep(c(df[,1], rev(df[,1])), times = dim(stack.df)[2]), y = c(apply(stack.df, 1, min), as.numeric(apply(stack.df, 2, function(x) c(rev(x),x)))[1:(length(df[,1])*length(colnames(stack.df))*2-length(df[,1]))]), id = rep(colnames(stack.df), each = 2*length(df[,1])))

  ##  qplot(x = x, y = y, data = coords.df, geom = ""polygon"", color = I(""white""), fill = id)

  # use this for smoothed data
  density.df &lt;- apply(stack.df, 2, function(x) spline(x = df[,1], y = x))
  id.df &lt;- sort(rep(colnames(stack.df), each = as.numeric(lapply(density.df, function(x) length(x$x)))))
  density.df &lt;- do.call(""rbind"", lapply(density.df, as.data.frame))
  density.df &lt;- data.frame(density.df, id = id.df)
  smooth.df &lt;- data.frame(x = unlist(tapply(density.df$x, density.df$id, function(x) c(x, rev(x)))), y = c(apply(unstack(density.df[,2:3]), 1, min), unlist(tapply(density.df$y, density.df$id, function(x) c(rev(x), x)))[1:(table(density.df$id)[1]+2*max(cumsum(table(density.df$id))[-dim(stack.df)[2]]))]), id = rep(names(table(density.df$id)), each = 2*table(density.df$id)))

  qplot(x = x, y = y, data = smooth.df, geom = ""polygon"", color = I(""white""), fill = id)
}

hourglass.plot(blue[,-12]) + opts(title = c(""Blue Whale Catch""))
</code></pre>

<p><a href=""http://probabilitynotes.files.wordpress.com/2010/06/bluewhalecatch.png"">alt text http://probabilitynotes.files.wordpress.com/2010/06/bluewhalecatch.png</a></p>
"
3242908,180033,2010-07-14T02:08:37Z,2151147,3,FALSE,"<p>You can convert the variable labels to variable names from within Stata before exporting it to a R or text file.<br>
As Ian mentions, variable labels usually do not make good variable names, but if you convert spaces and other characters to underscores and if your variable labels aren't too long, you can re-label your vars with the varlabels quite easily.  </p>

<p>Below is an example using the inbuilt Stata dataset ""cancer.dta"" to replace all variable names with var labels--importantly, this code will not try to rename variable with no variable labels.  Note that I also picked a dataset where there are lots of characters that aren't useful in naming a variable (e.g.: =, 1, ', .,  (), etc)...you can add any characters that might be lurking in your variable labels to the list in the 5th line:  ""local chars ""..."" "" and it will make the changes for you:</p>

<pre><code>****************! BEGIN EXAMPLE
//copy and paste this code into a Stata do-file and click ""do""//
sysuse  cancer, clear
desc
**
local chars """" "" ""("" "")"" ""."" ""1"" ""="" `""'""' ""___"" ""__"" ""
ds, not(varlab """")    // &lt;-- This will only select those vars with varlabs //
foreach v in `r(varlist)' {
    local `v'l ""`:var lab `v''""
    **variables names cannot have spaces or other symbols, so::
        foreach s in `chars' {
    local `v'l: subinstr local `v'l ""`s'"" ""_"", all
              }
    rename `v' ``v'l'
    **make the variable names all lower case**
    cap rename ``v'l' `=lower(""``v'l'"")'
      }
desc
****************! END EXAMPLE
</code></pre>

<p>You might also consider taking a look at <a href=""http://www.stattransfer.com/"" rel=""nofollow noreferrer"">Stat Transfer</a> and it's capabilities in converting Stata to R datafiles.</p>
"
3244029,78912,2010-07-14T07:09:39Z,1888151,0,FALSE,"<p>my current practice is not automated at all: I give the three coordinates of the <code>Version:</code> field the meaning as in the question, but the third coordinate (the svn revision number), I update it by hand (incrementing it by one) before each commit.  </p>

<p>working from Emacs (using <code>psvn</code>), I don't see this as a major limitation, but it is more work than I would like to perform by hand and it is definitely not satisfactory on larger repositories where the revision number is incremented by unrelated commits.</p>
"
3336726,402480,2010-07-26T16:14:54Z,652136,16,FALSE,"<p>Removing Null elements from a list in single line : </p>

<p><code>x=x[-(which(sapply(x,is.null),arr.ind=TRUE))]</code></p>

<p>Cheers</p>
"
3344043,403310,2010-07-27T13:22:49Z,2232699,14,FALSE,"<p>Thanks for the answers. I missed this thread when it was originally posted. data.table has moved on since February. 1.4.1 was released to CRAN a while ago and 1.5 is out soon. For example the DT() alias has been replaced with list(); as a primitive its much faster, and data.table now inherits from data.frame so it works with packages that <em>only</em> accept data.frame such as ggplot and lattice, without any conversion required (faster and more convenient).</p>

<p>Is it possible to subscribe to the data.table tag so I get an email when someone posts a question with that tag?  The datatable-help list has grown to about 30-40 messages a month, but I'm happy to answer here too if I can get some kind of notification.</p>

<p>Matthew</p>
"
3351143,404272,2010-07-28T08:33:38Z,1136709,2,FALSE,"<p>try RCaller, <a href=""http://www.mhsatman.com/rcaller.php"" rel=""nofollow noreferrer"">http://www.mhsatman.com/rcaller.php</a></p>
"
3390669,406309,2010-08-02T18:30:26Z,2095215,0,FALSE,"<p>I haven't tested this, but try the following instead:</p>

<pre><code>(add-hook 'noweb-select-mode-hook
              '(lambda () (hack-local-variables)))
</code></pre>

<p>Perhaps hack-local-variables-prop-line has been changed to merely parse the values, not instate them.</p>
"
3391538,182155,2010-08-02T20:36:43Z,2614767,1,FALSE,"<p>I don't know anything about R, but assuming that it can call a REST API and consume data in XML form, you can try the Mergent Company Fundamentals API at <a href=""http://www.mergent.com/servius/"" rel=""nofollow noreferrer"">http://www.mergent.com/servius/</a> - there's lots of very detailed financial statement data (balance sheets / income statements / cashflow statements / ratios), standardized across companies, going back more than 20 years</p>
"
3420304,65148,2010-08-06T01:04:26Z,1188544,3,TRUE,"<p>Simple answer.  On windows when running command use ""Rcmd"" not ""R CMD"".  There is a separate exe for running the commands.  Look in the bin folder of your R installation.</p>
"
3466308,322912,2010-08-12T09:30:53Z,1265129,3,FALSE,"<p>Another way, similar to Ken's is using the clipboard (on windows, and possibly linux). I would copy your code and run</p>

<pre><code>&gt; site.data &lt;- read.table(""clipboard"", header=T)
&gt; site.data
    site year     peak
1  ALBEN    5 101529.6
2  ALBEN   10 117483.4
3  ALBEN   20 132960.9
8  ALDER    5   6561.3
9  ALDER   10   7897.1
10 ALDER   20   9208.1
15 AMERI    5  43656.5
16 AMERI   10  51475.3
17 AMERI   20  58854.4
</code></pre>
"
3485309,420664,2010-08-14T22:08:20Z,1508889,6,FALSE,"<p><code>colSums(!is.na(x))</code> should work.</p>
"
3494395,421869,2010-08-16T15:01:50Z,1801064,2,FALSE,"<p>try command <code>x11()</code> before each plot, here's an example:</p>

<pre><code>x11()
plot(1:10)
x11()
plot(rnorm(10))
</code></pre>

<p>This will lead to different plot windows. You can add ""par"" command to any of these <code>x11()</code> windows and get more variety of plots, i.e. 4 plots in one window while a big plot in another window.</p>
"
3502527,355431,2010-08-17T12:48:31Z,1614331,3,TRUE,"<p>I've had success in getting the Binary of a plot into an R variable as a string.  Its got some read/write overhead.  In the snippet below, R saves the plot as a temp file and reads it back in.</p>

<pre><code>## create a plot
x &lt;- rnorm(100,0,1)
hist(x, col=""light blue"")

## save plot as temp file
png(filename=""temp.png"", width=500, height=500)
print(p)
dev.off()

## read temp file as a binary string
plot_binary &lt;- paste(readBin(""temp.png"", what=""raw"", n=1e6), collapse="""")
</code></pre>

<p>Maybe this is helpful to you.</p>
"
3517620,424640,2010-08-18T23:44:06Z,2614767,1,FALSE,"<p>I have written a C# program that I think does what you want.  It parses the html from nasdaq.com pages.  It parses html and creates 1 csv file per stock that includes income statement, cash flow, and balance sheet values going back 5 - 10 years depending on the age of the stock.  I am now working to add some analysis calculations (mostly historic ratios at this point).  I'm interested in learning about R and it's applications to fundamental analysis.  Maybe we can help each other.</p>
"
3584290,428790,2010-08-27T12:51:40Z,2275896,68,FALSE,"<p>Sometimes speedup can be substantial, like when you have to nest for-loops to get the average based on a grouping of more than one factor. Here you have two approaches that give you the exact same result :</p>

<pre><code>set.seed(1)  #for reproducability of the results

# The data
X &lt;- rnorm(100000)
Y &lt;- as.factor(sample(letters[1:5],100000,replace=T))
Z &lt;- as.factor(sample(letters[1:10],100000,replace=T))

# the function forloop that averages X over every combination of Y and Z
forloop &lt;- function(x,y,z){
# These ones are for optimization, so the functions 
#levels() and length() don't have to be called more than once.
  ylev &lt;- levels(y)
  zlev &lt;- levels(z)
  n &lt;- length(ylev)
  p &lt;- length(zlev)

  out &lt;- matrix(NA,ncol=p,nrow=n)
  for(i in 1:n){
      for(j in 1:p){
          out[i,j] &lt;- (mean(x[y==ylev[i] &amp; z==zlev[j]]))
      }
  }
  rownames(out) &lt;- ylev
  colnames(out) &lt;- zlev
  return(out)
}

# Used on the generated data
forloop(X,Y,Z)

# The same using tapply
tapply(X,list(Y,Z),mean)
</code></pre>

<p>Both give exactly the same result, being a 5 x 10 matrix with the averages and named rows and columns. But :</p>

<pre><code>&gt; system.time(forloop(X,Y,Z))
   user  system elapsed 
   0.94    0.02    0.95 

&gt; system.time(tapply(X,list(Y,Z),mean))
   user  system elapsed 
   0.06    0.00    0.06 
</code></pre>

<p>There you go. What did I win? ;-)</p>
"
3611619,430909,2010-08-31T17:34:19Z,1523126,46,FALSE,"<p>You can have read.table or read.csv do this conversion for you semi-automatically. First create a new class definition, then create a conversion function and set it as an ""as"" method using the setAs function like so:</p>

<pre><code>setClass(""num.with.commas"")
setAs(""character"", ""num.with.commas"", 
        function(from) as.numeric(gsub("","", """", from) ) )
</code></pre>

<p>Then run read.csv like:</p>

<pre><code>DF &lt;- read.csv('your.file.here', 
   colClasses=c('num.with.commas','factor','character','numeric','num.with.commas'))
</code></pre>
"
3635697,438947,2010-09-03T12:28:36Z,2578961,2,FALSE,"<p>You don't need <code>prop.tables</code> or count etc to do the 100% stacked bars. You just need <code>+geom_bar(position=""stack"")</code></p>
"
3738280,450956,2010-09-17T19:18:28Z,2436688,4,FALSE,"<p>Not sure why you don't think your first method won't work.  You have a bug in the lappend function: length(list) should be length(lst).  This works fine and returns a list with the appended obj.</p>
"
3784878,170352,2010-09-24T07:01:25Z,2578961,2,FALSE,"<p>Your second problem can be solved with melt and cast from the reshape package</p>

<p>After you've factored the elements in your data.frame called you can use something like: </p>

<pre><code>install.packages(""reshape"")
library(reshape)

x &lt;- melt(your.df, c()) ## Assume you have some kind of data.frame of all factors
x &lt;- na.omit(x) ## Be careful, sometimes removing NA can mess with your frequency calculations

x &lt;- cast(x, variable + value ~., length)
colnames(x) &lt;- c(""variable"",""value"",""freq"")
## Presto!
ggplot(x, aes(variable, freq, fill = value)) + geom_bar(position = ""fill"") + coord_flip() + scale_y_continuous("""", formatter=""percent"")
</code></pre>

<p>As an aside, I like to use grep to pull in columns from a messy import. For example: </p>

<pre><code>x &lt;- your.df[,grep(""int."",df)] ## pulls all columns starting with ""int_""
</code></pre>

<p>And factoring is easier when you don't have to type c(' ', ...) a million times.</p>

<pre><code>for(x in 1:ncol(x)) { 
df[,x] &lt;- factor(df[,x], labels = strsplit('
Very Interested
Somewhat Interested
Not Very Interested
Not At All interested
NA
NA
NA
NA
NA
NA
', '\n')[[1]][-1]
}
</code></pre>
"
3811680,134830,2010-09-28T10:09:15Z,1735540,1,FALSE,"<p>To simplify things, let's just consider only the estimates.</p>

<pre><code>estimates &lt;- subset(val, variable == ""estimate"")
</code></pre>

<p>First we reorder the factor levels, so that <code>State</code>s are plotted in decreasing order of <code>Value</code>.</p>

<pre><code>estimates$State &lt;- with(estimates, reorder(State, -Value))
</code></pre>

<p>Similarly, we reorder the dataset and calculate a cumulative value.</p>

<pre><code>estimates &lt;- estimates[order(estimates$Value, decreasing = TRUE),]
estimates$cumulative &lt;- cumsum(estimates$Value)
</code></pre>

<p>Now we are ready to draw the plot.  The trick to get a line and bar on the same axes is to convert the State variable (a factor) to be numeric.  </p>

<pre><code>p &lt;- ggplot(estimates, aes(State, Value)) + 
  geom_bar() +
  geom_line(aes(as.numeric(State), cumulative))
p
</code></pre>

<p>As mentioned in the question, trying to draw two Pareto plots of two variable groups right next to each other isn't very easy.  You'd probably be better off using facetting if you want multiple Pareto plots.</p>
"
3832235,256662,2010-09-30T15:38:57Z,1487320,3,FALSE,"<p>I see you asked this a year ago, and since there has been some updates since (relevant to WordPress blogs), I thought of referencing them.</p>

<p>I wrote two posts on the topic:</p>

<ol>
<li>For wordpress.com hosted blogs:  <a href=""http://www.r-statistics.com/2010/09/r-syntax-highlighting-for-bloggers-on-wordpress-com/"" rel=""nofollow"">R syntax highlighting for bloggers on WordPress.com</a></li>
<li>For wordpress.org self hosted blogs: <a href=""http://www.r-statistics.com/2010/02/r-syntax-highlight-on-your-blog-a-wordpress-plugin/"" rel=""nofollow"">Highlight the R syntax on your (WordPress) blog using the wp-syntax plugin</a></li>
</ol>

<p>p.s: I also published the talk I gave on useR2010 with other tips on <a href=""http://www.r-statistics.com/2010/07/blogging-about-r-presentation-and-audio/"" rel=""nofollow"">blogging about R</a>, but the other the posts linked to are the ones answering your question.</p>

<p>Cheers, Tal</p>
"
3840211,168747,2010-10-01T14:41:11Z,2597310,4,FALSE,"<p>I assume you use standard way to plot tree which is </p>

<pre><code>plot(f)
text(f)
</code></pre>

<p>As you can read in help to <code>text.rpart</code>, argument <code>pretty</code> on default factor variables are presented as letters, so <code>a</code> means <code>levels(Climate)[1]</code> and it means that on left node are observation with <code>Climate==levels(Climate)[1]</code> and on right the others.</p>

<p>You could print levels directly using </p>

<pre><code>plot(f)
text(f, pretty=1)
</code></pre>

<p><img src=""https://i.stack.imgur.com/VPfZ1.png"" alt=""Created by rpart""></p>

<p>but I recommend using <code>draw.tree</code> from <a href=""http://cran.r-project.org/web/packages/maptree/index.html"" rel=""nofollow noreferrer"">maptree package</a>:</p>

<pre><code>require(maptree)
draw.tree(f)
</code></pre>

<p><img src=""https://i.stack.imgur.com/QF34N.png"" alt=""Created by maptree""></p>

<p>I used fake data to do plots:</p>

<pre><code>X &lt;- data.frame(
    y=rep(1:4,25),
    Climate=rep(c(""Tropical"", ""Arid"", ""Temperate"", ""Snow""),25)
)
f &lt;- rpart(y~Climate, X)
</code></pre>
"
3871107,322912,2010-10-06T09:27:01Z,2001625,2,FALSE,"<p>If you open help pages for <code>Polygon</code> or <code>SpatialPolygons</code> (doesn't really matter, it will lead you to the same page), you can click on <code>SpatialPolygons-class</code> and <code>Polygons-class</code> links. There, you can see which slots each class contains.</p>

<p><code>Polygon-class</code> as an example:</p>

<pre><code>Slots

ringDir:
    Object of class ""integer""; the ring direction of the ring (polygon) coordinates, holes are expected to be anti-clockwise 
labpt:
    Object of class ""numeric""; an x, y coordinate pair forming the label point of the polygon 
area:
    Object of class ""numeric""; the area of the polygon 
hole:
    Object of class ""logical""; does the polygon seem to be a hole 
coords:
    Object of class ""matrix""; coordinates of the polygon; first point should equal the last point 
</code></pre>
"
3871730,455865,2010-10-06T10:55:22Z,1433523,1,FALSE,"<p>I am an Rweka user, and I can tell you it is amazingly quick, it outperforms weka alone, while using it's functions in the r environment. I think that the R package has a very special way to integrate inside the language java libraries, nevertheless these libraries need to be prepared to allow this. For being able to do a proper integration you will need to do an important amount of research in order to see how to make things fit properly. I recommend you to read the documentation that comes with R, which details which are the best practices for writing NEW LIBRARIES libraries.</p>
"
3878545,468660,2010-10-07T03:52:38Z,1649503,0,FALSE,"<p>I found this as the only sensible, short discussion of how to go back and forth from R objects and python. naufraghi's solution prompted the following approach to converting a data.frame, which retains the nicer slicing capabilities of the dataframe:</p>

<pre><code>In [69]: import numpy as np

In [70]: import rpy2.robjects as ro

In [71]: df = ro.r['data.frame'](a=r.c(1,2,3), b=r.c(4.0,5.0,6.3))

In [72]: df
Out[72]: &lt;RDataFrame - Python:0x5492200 / R:0x4d00a28&gt;

In [73]: print(df)
  a   b
1 1 4.0
2 2 5.0
3 3 6.3

In [74]: recdf = np.rec.fromarrays(df, names=tuple(df.names))

In [75]: recdf
Out[75]: 
rec.array([(1, 4.0), (2, 5.0), (3, 6.2999999999999998)], 
      dtype=[('a', '&lt;i4'), ('b', '&lt;f8')])
</code></pre>

<p>Seems a bit off-topic at this point, but I'm not sure what the appropriate procedure would be to capture this question &amp; answer of mine!</p>
"
3935429,134830,2010-10-14T16:34:09Z,2631780,24,FALSE,"<p>You have to manually choose the number of characters to wrap at, but the combination of <code>strwrap</code> and <code>paste</code> will do what you want.</p>

<pre><code>wrapper &lt;- function(x, ...) 
{
  paste(strwrap(x, ...), collapse = ""\n"")
}

my_title &lt;- ""This is a really long title of a plot that I want to nicely wrap and fit onto the plot without having to manually add the backslash n, but at the moment it does not""
r + 
  geom_smooth() + 
  opts(title = wrapper(my_title, width = 20))
</code></pre>
"
3935554,199217,2010-10-14T16:52:58Z,1249548,311,TRUE,"<h3>Any ggplots side-by-side (or n plots on a grid)</h3>

<p>The function <code>grid.arrange()</code> in the <a href=""https://cran.r-project.org/web/packages/gridExtra/index.html"" rel=""noreferrer""><code>gridExtra</code></a> package will combine multiple plots; this is how you put two side by side. </p>

<pre><code>require(gridExtra)
plot1 &lt;- qplot(1)
plot2 &lt;- qplot(1)
grid.arrange(plot1, plot2, ncol=2)
</code></pre>

<p>This is useful when the two plots are not based on the same data, for example if you want to plot different variables without using reshape(). </p>

<p>This will plot the output as a side effect. To print the side effect to a file, specify a device driver (such as <code>pdf</code>, <code>png</code>, etc), e.g.</p>

<pre><code>pdf(""foo.pdf"")
grid.arrange(plot1, plot2)
dev.off()
</code></pre>

<p>or, use <code>arrangeGrob()</code> in combination with <code>ggsave()</code>,</p>

<pre><code>ggsave(""foo.pdf"", arrangeGrob(plot1, plot2))
</code></pre>

<p>This is the equivalent of making two distinct plots using <code>par(mfrow = c(1,2))</code>. This not only saves time arranging data, it is necessary when you want two dissimilar plots.</p>

<hr>

<h3>Appendix: Using Facets</h3>

<p>Facets are helpful for making similar plots for different groups. This is pointed out below in many answers below, but I want to highlight this approach with examples equivalent to the above plots. </p>

<pre><code>mydata &lt;- data.frame(myGroup = c('a', 'b'), myX = c(1,1))

qplot(data = mydata, 
    x = myX, 
    facets = ~myGroup)

ggplot(data = mydata) + 
    geom_bar(aes(myX)) + 
    facet_wrap(~myGroup)
</code></pre>

<hr>

<h3>Update</h3>

<p>the <code>plot_grid</code> function in the <a href=""https://cran.r-project.org/web/packages/cowplot/index.html"" rel=""noreferrer""><code>cowplot</code></a> is worth checking out as an alternative to <code>grid.arrange</code>. See the <a href=""https://stackoverflow.com/a/31223588/199217"">answer</a> by @claus-wilke below and <a href=""https://stackoverflow.com/a/31223588/199217"">this vignette</a> for an equivalent approach; but the function allows finer controls on plot location and size, based on <a href=""https://stackoverflow.com/a/31223588/199217"">this vignette</a>. </p>
"
3977156,428790,2010-10-20T11:16:00Z,1401904,59,TRUE,"<p>Just for completeness, there are some ways to prevent you from having this problem. As Dirk said, save your packages in another directory on your computer. </p>

<pre><code>install.packages(""thepackage"",lib=""/path/to/directory/with/libraries"")
</code></pre>

<p>You can change the default <code>.Library</code> value using the function <code>.libPaths</code> too</p>

<pre><code>.libPaths(""/path/to/directory/with/libraries"")
</code></pre>

<p>This will put this path as a first value in the <code>.Library</code> variable, and will make it the default.</p>

<p>If you want to automate this further, you can specify this in the Rprofile.site file, which you find in the /etc/ directory of your R build. Then it will load automatically every time R loads, and you don't have to worry about that any more. You can just install and load packages from the specified directory.</p>

<p>Finally, I have some small code included in my Rprofile.site allowing me to reinstall all packages when I install a new R version. You just have to list them up <strong>before</strong> you update to the new R version. I do that using an .RData file containing an updated list with all packages.</p>

<pre><code>library(utils)

## Check necessary packages
load(""G:\Setinfo\R\packagelist.RData"") # includes a vector ""pkgs""
installed &lt;- pkgs %in% installed.packages()[, 'Package']
if (length(pkgs[!installed]) &gt;=1){
  install.packages(pkgs[!installed])
}
</code></pre>

<p>I make the packagelist.RData by specifying <code>.Last()</code> in my Rprofile.site. This updates the package list if I installed some :</p>

<pre><code>.Last &lt;- function(){
  pkgs &lt;- installed.packages()[,1]
  if (length(pkgs) &gt; length(installed)){
    save(pkgs,file=""G:\Setinfo\R\packagelist.RData"")
  }
}
</code></pre>

<p>When I install a new R version, I just add the necessary elements to the Rprofile.site file and all packages are reinstalled. I have to adjust the Rprofile.site anyway (using sum contrasts, adding the extra code for Tinn-R, these things), so it's not really extra work. It just takes extra time installing all packages anew.</p>

<p>This last bit is equivalent to what is given in the original question as a solution. I just don't need to worry about getting the ""installed"" list first.</p>

<p>Again, this doesn't work flawless if you have packages that are not installed from CRAN. But this code is easily extendible to include those ones too.</p>

<p>Edit: There was a missing parenthesis in the code</p>
"
3983656,481927,2010-10-21T01:35:22Z,2312913,0,FALSE,"<p>Last week I coded up such an estimate-the-number-of-clusters algorithm for a K-Means clustering program. I used the method outlined in:</p>

<p><a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.9687&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.9687&amp;rep=rep1&amp;type=pdf</a></p>

<p>My biggest implementation problem was that I had to find a suitable Cluster Validation Index (ie error metric) that would work. Now it is a matter of processing speed, but the results currently look reasonable.</p>
"
4010172,485832,2010-10-24T20:04:43Z,1377003,3,FALSE,"<p>If you really want the density function, why not use it directly:</p>

<pre><code>$pi = 3.141593;
$x = 2.02;
$mean = 2;
$sd = .24;
print 1/($sd * sqrt(2*$pi)) * exp(-($x-$mean)**2 / (2 * $sd**2));
</code></pre>

<p>It gives 1.65649768474891 about the same as dnorm in R.</p>
"
4117662,351825,2010-11-07T12:17:01Z,2349820,0,FALSE,"<p>Another option is using the <a href=""http://data-sorcery.org/about/"" rel=""nofollow"">Incanter</a> library for Clojure. It's an R-like library in Clojure, pretty fast due to the (pure java) Parallel Colt numerics library, with built-in JFreeChart support for quick charting as well as a wrapper for using Processing through Clojure.</p>

<p>You could just push the resulting app to the web as a normal (though probably pretty big) Java Applet.</p>
"
4205784,483620,2010-11-17T15:11:34Z,2471750,1,FALSE,"<p>I think there might be a bit of confusion about what a boxplot does/is. While it is possible to create groups on the x axis, as far as I know, the y axis shows the distribution of a certain measure (I assume either col3 or col4, in your case), not the RMSE or MBE of those measurements, which would be a single value for each group.</p>

<p>I am not sure if your grouping variable (for the x axis) is col5, the files or the criteria you list for col2, or all of them? Regardless, you would need more data for the plots to be meaningful.</p>

<p>This is a basic example of a boxplot of col3 grouped by col5 and file:</p>

<pre><code>col3 = c(56.625, 50.625, 65.875, 52.875, 70, 67.750, 65.750, 56.625, 50.625, 65.875, 52.875, 70, 67.750, 65.750)
col5 = c(""RED"", ""GREEN"", ""BLUE"", ""RED"", ""BLUE"", ""RED"", ""GREEN"", ""RED"", ""GREEN"", ""BLUE"",""RED"",""BLUE"",""RED"",""GREEN"")
myfile = c(1,1,1,1,1,1,1,2,2,2,2,2,2,2)
mydata = data.frame(col3, col5, myfile)
boxplot(col3 ~ col5 + myfile, data = mydata)
</code></pre>

<p>Note that because the number of cases is limited, you do not see the whiskers on some categories, nor the outliers. You would need more data for this plot to be useful, right now all it is showing is a comparison of medians.</p>

<p>Can you clarify what you were hoping the plot would show?</p>
"
4284931,322912,2010-11-26T11:37:26Z,1195826,414,FALSE,"<p>Since R version 2.12, there's a <code>droplevels()</code> function.</p>

<pre><code>levels(droplevels(subdf$letters))
</code></pre>
"
4285037,322912,2010-11-26T11:50:47Z,2568234,3,FALSE,"<p>I think there's a better solution (I wrote it for a workshop a few days ago), but it slipped my mind. Here's an ugly substitute with base graphics. Feel free to annotate the x axis <em>ad libitum</em>. Personally, I like Greg's solution.</p>

<pre><code>plot(0, 0, xlim = c(1, 4), ylim = range(casp6), type = ""n"")
points(casp6 ~ trans.factor)
</code></pre>
"
4294254,522575,2010-11-27T22:27:44Z,2678141,5,FALSE,"<p>See grid level editing for another option:</p>

<p><a href=""https://github.com/hadley/ggplot2/wiki/Editing-raw-grid-objects-from-a-ggplot"" rel=""noreferrer"">https://github.com/hadley/ggplot2/wiki/Editing-raw-grid-objects-from-a-ggplot</a></p>
"
4360358,1855677,2010-12-05T18:15:05Z,1358003,39,FALSE,"<p>I make aggressive use of the <code>subset</code> parameter with selection of only the required variables when passing dataframes to the <code>data=</code> argument of regression functions. It does result in some errors if I forget to add variables to both the formula and the <code>select=</code> vector, but it still saves a lot of time due to decreased copying of objects and reduces the memory footprint significantly. Say I have 4 million records with 110 variables (and I do.) Example:</p>

<pre><code># library(rms); library(Hmisc) for the cph,and rcs functions
Mayo.PrCr.rbc.mdl &lt;- 
cph(formula = Surv(surv.yr, death) ~ age + Sex + nsmkr + rcs(Mayo, 4) + 
                                     rcs(PrCr.rat, 3) +  rbc.cat * Sex, 
     data = subset(set1HLI,  gdlab2 &amp; HIVfinal == ""Negative"", 
                           select = c(""surv.yr"", ""death"", ""PrCr.rat"", ""Mayo"", 
                                      ""age"", ""Sex"", ""nsmkr"", ""rbc.cat"")
   )            )
</code></pre>
"
4365087,531933,2010-12-06T09:49:58Z,2477398,1,FALSE,"<p>My work-around was to copy the package from my personal library (%USERPROFILE%\Documents\R) to the global library (%R_HOME%\library).</p>

<p>It's not the best because this requires Administrator privileges which not all users will have...</p>
"
4420854,539416,2010-12-12T08:31:59Z,520810,5,FALSE,"<pre><code>qw = function(s) unlist(strsplit(s,' '))
</code></pre>
"
4485352,107409,2010-12-19T21:58:45Z,659725,1,FALSE,"<p>Here is my 2 cents: SQL server does not scale well. We attempted to use SQL server to store financial data in real time (i.e. prices ticks coming in for 100 symbols). It worked perfectly for the first 2 weeks - then it went slower and slower as the database size increased, and finally ground to a halt, too slow to insert each price as it was received. We tried to work around it by moving data from the active database to offline storage every night, but ultimately the project was abandoned as it just didn't work.</p>

<p>Bottom line: if you're planning on storing a lot of data ( >1GB) you need something that scales properly, and that probably means a column database.</p>
"
4486860,548147,2010-12-20T04:24:57Z,1330989,43,FALSE,"<p>To make the text on the tick labels fully visible and read in the same direction as the y-axis label, change the last line to </p>

<pre><code>q + theme(axis.text.x=element_text(angle=90, hjust=1))
</code></pre>
"
4551018,1855677,2010-12-29T02:16:07Z,2394902,4,TRUE,"<p>You can avoid creating ""labelled"" variables in spss.get with the argument: , use.value.labels=FALSE.</p>

<pre><code>w &lt;- spss.get('/tmp/my.sav', use.value.labels=FALSE, datevars=c('birthdate','deathdate'))
</code></pre>

<p>The code from Bhattacharya could fail if the class of the labelled vector were simply ""labelled"" rather than c(""labelled"", ""factor"") in which case it should have been:</p>

<pre><code>class(x[[i]]) &lt;- NULL  # no error from assignment of empty vector
</code></pre>

<p>The error you report can be reproduced with this code:</p>

<pre><code>&gt; b &lt;- 4:6
&gt; label(b) &lt;- 'B Label'
&gt; str(b)
Class 'labelled'  atomic [1:3] 4 5 6
  ..- attr(*, ""label"")= chr ""B Label""
&gt; class(b) &lt;- class(b)[-1]
Error in class(b) &lt;- class(b)[-1] : 
  invalid replacement object to be a class string
</code></pre>
"
4552030,180892,2010-12-29T06:40:43Z,1429907,13,FALSE,"<p>For creating custom reports, I've found it useful to incorporate many of the existing tips  suggested here.</p>

<p><strong>Generating reports:</strong>
A good strategy for generating reports involves the combination of Sweave, make, and R.</p>

<p><strong>Editor:</strong> 
Good editors for preparing Sweave documents include:</p>

<ul>
<li>StatET and Eclipse</li>
<li>Emacs and ESS</li>
<li>Vim and Vim-R</li>
<li>R Studio</li>
</ul>

<p><strong>Code organisation:</strong>
In terms of code organisation, I find two strategies useful:</p>

<ul>
<li>Read up about analysis workflow (e.g., <a href=""http://www.johnmyleswhite.com/notebook/2010/08/26/projecttemplate/"" rel=""nofollow noreferrer"">ProjectTemplate</a>, 
Josh Reich's ideas, my own presentation on R workflow
<a href=""http://jeromyanglim.blogspot.com/2010/12/r-workflow-slides-from-talk-at.html"" rel=""nofollow noreferrer"">Slides</a>
and <a href=""http://jeromyanglim.blogspot.com/2010/12/video-of-reproducible-research-with-r.html"" rel=""nofollow noreferrer"">Video</a> )</li>
<li>Study example reports and discern the workflow

<ul>
<li><a href=""https://stackoverflow.com/questions/1429907/workflow-for-statistical-analysis-and-report-writing/1430569#1430569"">Hadley Wickham's examples</a></li>
<li><a href=""https://github.com/jeromyanglim"" rel=""nofollow noreferrer"">My examples on github</a></li>
<li><a href=""https://stats.stackexchange.com/questions/1980/complete-substantive-examples-of-reproducible-research-using-r"">Examples of reproducible research listed on Cross Validated</a></li>
</ul></li>
</ul>
"
4552176,180892,2010-12-29T07:11:18Z,2712421,14,FALSE,"<p>For the sake of completeness, I thought I'd provide an update on my adoption of version control.</p>

<p>I have found version control for solo data analysis projects to be very useful.</p>

<p>I've adopted git as my main version control tool. I first starteed using Egit within Eclipse with StatET. Now I generally just use the command-line interface, although integration with RStudio is quite good.</p>

<p>I've blogged about my experience <a href=""http://jeromyanglim.blogspot.com/2010/11/getting-started-with-git-egit-eclipse.html"" rel=""noreferrer"">getting set up with version control</a> from the perspective of data analysis projects.</p>

<p>As stated in the post, I've found adopting version control has had many secondary benefits in how I think about data analysis projects including clarifying:</p>

<ul>
<li>the distinction between source and derived files</li>
<li>the nature of dependencies:

<ul>
<li>dependencies between elements of code</li>
<li>dependencies between files within a project</li>
<li>and dependencies with files and programs external to the repository</li>
</ul></li>
<li>the nature of a repository and how repositories should be divided</li>
<li>the nature of committing and documenting changes and project milestones</li>
</ul>
"
4606214,NA,2011-01-05T16:04:07Z,2603184,4,FALSE,"<p>Actually the <strong>R.oo</strong> package emulates the pass-by-reference behaviour by using environments.</p>
"
4639626,568812,2011-01-09T14:19:13Z,2151212,4,FALSE,"<p>In bash, you can construct a command line like the following:<BR></p>

<pre><code>$ z=10
$ echo $z
10
$ Rscript -e ""args&lt;-commandArgs(TRUE);x=args[1]:args[2];x;mean(x);sd(x)"" 1 $z
 [1]  1  2  3  4  5  6  7  8  9 10
[1] 5.5
[1] 3.027650
$
</code></pre>

<p>You can see that the variable <code>$z</code> is substituted by bash shell with ""10"" and this value is picked up by <code>commandArgs</code> and fed into <code>args[2]</code>, and the range command <code>x=1:10</code> executed by R successfully, etc etc.</p>
"
4685169,439792,2011-01-13T20:58:21Z,652136,23,FALSE,"<p>Here is how the remove the last element of a <strong>list</strong> in R:</p>

<pre><code>x &lt;- list(""a"", ""b"", ""c"", ""d"", ""e"")
x[length(x)] &lt;- NULL
</code></pre>

<p>If x might be a vector then you would need to create a new object:</p>

<pre><code>x &lt;- c(""a"", ""b"", ""c"", ""d"", ""e"")
x &lt;- x[-length(x)]
</code></pre>

<ul>
<li>Work for <strong>lists</strong> and <strong>vectors</strong></li>
</ul>
"
4744682,399470,2011-01-20T07:57:14Z,2258784,56,TRUE,"<p>The closest thing to a comprehensive list that I have been able to find is in the <a href=""https://github.com/hadley/ggplot2/wiki/-opts%28%29----theme%28%29-List"" rel=""nofollow"" title=""ggplot2 wiki"">ggplot2 wiki</a> on github. I think that most of the options are covered there with examples.</p>

<p><strong>Update</strong>
If you would like to see the options that are in use on a current plot, you can use <code>plot_theme(x)</code> to see all of the options that are currently set for the plot named x. It will not be a comprehensive list, but should help if you want to change something like the font size for an axis label.</p>

<p><strong>Update 2</strong>
With the transition to version 0.9.0 it's worth noting that the built in documentation has been dramatically improved, and the <a href=""https://groups.google.com/forum/?fromgroups#!topic/ggplot2-dev/iiS6tMF0fhk"" rel=""nofollow"">transition guide</a> from 0.8.9 to 0.9.0 is also worth checking out.</p>

<p><strong>Update 3</strong>
There is now a ggplot2 documentation website. Look at the documentation for <a href=""http://docs.ggplot2.org/current/theme.html"" rel=""nofollow"">theme</a> for a complete list. Also, <code>?theme</code> has a pretty complete list as of 0.9.3.</p>

<p><strong>Update 4</strong>
There is now a ggthemes package that has some nice themes and scales to choose from. It might save you from having to create your own. See their <a href=""https://github.com/jrnold/ggthemes"" rel=""nofollow"">github</a> page for more information.</p>
"
4761634,584763,2011-01-21T17:08:48Z,2464721,2,FALSE,"<p>I got to this page trying to work out why my JDK was reporting 64 bit despite the PATH and JAVA_HOME were pointing to 32 bit.</p>

<p>I dont even know what R is, but this article might help (it solved it for me)</p>

<p><a href=""http://www.tipandtrick.net/2008/how-to-open-and-run-32-bit-command-prompt-in-64-bit-x64-windows/"" rel=""nofollow"">http://www.tipandtrick.net/2008/how-to-open-and-run-32-bit-command-prompt-in-64-bit-x64-windows/</a></p>

<p>In a nutshell, dont run from 'cmd' use '%windir%\SysWoW64\cmd.exe' instead.
Or, put your JDK at the front of the path instead of the end (I dont think this is ideal).</p>
"
4767111,564164,2011-01-22T09:28:47Z,2232699,8,FALSE,"<p>I think using the <code>base::merge</code> function is not needed, as using <code>data.table</code> joins can be a lot faster. E.g. see the following. I make <code>x</code> and <code>y</code> data.tables with 3-3 columns:</p>

<pre><code>&gt; x &lt;- data.table( foo = 1:5, a=20:24, zoo = 5:1 )
&gt; y &lt;- data.table( foo = 1:5, b=30:34, boo = 10:14)
&gt; setkey(x, foo)
&gt; setkey(y, foo)
</code></pre>

<p>And merge both with <code>base:merge</code> and <code>data.table</code> joins to see the speed of executions:</p>

<pre><code>&gt; system.time(merge(x,y))
   user  system elapsed 
  0.027   0.000   0.023 
&gt; system.time(x[,list(y,x)])
   user  system elapsed 
  0.003   0.000   0.006 
</code></pre>

<p>The results are not identical, as the latter has one extra column:</p>

<pre><code>&gt; merge(x,y)
     foo  a zoo  b boo
[1,]   1 20   5 30  10
[2,]   2 21   4 31  11
[3,]   3 22   3 32  12
[4,]   4 23   2 33  13
[5,]   5 24   1 34  14
&gt; x[,list(x,y)]
     foo  a zoo foo.1  b boo
[1,]   1 20   5     1 30  10
[2,]   2 21   4     2 31  11
[3,]   3 22   3     3 32  12
[4,]   4 23   2     4 33  13
[5,]   5 24   1     5 34  14
</code></pre>

<p>Which could not make a big trouble :)</p>
"
4793908,589031,2011-01-25T13:10:21Z,1296646,30,FALSE,"<p>Suppose you have a <code>data.frame</code> <code>A</code> and you want to sort it using column called <code>x</code> descending order. Call the sorted <code>data.frame</code> <code>newdata</code></p>

<pre><code>newdata &lt;- A[order(-A$x),]
</code></pre>

<p>If you want ascending order then replace <code>""-""</code> with nothing. You can have something like </p>

<pre><code>newdata &lt;- A[order(-A$x, A$y, -A$z),]
</code></pre>

<p>where <code>x</code> and <code>z</code> are some columns in <code>data.frame</code> <code>A</code>. This means sort <code>data.frame</code> <code>A</code> by <code>x</code> descending, <code>y</code> ascending and <code>z</code> descending.</p>
"
4801972,420385,2011-01-26T06:45:22Z,2161152,3,FALSE,"<p>I've restructured Matti's Pweave a bit, so that it is possible to define arbitrary ""chunk-processors"" as plugin-modules.  This makes it easy to extend for several chunk-based text-preprocessing applications.  The restructured version is available at <a href=""https://bitbucket.org/edgimar/pweave/src"" rel=""nofollow"">https://bitbucket.org/edgimar/pweave/src</a>.  As an example, you could write the following LaTeX-Pweave document (notice the ""processor name"" in this example is specified with the name 'mplfig'):</p>

<pre><code>\documentclass[a4paper]{article}
\usepackage{graphicx}
\begin{document}
\title{Test document}
\maketitle

Don't miss the great information in Figure \ref{myfig}!


&lt;&lt;p=mplfig, label=myfig, caption = ""Figure caption...""&gt;&gt;=
import sys
import pylab as pl

pl.plot([1,2,3,4,5],['2,4,6,8,10'], 'b.', markersize=15)
pl.axis('scaled')
pl.axis([-3,3, -3,3]) # [xmin,xmax, ymin,ymax]
@

\end{document}
</code></pre>
"
4827843,300123,2011-01-28T11:46:53Z,1358003,85,FALSE,"<p>Saw this on a twitter post and think it's an awesome function by Dirk! Following on from JD Long's answer, I would do this for user friendly reading:</p>

<pre><code># improved list of objects
.ls.objects &lt;- function (pos = 1, pattern, order.by,
                        decreasing=FALSE, head=FALSE, n=5) {
    napply &lt;- function(names, fn) sapply(names, function(x)
                                         fn(get(x, pos = pos)))
    names &lt;- ls(pos = pos, pattern = pattern)
    obj.class &lt;- napply(names, function(x) as.character(class(x))[1])
    obj.mode &lt;- napply(names, mode)
    obj.type &lt;- ifelse(is.na(obj.class), obj.mode, obj.class)
    obj.prettysize &lt;- napply(names, function(x) {
                           capture.output(format(utils::object.size(x), units = ""auto"")) })
    obj.size &lt;- napply(names, object.size)
    obj.dim &lt;- t(napply(names, function(x)
                        as.numeric(dim(x))[1:2]))
    vec &lt;- is.na(obj.dim)[, 1] &amp; (obj.type != ""function"")
    obj.dim[vec, 1] &lt;- napply(names, length)[vec]
    out &lt;- data.frame(obj.type, obj.size, obj.prettysize, obj.dim)
    names(out) &lt;- c(""Type"", ""Size"", ""PrettySize"", ""Rows"", ""Columns"")
    if (!missing(order.by))
        out &lt;- out[order(out[[order.by]], decreasing=decreasing), ]
    if (head)
        out &lt;- head(out, n)
    out
}

# shorthand
lsos &lt;- function(..., n=10) {
    .ls.objects(..., order.by=""Size"", decreasing=TRUE, head=TRUE, n=n)
}

lsos()
</code></pre>

<p>Which results in something like the following:</p>

<pre><code>                      Type   Size PrettySize Rows Columns
pca.res                 PCA 790128   771.6 Kb    7      NA
DF               data.frame 271040   264.7 Kb  669      50
factor.AgeGender   factanal  12888    12.6 Kb   12      NA
dates            data.frame   9016     8.8 Kb  669       2
sd.                 numeric   3808     3.7 Kb   51      NA
napply             function   2256     2.2 Kb   NA      NA
lsos               function   1944     1.9 Kb   NA      NA
load               loadings   1768     1.7 Kb   12       2
ind.sup             integer    448  448 bytes  102      NA
x                 character     96   96 bytes    1      NA
</code></pre>

<p>NOTE: The main part I added was (again, adapted from JD's answer) :</p>

<pre><code>obj.prettysize &lt;- napply(names, function(x) {
                           capture.output(print(object.size(x), units = ""auto"")) })
</code></pre>

<p>I couldn't think of any other way to get the output from print(...) and so used capture.output(), which I'm sure is very inefficient :)</p>
"
4831793,210673,2011-01-28T18:34:34Z,1741820,24,FALSE,"<p>According to John Chambers, the operator <code>=</code> is only allowed at ""the top level,"" which means it is not allowed in control structures like <code>if</code>, making the following programming error illegal.</p>

<pre><code>&gt; if(x = 0) 1 else x
Error: syntax error
</code></pre>

<p>As he writes, ""Disallowing the new assignment form [=] in control expressions avoids programming errors (such as the example above) that are more likely with the equal operator than with other S assignments.""</p>

<p>You can manage to do this if it's ""isolated from surrounding logical structure, by braces or an extra pair of parentheses,"" so <code>if ((x = 0)) 1 else x</code> would work.</p>

<p>See <a href=""http://developer.r-project.org/equalAssign.html"" rel=""noreferrer"">http://developer.r-project.org/equalAssign.html</a></p>
"
4859921,180892,2011-02-01T07:16:01Z,2462708,1,FALSE,"<p>There are several R plugins for vim.</p>

<p>Check out <a href=""http://www.vim.org/scripts/script.php?script_id=2628"" rel=""nofollow"">Vim-R-Plugin</a>. It supports syntax highlighting.</p>
"
4862334,217595,2011-02-01T12:06:50Z,743812,161,FALSE,"<p>Or you can simply calculate it using filter, here's the function I use:</p>

<p><code>ma &lt;- function(x,n=5){filter(x,rep(1/n,n), sides=2)}</code></p>
"
4864005,429739,2011-02-01T14:52:09Z,1358003,20,FALSE,"<p>For both speed and memory purposes, when building a large data frame via some complex series of steps, I'll periodically flush it (the in-progress data set being built) to disk, appending to anything that came before, and then restart it. This way the intermediate steps are only working on smallish data frames (which is good as, e.g., <em>rbind</em> slows down considerably with larger objects). The entire data set can be read back in at the end of the process, when all the intermediate objects have been removed.</p>

<pre><code>dfinal &lt;- NULL
first &lt;- TRUE
tempfile &lt;- ""dfinal_temp.csv""
for( i in bigloop ) {
    if( !i %% 10000 ) { 
        print( i, ""; flushing to disk..."" )
        write.table( dfinal, file=tempfile, append=!first, col.names=first )
        first &lt;- FALSE
        dfinal &lt;- NULL   # nuke it
    }

    # ... complex operations here that add data to 'dfinal' data frame  
}
print( ""Loop done; flushing to disk and re-reading entire data set..."" )
write.table( dfinal, file=tempfile, append=TRUE, col.names=FALSE )
dfinal &lt;- read.table( tempfile )
</code></pre>
"
4875860,321622,2011-02-02T14:53:11Z,2275896,24,FALSE,"<p>I've written elsewhere that an example like Shane's doesn't really stress the difference in performance among the various kinds of looping syntax because the time is all spent within the function rather than actually stressing the loop.  Furthermore, the code unfairly compares a for loop with no memory with apply family functions that return a value.  Here's a slightly different example that emphasizes the point.</p>

<pre><code>foo &lt;- function(x) {
   x &lt;- x+1
 }
y &lt;- numeric(1e6)
system.time({z &lt;- numeric(1e6); for(i in y) z[i] &lt;- foo(i)})
#   user  system elapsed 
#  4.967   0.049   7.293 
system.time(z &lt;- sapply(y, foo))
#   user  system elapsed 
#  5.256   0.134   7.965 
system.time(z &lt;- lapply(y, foo))
#   user  system elapsed 
#  2.179   0.126   3.301 
</code></pre>

<p>If you plan to save the result then apply family functions can be <em>much</em> more than syntactic sugar. </p>

<p>(the simple unlist of z is only 0.2s so the lapply is much faster.  Initializing the z in the for loop is quite fast because I'm giving the average of the last 5 of 6 runs so moving that outside the system.time would hardly affect things)</p>

<p>One more thing to note though is that there is another reason to use apply family functions independent of their performance, clarity, or lack of side effects. A <code>for</code> loop typically promotes putting as much as possible within the loop. This is because each loop requires setup of variables to store information (among other possible operations). Apply statements tend to be biased the other way. Often times you want to perform multiple operations on your data, several of which can be vectorized but some might not be able to be. In R, unlike other languages, it is best to separate those operations out and run the ones that are not vectorized in an apply statement (or vectorized version of the function) and the ones that are vectorized as true vector operations. This often speeds up performance tremendously.</p>

<p>Taking Joris Meys example where he replaces a traditional for loop with a handy R function we can use it to show the efficiency of writing code in a more R friendly manner for a similar speedup without the specialized function.</p>

<pre><code>set.seed(1)  #for reproducability of the results

# The data - copied from Joris Meys answer
X &lt;- rnorm(100000)
Y &lt;- as.factor(sample(letters[1:5],100000,replace=T))
Z &lt;- as.factor(sample(letters[1:10],100000,replace=T))

# an R way to generate tapply functionality that is fast and 
# shows more general principles about fast R coding
YZ &lt;- interaction(Y, Z)
XS &lt;- split(X, YZ)
m &lt;- vapply(XS, mean, numeric(1))
m &lt;- matrix(m, nrow = length(levels(Y)))
rownames(m) &lt;- levels(Y)
colnames(m) &lt;- levels(Z)
m
</code></pre>

<p>This winds up being much faster than the <code>for</code> loop and just a little slower than the built in optimized <code>tapply</code> function. It's not because <code>vapply</code> is so much faster than <code>for</code> but because it is only performing one operation in each iteration of the loop. In this code everything else is vectorized. In Joris Meys traditional <code>for</code> loop many (7?) operations are occurring in each iteration and there's quite a bit of setup just for it to execute. Note also how much more compact this is than the <code>for</code> version.</p>
"
4915855,520172,2011-02-06T20:22:24Z,1475360,1,FALSE,"<p>Why don't you try <code>quantmod::Lag</code> function for generating a matrix consisting of various lagged-series of a series, at different lag values? For example</p>

<pre><code>&gt; quantmod::Lag (1:10, k=c(0,5,2))
</code></pre>

<p>will return</p>

<blockquote>
<pre><code>Lag.0 Lag.5 Lag.2
 [1,]     1    NA    NA
 [2,]     2    NA    NA
 [3,]     3    NA     1
 [4,]     4    NA     2
 [5,]     5    NA     3
 [6,]     6     1     4
 [7,]     7     2     5
 [8,]     8     3     6
 [9,]     9     4     7
[10,]    10     5     8
</code></pre>
</blockquote>
"
4935202,261210,2011-02-08T15:51:58Z,2399027,1,FALSE,"<p>I had a similar error and had to do an additional fix: Setting the R path explicitly to ...bin\x64, and also being consistent in using x64 Java and R.</p>
"
4943503,609546,2011-02-09T10:21:17Z,2646402,1,FALSE,"<pre><code> y &lt;- new.env()
 with(y, x &lt;- 1)
 f &lt;- function(env,z) {
    with(env, x+z)
 }
 f(y,z=1)
</code></pre>

<p>mind the parentheses:) The following will work:</p>

<pre><code>with(env, x)+z
</code></pre>
"
4958649,611554,2011-02-10T14:58:54Z,498932,2,FALSE,"<p>I think Badjer is most of the way there. </p>

<p>If your 600 files are part of a project and the ""Build action"" for each of these is set as content you can add all of these by simply:</p>

<ol>
<li>Going to the setup project</li>
<li>Selecting Add > Project Output</li>
<li>Selecting the project the files belong to from the drop down</li>
<li>Selecting the ""Content Files"" option from the list below </li>
<li>Clicking OK.</li>
</ol>

<p>You can check the files will be added to the appropriate place by going to View > File System on the setup project and checking that the content files output is being added to the correct folder. </p>

<p>The files will be added to the install directory in the same hierarchy as they are specified in the project they belong to. </p>
"
5019242,308375,2011-02-16T16:36:43Z,2572559,0,FALSE,"<p>I have a solution I figured out over time (sorry I haven't checked this in a while)</p>

<pre><code>checkIt &lt;- function(bind) {

    print(bind)

    cmpfun &lt;- function(r) {all(r == heeds.data[bind,23:47,drop=FALSE])}
    brows &lt;- apply(heeds.data[,23:47], 1, cmpfun)

    #print(heeds.data[brows,c(""eval.num"",""fitness"",""green.h.1"",""green.h.2"",""green.v.5"")])
    print(nrow(heeds.data[brows,c(""eval.num"",""fitness"",""green.h.1"",""green.h.2"",""green.v.5"")]))
}
</code></pre>

<p>Note that heeds.data is my actual data frame and I just printed a few columns originally to make sure that it was working (now commented out).  Also, 23:47 is the part that needs to be checked for duplicates</p>

<p>Also, I really haven't learned as much R as I should so I'm open to suggestions.</p>

<p>Hope this helps!</p>
"
5033018,621919,2011-02-17T18:32:08Z,2399027,11,FALSE,"<p>If you have read this threat and neither of the suggestions above has worked so far, then it might be worth trying one further:</p>

<ul>
<li>Windows 7</li>
<li>R version 2.12.1 (2010-12-16) 64-bit </li>
<li><p>Java(TM) SE Runtime Environment (build 1.6.0_23-b05), Java HotSpot(TM) 64-Bit Server VM (build 19.0-b09, mixed mode)</p>

<p><code>JAVA_HOME=C:\Program Files\Java\jre6\bin\</code></p>

<p><code>Path= ...;C:\Program Files\Java\jre6\bin\server\;C:\Program Files\R\R-2.12.1\bin\x64\</code></p></li>
</ul>

<p>The thing that finally solved my problem was to explicitly add <code>\server\</code> to the PATH variable.</p>
"
5046762,402681,2011-02-18T21:27:43Z,2564258,143,FALSE,"<p>You can also use <code>par</code> and plot on the same graph but different axis. Something as follows:</p>

<pre><code>plot( x, y1, type=""l"", col=""red"" )
par(new=TRUE)
plot( x, y2, type=""l"", col=""green"" )
</code></pre>

<p>If you read in detail about <code>par</code> in <code>R</code>, you will be able to generate really interesting graphs.  Another book to look at is Paul Murrel's R Graphics.</p>
"
5062127,597925,2011-02-21T03:53:59Z,2504543,1,FALSE,"<p>Not sure I can help unfortunately but thought I would post as I found this searching for help on this error. What I effectively had was:</p>

<pre><code>a &lt;- cbind(b,c)
d &lt;- merge(a,e)
</code></pre>

<p>And I got that same error. Using <code>a &lt;- data.frame(b,c)</code> fixed the problem, but I can't work out why.</p>

<pre><code>object.size(a);1248124200 bytes

object.size(c);1248124032 bytes
</code></pre>

<p>So something is different. All classes are the same, <code>str()</code> reveals nothing. I'm stumped.</p>

<p>Hopefully that aids someone else in the know.</p>
"
5098167,631251,2011-02-23T22:42:54Z,2436688,0,FALSE,"<pre><code>&gt; LL&lt;-list(1:4)

&gt; LL

[[1]]
[1] 1 2 3 4

&gt; LL&lt;-list(c(unlist(LL),5:9))

&gt; LL

[[1]]
 [1] 1 2 3 4 5 6 7 8 9
</code></pre>
"
5100147,631597,2011-02-24T03:47:37Z,1567718,35,FALSE,"<p>I was wanting the same thing, and remembered <code>library(foo)</code> didn't need quotes, this is what it does:</p>

<pre><code>package &lt;- as.character(substitute(package))
</code></pre>
"
5120504,373025,2011-02-25T17:26:24Z,2003465,1,FALSE,"<p>1) Solve for the derivative polynomial P' to locate your three roots. See <a href=""https://stackoverflow.com/questions/4503849/quadratic-equation-in-ada/4504415#4504415"">there</a> to know how to do it properly. Call those roots a and b (with a &lt; b)</p>

<p>2) For the middle root, use a few steps of bisection between a and b, and when you're close enough, finish with Newton's method.</p>

<p>3) For the min and max root, ""hunt"" the solution. For the max root:</p>

<ul>
<li>Start with x0 = b, x1 = b + (b - a) * lambda, where lambda is a moderate number (say 1.6)</li>
<li>do x_n = b + (x_{n - 1} - a) * lambda until P(x_n) and P(b) have different signs</li>
<li>Perform bisection + newton between x_{n - 1} and x_n</li>
</ul>
"
5138799,636656,2011-02-28T06:15:33Z,1386767,5,FALSE,"<p>The <a href=""http://www.walware.de/goto/statet"" rel=""noreferrer"">StatET plugin</a> for Eclipse, which provides a nice cross-platform IDE for R, LaTeX, and Sweave, has an integrated object browser for R once you suffer through the pain of getting it all set up.</p>
"
5147390,625056,2011-02-28T20:47:26Z,1493969,6,FALSE,"<pre><code>probes &lt;- rep(TRUE, 1000000)
ind &lt;- c(50:100)
val &lt;- rep(FALSE,length(ind))

new.probes &lt;- vector(mode=""logical"",length(probes)+length(val))
new.probes[-ind] &lt;- probes
new.probes[ind] &lt;- val
</code></pre>

<p>Some timings:
My method
   user  system elapsed 
   0.03    0.00    0.03 </p>

<p>Marek method
   user  system elapsed 
   0.18    0.00    0.18 </p>

<p>R append with for loop
   user  system elapsed 
   1.61    0.48    2.10 </p>
"
5251254,377118,2011-03-09T19:42:16Z,2394902,0,FALSE,"<p>Suppose:</p>

<pre><code>library(Hmisc)
w &lt;- spss.get('...')
</code></pre>

<p>You could remove the labels of a variable called ""var1"" by using: </p>

<pre><code>attributes(w$var1)$label &lt;- NULL
</code></pre>

<p>If you also want to remove the class ""labbled"", you could do:</p>

<pre><code>class(w$var1) &lt;- NULL 
</code></pre>

<p>or if the variable has more than one class:</p>

<pre><code>class(w$var1) &lt;- class(w$var1)[-which(class(w$var1)==""labelled"")]
</code></pre>

<p>Hope this helps!</p>
"
5254670,652749,2011-03-10T02:09:58Z,2678141,4,FALSE,"<p>This leaves you only with the data points: </p>

<pre><code>ggplot(out, aes(X1, X2)) + 
    geom_point() + 
    scale_x_continuous(breaks = NULL) + 
    scale_y_continuous(breaks = NULL) + 
    opts(panel.background = theme_blank()) + 
    opts(axis.title.x = theme_blank(), axis.title.y = theme_blank())
</code></pre>
"
5262141,614735,2011-03-10T15:50:08Z,2397097,23,FALSE,"<p>Maybe this could help you:</p>

<pre><code>#bootstrap
set.seed(101)
n &lt;- 1000
x &lt;- rnorm(n, mean=2)
y &lt;- 1.5 + 0.4*x + rnorm(n)
df &lt;- data.frame(x=x, y=y, group=""A"")
x &lt;- rnorm(n, mean=2)
y &lt;- 1.5*x + 0.4 + rnorm(n)
df &lt;- rbind(df, data.frame(x=x, y=y, group=""B""))

#calculating ellipses
library(ellipse)
df_ell &lt;- data.frame()
for(g in levels(df$group)){
df_ell &lt;- rbind(df_ell, cbind(as.data.frame(with(df[df$group==g,], ellipse(cor(x, y), 
                                         scale=c(sd(x),sd(y)), 
                                         centre=c(mean(x),mean(y))))),group=g))
}
#drawing
library(ggplot2)
p &lt;- ggplot(data=df, aes(x=x, y=y,colour=group)) + geom_point(size=1.5, alpha=.6) +
  geom_path(data=df_ell, aes(x=x, y=y,colour=group), size=1, linetype=2)
</code></pre>

<p>Output looks like this:</p>

<p><img src=""https://i.stack.imgur.com/JeiWA.png"" alt=""enter image description here""></p>

<p><a href=""https://stats.stackexchange.com/questions/7899/complex-regression-plot-in-r"">Here</a> is more complex example.</p>
"
5298424,658755,2011-03-14T12:13:40Z,750786,6,FALSE,"<p>If you are interested in parsing command line arguments to an R script try RScript which is bundled with R as of version 2.5.x</p>

<p><a href=""http://stat.ethz.ch/R-manual/R-patched/library/utils/html/Rscript.html"">http://stat.ethz.ch/R-manual/R-patched/library/utils/html/Rscript.html</a></p>
"
5322613,211116,2011-03-16T08:21:31Z,2712421,7,FALSE,"<p>Step back a bit first, and learn the advantages of writing R packages! You say you have projects with several thousand lines of code, yet these aren't structured or documented like package code is? You get big wins with conforming to the package ideals, including documentation for every function, tests for many of the usual hard-to-catch errors, the facility to write test suites of your own etc etc.</p>

<p>If you haven't got the discipline to produce a package, then I'm not sure you've got the discipline to do proper revision control.</p>
"
5373293,597925,2011-03-21T02:38:25Z,1301759,0,FALSE,"<p>NOTE: Check the 5th comment on the answer above. Solution should be </p>

<pre><code>s1$index &lt;- with(s1,ave(value1,state,FUN=seq_along))
s2$index &lt;- with(s2,ave(value2,state,FUN=seq_along))
</code></pre>

<p>Tested and working.</p>
"
5381477,659986,2011-03-21T17:44:48Z,1894190,0,FALSE,"<p>You may also want to look at <a href=""http://sna.stanford.edu/rlabs.php"" rel=""nofollow"">Social Network Analysis Labs in R and SoNIA</a>, specifically the <a href=""http://sna.stanford.edu/lab.php?l=5"" rel=""nofollow"">lab on affiliation data</a>. </p>
"
5412149,662787,2011-03-23T22:00:07Z,2275896,43,FALSE,"<p>...and as I just wrote elsewhere, vapply is your friend!
...it's like sapply, but you also specify the return value type which makes it much faster.</p>

<pre><code>&gt; system.time({z &lt;- numeric(1e6); for(i in y) z[i] &lt;- foo(i)})
   user  system elapsed 
   3.54    0.00    3.53 
&gt; system.time(z &lt;- lapply(y, foo))
   user  system elapsed 
   2.89    0.00    2.91 
&gt; system.time(z &lt;- vapply(y, foo, numeric(1)))
   user  system elapsed 
   1.35    0.00    1.36 
</code></pre>
"
5439071,586927,2011-03-25T23:14:17Z,1395391,1,FALSE,"<p>If you haven't stumbled upon this webpage, it looks useful (I haven't tried it myself...)</p>

<p><a href=""http://blog.revolutionanalytics.com/2009/09/how-to-use-a-google-spreadsheet-as-data-in-r.html"" rel=""nofollow"">http://blog.revolutionanalytics.com/2009/09/how-to-use-a-google-spreadsheet-as-data-in-r.html</a></p>

<p>It looks like the package that used to be used (RGoogleData) is currently being maintained.</p>

<p>Good Luck!</p>
"
5458246,564164,2011-03-28T11:22:49Z,1395391,1,FALSE,"<p>Not a clever solution, but evaluating the formula does work. For example with the following function:</p>

<pre><code>getValues &lt;- function(x) {
    m &lt;- apply(x, 2, function(x) as.character(x))
    for (i in 1:nrow(m)) {
        formulas &lt;- which(substr(m[i,], 1, 4) == ""=RC["")
        t &lt;- sub('=RC[', '', m[i, formulas], fixed=TRUE)
        t &lt;- sub(']', '', t, fixed=TRUE)
        t &lt;- as.numeric(t)
        m[i, formulas] &lt;- m[i, (formulas + t)]
    }
    return(m)
}
</code></pre>

<p><code>getValues(y2009)</code> should return a data frame with all required values. I know this is a quite dumb ""solution"" with lot of compromises but I hope you could code a lot cleaner function for the task! :)</p>
"
5524435,429846,2011-04-02T16:38:38Z,1412775,1,TRUE,"<p>If I understand you correctly, this will do what you want:</p>

<pre><code>## example data
set.seed(1)
judge &lt;- data.frame(judge1 = sample(1:10), judge2 = sample(1:10),
                    judge3 = sample(1:10), judge4 = sample(1:10))
</code></pre>

<p>We compute the ranks for each judge using <code>sapply()</code> which returns a matrix of ranks. Then we use apply <code>rank()</code> on the rows of this matrix to compute the performer/row ranks. A final transpose gets the result back in the required orientation.</p>

<pre><code>&gt; t(apply(sapply(judge, rank), 1, rank))
      judge1 judge2 judge3
 [1,]    1.5    1.5    3.0
 [2,]    3.0    1.5    1.5
 [3,]    1.0    2.5    2.5
 [4,]    2.0    3.0    1.0
 [5,]    1.0    2.0    3.0
 [6,]    2.5    1.0    2.5
 [7,]    3.0    2.0    1.0
 [8,]    3.0    1.0    2.0
 [9,]    3.0    1.0    2.0
[10,]    1.0    3.0    2.0
</code></pre>

<p>Wrap that in a function and you are good to go:</p>

<pre><code>prank &lt;- function(df, ...) {
    t(apply(sapply(df, rank, ...), 1, rank, ...))
}
</code></pre>

<p>Which gives:</p>

<pre><code>&gt; prank(judge)
      judge1 judge2 judge3
 [1,]    1.5    1.5    3.0
 [2,]    3.0    1.5    1.5
 [3,]    1.0    2.5    2.5
 [4,]    2.0    3.0    1.0
 [5,]    1.0    2.0    3.0
 [6,]    2.5    1.0    2.5
 [7,]    3.0    2.0    1.0
 [8,]    3.0    1.0    2.0
 [9,]    3.0    1.0    2.0
[10,]    1.0    3.0    2.0
</code></pre>

<p>The <code>...</code> allows arguments to <code>rank()</code> to be passed in, for example the <code>ties.method</code> argument:</p>

<pre><code>&gt; prank(judge, ties = ""min"")
      judge1 judge2 judge3
 [1,]      1      1      3
 [2,]      3      1      1
 [3,]      1      2      2
 [4,]      2      3      1
 [5,]      1      2      3
 [6,]      2      1      2
 [7,]      3      2      1
 [8,]      3      1      2
 [9,]      3      1      2
[10,]      1      3      2
</code></pre>
"
5544773,662787,2011-04-04T21:51:43Z,1719447,12,FALSE,"<p>The outer function actually DOES work on lists, but the function that you provide gets the two input vectors repeated so that they contain all possible combinations... </p>

<p>As for which is faster, combining outer with vapply is 3x faster than the double for-loop on my machine. If the actual kernel function does ""real work"", the difference in looping speed is probably not so important.</p>

<pre><code>f1 &lt;- function(a,b, fun) {
  outer(a, b, function(x,y) vapply(seq_along(x), function(i) fun(x[[i]], y[[i]]), numeric(1)))
}

f2 &lt;- function(a,b, fun) {
    kernelMatrix &lt;- matrix(0L, length(a), length(b))
    for (i in seq_along(a))
    {
       for (j in seq_along(b))
       {
          kernelMatrix[i,j] = fun(a[[i]], b[[j]])
       }
    }
    kernelMatrix
}

n &lt;- 300
m &lt;- 2
a &lt;- lapply(1:n, function(x) matrix(runif(m*m),m))
b &lt;- lapply(1:n, function(x) matrix(runif(m*m),m))
kernelFunction &lt;- function(x,y) 0 # dummy, so we only measure the loop overhead

&gt; system.time( r1 &lt;- f1(a,b, kernelFunction) )
   user  system elapsed 
   0.08    0.00    0.07 
&gt; system.time( r2 &lt;- f2(a,b, kernelFunction) )
   user  system elapsed 
   0.23    0.00    0.23 
&gt; identical(r1, r2)
[1] TRUE
</code></pre>
"
5570354,695300,2011-04-06T17:16:31Z,1452235,35,TRUE,"<p>A byte code compiler will be part of the R 2.13 release.  By default it is not used in this release but it is available; I expect the 2.14 release will by default byte compile all base and recommended packages. The compiler::compile help page and the <em>R Installation and Administration Manual</em> give some more details.</p>
"
5570426,211116,2011-04-06T17:22:49Z,1452235,6,FALSE,"<p>Why do people get the fear when deploying R? I'm fairly sure I've seen this question before. </p>

<p>Installing R is a piece of cake (you don't actually say which OS you care about). For Windows its one .exe. file, run it, say ""yes"" a few times and its done. I suspect the installer exe probably has flags for unattended installation too. </p>
"
5572986,419132,2011-04-06T21:09:22Z,1677021,0,FALSE,"<p>R is definitely written in C.  I asked myself this question alittle while ago, and resolved it by downloaded the source code from <a href=""http://www.r-project.org/"" rel=""nofollow"">http://www.r-project.org/</a>.</p>
"
5600716,210673,2011-04-08T20:59:44Z,1395105,2,FALSE,"<p>I did this a few years ago by outputting to a .fig format instead of directly to a .pdf; you write the titles including the latex code and use fig2ps or fig2pdf to create the final graphic file.  The setup I had to do this broke with R 2.5; if I had to do it again I'd look into tikz instead, but am including this here anyway as another potential option.</p>

<p>My notes on how I did it using Sweave are here: <a href=""http://www.stat.umn.edu/~arendahl/computing"" rel=""nofollow"">http://www.stat.umn.edu/~arendahl/computing</a></p>
"
5609869,120292,2011-04-10T04:56:47Z,1245273,8,FALSE,"<p>Dirk's answer is a great one. If you want an appearance like what <code>hist</code> produces, you can also try this:</p>

<pre><code>buckets &lt;- c(0,1,2,3,4,5,25)
mydata_hist &lt;- hist(mydata$V3, breaks=buckets, plot=FALSE)
bp &lt;- barplot(mydata_hist$count, log=""y"", col=""white"", names.arg=buckets)
text(bp, mydata_hist$counts, labels=mydata_hist$counts, pos=1)
</code></pre>

<p>The last line is optional, it adds value labels just under the top of each bar. This can be useful for log scale graphs, but can also be omitted.</p>

<p>I also pass <code>main</code>, <code>xlab</code>, and <code>ylab</code> parameters to provide a plot title, x-axis label, and y-axis label.</p>
"
5706756,713882,2011-04-18T17:46:38Z,1452235,3,FALSE,"<p>You may check out the P compiler which implements a subset of R. Especially, lists, matrices, vectors etc. are implemented as well as lsfit, chol, svd, ... </p>

<p>You can download a free version at </p>

<p><a href=""http://www.ptechnologies.org"" rel=""nofollow"">www.ptechnologies.org</a></p>

<p>It speeds up computations substantially.</p>

<p>Best,</p>

<p>AS</p>
"
5709346,381788,2011-04-18T21:54:20Z,1725609,6,FALSE,"<p>You can also try the h5r package on CRAN. This package uses the new hdf5 libraries and provides more functionality than hdf5 library.</p>
"
5764087,577857,2011-04-23T12:11:35Z,1386767,12,FALSE,"<p><a href=""http://www.rstudio.org/"" rel=""noreferrer"">RStudio</a> has a nice object browser.</p>
"
5796821,374753,2011-04-26T21:43:05Z,952275,13,FALSE,"<p>gsub() can do this and return only the capture group:</p>

<p>However, in order for this to work, you must explicitly select elements outside your capture group as mentioned in the gsub() help.</p>

<blockquote>
  <p>(...) elements of character vectors 'x' which are not substituted will be returned unchanged.    </p>
</blockquote>

<p>So if your text to be selected lies in the middle of some string, adding .* before and after the capture group should allow you to only return it.</p>

<p><code>gsub("".*\\((.*?) :: (0\\.[0-9]+)\\).*"",""\\1 \\2"", ""(sometext :: 0.1231313213)"")
    [1] ""sometext 0.1231313213""</code></p>
"
5814643,728699,2011-04-28T06:40:16Z,1163640,2,FALSE,"<p>in GS view, convert our files to pdf, then convert to PS or EPS again, final file's size is reduced 5-7 times.</p>
"
5832647,731140,2011-04-29T13:25:16Z,2296694,14,TRUE,"<p>Renjin is a pure-JVM implementation of the R language.
It's very much under development and not production ready, but making good progress.</p>

<p><a href=""http://code.google.com/p/renjin"" rel=""noreferrer"">http://code.google.com/p/renjin</a></p>
"
5851081,733627,2011-05-01T20:14:41Z,1310247,2,FALSE,"<p>While I, too, prefer not to use <code>attach()</code>, it does have its place when you need to persist an object (in this case, a <code>data.frame</code>) through the life of your program when you have several functions using it.  Instead of passing the object into every R function that uses it, I think it is more convenient to keep it in one place and call its elements as needed.</p>

<p>That said, I would only use it if I know how much memory I have available and only if I make sure that I <code>detach()</code> this <code>data.frame</code> once it is out of scope.</p>

<p>Am I making sense?</p>
"
5905035,190277,2011-05-05T22:58:57Z,2443556,5,FALSE,"<p>Most simply (using the older <code>nlme</code> rather than the new <code>lme4</code>),</p>

<pre><code>library(nlme)
lme(fixed = y ~ 1, random = ~1|x1, data = p)
</code></pre>

<p>The equivalent in <code>lme4</code> is:</p>

<pre><code>library(lme4)
lmer(y~1+(1|x1), data = p)
</code></pre>
"
5908270,741308,2011-05-06T07:41:36Z,2185958,2,FALSE,"<p>Have you already checked the <a href=""http://cran.r-project.org/package=CorrBin"" rel=""nofollow"">CorrBin</a> package in R?<br>
It is for analysis of correlated binary data, there is a paper named: <a href=""http://cran.r-project.org/web/packages/CorrBin/vignettes/CorrBinVignette.pdf"" rel=""nofollow"">Using the CorrBin package for nonparametric analysis of
correlated binary data</a> by Szabo, it includes the Rao-Scott, stochastic ordering and three versions of a GEE-based test.</p>
"
5969414,345660,2011-05-11T19:17:02Z,2370094,1,FALSE,"<p>You need to melt the data frame <strong>before</strong> you cast it.  Casting without melting first will yield all kinds of unexpected behavior, because reshape has to guess the structure of your data.</p>
"
5971248,10396,2011-05-11T22:15:01Z,1309263,11,FALSE,"<p>I've done a <a href=""http://ideone.com/XPbl6"" rel=""nofollow noreferrer"">C implementation</a><a href=""https://gist.github.com/ashelly/5665911"" rel=""nofollow noreferrer""> here</a>.  A few more details are in this question: <a href=""https://stackoverflow.com/questions/5527437/rolling-median-in-c-turlach-implementation"">Rolling median in C - Turlach implementation</a>.</p>

<p>Sample usage:</p>

<pre><code>int main(int argc, char* argv[])
{
   int i,v;
   Mediator* m = MediatorNew(15);

   for (i=0;i&lt;30;i++)
   {
      v = rand()&amp;127;
      printf(""Inserting %3d \n"",v);
      MediatorInsert(m,v);
      v=MediatorMedian(m);
      printf(""Median = %3d.\n\n"",v);
      ShowTree(m);
   }
}
</code></pre>
"
6004726,352319,2011-05-14T20:54:31Z,2218395,1,FALSE,"<p>Take a look at <a href=""http://bioinfo.unice.fr/biodiv/Tree_editors.html"" rel=""nofollow"">this</a> page that has lots of information about software that deals with trees, including dendrograms.  I noticed several tools that deal with tree comparison, although I haven't personally used any of them yet.  There are a number of references cited there also.</p>
"
6054174,760390,2011-05-19T05:26:00Z,1532535,10,FALSE,"<p>You can do this by including the scales=""free"" option in your facet_wrap call:</p>

<pre><code>myGroups &lt;- sample(c(""Mo"", ""Larry"", ""Curly""), 100, replace=T)
myValues &lt;- rnorm(300)
df &lt;- data.frame(myGroups, myValues)


p &lt;- ggplot(df)  + 
  geom_density(aes(myValues), fill = alpha(""#335785"", .6)) + 
  facet_wrap(~ myGroups, scales=""free"")
p
</code></pre>
"
6130706,561698,2011-05-25T20:45:00Z,2258784,22,FALSE,"<p>Entering in </p>

<pre><code>theme_get()
</code></pre>

<p>will show a comprehensive listing of theme values and options.  You can then follow the syntax to modify these attributes in opts().</p>
"
6140825,561698,2011-05-26T15:08:39Z,1296646,12,FALSE,"<p>Dirk's answer is good but if you need the sort to persist you'll want to apply the sort back onto the name of that data frame.  Using the example code:</p>

<pre><code>dd &lt;- dd[with(dd, order(-z, b)), ] 
</code></pre>
"
6310757,345660,2011-06-10T18:48:07Z,2712421,4,FALSE,"<p>Dropbox has a ""ppor man's"" version control that gets you part of the way there for little effort with lots of extra benefits.</p>
"
6407026,645245,2011-06-20T05:08:54Z,2370515,7,FALSE,"<p>See <code>row</code> in <code>?base::row</code>. This gives the row indices for any matrix-like object.</p>
"
6461822,198401,2011-06-23T23:31:06Z,1815606,10,FALSE,"<p>A slimmed down variant of Supressingfire's answer:</p>

<pre><code>source_local &lt;- function(fname){
    argv &lt;- commandArgs(trailingOnly = FALSE)
    base_dir &lt;- dirname(substring(argv[grep(""--file="", argv)], 8))
    source(paste(base_dir, fname, sep=""/""))
}
</code></pre>
"
6473857,654923,2011-06-24T21:17:04Z,1567718,2,FALSE,"<p>Just want to provide an example to show the advantage and limitation in this issue:</p>

<p>I want to ""save"" a function with its name, as an option that would be used in another function:</p>

<pre><code>R&gt; foreach(..., .combine=test_fun) {...}
</code></pre>

<p>The <code>test_fun</code> is the name of the function, and of course</p>

<pre><code>R&gt; mode(test_fun)  
[1] ""function""
</code></pre>

<p>When I use it in foreach, I just need the function name, while <code>test_fun</code> can be an existing function (e.g. <code>cbind</code>). So, <code>test_fun</code> is assigned by</p>

<pre><code>R&gt; test_fun &lt;- get('cbind')
</code></pre>

<p>or</p>

<pre><code>R&gt; test_fun &lt;- assign('cbind', get('cbind'))
</code></pre>

<p>So, you got the function here</p>

<pre><code>R&gt; test_fun  
function (..., deparse.level = 1)   
.Internal(cbind(deparse.level, ...))  
</code></pre>

<p>actually, the original name can't be maintained, so you have no way to convert <code>test_fun</code> back to string <code>""cbind""</code>. </p>

<pre><code>R&gt; deparse(substitute(test_fun))  
[1] ""test_fun""
</code></pre>

<p>I unfortunately need to deparse the foreach code so want the original name shown in the string. This means that the only way is to save <code>'cbind'</code> as a string and creating such a function object brings no benefit in this case.</p>
"
6514175,235524,2011-06-28T23:17:07Z,520810,8,FALSE,"<p>The popular <a href=""http://biostat.mc.vanderbilt.edu/wiki/Main/Hmisc"" rel=""nofollow"">Hmisc package</a> offers the function <code>Cs()</code> to do this:</p>

<pre><code>library(Hmisc)
Cs(foo,bar)
[1] ""foo"" ""bar""
</code></pre>

<p>which uses a similar strategy to hadley's answer:</p>

<pre><code>Cs
function (...) 
{
    if (.SV4. || .R.) 
        as.character(sys.call())[-1]
    else {
        y &lt;- ((sys.frame())[[""...""]])[[1]][-1]
        unlist(lapply(y, deparse))
    }
}
&lt;environment: namespace:Hmisc&gt;
</code></pre>
"
6534016,822925,2011-06-30T11:29:16Z,2568234,4,FALSE,"<p>I find the following solution:</p>

<pre><code>stripchart(casp6~trans.factor,data.frame(casp6,trans.factor),pch=1,vertical=T)
</code></pre>

<p>simple and direct.</p>

<p>(Refer eg to <a href=""http://www.mail-archive.com/r-help@r-project.org/msg34176.html"" rel=""nofollow"">http://www.mail-archive.com/r-help@r-project.org/msg34176.html</a>)</p>
"
6540933,631237,2011-06-30T20:47:09Z,1432867,11,FALSE,"<p>This question is almost 2 years old now, but as a new R user in an experimental field, this was a big question for me, and this page is prominent in google results. I just discovered an answer I like better than the current set, so I thought I'd add it.</p>

<p>the package sciplot makes the task super easy.  It gets the job done in a single command</p>

<pre><code>#only necessary to get the MPG dataset from ggplot for direct comparison
library(ggplot2)
data(mpg)
attach(mpg)

#the bargraph.CI function with a couple of parameters to match the ggplot example
#see also lineplot.CI in the same package
library(sciplot)
bargraph.CI(
  class,  #categorical factor for the x-axis
  hwy,    #numerical DV for the y-axis
  year,   #grouping factor
  legend=T, 
  x.leg=19,
  ylab=""Highway MPG"",
  xlab=""Class"")
</code></pre>

<p>produces this very workable graph with mostly default options.  Note that the error bars are standard errors by default, but the parameter takes a function, so they can be anything you want! <img src=""https://i.stack.imgur.com/6RyTe.png"" alt=""sciplot bargraph.CI with mpg data""></p>
"
6644840,216064,2011-07-11T01:04:19Z,1487320,0,FALSE,"<p>For sweaving markdown to blogspot, a combination of the commandline tool Pandoc, R-package ascii and Python gdata module can be used. See <a href=""http://factbased.blogspot.com/2011/07/reproducible-blogging.html"" rel=""nofollow"">my blogpost</a></p>
"
6653788,839321,2011-07-11T17:08:35Z,2186015,16,TRUE,"<p>For anyone coming to this question like I just did after googling for rsqlite and dbgetpreparedquery, it seems that in the latest version of rsqlite you can run a SELECT query with bind variables. I just ran the following:</p>

<pre><code>query &lt;- ""SELECT probe_type,next_base,color_channel FROM probes WHERE probeid=?""
probe.types.df &lt;- dbGetPreparedQuery(con,que,bind.data=data.frame(probeids=ids))
</code></pre>

<p>This was relatively fast (selecting 2,000 rows out of a 450,000 row table) and is <em>incredibly</em> useful.</p>

<p>FYI.</p>
"
6660258,602276,2011-07-12T06:20:53Z,2119750,6,TRUE,"<p>Since you don't provide sample data, I shall demonstrate a solution using random data.</p>

<pre><code>set.seed(1)
n &lt;- 100
dat &lt;- data.frame(
    ds = sample(paste(""x"", 1:8, sep=""""), n, replace=TRUE),
    size = runif(n, 0, 250),
    root = sample(c(TRUE, FALSE), n, replace=TRUE),
    test = sample(c(""se"", ""cb"", ""cb.se""), n, replace=TRUE) 
)


head(dat)
  ds      size  root  test
1 x3 163.68098  TRUE cb.se
2 x3  88.29932  TRUE    se
3 x5  67.56504 FALSE    cb
4 x8 248.17102  TRUE    cb
5 x2 158.37332  TRUE    cb
6 x8  53.30203 FALSE cb.se

p &lt;- ggplot(dat, aes(x=ds, y=size)) + 
  geom_jitter(aes(colour=root)) + 
  facet_grid(test~.) 
</code></pre>

<p>Create the data frame containing label data.  Note the use of <code>summarize</code>.  This tells <code>ddply</code> to create a new column to the data.frame    </p>

<pre><code>labels &lt;- ddply(dat, .(ds, test), summarize, size=round(sum(size), 0))
head(labels)
  ds  test size
1 x1    cb  193
2 x1 cb.se  615
3 x1    se  274
4 x2    cb  272
5 x2 cb.se  341
6 x2    se 1012

p + geom_text(aes(x=ds, label=size, y=128), data=labels, size=2) 
</code></pre>

<p><img src=""https://i.stack.imgur.com/Bx4Zw.png"" alt=""enter image description here""></p>
"
6709601,657147,2011-07-15T15:37:58Z,1401904,2,FALSE,"<p>Two options:</p>

<ol>
<li>Implement my answer <a href=""https://stackoverflow.com/questions/5721942/making-r-installation-self-contained-user-independent/6709445#6709445"">here</a></li>
<li>If you use R under Eclipse with StatET, open <em>Run Configurations</em>, click on <em>Console</em> tab and in the box called <em>R snippet run after startup</em> add this line with your choice of directory: <code>.libPaths(""C:/R/library"")</code></li>
</ol>
"
6871968,636656,2011-07-29T10:48:00Z,1296646,303,FALSE,"<h3>Your choices</h3>

<ul>
<li><code>order</code> from <code>base</code></li>
<li><code>arrange</code> from <code>dplyr</code></li>
<li><code>setorder</code> and <code>setorderv</code> from <code>data.table</code></li>
<li><code>arrange</code> from <code>plyr</code></li>
<li><code>sort</code> from <code>taRifx</code></li>
<li><code>orderBy</code> from <code>doBy</code></li>
<li><code>sortData</code> from <code>Deducer</code></li>
</ul>

<p>Most of the time you should use the <code>dplyr</code> or <code>data.table</code> solutions, unless having no-dependencies is important, in which case use <code>base::order</code>.</p>

<hr>

<p>I recently added sort.data.frame to a CRAN package, making it class compatible as discussed here:
<a href=""https://stackoverflow.com/questions/6836963/best-way-to-create-generic-method-consistency-for-sort-data-frame"">Best way to create generic/method consistency for sort.data.frame?</a></p>

<p>Therefore, given the data.frame dd, you can sort as follows:</p>

<pre><code>dd &lt;- data.frame(b = factor(c(""Hi"", ""Med"", ""Hi"", ""Low""), 
      levels = c(""Low"", ""Med"", ""Hi""), ordered = TRUE),
      x = c(""A"", ""D"", ""A"", ""C""), y = c(8, 3, 9, 9),
      z = c(1, 1, 1, 2))
library(taRifx)
sort(dd, f= ~ -z + b )
</code></pre>

<p>If you are one of the original authors of this function, please contact me.  Discussion as to public domaininess is here: <a href=""http://chat.stackoverflow.com/transcript/message/1094290#1094290"">http://chat.stackoverflow.com/transcript/message/1094290#1094290</a></p>

<hr>

<p>You can also use the <code>arrange()</code> function from <code>plyr</code> as Hadley pointed out in the above thread:</p>

<pre><code>library(plyr)
arrange(dd,desc(z),b)
</code></pre>

<hr>

<p>Benchmarks: Note that I loaded each package in a new R session since there were a lot of conflicts.  In particular loading the doBy package causes <code>sort</code> to return ""The following object(s) are masked from 'x (position 17)': b, x, y, z"", and loading the Deducer package overwrites <code>sort.data.frame</code> from Kevin Wright or the taRifx package.</p>

<pre><code>#Load each time
dd &lt;- data.frame(b = factor(c(""Hi"", ""Med"", ""Hi"", ""Low""), 
      levels = c(""Low"", ""Med"", ""Hi""), ordered = TRUE),
      x = c(""A"", ""D"", ""A"", ""C""), y = c(8, 3, 9, 9),
      z = c(1, 1, 1, 2))
library(microbenchmark)

# Reload R between benchmarks
microbenchmark(dd[with(dd, order(-z, b)), ] ,
    dd[order(-dd$z, dd$b),],
    times=1000
)
</code></pre>

<p>Median times:</p>

<p><code>dd[with(dd, order(-z, b)), ]</code> <strong>778</strong></p>

<p><code>dd[order(-dd$z, dd$b),]</code> <strong>788</strong></p>

<pre><code>library(taRifx)
microbenchmark(sort(dd, f= ~-z+b ),times=1000)
</code></pre>

<p>Median time: <strong>1,567</strong></p>

<pre><code>library(plyr)
microbenchmark(arrange(dd,desc(z),b),times=1000)
</code></pre>

<p>Median time: <strong>862</strong></p>

<pre><code>library(doBy)
microbenchmark(orderBy(~-z+b, data=dd),times=1000)
</code></pre>

<p>Median time: <strong>1,694</strong></p>

<p>Note that doBy takes a good bit of time to load the package.</p>

<pre><code>library(Deducer)
microbenchmark(sortData(dd,c(""z"",""b""),increasing= c(FALSE,TRUE)),times=1000)
</code></pre>

<p>Couldn't make Deducer load.  Needs JGR console.</p>

<pre><code>esort &lt;- function(x, sortvar, ...) {
attach(x)
x &lt;- x[with(x,order(sortvar,...)),]
return(x)
detach(x)
}

microbenchmark(esort(dd, -z, b),times=1000)
</code></pre>

<p>Doesn't appear to be compatible with microbenchmark due to the attach/detach.</p>

<hr>

<pre><code>m &lt;- microbenchmark(
  arrange(dd,desc(z),b),
  sort(dd, f= ~-z+b ),
  dd[with(dd, order(-z, b)), ] ,
  dd[order(-dd$z, dd$b),],
  times=1000
  )

uq &lt;- function(x) { fivenum(x)[4]}  
lq &lt;- function(x) { fivenum(x)[2]}

y_min &lt;- 0 # min(by(m$time,m$expr,lq))
y_max &lt;- max(by(m$time,m$expr,uq)) * 1.05

p &lt;- ggplot(m,aes(x=expr,y=time)) + coord_cartesian(ylim = c( y_min , y_max )) 
p + stat_summary(fun.y=median,fun.ymin = lq, fun.ymax = uq, aes(fill=expr))
</code></pre>

<p><img src=""https://i.stack.imgur.com/9z6Oq.png"" alt=""microbenchmark plot""></p>

<p>(lines extend from lower quartile to upper quartile, dot is the median)</p>

<hr>

<p>Given these results and weighing simplicity vs. speed, I'd have to give the nod to <strong><code>arrange</code> in the <code>plyr</code> package</strong>.  It has a simple syntax and yet is almost as speedy as the base R commands with their convoluted machinations.  Typically brilliant Hadley Wickham work.  My only gripe with it is that it breaks the standard R nomenclature where sorting objects get called by <code>sort(object)</code>, but I understand why Hadley did it that way due to issues discussed in the question linked above.</p>
"
6927372,321622,2011-08-03T13:40:46Z,1380694,3,FALSE,"<p>(using lme4 library)
This fits your subject effect as random and also the variable that your random effects are grouped under.  In this model the random effect is the intercept varying by subject.</p>

<pre><code>m &lt;- lmer( Score ~ Condition + Time + (1|Subject), data=myDat )
</code></pre>

<p>To see the random effects you can just use</p>

<pre><code>ranef(m)
</code></pre>

<p>As Ian Fellows mentioned, your data may also have random Condition and Time components.  You can test that with another model.  In the one below Condition, Time, and the intercept are allowed to vary randomly by subject.  It also evaluates their correlations.</p>

<pre><code>mi &lt;- lmer( Score ~ Condition + Time + (Condition + Time|Subject), data=myDat )
</code></pre>

<p>and try</p>

<pre><code>summary(mi)
ranef(mi)
</code></pre>

<p>You could also test for this without correlations with the intercept, with interactions between Condition and Time, and numerous other models to see which best fits your data and / or your theory.  Your question is a bit vague but these few commands should get you started.</p>

<p>Note that Subject is your grouping factor so it's what you fit other effects as random under.  It's not something you then explicitly fit as a predictor as well.</p>
"
7178628,910024,2011-08-24T16:02:11Z,2436688,2,FALSE,"<p>I think what you want to do is <strong>actually</strong> pass by reference (pointer) to the function-- create a new environment (which are passed by reference to functions) with the list added to it:</p>

<pre><code>listptr=new.env(parent=globalenv())
listptr$list=mylist

#Then the function is modified as:
lPtrAppend &lt;- function(lstptr, obj) {
    lstptr$list[[length(lstptr$list)+1]] &lt;- obj
}
</code></pre>

<p>Now you are only modifying the existing list (not creating a new one)</p>
"
7184895,324364,2011-08-25T03:39:44Z,2248261,0,FALSE,"<p>Here's a very basic solution using <code>lattice</code>:</p>

<pre><code>dat &lt;- data.frame(x = rnorm(10), y = rnorm(10),
                  labs = as.character(1:10))
xyplot(y~x,data = dat,panel = function(x,labs){
                                 panel.text(x,y,labels = labs)},
       labs = dat$labs)
</code></pre>

<p><img src=""https://i.stack.imgur.com/3Z7J9.png"" alt=""enter image description here""></p>
"
7197774,633294,2011-08-25T22:02:44Z,2233584,11,FALSE,"<p>@Nick:</p>

<p>You can control your error message if you write a function with a descriptive name to test the condition that will throw an error in your program.  Here's an example:</p>

<pre><code>Less_Than_8 = function(x) return(x &lt; 8)

for (i in 1:10)
{
  print(i)
  stopifnot(Less_Than_8(i))
}
</code></pre>

<p>This will print the numbers 1 through 8, then print a message that says </p>

<pre><code>Error: Less_Than_8(i) is not TRUE
</code></pre>

<p>It would be nice if the ""i"" in parentheses was replaced with the value that failed the test, but you get what you pay for.</p>

<p>If you need anything fancier than that, look into Runit and testthat as Harlan suggested.</p>
"
7220087,251191,2011-08-28T09:27:19Z,2676554,51,FALSE,"<p>A version of John's answer above that removes the pesky NA's:</p>

<pre><code>stderr &lt;- function(x) sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))
</code></pre>
"
7315040,929934,2011-09-06T04:44:56Z,1712316,1,FALSE,"<p>I kept getting the same error while using family='binomial'. Turns out I had too few events/cases in my response variable. Using nfold=10 in cv.glmnet, I would get the error if my binary response [0,1] occured less than ~2% of the time. I recommend having your script check for this scenario and use glmnet when true. You won't get the benefit of the x validation for selecting lambda though.</p>
"
7328639,929934,2011-09-07T03:41:56Z,1712316,0,FALSE,"<p>Need to clarify the above. I had this problem when using cv.glmnet(). By identifying those instances where there are too few cases, I used glmnet() instead. You lose the benefit of the cross validation (i.e. estimating lambda), but it will complile. </p>
"
7370158,935950,2011-09-10T06:59:54Z,1379549,3,FALSE,"<p>Try <a href=""http://rstudio.org/"" rel=""nofollow"">RStudio</a>.</p>

<p>I have been a fan of <a href=""http://aquamacs.org/"" rel=""nofollow"">Emacs</a> and <a href=""http://pages.uoregon.edu/koch/texshop/obtaining.html"" rel=""nofollow"">TeXShop</a> as mentioned in previous answers. </p>

<p>However, Rstudio is starting to win me over. It is a rapidly-improving dedicated IDE for R. Definitely worth checking out.</p>

<p>I still love doing certain R-only development tasks in the <a href=""http://cran.r-project.org/"" rel=""nofollow"">standard R IDE</a> for mac. But for Sweave documents and some associated R devel at the same time, RStudio wins. It works with virtually zero tweaking. I'm not sure about the PDF-related features in the latter half of the original question.</p>
"
7438237,176995,2011-09-15T22:36:06Z,1395945,8,FALSE,"<p>If your data have discrete categories that you wish to colour, then your task is a bit easier. For example, if your data look like this, with each row representing a transaction,</p>

<pre><code>&gt; d &lt;- data.frame(customer = sample(letters[1:5], size = 20, replace = TRUE),
&gt;                sales = rnorm(20, 8000, 2000),
&gt;                profit = rnorm(20, 40, 15))
&gt; head(d,6)
customer    sales   profit
        a 8414.617 15.33714
        a 8759.878 61.54778
        e 8737.289 56.85504
        d 9516.348 24.60046
        c 8693.642 67.23576
        e 7291.325 26.12234
</code></pre>

<p>and you want to make a scatter plot of transactions coloured by customer, then you can do this</p>

<pre><code>p &lt;- ggplot(d, aes(sales,profit))
p + geom_point(aes(colour = customer))
</code></pre>

<p>to get....</p>

<p><img src=""https://i.stack.imgur.com/XgJ2H.png"" alt=""sales vs profit coloured on customer""></p>
"
7483495,954495,2011-09-20T10:11:23Z,2470248,82,FALSE,"<p>I would use the <code>cat()</code> command as in this example:</p>

<pre><code>&gt; cat(""Hello"",file=""outfile.txt"",sep=""\n"")
&gt; cat(""World"",file=""outfile.txt"",append=TRUE)
</code></pre>

<p>You can then view the results from with R with</p>

<pre><code>&gt; file.show(""outfile.txt"")
hello
world
</code></pre>
"
7485717,259795,2011-09-20T13:13:06Z,1309263,6,FALSE,"<p>I use this incremental median estimator:</p>

<pre><code>median += eta * sgn(sample - median)
</code></pre>

<p>which has the same form as the more common mean estimator:</p>

<pre><code>mean += eta * (sample - mean)
</code></pre>

<p>Here <em>eta</em> is a small learning rate parameter (e.g. 0.001), and <em>sgn</em>() is the signum function which returns one of {-1, 0, 1}. (Use a constant <em>eta</em> like this if the data is non-stationary and you want to track changes over time; otherwise, for stationary sources use something like <em>eta</em>=1/n to converge, where n is the number of samples seen so far.)</p>

<p>Also, I modified the median estimator to make it work for arbitrary quantiles. In general, a quantile function (http://en.wikipedia.org/wiki/Quantile_function) tells you the value that divides the data into two fractions: p and 1-p. The following estimates this value incrementally:</p>

<pre><code>quantile += eta * (sgn(sample - quantile) + 2.0 * p - 1.0)
</code></pre>

<p>The value p should be within [0,1]. This essentially shifts the sgn() function's symmetrical output {-1,0,1} to lean toward one side, partitioning the data samples into two unequally-sized bins (fractions p and 1-p of the data are less than/greater than the quantile estimate, respectively). Note that for p=0.5, this reduces to the median estimator.</p>
"
7486833,954977,2011-09-20T14:27:08Z,1107605,0,FALSE,"<p>If you just want to comment out a line, use ""##"" instead of ""#"". Double # will put the line at the right position.</p>
"
7553787,964857,2011-09-26T10:33:02Z,1449266,-1,FALSE,"<p>I found this method and it's very useful:</p>

<pre><code>for i=1:6
text(x(i),y(i),num2str(y(i)));
end
</code></pre>

<p>Repeat this line for every member in the plot.</p>
"
7585599,NA,2011-09-28T15:26:48Z,1815606,7,FALSE,"<p>This works for me. Just greps it out of the command line arguments, strips off the unwanted text, does a dirname and finally gets the full path from that:</p>

<pre><code>args &lt;- commandArgs(trailingOnly = F)  
scriptPath &lt;- normalizePath(dirname(sub(""^--file="", """", args[grep(""^--file="", args)])))
</code></pre>
"
7636139,959876,2011-10-03T14:07:31Z,2614767,8,FALSE,"<p>I have found on the net only a partial solution to your issue, for I managed to retrieve only the Balance sheet info and financial statement for one year. I don't know how to do it for more years.
There is a package in R, called <strong>quantmod</strong>, which you can install from CRAN</p>

<pre><code>install.packages('quantmod')
</code></pre>

<p>Then you can do the following: Suppose you want to get the financial info from a company listed at NYSE : General Electric. ticker: GE</p>

<pre><code> library(quantmod)
 getFinancials('GE')
 viewFinancials(GE.f)
</code></pre>

<p>To get only the income statement, reported anually, as a data frame use this:</p>

<pre><code>viewFinancials(GE.f, ""IS"", ""A"")
</code></pre>

<p>Please let me know if you find out how to do this for multiple years.</p>
"
7640738,977454,2011-10-03T21:15:11Z,1260965,4,FALSE,"<p>It is just three lines!</p>

<pre><code>library(maps);
colors = floor(runif(63)*657);
map(""state"", col = colors, fill = T, resolution = 0)
</code></pre>

<p>Done!! 
Just change the second line to any vector of 63 elements (each element between 0 and 657, which are members of colors())</p>

<p>Now if you want to get fancy you can write:</p>

<pre><code>library(maps);
library(mapproj);
colors = floor(runif(63)*657);
map(""state"", col = colors, fill = T, projection = ""polyconic"", resolution = 0);
</code></pre>

<p>The 63 elements represent the 63 regions whose names you can get by running:</p>

<pre><code>map(""state"")$names;
</code></pre>
"
7671706,981774,2011-10-06T08:13:45Z,2282892,1,FALSE,"<p>In Linux, there is a great package RKWard, which takes care of all the issues that Notepad++ is missing.</p>

<p>Unfotunately, I was unable to make it run under Windows (it says it is missing a dll)</p>

<p>So I am using Notepad++ as my development environment and RStudio for runtime environment</p>
"
7685471,983722,2011-10-07T09:37:35Z,1358003,27,FALSE,"<p>Unfortunately I did not have time to test it extensively but here is a memory tip that I have not seen before. For me the required memory was reduced with more than 50%.
When you read stuff into R with for example read.csv they require a certain amount of memory.
After this you can save them with <code>save(""Destinationfile"",list=ls())</code>
The next time you open R you can use <code>load(""Destinationfile"")</code>
Now the memory usage might have decreased.
It would be nice if anyone could confirm whether this produces similar results with a different dataset.</p>
"
7706780,321622,2011-10-09T21:48:57Z,1407449,3,FALSE,"<p>First do a simple aggregate to get it summarized.</p>

<pre><code>df &lt;- aggregate(cbind(var0, var1, var2, var3, var4) ~ year + group, test_data, mean)
</code></pre>

<p>That makes a data.frame like this...</p>

<pre><code>   year group     var0      var1     var2     var3     var4
1  2007     a 42.25000 0.2031277 2.145394 2.801812 3.571999
2  2009     a 30.50000 1.2033653 1.475158 3.618023 4.127601
3  2007     b 52.60000 1.4564604 2.224850 3.053322 4.339109
...
</code></pre>

<p>That, by itself, is pretty close to what you wanted.  You could just break it up by group now.</p>

<pre><code>l &lt;- split(df, df$group)
</code></pre>

<p>OK, so that's not quite it but we can refine the output if you really want to.</p>

<pre><code>lapply(l, function(x) {d &lt;- t(x[,3:7]); colnames(d) &lt;- x[,2]; d})

$a
           2007      2009
var0 42.2500000 30.500000
var1  0.2031277  1.203365
var2  2.1453939  1.475158
...
</code></pre>

<p>That doesn't have all your table formatting but it's organized exactly as you describe and is darn close.  This last step you could pretty up how you like.</p>

<p>This is the only answer here that matches the requested organization, and it's the fastest way to do it in R.  BTW, I wouldn't bother doing that last step and just stick with the very first output from the aggregate... or maybe the split.</p>
"
7718570,988388,2011-10-10T20:55:27Z,1530999,-1,FALSE,"<p>It is an old post but here is one possible solution that can be helpful, using a model comparison method to test if hd_lmer2 produces a better fit than hdlmer1 (i.e., if the addition of the random effect is significative or not).</p>

<pre><code>hdlmer1ml&lt;-update(hdlmer1,REML=FALSE)
hdlmer2ml&lt;-update(hdlmer2,REML=FALSE)

anova(hdlmer2ml,hdlmer1ml)
</code></pre>
"
7737742,78912,2011-10-12T09:13:24Z,2737680,0,FALSE,"<p>I contacted two of the maintainers at r-forge and CRAN and they pointed me to the <a href=""https://svn.r-project.org/R/trunk/"" rel=""nofollow"">sources of R</a>, in particular the <a href=""https://svn.r-project.org/R/trunk/src/library/tools/R/check.R"" rel=""nofollow""><code>check.R</code></a> script.</p>

<p>if I understand it correctly: </p>

<ul>
<li>scripts in the tests directory are called using a <code>system</code> call,</li>
<li>output (stdout and stderr) go to one single file,</li>
<li>there are two possible outcomes: <strong>ok</strong> or <strong>not ok</strong>,</li>
<li>to answer this question we need patching the library/tools package.</li>
</ul>

<hr>

<p>I opened a <a href=""https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=14702"" rel=""nofollow"">change request</a> on R, my first guess is something like bit-coding the return status, bit-0 for ERROR (as it is now), bit-1 for WARNING, bit-2 for NOTE.  </p>

<p>from doSvUnit.R, I would <code>quit</code> with <code>status</code> 2 in case of failures and 4 in case of skipped tests.</p>

<p>the patch would look like this:</p>

<pre><code>Index: src/library/tools/R/testing.R
===================================================================
--- src/library/tools/R/testing.R   (revision 57214)
+++ src/library/tools/R/testing.R   (working copy)
@@ -352,10 +352,16 @@
         } else
             cmd &lt;- paste(""LANGUAGE=C"", ""R_TESTS=startup.Rs"", cmd)
         res &lt;- system(cmd)
-        if (res) {
+        if (res%/%4 %% 2) {
+            message(""NOTE"")
+        }
+        if (res%/%2 %% 2) {
+            message(""WARNING"")
+        }
+        if (res %% 2) {
             file.rename(outfile, paste(outfile, ""fail"", sep="".""))
             return(1L)
         }
         savefile &lt;- paste(outfile, ""save"", sep = ""."" )
         if (file.exists(savefile)) {
             message(""  Comparing "", sQuote(outfile), "" to "",
</code></pre>

<p>unpatched R sees anything different from 0 as ERROR.</p>
"
7812713,953348,2011-10-18T19:41:58Z,1975110,8,FALSE,"<p>If something that triggers on option(error...) is of interest, you can also do this:</p>

<pre><code>options(error=traceback)
</code></pre>

<p>From what I can tell, it does most of what Bob's suggested solution do, but has the advantage of being much shorter.</p>

<p>(Feel free to combine with keep.source=TRUE, warn=2, etc. as needed.)</p>
"
7816636,673826,2011-10-19T04:46:45Z,2682629,1,FALSE,"<p>While the question title is ambiguous whether the talk about forcing visual refresh or just changing the content, I've recently had similar issue with gstatusbar update before and after long operation. While there is an alternative to REPL named <a href=""http://www.omegahat.org/REventLoop/"" rel=""nofollow"">REventLoop</a>, I've found the use of tcl timer quite handy.</p>

<pre><code>tcl(""after"", 300, my_long_operation)
</code></pre>

<p>So I update gstatusbar before long operation, then set up timer that in less than a second will fire my function that takes a while, and at the end of that function I update gstatusbar using something like </p>

<pre><code>svalue(sb) &lt;- ""Ready""
</code></pre>
"
7920543,1017068,2011-10-27T18:27:31Z,1163640,1,FALSE,"<p>The OP solved the problem by generating a PNG file directly. I had to use EPS because PNG and other formats aliased the image. I would have to convert to EPS anyway to include into a LaTeX file.</p>

<p>I used GIMP to import the 10 MB eps file generated from R image function. Then rotated, flattened, and saved as a 300KB eps file. Flattening merges all layers into a single layer and removes the alpha channel for transparency. Easily handled by LaTeX after this transformation.</p>

<p>Before transformation the image rendered very slowly in Ghost Script and couldn't be rendered at all in epsviewer. GIMP uses Ghost Script as a front end, so import is slow, but, once imported, all processing and rendering was very fast. </p>
"
7973154,805808,2011-11-01T21:29:01Z,2194516,0,FALSE,"<p>Belated answer, but I'd recommend reading the data into a memory mapped file, using the <code>bigmemory</code> package.  After that, I'd look for the non-zero entries, which can then be represented as a 3 column matrix: (ix_row, ix_col, value).  This is called a coordinate object list (COO), though the name is unimportant.  From there, <code>Matrix</code> supports the creation of sparse matrices (via <code>sparseMatrix</code>).  After you get the COO, you're pretty much set - conversion to and from the sparse matrix format is reasonably fast.  Multiplying the matrix by <code>Beta</code> should be reasonably fast.  If you need even greater speed, you could use an optimized BLAS library, but that opens up more questions.  :)</p>
"
8028577,172382,2011-11-06T16:43:53Z,2737680,1,FALSE,"<p>You should be able to alter the doSvUnit.R script to at least emit warnings and errors as you describe. What you want to do is to run the tests and then inspect the return value of the test runner and have R code that calls, warning() or stop().</p>

<p>For an example of how this was done using RUnit, take a look at the codetoolsBioC package in the Bioconductor repository. The relevant code is in inst/templates and copied below:</p>

<pre><code>.test &lt;- function(dir, pattern = "".*_test\\.R$"")
{
    .failure_details &lt;- function(result) {
        res &lt;- result[[1L]]
        if (res$nFail &gt; 0 || res$nErr &gt; 0) {
            Filter(function(x) length(x) &gt; 0,
                   lapply(res$sourceFileResults,
                          function(fileRes) {
                              names(Filter(function(x) x$kind != ""success"",
                                           fileRes))
                          }))
        } else list()
    }

    if (missing(dir)) {
        dir &lt;- system.file(""unitTests"", package=""@PKG@"")
        if (!nzchar(dir)) {
            dir &lt;- system.file(""UnitTests"", package=""@PKG@"")
            if (!nzchar(dir))
                stop(""unable to find unit tests, no 'unitTests' dir"")
        }
    }

    ## Run unit tests from the directory containing the test files.
    ## This allows tests to refer to data files with relative paths
    cwd &lt;- getwd()
    on.exit(setwd(cwd))
    setwd(dir)

    require(""RUnit"", quietly=TRUE) || stop(""RUnit package not found"")
    RUnit_opts &lt;- getOption(""RUnit"", list())
    RUnit_opts$verbose &lt;- 0L
    RUnit_opts$silent &lt;- TRUE
    RUnit_opts$verbose_fail_msg &lt;- TRUE
    options(RUnit = RUnit_opts)
    suite &lt;- defineTestSuite(name=""@PKG@ RUnit Tests"", dirs=getwd(),
                             testFileRegexp=pattern,
                             rngKind=""default"",
                             rngNormalKind=""default"")
    result &lt;- runTestSuite(suite)
    cat(""\n\n"")
    printTextProtocol(result, showDetails=FALSE)
    if (length(details &lt;- .failure_details(result)) &gt;0) {
        cat(""\nTest files with failing tests\n"")
        for (i in seq_along(details)) {
            cat(""\n  "", basename(names(details)[[i]]), ""\n"")
            for (j in seq_along(details[[i]])) {
                cat(""    "", details[[i]][[j]], ""\n"")
            }
        }
        cat(""\n\n"")
        stop(""unit tests failed for package @PKG@"")
    }
    result
}
</code></pre>
"
8040942,552683,2011-11-07T18:35:20Z,2686438,0,FALSE,"<p>There a solution that lets you stay in Sweave without having to use Brew . The key is to turn the code that's applied in the loop into a Latex macro with <code>\newcommand</code>, and then have an R chunk that loops over your variable and emits a call to your Latex macro for each value</p>

<p>A complete example is available at <a href=""https://stat.ethz.ch/pipermail/r-help/2008-June/164783.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2008-June/164783.html</a>, but here is the gist of it:</p>

<pre><code>\documentclass{article}
\SweaveOpts{echo=FALSE}
\newcommand\digit[2]{%
  \section{Digit #1}
  Digit #1 of $\pi$ is $#2$.
}
\title{Digits of $\pi$}
\begin{document}
\maketitle
&lt;&lt;results=tex&gt;&gt;=
digits = c(3, 1, 4, 1, 5, 9)
for (i in seq(digits)) {
  cat(paste(""\\digit{"", i, ""}{"", digits[i], ""}\n"", sep=""""))
}
@ 
\end{document}
</code></pre>
"
8060292,1036586,2011-11-09T04:18:43Z,2151212,3,FALSE,"<p>FYI:   there is a function args(), which retrieves the arguments of R functions, not to be confused with a vector of arguments named args</p>
"
8069844,37751,2011-11-09T18:39:44Z,2161152,14,FALSE,"<p><a href=""http://www.dexy.it/"">Dexy</a> is a very similar product to Sweave. One advantage of Dexy is that it is not exclusive to one single language. You could create a Dexy document that included R code, Python code, or about anything else. </p>
"
8158306,1031175,2011-11-16T20:37:01Z,2463437,7,FALSE,"<p>I don't think any of the above has answered the question - which was to evaluate 2 + 2 ;). To use a string expression would be something like:</p>

<pre><code>#include &lt;Rinternals.h&gt;
#include &lt;R_ext/Parse.h&gt;
#include &lt;Rembedded.h&gt;

int main(int argc, char **argv) {
    SEXP x;
    ParseStatus status;
    const char* expr = ""2 + 2"";

    Rf_initEmbeddedR(argc, argv);

    x = R_ParseVector(mkString(expr), 1, &amp;status, R_NilValue);
    if (TYPEOF(x) == EXPRSXP) { /* parse returns an expr vector, you want the first */
        x = eval(VECTOR_ELT(x, 0), R_GlobalEnv);
        PrintValue(x);
    }

    Rf_endEmbeddedR(0);

    return 0;
}
</code></pre>

<p>This lacks error checking, obviously, but works:</p>

<pre><code>Z:\&gt;gcc -o e.exe e.c -IC:/PROGRA~1/R/R-213~1.0/include -LC:/PROGRA~1/R/R-213~1.0/bin/i386 -lR
Z:\&gt;R CMD e.exe
[1] 4
</code></pre>

<p>(To get the proper commands for your R use <code>R CMD SHLIB e.c</code> which gives you the relevant compiler flags)</p>

<p>You can also construct the expression by hand if it's simple enough - e.g., for <code>rnorm(10)</code> you would use</p>

<pre><code>SEXP rnorm = install(""rnorm"");
SEXP x = eval(lang2(rnorm, ScalarInteger(10)), R_GlobalEnv);
</code></pre>
"
8177857,1000343,2011-11-18T04:41:03Z,2547402,6,FALSE,"<p>I've written the following code in order to generate the mode.</p>

<pre><code>MODE &lt;- function(dataframe){
    DF &lt;- as.data.frame(dataframe)

    MODE2 &lt;- function(x){      
        if (is.numeric(x) == FALSE){
            df &lt;- as.data.frame(table(x))  
            df &lt;- df[order(df$Freq), ]         
            m &lt;- max(df$Freq)        
            MODE1 &lt;- as.vector(as.character(subset(df, Freq == m)[, 1]))

            if (sum(df$Freq)/length(df$Freq)==1){
                warning(""No Mode: Frequency of all values is 1"", call. = FALSE)
            }else{
                return(MODE1)
            }

        }else{ 
            df &lt;- as.data.frame(table(x))  
            df &lt;- df[order(df$Freq), ]         
            m &lt;- max(df$Freq)        
            MODE1 &lt;- as.vector(as.numeric(as.character(subset(df, Freq == m)[, 1])))

            if (sum(df$Freq)/length(df$Freq)==1){
                warning(""No Mode: Frequency of all values is 1"", call. = FALSE)
            }else{
                return(MODE1)
            }
        }
    }

    return(as.vector(lapply(DF, MODE2)))
}
</code></pre>

<p>Let's try it:</p>

<pre><code>MODE(mtcars)
MODE(CO2)
MODE(ToothGrowth)
MODE(InsectSprays)
</code></pre>
"
8189441,169947,2011-11-18T21:33:10Z,2547402,268,TRUE,"<p>One more solution, which works for both numeric &amp; character/factor data:</p>

<pre><code>Mode &lt;- function(x) {
  ux &lt;- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
</code></pre>

<p>On my dinky little machine, that can generate &amp; find the mode of a 10M-integer vector in about half a second.</p>
"
8212428,403310,2011-11-21T13:16:35Z,2053397,12,FALSE,"<p>After this question was asked, packages <a href=""http://cran.r-project.org/web/packages/int64/index.html"" rel=""noreferrer"">int64</a> by Romain Francois and <a href=""http://www.stats.bris.ac.uk/R/web/packages/bit64/index.html"" rel=""noreferrer"">bit64</a> by Jens Oehlschlägel are now available.</p>
"
8261291,1064491,2011-11-24T18:29:13Z,2447454,10,FALSE,"<p>For rpy2 2.2.4 I had to add:</p>

<pre><code>import rpy2.robjects.numpy2ri
rpy2.robjects.numpy2ri.activate()
</code></pre>
"
8336496,260197,2011-12-01T04:20:56Z,1114699,11,FALSE,"<p>Since you tagged this <a href=""/questions/tagged/igraph"" class=""post-tag"" title=""show questions tagged 'igraph'"" rel=""tag"">igraph</a>, how about using built in functionality?</p>

<pre><code>&gt; g &lt;- graph.data.frame( edges )
&gt; adjlist &lt;- get.adjedgelist(g)
</code></pre>

<p>Only caveat is the vertices are zero indexed, which will be changing with igraph 0.6.</p>
"
8356858,125663,2011-12-02T13:15:09Z,1245273,2,FALSE,"<p>I've put together a function that behaves identically to hist in the default case, but accepts the log argument. It uses several tricks from other posters, but adds a few of its own. <code>hist(x)</code> and <code>myhist(x)</code> look identical.</p>

<p>The original problem would be solved with:</p>

<pre><code>myhist(mydata$V3, breaks=c(0,1,2,3,4,5,25), log=""xy"")
</code></pre>

<p>The function:</p>

<pre><code>myhist &lt;- function(x, ..., breaks=""Sturges"",
                   main = paste(""Histogram of"", xname),
                   xlab = xname,
                   ylab = ""Frequency"") {
  xname = paste(deparse(substitute(x), 500), collapse=""\n"")
  h = hist(x, breaks=breaks, plot=FALSE)
  plot(h$breaks, c(NA,h$counts), type='S', main=main,
       xlab=xlab, ylab=ylab, axes=FALSE, ...)
  axis(1)
  axis(2)
  lines(h$breaks, c(h$counts,NA), type='s')
  lines(h$breaks, c(NA,h$counts), type='h')
  lines(h$breaks, c(h$counts,NA), type='h')
  lines(h$breaks, rep(0,length(h$breaks)), type='S')
  invisible(h)
}
</code></pre>

<p>Exercise for the reader: Unfortunately, not everything that works with hist works with myhist as it stands. That should be fixable with a bit more effort, though.</p>
"
8372571,1078621,2011-12-04T01:36:09Z,2443556,2,FALSE,"<p>Zuur et al 2011, Mixed Effects Models and Extensions in Ecology with R, has an excellent walk through of random effects using the nlme package.  They explain the differences between fixed, random, and mixed models and how to specify random intercepts and random slopes.</p>
"
8391998,199217,2011-12-05T21:17:39Z,1249548,42,FALSE,"<p>You can use the following <code>multiplot</code> function from  <a href=""http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_%28ggplot2%29/"" rel=""noreferrer"">Winston Chang's R cookbook</a></p>

<pre><code>multiplot(plot1, plot2, cols=2)
</code></pre>

<hr>

<pre><code>multiplot &lt;- function(..., plotlist=NULL, cols) {
    require(grid)

    # Make a list from the ... arguments and plotlist
    plots &lt;- c(list(...), plotlist)

    numPlots = length(plots)

    # Make the panel
    plotCols = cols                          # Number of columns of plots
    plotRows = ceiling(numPlots/plotCols) # Number of rows needed, calculated from # of cols

    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(plotRows, plotCols)))
    vplayout &lt;- function(x, y)
        viewport(layout.pos.row = x, layout.pos.col = y)

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
        curRow = ceiling(i/plotCols)
        curCol = (i-1) %% plotCols + 1
        print(plots[[i]], vp = vplayout(curRow, curCol ))
    }

}
</code></pre>
"
8468373,868718,2011-12-11T23:27:36Z,2665532,3,FALSE,"<p>This may partially answer the question, or help others who want to begin by only downloading FROM public google spreadsheets: <a href=""http://blog.revolutionanalytics.com/2009/09/how-to-use-a-google-spreadsheet-as-data-in-r.html#"" rel=""nofollow"">http://blog.revolutionanalytics.com/2009/09/how-to-use-a-google-spreadsheet-as-data-in-r.html#</a></p>

<p>I had a problem with certificates, and instead of figuring that out, I use the option ssl.verifypeer=FALSE.  E.g.:</p>

<pre><code>getURL(""https://&lt;googledocs URL for sharing CSV&gt;, ssl.verifypeer=FALSE)
</code></pre>
"
8516373,1099318,2011-12-15T07:13:21Z,2453326,7,FALSE,"<p>For nth highest value,</p>

<pre><code>sort(x, TRUE)[n]
</code></pre>
"
8598440,1031175,2011-12-22T01:26:18Z,2477398,3,FALSE,"<p>You can modify the library path - see <code>?.libPaths</code> in R, you simply want to add your private library to the path. The GUI does that for you, but if you are outside it doesn't happen. For example:</p>

<pre><code> re.eval("".libPaths('c:/users/foo/Documents/R')"");
</code></pre>

<p>Then load your package.</p>
"
8610156,1112546,2011-12-22T21:54:34Z,2566128,6,TRUE,"<pre><code>#!/usr/bin/perl
use strict;
use File::Temp qw/tempfile/;
use File::Copy;

my $usage = &lt;&lt;EOD

  $0 file1.Rd [file2.Rd ...]

  When using roxygen to generate documentation for an R pacakge, if a default
  argument has a percent sign in it then roxygen will copy it directly into
  the .Rd file. Since .Rd is basically latex, this will be interpreted as a
  comment and case the file to be parsed incorrectly.

  For percent signs elsewhere in your documentation, for example in the
  description of one of the parameters, you should use ""\%"" so parse_Rd
  interprets it correctly.

  But you cannot do this in the default arguments because they have to be
  valid R code, too.

  Since the .Rd files are automatically generated they should not have
  any latex comments in them anyway.

  This script escapes every unescaped % within the file.

  The .Rd files are modified in place, since it would be easy to
  generate them again with R CMD roxygen.

EOD
;

my $n_tot = 0;
my $n_file = @ARGV;
my $n_esc_file = 0;
foreach my $fn (@ARGV)  {

  print STDERR ' ' x 100, ""\rWorking on $fn\t"";
  open my $fh, $fn or die ""Couldn't open $fn: $!"";
  my ($tmp_fh, $tmp_fn) = tempfile();

  my $n;
  while(&lt;$fh&gt;)  {
    $n += s/(?&lt;!\\)%/\\%/g;  # if % is not preceded with backslash then replace it with '\%'
    print $tmp_fh $_;
  }
  $n_tot += $n;
  $n_esc_file ++ if $n;
  print ""Escaped $n '%'\n"" if $n;
  close $tmp_fh;
  move($tmp_fn =&gt; $fn);
}

print ""\n"";

print ""$n_file files parsed\n"";
print ""$n_esc_file contained at least one unescaped %\n"";
print ""$n_tot total % were escaped\n"";
</code></pre>

<p>This is my inelegant solution. Save the perl script as, for example, escape_percents.pl, then the sequence would be like this:</p>

<pre><code>R CMD roxygen my.package
perl escape_percents.pl my.package.roxygen/man/*.Rd
R CMD install my.package.roxygen
</code></pre>

<p>This may introduce more problems, for example if you have example code using %% as the modulus operator, but it has been handy for me.</p>
"
8617406,983722,2011-12-23T14:55:51Z,1982667,0,FALSE,"<p>Perhaps a shift of perspective can solve this problem.
Rather than seeing the constant as something special, just consider the problem without constant and with a variable that is a vector of ones.</p>
"
8862950,689127,2012-01-14T14:50:23Z,2754658,3,FALSE,"<p>Additional answer which may help: geom_ribbon can be used to produce a filled area between two lines without needing to explicitly construct a polygon. There is good documentation here:
<a href=""http://had.co.nz/ggplot2/geom_ribbon.html"" rel=""nofollow"">http://had.co.nz/ggplot2/geom_ribbon.html</a></p>
"
8992102,800044,2012-01-24T18:32:55Z,2678141,83,FALSE,"<p>As of ggplot2 0.9.2, this has become <strong>much</strong> easier to do using ""themes."" You can now assign themes separately to panel.grid.major.x and panel.grid.major.y, as demonstrated below. </p>

<pre><code>#   simulate data for the bar graph
data &lt;- data.frame( X = c(""A"",""B"",""C""), Y = c(1:3) )    

#   make the bar graph
ggplot( data  ) +
    geom_bar( aes( X, Y ) ) +
    theme( # remove the vertical grid lines
           panel.grid.major.x = element_blank() ,
           # explicitly set the horizontal lines (or they will disappear too)
           panel.grid.major.y = element_line( size=.1, color=""black"" ) 
    )
</code></pre>

<p>The result of this example is quite ugly looking, but it demonstrates how to remove the vertical lines while preserving the horizontal lines and x-axis tick-marks.</p>
"
9012899,981774,2012-01-26T01:27:20Z,2282892,0,FALSE,"<p>Actually, with NppToR, it becomes a real blast!  No need for RStudio - although if you have a big screen, RStudio is still a great runtime environment</p>
"
9019751,1169233,2012-01-26T14:38:38Z,1310247,3,FALSE,"<p>Just like Leoni said, <code>with</code> and <code>within</code> are perfect substitutes for <code>attach</code>, but I wouldn't completely dismiss it. I use it sometimes, when I'm working directly at the R prompt and want to test some commands before writing them on a script. Especially when testing multiple commands, <code>attach</code> can be a more interesting, convenient and even harmless alternative to <code>with</code> and <code>within</code>, since after you run <code>attach</code>, the command prompt is clear for you to write inputs and see outputs.</p>

<p>Just make sure to <code>detach</code> your data after you're done!</p>
"
9071687,236247,2012-01-30T22:21:07Z,2658752,0,FALSE,"<p><a href=""http://code.google.com/p/efficient-java-matrix-library/"" rel=""nofollow"">EJML</a>  seems to be a promising new one for fast multiplication.  They have benchmarks on their site to show what they claim to be fast times for matrix multiplication.</p>
"
9073730,107156,2012-01-31T02:44:23Z,2603184,4,FALSE,"<p>R does have a library now that allows you to do OOP using references. See <a href=""http://stat.ethz.ch/R-manual/R-devel/library/methods/html/refClass.html"" rel=""nofollow"">ReferenceClasses</a> which is part of the methods package.</p>
"
9081281,1079872,2012-01-31T14:34:16Z,2409357,56,FALSE,"<p>I had a similar problem and solved it with JD Long answer. But as a results of <code>ggplot2</code> updating to version 0.9.0 I noticed that all <code>geom_text()</code>calls rendered somewhat blurred on the plots.</p>

<p>Thanks to <a href=""https://stackoverflow.com/users/314020/kohske"">kohske</a> I discovered that this code </p>

<pre><code>ggplot(data2, aes(x=time, y=value, group=type, col=type))+
geom_line()+
geom_point()+
theme_bw() +
geom_text(aes(7, .9, label=""correct color"", color=""NA*"")) +
geom_text(aes(15, .6, label=""another correct color!"", color=""MVH"")) 
</code></pre>

<p>plots the geom_text <code>nrow(data2)</code>times! </p>

<p>The correct way for supplying data to geom_text is building a different data.frame holding coordinates, labels and colors for the strings you want to be plotted:</p>

<pre><code>data2.labels &lt;- data.frame(
  time = c(7, 15), 
  value = c(.9, .6), 
  label = c(""correct color"", ""another correct color!""), 
  type = c(""NA*"", ""MVH"")
  )

ggplot(data2, aes(x=time, y=value, group=type, col=type))+
  geom_line()+
  geom_point()+
  theme_bw() +
  geom_text(data = data2.labels, aes(x = time, y = value, label = label))
</code></pre>
"
9082552,1180706,2012-01-31T15:53:25Z,1523126,1,FALSE,"<p>Another solution:</p>

<pre><code> y &lt;- c(""1,200"",""20,000"",""100"",""12,111"") 

 as.numeric(unlist(lapply( strsplit(y,"",""),paste, collapse="""")))
</code></pre>

<p>It will be considerably slower than <code>gsub</code>,though. </p>
"
9117474,949167,2012-02-02T18:02:32Z,1154242,13,FALSE,"<p>@Richie Cotton has a pretty good answer above.  I can only add that this <a href=""http://www.statmethods.net/advgraphs/axes.html"" rel=""noreferrer"">page</a> provides some examples.  Try the following:</p>

<pre><code>x &lt;- 1:20
y &lt;- runif(20)
plot(x,y,xaxt = ""n"")
axis(side = 1, at = x, labels = FALSE, tck = -0.01)
</code></pre>
"
9154105,10559,2012-02-05T23:31:17Z,1898101,11,FALSE,"<p>As others have mentioned, R's built-in Windows graphics device does not do anti-aliasing. But nowadays it's easy to install the Cairo graphics device which does.</p>

<p>At the R console:</p>

<pre><code>install.packages('Cairo',,'http://www.rforge.net/')
</code></pre>

<p>To test:</p>

<pre><code>plot(rnorm(1000)) # non-antialiased (on Windows)
library('Cairo')
CairoWin()
plot(rnorm(1000)) # antialiased!
</code></pre>

<p><a href=""http://www.rforge.net/Cairo/files/"" rel=""noreferrer"">More</a></p>
"
9164802,445565,2012-02-06T17:58:12Z,2165342,11,FALSE,"<p>When developing, ?<a href=""http://stat.ethz.ch/R-manual/R-patched/library/utils/html/getFromNamespace.html"" rel=""nofollow noreferrer"">assignInNamespace</a>  is very useful.</p>

<p>This allows you to inject a new copy of a non-exported function into a package's namespace.</p>

<p>It would be nice if R's error message was more helpful. Instead of:</p>

<pre><code>Error: 'matrixToPaths' is not an exported object from 'namespace:OpenMx'
</code></pre>

<p>Why not add:</p>

<pre><code>You might try OpenMx:::matrixToPaths""
</code></pre>
"
9167337,771009,2012-02-06T21:12:29Z,1632772,3,FALSE,"<p>I had the same problem with type mismatches, especially with factors. I had to glue together two otherwise compatible datasets.</p>

<p>My solution is to convert factors in both dataframes to ""character"". Then it works like a charm :-)</p>

<pre><code>    convert.factors.to.strings.in.dataframe &lt;- function(dataframe)
    {
        class.data  &lt;- sapply(dataframe, class)
        factor.vars &lt;- class.data[class.data == ""factor""]
        for (colname in names(factor.vars))
        {
            dataframe[,colname] &lt;- as.character(dataframe[,colname])
        }
        return (dataframe)
    }
</code></pre>

<p>If you want to see the types in your two dataframes run (change var names):</p>

<pre><code>    cbind(""orig""=sapply(allSurveyData, class), 
          ""merge"" = sapply(curSurveyDataMerge, class),
          ""eq""=sapply(allSurveyData, class) == sapply(curSurveyDataMerge, class)
    )
</code></pre>
"
9238777,1172558,2012-02-11T08:12:21Z,1810662,1,FALSE,"<p>You could use the line:</p>

<pre><code>par(mfrow=c(2,2))
</code></pre>

<p>before </p>

<pre><code>plot(r)
</code></pre>

<p>In that way you say to R to show 4 pictures at the same frame. You can also set 4,1 to have 4 graphs one above the other, or 1,4 to have 4 graphs one on the side of the other.</p>
"
9256744,103225,2012-02-13T07:10:03Z,1497539,11,FALSE,"<p><a href=""https://stackoverflow.com/a/1497697/103225"">Dirk</a> has explained how to plot the density function over the histogram. But sometimes you might want to go with the stronger assumption of a skewed normal distribution and plot that instead of density. You can estimate the parameters of the distribution and plot it using the <a href=""http://cran.r-project.org/web/packages/sn/index.html"" rel=""nofollow noreferrer"">sn package</a>:</p>

<pre><code>&gt; sn.mle(y=c(rep(65, times=5), rep(25, times=5), rep(35, times=10), rep(45, times=4)))
$call
sn.mle(y = c(rep(65, times = 5), rep(25, times = 5), rep(35, 
    times = 10), rep(45, times = 4)))

$cp
    mean     s.d. skewness 
41.46228 12.47892  0.99527 
</code></pre>

<p><img src=""https://i.stack.imgur.com/xQJYD.png"" alt=""Skew-normal distributed data plot""></p>

<p>This probably works better on data that is more skew-normal:</p>

<p><img src=""https://i.stack.imgur.com/HgX0f.png"" alt=""Another skew-normal plot""></p>
"
9260430,1144673,2012-02-13T12:21:57Z,2658752,0,FALSE,"<p><a href=""http://sourceforge.net/projects/parallelcolt/"" rel=""nofollow"">Parallel colt</a> is an effective library to perform matrix operations . </p>

<p>Other good matrix libraries would include <a href=""http://jblas.org/"" rel=""nofollow"">jblas</a> and <a href=""http://sourceforge.net/projects/ujmp/"" rel=""nofollow"">ujmp</a></p>

<p>All of these packages are effective. jblas is my personal favourite . It has good documentation and straight forward unlike ujmp</p>
"
9316377,1214413,2012-02-16T17:49:49Z,1581232,11,FALSE,"<p>Huge thanks to Jonathan Chang for his answer. <code>formatC</code> looks to be an extremely useful function. This inspired me to read the documentation for it, wherein I found <code>prettyNum</code>, which turned out to be a pretty elegant solution to a similar problem I was having. Here's a minimum viable example of what I did to add commas to numbers in a data frame named <code>enrollment.summary</code>.</p>

<p><code>xtable(prettyNum(enrollment.summary,big.mark="",""))</code></p>
"
9382261,1223966,2012-02-21T17:33:25Z,2399027,3,FALSE,"<p>For what it's worth, putting <code>C:\Program Files\Java\jre6\bin\[server]</code> in my <code>PATH</code>  worked for me. It seems the rJava module could jot find <code>jvm.dll</code> there.</p>

<p>Here are the versions of R and Java that I'm using (on 64-bit Windows 7).</p>

<p>Java:</p>

<pre><code>java version ""1.6.0_30""
Java(TM) SE Runtime Environment (build 1.6.0_30-b12)
Java HotSpot(TM) 64-Bit Server VM (build 20.5-b03, mixed mode)
</code></pre>

<p>R:</p>

<pre><code>R version 2.14.1 (2011-12-22)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-pc-mingw32/x64 (64-bit)
</code></pre>
"
9477542,150474,2012-02-28T06:41:21Z,1309263,1,FALSE,"<p>For those who need a running median in Java...PriorityQueue is your friend. O(log N) insert, O(1) current median, and O(N) remove. If you know the distribution of your data you can do a lot better than this. </p>

<pre><code>public class RunningMedian {
  // Two priority queues, one of reversed order.
  PriorityQueue&lt;Integer&gt; lower = new PriorityQueue&lt;Integer&gt;(10,
          new Comparator&lt;Integer&gt;() {
              public int compare(Integer arg0, Integer arg1) {
                  return (arg0 &lt; arg1) ? 1 : arg0 == arg1 ? 0 : -1;
              }
          }), higher = new PriorityQueue&lt;Integer&gt;();

  public void insert(Integer n) {
      if (lower.isEmpty() &amp;&amp; higher.isEmpty())
          lower.add(n);
      else {
          if (n &lt;= lower.peek())
              lower.add(n);
          else
              higher.add(n);
          rebalance();
      }
  }

  void rebalance() {
      if (lower.size() &lt; higher.size() - 1)
          lower.add(higher.remove());
      else if (higher.size() &lt; lower.size() - 1)
          higher.add(lower.remove());
  }

  public Integer getMedian() {
      if (lower.isEmpty() &amp;&amp; higher.isEmpty())
          return null;
      else if (lower.size() == higher.size())
          return (lower.peek() + higher.peek()) / 2;
      else
          return (lower.size() &lt; higher.size()) ? higher.peek() : lower
                  .peek();
  }

  public void remove(Integer n) {
      if (lower.remove(n) || higher.remove(n))
          rebalance();
  }
}
</code></pre>
"
9652931,742447,2012-03-11T06:24:15Z,1299871,143,FALSE,"<p>There is the <strong>data.table</strong> approach for an inner join, which is very time and memory efficient (and necessary for some larger data.frames):</p>

<pre><code>library(data.table)

dt1 &lt;- data.table(df1, key = ""CustomerId"") 
dt2 &lt;- data.table(df2, key = ""CustomerId"")

joined.dt1.dt.2 &lt;- dt1[dt2]
</code></pre>

<p><code>merge</code> also works on data.tables (as it is generic and calls <code>merge.data.table</code>)</p>

<pre><code>merge(dt1, dt2)
</code></pre>

<p>data.table documented on stackoverflow:<br>
<a href=""https://stackoverflow.com/questions/2232699/r-how-to-do-a-data-table-merge-operation"">How to do a data.table merge operation</a><br>
<a href=""https://stackoverflow.com/questions/9914734/translating-sql-joins-on-foreign-keys-to-r-data-table-syntax"">Translating SQL joins on foreign keys to R data.table syntax</a><br>
<a href=""https://stackoverflow.com/questions/11146967/efficient-alternatives-to-merge-for-larger-data-frames-r"">Efficient alternatives to merge for larger data.frames R</a><br>
<a href=""https://stackoverflow.com/questions/7090621/how-to-do-a-basic-left-outer-join-with-data-table-in-r"">How to do a basic left outer join with data.table in R?</a></p>

<p>Yet another option is the <code>join</code> function found in the <a href=""http://cran.r-project.org/web/packages/plyr/index.html"" rel=""noreferrer""><strong>plyr</strong></a> package</p>

<pre><code>library(plyr)

join(df1, df2,
     type = ""inner"")

#   CustomerId Product   State
# 1          2 Toaster Alabama
# 2          4   Radio Alabama
# 3          6   Radio    Ohio
</code></pre>

<p>Options for <code>type</code>: <code>inner</code>, <code>left</code>, <code>right</code>, <code>full</code>.</p>

<p>From <code>?join</code>: Unlike <code>merge</code>, [<code>join</code>] preserves the order of x no matter what join type is used.</p>
"
9667903,989691,2012-03-12T13:27:26Z,2603184,12,FALSE,"<p>As several have pointed out before, this can be done via using objects of class <code>environment</code>. There exists a formal approach building upon the use of <code>environment</code>s. It's called <em>Reference Classes</em> and makes things really easy for you. Check <code>?setRefClass</code> for the main entry help page. It also describes how to use formal methods with Reference Classes.</p>

<h2>Example</h2>

<pre><code>setRefClass(""MyClass"",
    fields=list(
        name=""character""
    )
)

instance1 &lt;- new(""MyClass"",name=""Hello1"")
instance2 &lt;- new(""MyClass"",name=""Hello2"")

array = c(instance1,instance2)

instance1$name &lt;- ""World!""
</code></pre>

<h2>Output</h2>

<pre><code>&gt; instance1
Reference class object of class ""MyClass""
Field ""name"":
[1] ""World!""

&gt; array
[[1]]
Reference class object of class ""MyClass""
Field ""name"":
[1] ""World!""

[[2]]
Reference class object of class ""MyClass""
Field ""name"":
[1] ""Hello2""
</code></pre>
"
9693045,944336,2012-03-13T22:15:02Z,1136709,2,FALSE,"<p>Talking to R from Java has been standardised using the REngine API. There are basically two implementations for this.</p>

<p>The first implementation is <strong>JRI</strong>. It is based on JNI, and executes the R dll inside the JVM. This means the connection is very fast. You can use full R functionality, including objects which live inside R but are accessible/modifiable in Java. The disadvantage is you cannot use multithreading.</p>

<p>The second implementation is <strong>RServe</strong>. RServe consists of a server written in C, together with a Java client. The server is started from the command line, and includes the R dll. The java client then makes a socket connection and calls R in a serialised manner. This implementation works well. The disadvantage is that, on Windows, the RServe component is not able to fork itself to handle multiple connections. Every RServe instance can only server one single user.</p>

<p>An alternative implementation to look out for is a Java RMI client which calls a Java server calling R using JRI. The idea is that you can use multithreading because you can talk to multiple servers at once. See <a href=""http://www.londonr.org/remoterengine-londonR.pdf"" rel=""nofollow"">http://www.londonr.org/remoterengine-londonR.pdf</a></p>

<p>In practice, we have used RServe together with a lot of boilerplate code for launching and managing the RServe instances. It's basically a major PITA, but it works well and has good performance.</p>
"
9722181,988463,2012-03-15T14:47:43Z,2684966,1,FALSE,"<p>You can do this editing the <code>grob</code> directly, try:</p>

<pre><code>grid.remove(gPath(""panel.grid.minor.x.polyline""),grep=T)
grid.remove(gPath(""panel.grid.major.x.polyline""),grep=T) 
</code></pre>

<p>It will strip off your vertical lines. I'm just having problems to use it inside a function, because I guess that it only works when the ggplot is printed.</p>

<p>But, if that's not your case and you'll just need the graphic, than it will work.</p>
"
9789710,1281312,2012-03-20T15:26:47Z,1377003,2,FALSE,"<p>I don't think Jouni is quite right.  This seems to give a reasonable version of the PDF (extract the middle of the loop if you just want a specific x-y point):</p>

<pre><code>!/usr/bin/perl

use strict;
use Getopt::Std;
use POSIX qw(ceil floor);

# Usage
# Outputs normal density function given a mean and sd
# -s standard deviation
# -m mean
# -n normalization factor (multiply result by this amount), optional

my %para = ();
getopts('s:m:n:', \%para);
if (!exists ($para{'s'}) || !exists ($para{'m'})) {
   die (""mean and standard deviation required"");
}

my $norm = 1.0;
if (exists ($para{'n'})) {
   $norm = $para{'n'};
}

my $sd = $para{'s'};
my $mean = $para{'m'};

my $start = floor($mean - ($sd * 5));
my $end = ceil($mean + ($sd * 5));

my $pi = 3.141593;

my $var = $sd**2;

for (my $x = $start; $x &lt; $end; $x+=0.1) {
    my $e = exp( -1 * (($x-$mean)**2) / (2*$var));
    my $d = sqrt($var) * sqrt(2*$pi);
    my $y = 1.0/$d*$e * $norm;
    printf (""%5.5f %5.5f\n"", $x, $y);
}
</code></pre>
"
9839949,1288311,2012-03-23T13:21:49Z,1358003,27,FALSE,"<p>I quite like the improved objects function developed by Dirk. Much of the time though, a more basic output with the object name and size is sufficient for me. Here's a simpler function with a similar objective. Memory use can be ordered alphabetically or by size, can be limited to a certain number of objects, and can be ordered ascending or descending. Also, I often work with data that are 1GB+, so the function changes units accordingly.</p>

<pre><code>showMemoryUse &lt;- function(sort=""size"", decreasing=FALSE, limit) {

  objectList &lt;- ls(parent.frame())

  oneKB &lt;- 1024
  oneMB &lt;- 1048576
  oneGB &lt;- 1073741824

  memoryUse &lt;- sapply(objectList, function(x) as.numeric(object.size(eval(parse(text=x)))))

  memListing &lt;- sapply(memoryUse, function(size) {
        if (size &gt;= oneGB) return(paste(round(size/oneGB,2), ""GB""))
        else if (size &gt;= oneMB) return(paste(round(size/oneMB,2), ""MB""))
        else if (size &gt;= oneKB) return(paste(round(size/oneKB,2), ""kB""))
        else return(paste(size, ""bytes""))
      })

  memListing &lt;- data.frame(objectName=names(memListing),memorySize=memListing,row.names=NULL)

  if (sort==""alphabetical"") memListing &lt;- memListing[order(memListing$objectName,decreasing=decreasing),] 
  else memListing &lt;- memListing[order(memoryUse,decreasing=decreasing),] #will run if sort not specified or ""size""

  if(!missing(limit)) memListing &lt;- memListing[1:limit,]

  print(memListing, row.names=FALSE)
  return(invisible(memListing))
}
</code></pre>

<p>And here is some example output:</p>

<pre><code>&gt; showMemoryUse(decreasing=TRUE, limit=5)
      objectName memorySize
       coherData  713.75 MB
 spec.pgram_mine  149.63 kB
       stoch.reg  145.88 kB
      describeBy    82.5 kB
      lmBandpass   68.41 kB
</code></pre>
"
9938612,990744,2012-03-30T07:21:55Z,713878,0,FALSE,"<p>It uses the QR algo. See Wilkinson, J. H. (1965) The Algebraic Eigenvalue Problem. Clarendon Press, Oxford. It does not exploit sparsity.</p>
"
9968623,1292820,2012-04-01T21:46:56Z,2492947,0,FALSE,"<p>Based on the answers by @James and @Jyotirmoy Bhattacharya I came up with this solution:</p>

<pre><code>zx &lt;- replicate (5, rnorm(50))
zx_means &lt;- (colMeans(zx, na.rm = TRUE))
boxplot(zx, horizontal = FALSE, outline = FALSE)
points(zx_means, pch = 22, col = ""darkgrey"", lwd = 7)
</code></pre>

<p>(See <a href=""https://stackoverflow.com/questions/9871106/boxplots-ranked-by-mean-value"">this</a> post for more details)</p>

<p>If you would like to add points to horizontal box plots, please see <a href=""https://stackoverflow.com/questions/9874512/adding-points-to-horizontal-boxplots"">this</a> post. </p>
"
10004019,1201032,2012-04-04T01:56:45Z,520810,11,FALSE,"<p>I have added this function to my Rprofile.site file (see <code>?Startup</code> if you are not familiar)</p>

<pre><code>qw &lt;- function(x) unlist(strsplit(x, ""[[:space:]]+""))

qw(""You can type    text here
    with    linebreaks if you
    wish"")
#  [1] ""You""        ""can""        ""type""       ""text""      
#  [5] ""here""       ""with""       ""linebreaks"" ""if""        
#  [9] ""you""        ""wish""    
</code></pre>
"
10005644,1312033,2012-04-04T05:54:24Z,2436688,15,FALSE,"<p>In the Lisp we did it this way:</p>

<pre><code>&gt; l &lt;- c(1)
&gt; l &lt;- c(2, l)
&gt; l &lt;- c(3, l)
&gt; l &lt;- rev(l)
&gt; l
[1] 1 2 3
</code></pre>

<p>though it was 'cons', not just 'c'. If you need to start with an empy list, use l &lt;- NULL.</p>
"
10038710,80626,2012-04-06T03:13:48Z,952275,75,TRUE,"<p><code>str_match()</code>, from the <a href=""http://cran.r-project.org/web/packages/stringr/index.html"" rel=""noreferrer""><code>stringr</code></a> package, will do this. It returns a character matrix with one column for each group in the match (and one for the whole match):</p>

<pre><code>&gt; s = c(""(sometext :: 0.1231313213)"", ""(moretext :: 0.111222)"")
&gt; str_match(s, ""\\((.*?) :: (0\\.[0-9]+)\\)"")
     [,1]                         [,2]       [,3]          
[1,] ""(sometext :: 0.1231313213)"" ""sometext"" ""0.1231313213""
[2,] ""(moretext :: 0.111222)""     ""moretext"" ""0.111222""    
</code></pre>
"
10178691,264696,2012-04-16T17:22:06Z,1721536,3,FALSE,"<p>In some cases I don't think this is possible:</p>

<pre><code>layerCake&lt;-data.frame(group=c(rep(""normal"",4),rep(""tumor"",4)),
                      class=factor(rep(c(""exon"",""intron"",""intergenic"",""unmapped""),2),levels=rev(c(""exon"",""intron"",""intergenic"",""unmapped"")),ordered=TRUE),
                      fraction=c(.02,.25,.50,.23,.015,.20,.555,.23)
)
layerCake[layerCake$group=='normal',""reads""]&lt;-130948403*layerCake[layerCake$group=='normal',""fraction""]
layerCake[layerCake$group=='tumor',""reads""]&lt;-200948403*layerCake[layerCake$group=='tumor',""fraction""]
g&lt;-ggplot(layerCake, aes(x=factor(group),y=reads, fill=factor(class),order = as.numeric(class)))+xlab(""Group"")+scale_fill_discrete(name=""Anno Class"",breaks=c(""exon"",""intron"",""intergenic"",""unmapped""))
</code></pre>

<p>correct order in stacked:<br>
    g+geom_bar(stat=""identity"",position=""stack"")
<img src=""https://i.stack.imgur.com/ksrUF.png"" alt=""enter image description here""></p>

<p>incorrect order in dodge:</p>

<pre><code>g+geom_bar(stat=""identity"",position=""dodge"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/mld4K.png"" alt=""enter image description here""></p>

<p>let's try to reverse the order in ggplot:</p>

<pre><code>g&lt;-ggplot(lc, aes(x=factor(group),y=reads, fill=factor(class),order = -as.numeric(class)))+xlab(""Group"")+scale_fill_discrete(name=""Anno Class"",breaks=c(""exon"",""intron"",""intergenic"",""unmapped""))
g+geom_bar(stat=""identity"",position=""dodge"")
</code></pre>

<p>no dice</p>

<p>let's try to reorder the data frame</p>

<pre><code>lc &lt;- with(lc, lc[order(-as.numeric(class)), ])
g&lt;-ggplot(lc, aes(x=factor(group),y=reads, fill=factor(class),order = -as.numeric(class)))+xlab(""Group"")+scale_fill_discrete(name=""Anno Class"",breaks=c(""exon"",""intron"",""intergenic"",""unmapped""))
g+geom_bar(stat=""identity"",position=""dodge"")
</code></pre>

<p>nope</p>
"
10190581,742447,2012-04-17T11:45:22Z,2397097,20,FALSE,"<p>Keelan Evanini, Ingrid Rosenfelder and Josef Fruehwald (JoFrhwld@gmail.com) have created a ggplot2 stat implementation of a 95% confidence interval ellipses (and an easier way to plot ellipses in ggplot2):</p>

<p><a href=""https://github.com/JoFrhwld/FAAV/blob/master/r/stat-ellipse.R"" rel=""noreferrer"">GitHub stat-ellipse.R</a></p>

<p><a href=""http://jofrhwld.github.com/FAAV/"" rel=""noreferrer"">their site</a></p>

<p>You can use it as:</p>

<pre><code>library(ggplot2)
library(devtools)
library(digest)
source_url(""https://raw.github.com/low-decarie/FAAV/master/r/stat-ellipse.R"")    
qplot(data=df, x=x, y=y, colour=colour)+stat_ellipse()
</code></pre>

<p><img src=""https://i.stack.imgur.com/w197v.jpg"" alt=""enter image description here""></p>

<p>To create the data</p>

<pre><code>set.seed(101)
n &lt;- 1000
x &lt;- rnorm(n, mean=2)
y &lt;- 1.5 + 0.4*x + rnorm(n)
colour &lt;- sample(c(""first"", ""second""), size=n, replace=T)
df &lt;- data.frame(x=x, y=y, colour=colour)
</code></pre>
"
10228871,79708,2012-04-19T13:13:15Z,1923273,15,FALSE,"<p>There is a standard function in R for that</p>

<p><code>tabulate(numbers)</code></p>
"
10421875,1855677,2012-05-02T21:17:50Z,2540129,56,FALSE,"<p>The 'lattice' package is built on the grid package and attaches its namespace when 'lattice' loaded. However, in order to use the <code>grid.layout</code> function, you need to explicitly <code>load()</code> pkg::grid. The other alternative, that is probably easier, is the <code>grid.arrange</code> function in pkg::gridExtra:</p>

<pre><code> install.packages(""gridExtra"")
 require(gridExtra) # also loads grid
 require(lattice)
 x &lt;- seq(pi/4, 5 * pi, length.out = 100)
 y &lt;- seq(pi/4, 5 * pi, length.out = 100)
 r &lt;- as.vector(sqrt(outer(x^2, y^2, ""+"")))

 grid &lt;- expand.grid(x=x, y=y)
 grid$z &lt;- cos(r^2) * exp(-r/(pi^3))
 plot1 &lt;- levelplot(z~x*y, grid, cuts = 50, scales=list(log=""e""), xlab="""",
           ylab="""", main=""Weird Function"", sub=""with log scales"",
           colorkey = FALSE, region = TRUE)

 plot2 &lt;- levelplot(z~x*y, grid, cuts = 50, scales=list(log=""e""), xlab="""",
           ylab="""", main=""Weird Function"", sub=""with log scales"",
           colorkey = FALSE, region = TRUE)
 grid.arrange(plot1,plot2, ncol=2)
</code></pre>

<p><img src=""https://i.stack.imgur.com/D30i2.png"" alt=""enter image description here""></p>
"
10456146,968132,2012-05-04T21:07:50Z,1169456,7,FALSE,"<p>For yet another concrete use case, use double brackets when you want to select a data frame created by the <code>split()</code> function. If you don't know, <code>split()</code> groups a list/data frame into subsets based on a key field. It's useful if when you want to operate on multiple groups, plot them, etc.</p>

<pre><code>&gt; class(data)
[1] ""data.frame""

&gt; dsplit&lt;-split(data, data$id)
&gt; class(dsplit)
[1] ""list""

&gt; class(dsplit['ID-1'])
[1] ""list""

&gt; class(dsplit[['ID-1']])
[1] ""data.frame""
</code></pre>
"
10513504,662787,2012-05-09T09:39:32Z,1467201,12,FALSE,"<p>A bit late to the party, but:</p>

<p>Explicitly calling <code>gc</code> will free some memory ""now"". ...so if <em>other processes</em> need the memory, it might be a good idea. For example before calling <code>system</code> or similar. Or perhaps when you're ""done"" with the script and R will sit idle for a while until the next job arrives - again, so that <em>other processes</em> get more memory.</p>

<p>If you just want your script to run faster, it won't matter since R will call it later if it needs to. It might even be slower since the normal GC cycle might never have needed to call it.</p>

<p>...but if you want to measure time for instance, it is typically a good idea to do a GC before running your test. This is what <code>system.time</code> does by default.</p>

<p><strong>UPDATE</strong> As @DWin points out, R (or C#, or Java etc) doesn't always know when memory is low and the GC needs to run. So you could sometimes need to do GC as a work-around for deficiencies in the memory system.</p>
"
10521814,1855677,2012-05-09T18:17:03Z,1467201,10,FALSE,"<p>Supposedly R uses only RAM. That's just not true on a Mac (and I suspect it's not true on Windows either.) If it runs out of RAM, it will start using virtual memory. Sometimes, but not always, processes will 'recognize' that they need to run gc() and free up memory. When they do not do so, you can see this by using the ActivityMonitor.app and seeing that all the RAM is occupied and disk access has jumped up.  I find that when I am doing large Cox regression runs that I can avoid spilling over into virtual memory (with slow disk access) by preceding calls with <code>gc(); cph(...)</code></p>
"
10575323,878098,2012-05-13T21:17:07Z,2684966,0,FALSE,"<p>Well I found this solution while googling for this problem. I have not tried it, yet.</p>

<p><a href=""http://wiki.stdout.org/rcookbook/Graphs/Axes%20(ggplot2)/"" rel=""nofollow"">http://wiki.stdout.org/rcookbook/Graphs/Axes%20(ggplot2)/</a></p>

<p>You have to scroll down a bit.</p>

<p>Best,</p>

<p>Felix</p>
"
10702732,755257,2012-05-22T13:12:00Z,1358003,11,FALSE,"<ol>
<li><p>I'm fortunate and my large data sets are saved by the instrument in ""chunks"" (subsets) of roughly 100 MB (32bit binary). Thus I can do pre-processing steps (deleting uninformative parts, downsampling) sequentially before fusing the data set. </p></li>
<li><p>Calling <code>gc ()</code> ""by hand"" can help if the size of the data get close to available memory.</p></li>
<li><p>Sometimes a different algorithm needs much less memory.<br/>
Sometimes there's a trade off between vectorization and memory use.<br/>
compare: <code>split</code> &amp; <code>lapply</code> vs. a <code>for</code> loop.</p></li>
<li><p>For the sake of fast &amp; easy data analysis, I often work first with a small random subset (<code>sample ()</code>) of the data. Once the data analysis script/.Rnw is finished data analysis code and the complete data go to the calculation server for over night / over weekend / ... calculation.</p></li>
</ol>
"
10758086,403310,2012-05-25T16:25:56Z,1296646,109,FALSE,"<p>Dirk's answer is great. It also highlights a key difference in the syntax used for indexing <code>data.frame</code>s and <code>data.table</code>s:</p>

<pre><code>## The data.frame way
dd[with(dd, order(-z, b)), ]

## The data.table way: (7 fewer characters, but that's not the important bit)
dd[order(-z, b)]
</code></pre>

<p>The difference between the two calls is small, but it can have important consequences. Especially if you write production code and/or are concerned with correctness in your research, it's best to avoid unnecessary repetition of variable names. <code>data.table</code>
 helps you do this.</p>

<p>Here's an example of how repetition of variable names might get you into trouble:</p>

<p>Let's change the context from Dirk's answer, and say this is part of a bigger project where there are a lot of object names and they are long and meaningful; instead of <code>dd</code> it's called <code>quarterlyreport</code>. It becomes :</p>

<pre><code>quarterlyreport[with(quarterlyreport,order(-z,b)),]
</code></pre>

<p>Ok, fine. Nothing wrong with that. Next your boss asks you to include last quarter's report in the report. You go through your code, adding an object <code>lastquarterlyreport</code> in various places and somehow (how on earth?) you end up with this :</p>

<pre><code>quarterlyreport[with(lastquarterlyreport,order(-z,b)),]
</code></pre>

<p>That isn't what you meant but you didn't spot it because you did it fast and it's nestled on a page of similar code. The code doesn't fall over (no warning and no error) because R thinks it is what you meant. You'd hope whoever reads your report spots it, but maybe they don't. If you work with programming languages a lot then this situation may be all to familiar. It was a ""typo"" you'll say. I'll fix the ""typo"" you'll say to your boss.</p>

<p>In <a href=""http://datatable.r-forge.r-project.org/"" rel=""noreferrer""><code>data.table</code></a> we're concerned about tiny details like this. So we've done something simple to avoid typing variable names twice. Something very simple. <code>i</code> is evaluated within the frame of <code>dd</code> already, automatically. You don't need <code>with()</code> at all.</p>

<p>Instead of</p>

<pre><code>dd[with(dd, order(-z, b)), ]
</code></pre>

<p>it's just</p>

<pre><code>dd[order(-z, b)]
</code></pre>

<p>And instead of</p>

<pre><code>quarterlyreport[with(lastquarterlyreport,order(-z,b)),]
</code></pre>

<p>it's just</p>

<pre><code>quarterlyreport[order(-z,b)]
</code></pre>

<p>It's a very small difference, but it might just save your neck one day. When weighing up the different answers to this question, consider counting the repetitions of variable names as one of your criteria in deciding.  Some answers have quite a few repeats, others have none.</p>
"
10759521,812102,2012-05-25T18:20:10Z,2349205,2,FALSE,"<p>Note that you could as well plot directly from <code>ce</code> (after the comma removing) using the column name :</p>

<pre><code>hist(ce$Weight)
</code></pre>

<p>(As opposed to using <code>hist(ce[1])</code>, which would lead to the same ""must be numeric"" error.)</p>

<p>This also works for a database query result.</p>
"
10845731,1207120,2012-06-01T06:55:41Z,2558191,4,FALSE,"<p>For those familiar with SQL, another way to manipulate dataframes can be the sqldf command in the sqldf package.</p>

<pre><code>library(sqldf)
sqldf(""SELECT Widget, avg(Energy) FROM yourDataFrame GROUP BY Widget"")
</code></pre>
"
10854459,892313,2012-06-01T16:57:25Z,2558191,1,FALSE,"<p>@Jyotirmoy mentioned that this can be done with the <code>plyr</code> library. Here is what that would look like:</p>

<pre><code>DF &lt;- read.table(text=
""Widget Type Energy  
egg 1 20  
egg 2 30  
jap 3 50  
jap 1 60"", header=TRUE)

library(""plyr"")
ddply(DF, .(Widget), summarise, Energy=mean(Energy))
</code></pre>

<p>which gives</p>

<pre><code>&gt; ddply(DF, .(Widget), summarise, Energy=mean(Energy))
  Widget Energy
1    egg     25
2    jap     55
</code></pre>
"
10880504,1430950,2012-06-04T11:29:47Z,1963492,4,FALSE,"<p>I agree with cbare.
Use <code>abline</code> to draw lines only where you really need.</p>

<p>Example from my last code:</p>

<pre><code>abline(v=c(39448, 39814), col=""grey40"")
abline(h=c(-0.6, -0.4, -0.2, 0.2,0.4,0.6), col=""grey10"", lty=""dotted"") 
</code></pre>

<p>remember that:</p>

<p><code>v</code> is for vertical lines.
<code>h</code> for horizontal.</p>

<p>exploit the commands</p>

<p><code>lty</code> for dotted line
<code>color</code> for light coloured line</p>

<p>in order to obtain ""no heavy grid"".</p>
"
10884093,1382740,2012-06-04T15:42:47Z,2447454,1,FALSE,"<p>For me (2.2.1) the following also worked (as documented on <a href=""http://rpy.sourceforge.net/rpy2/doc-2.2/html/numpy.html"" rel=""nofollow"">http://rpy.sourceforge.net/rpy2/doc-2.2/html/numpy.html</a>):</p>

<pre><code>import rpy2.robjects as ro
from rpy2.robjects.numpy2ri import numpy2ri
ro.conversion.py2ri = numpy2ri
</code></pre>
"
10959020,468660,2012-06-09T06:48:42Z,2147084,0,FALSE,"<p>Since I had to do this anyway, here's a close-enough-to-figure it out code sample along the lines of what @Alex Brown suggests (scores is a 2D array of some sort, which'll get turned into a grouped vector):</p>

<pre><code>barchart(scores, horizontal=FALSE, stack=FALSE, 
     xlab='Sample', ylab='Mean Score (max of 9)',
     auto.key=list(rectangles=TRUE, points=FALSE),
     panel=function(x, y, box.ratio, groups, errbars, ...) {
         # We need to specify groups because it's not actually the 4th 
         # parameter
         panel.barchart(x, y, box.ratio, groups=groups, ...)
         x &lt;- as.numeric(x)
         nvals &lt;- nlevels(groups)
         groups &lt;- as.numeric(groups)
         box.width &lt;- box.ratio / (1 + box.ratio)
         for(i in unique(x)) {
             ok &lt;- x == i
             width &lt;- box.width / nvals
             locs &lt;- i + width * (groups[ok] - (nvals + 1)/2)
             panel.arrows(locs, y[ok] + 0.5, scores.ses[,i], ...)
         }
     } )
</code></pre>

<p>I haven't tested this, but the important bits (the parts determining the locs etc. within the panel function) do work. That's the hard part to figure out. In my case, I was actually using panel.arrows to make errorbars (the horror!). But scores.ses is meant to be an array of the same dimension as scores.</p>

<p>I'll try to clean this up later - but if someone else wants to, I'm happy for it!</p>
"
10968755,1447332,2012-06-10T12:37:07Z,2258784,7,FALSE,"<p>Besides the obvious references to <a href=""http://had.co.nz/ggplot2/"" rel=""noreferrer"">ggplot2 reference manual</a> and to the <a href=""http://wiki.stdout.org/rcookbook/Graphs/"" rel=""noreferrer"">graphs section of Cookbook for R</a>, Hadley Wickham provides a nice opts() List on <a href=""https://github.com/hadley/ggplot2/wiki/-opts%28%29-List"" rel=""noreferrer"">github</a>.</p>
"
11030558,117280,2012-06-14T09:47:13Z,2123968,7,FALSE,"<p>In case you are interested in slicing the last n elements of the array then you could use:</p>

<pre><code>x[seq(length=n, from=length(x), by=-1)] 
</code></pre>
"
11052529,1409930,2012-06-15T14:21:45Z,467110,4,FALSE,"<p>R is generally an interpreted language. However, package <code>compiler</code> offers bytecode compilation that can improve performance. You can also call compiled code from R.</p>

<p>In terms of how fast, it depends on what you are trying to do and how you are trying to do it. Some looping operations can be very slow. However, in many cases, with well written code, the performance of R scripting is determined by the speed of the underlying internal C-based libraries and system memory read-write speeds, and so R is about as fast as anything else.</p>
"
11086822,331801,2012-06-18T16:12:12Z,1377130,4,FALSE,"<p>If you are willing to consider a library, pandas (http://pandas.pydata.org/) is a library built on top of numpy which amongst many other things provides:</p>

<blockquote>
  <p>Intelligent data alignment and integrated handling of missing data: gain automatic label-based alignment in computations and easily manipulate messy data into an orderly form</p>
</blockquote>

<p>I've been using it for almost one year in the financial industry where missing and badly aligned data is the norm and it really made my life easier.</p>
"
11112125,1467981,2012-06-20T02:40:50Z,1670803,1,FALSE,"<p>This error message does not originate from the spatstat package.</p>

<p>To identify the location of the error, type <code>traceback()</code> immediately after the error has occurred. This will give you a list of the nested commands that were being executed at the time the error occurred. The top one is the location which raised the error. </p>
"
11228573,579584,2012-06-27T14:31:53Z,1309263,15,FALSE,"<p>I couldn't find a modern implementation of a c++ data structure with order-statistic so ended up implementing both ideas in top coders link suggested by MAK ( <a href=""http://www.topcoder.com/tc?module=Static&amp;d1=match_editorials&amp;d2=srm310"">Match Editorial</a>: scroll down to FloatingMedian).</p>

<h2>Two multisets</h2>

<p>The first idea partitions the data into two data structures (heaps, multisets etc) with O(ln N) per insert/delete does not allow the quantile to be changed dynamically without a large cost. I.e. we can have a rolling median, or a rolling 75% but not both at the same time.</p>

<h2>Segment tree</h2>

<p>The second idea uses a segment tree which is O(ln N) for insert/deletions/queries but is more flexible. Best of all the ""N"" is the size of your data range. So if your rolling median has a window of a million items, but your data varies from 1..65536, then only 16 operations are required per movement of the rolling window of 1 million!!</p>

<p>The c++ code is similar to what Denis posted above (""Here's a simple algorithm for quantized data"")</p>

<h2>GNU Order Statistic Trees</h2>

<p>Just before giving up, I found that stdlibc++ contains order statistic trees!!!</p>

<p>These have two critical operations:</p>

<pre><code>iter = tree.find_by_order(value)
order = tree.order_of_key(value)
</code></pre>

<p>See <a href=""http://gcc.gnu.org/onlinedocs/libstdc++/manual/policy_based_data_structures_test.html"">libstdc++ manual policy_based_data_structures_test</a> (search for ""split and join"").</p>

<p>I have wrapped the tree for use in a convenience header for compilers supporting c++0x/c++11 style partial typedefs:</p>

<pre><code>#if !defined(GNU_ORDER_STATISTIC_SET_H)
#define GNU_ORDER_STATISTIC_SET_H
#include &lt;ext/pb_ds/assoc_container.hpp&gt;
#include &lt;ext/pb_ds/tree_policy.hpp&gt;

// A red-black tree table storing ints and their order
// statistics. Note that since the tree uses
// tree_order_statistics_node_update as its update policy, then it
// includes its methods by_order and order_of_key.
template &lt;typename T&gt;
using t_order_statistic_set = __gnu_pbds::tree&lt;
                                  T,
                                  __gnu_pbds::null_type,
                                  std::less&lt;T&gt;,
                                  __gnu_pbds::rb_tree_tag,
                                  // This policy updates nodes'  metadata for order statistics.
                                  __gnu_pbds::tree_order_statistics_node_update&gt;;

#endif //GNU_ORDER_STATISTIC_SET_H
</code></pre>
"
11257998,1308011,2012-06-29T07:47:20Z,2563824,6,FALSE,"<p>If all your values have the same type and you know the number of rows, you can use a matrix in the following way (this will be very fast):</p>

<pre><code>d &lt;- matrix(nrow=20, ncol=3) 
for (i in 1:20) { d[i,] &lt;- c(i+i, i*i, i/1)}
</code></pre>

<p>If you need a data frame, you can use rbind (as another answer suggests), 
or functions from package plyr like this:</p>

<pre><code>library(plyr)
ldply(1:20, function(i)c(i+i, i*i, i/1))
</code></pre>
"
11259323,322912,2012-06-29T09:31:20Z,2563824,10,FALSE,"<p>Another way would be</p>

<pre><code>do.call(""rbind"", sapply(1:20, FUN = function(i) c(i+i,i*i,i/1), simplify = FALSE))


     [,1] [,2] [,3]
 [1,]    2    1    1
 [2,]    4    4    2
 [3,]    6    9    3
 [4,]    8   16    4
 [5,]   10   25    5
 [6,]   12   36    6
</code></pre>

<p>If you don't specify <code>simplify = FALSE</code>, you have to transpose the result using <code>t</code>. This can be tedious for large structures.</p>

<p>This solution is especially handy if you have a data set on the large side and/or you need to repeat this many many times.</p>

<p>I offer some timings of solutions in this ""thread"".</p>

<pre><code>&gt; system.time(do.call(""rbind"", sapply(1:20000, FUN = function(i) c(i+i,i*i,i/1), simplify = FALSE)))
   user  system elapsed 
   0.05    0.00    0.05 

&gt; system.time(ldply(1:20000, function(i)c(i+i, i*i, i/1)))
   user  system elapsed 
   0.14    0.00    0.14 

&gt; system.time({d &lt;- matrix(nrow=20000, ncol=3) 
+ for (i in 1:20000) { d[i,] &lt;- c(i+i, i*i, i/1)}})
   user  system elapsed 
   0.10    0.00    0.09 

&gt; system.time(ldply(1:20000, function(i)c(i+i, i*i, i/1)))
   user  system elapsed 
  62.88    0.00   62.99 
</code></pre>
"
11268379,1195861,2012-06-29T20:13:54Z,2686437,0,FALSE,"<p>the problem here was data prep error.</p>

<p>a header was re-written far down in the middle of the data set.</p>
"
11302515,1497338,2012-07-02T23:25:16Z,2564258,5,FALSE,"<p>Rather than keeping the values to be plotted in an array, store them in a matrix.  By default the entire matrix will be treated as one data set.  However if you add the same number of modifiers to the plot, e.g. the col(), as you have rows in the matrix, R will figure out that each row should be treated independently.  For example:</p>

<pre><code>x = matrix( c(21,50,80,41), nrow=2 )
y = matrix( c(1,2,1,2), nrow=2 )
plot(x, y, col(""red"",""blue"")
</code></pre>

<p>This should work unless your data sets are of differing sizes.</p>
"
11330586,1490142,2012-07-04T14:01:02Z,1395191,2,FALSE,"<p>It is also very easy to generate your plots with the lattice package:</p>

<pre><code>library(lattice)
xyplot(year~peak | site, data)
</code></pre>
"
11421185,1515917,2012-07-10T20:13:04Z,1528428,0,FALSE,"<p>For anyone stumbling into this same problem :</p>

<p>It seems that the MCMClogit function cannot handle anything but B0=0 if your model only has an intercept.</p>

<p>If you add a covariate, then you can specify a precision just fine. </p>

<p>I would consider other packages (such as arm or rjags) if you really want to sample from this model. For a list of options available for Bayesian regression, see <a href=""http://cran.r-project.org/web/views/Bayesian.html"" rel=""nofollow"">http://cran.r-project.org/web/views/Bayesian.html</a></p>
"
11539265,1395462,2012-07-18T10:26:31Z,1770787,1,FALSE,"<p>You can get the specified field value via @ for this S4 type class as Dirk Eddelbuettel mentioned.</p>

<pre><code>summary(dx.ct)@teststat

               tau3      phi2     phi3
statistic -1.668368 0.9731316 1.444461

summary(dx.ct)@cval

1pct  5pct 10pct
tau3 -3.96 -3.41 -3.12
phi2  6.09  4.68  4.03
phi3  8.27  6.25  5.34
</code></pre>
"
11554019,178297,2012-07-19T04:36:58Z,1710853,8,FALSE,"<p>Dirk's suggestion indeed works well, if you have control over the server &amp; can run xvfb.
If not, read on...</p>

<p>in newer versions of R (>= 2.10 &amp; maybe earlier), this is no longer an error, it's a warning:</p>

<pre><code>&gt; library(tcltk)
Loading Tcl/Tk interface ... done
Warning message:
In fun(libname, pkgname) : no DISPLAY variable so Tk is not available
</code></pre>

<p>You can now suppress this warning, and the subsequent package loading message via:</p>

<pre><code>&gt; suppressPackageStartupMessages(suppressWarnings(library(tcltk)))
</code></pre>

<p>Often you will see this message due to loading a package like <code>qvalue</code> which depends on <code>tcltk</code>; if you're after silent operation, you should silently load tcltk first, then the package of interest:</p>

<pre><code>&gt; suppressPackageStartupMessages(suppressWarnings(library(tcltk)))
&gt; library(qvalue)
</code></pre>

<p>Mark</p>

<p>resurrected due to: <a href=""http://dev.list.galaxyproject.org/wrapping-qvalue-in-Galaxy-td4655164.html"" rel=""noreferrer"">http://dev.list.galaxyproject.org/wrapping-qvalue-in-Galaxy-td4655164.html</a></p>
"
11653723,1137731,2012-07-25T15:51:58Z,2535234,5,FALSE,"<p>You can also check the vegan package: <a href=""http://cran.r-project.org/web/packages/vegan//index.html"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/packages/vegan//index.html</a></p>

<p>The function vegdist in this package has a variety of dissimilarity (distance) functions, such as <code>manhattan</code>, <code>euclidean</code>, <code>canberra</code>, <code>bray</code>, <code>kulczynski</code>, <code>jaccard</code>, <code>gower</code>, <code>altGower</code>, <code>morisita</code>, <code>horn</code>,<code>mountford</code>, <code>raup</code> , <code>binomial</code>, <code>chao</code> or <code>cao</code>. Please check the .pdf in the package for a definition or consult references <a href=""https://stats.stackexchange.com/a/33001/12733"">https://stats.stackexchange.com/a/33001/12733</a>.</p>
"
11758240,636656,2012-08-01T11:27:33Z,2603184,34,FALSE,"<p>Note that if you hope to use pass-by-reference simply to avoid the performance implications of copying an object that isn't modified (as is common in other languages with constant references), R does this automatically:</p>

<pre><code>n &lt;- 10^7
bigdf &lt;- data.frame( x=runif(n), y=rnorm(n), z=rt(n,5) )
myfunc &lt;- function(dat) invisible(with( dat, x^2+mean(y)+sqrt(exp(z)) ))
myfunc2 &lt;- function(dat) {
    x &lt;- with( dat, x^2+mean(y)+sqrt(exp(z)) )
    invisible(x)
}
myfunc3 &lt;- function(dat) {
    dat[1,1] &lt;- 0
    invisible( with( dat, x^2+mean(y)+sqrt(exp(z)) ) )
}
tracemem(bigdf)
&gt; myfunc(bigdf)
&gt; # nothing copied
&gt; myfunc2(bigdf)
&gt; # nothing copied!
&gt; myfunc3(bigdf)
tracemem[0x6e430228 -&gt; 0x6b75fca0]: myfunc3 
tracemem[0x6b75fca0 -&gt; 0x6e4306f0]: [&lt;-.data.frame [&lt;- myfunc3 
tracemem[0x6e4306f0 -&gt; 0x6e4304f8]: [&lt;-.data.frame [&lt;- myfunc3 
&gt; 
&gt; library(microbenchmark)
&gt; microbenchmark(myfunc(bigdf), myfunc2(bigdf), myfunc3(bigdf), times=5)
Unit: milliseconds
            expr       min        lq    median        uq       max
1 myfunc2(bigdf)  617.8176  641.7673  644.3764  683.6099  698.1078
2 myfunc3(bigdf) 1052.1128 1134.0822 1196.2832 1202.5492 1206.5925
3  myfunc(bigdf)  598.9407  622.9457  627.9598  642.2727  654.8786
</code></pre>
"
11787365,180892,2012-08-02T23:40:17Z,1853703,3,FALSE,"<p>Given that your question title is ""plotting functions in R"", here's how to use <code>curve</code> to add a function to a base R plot.</p>

<p>Create data as before</p>

<pre><code>eq = function(x){x*x}; x = (1:50); y = eq(x)
</code></pre>

<p>Then use <code>plot</code> from base graphics to plot the points followed by <code>curve</code> with the <code>add=TRUE</code> argument, to add the curve.</p>

<pre><code>plot(x, y,  xlab = ""X-axis"", ylab = ""Y-axis"") 
curve(eq, add=TRUE)
</code></pre>
"
11797879,1571326,2012-08-03T14:38:30Z,1484307,-3,FALSE,"<p>This is rather late, but I was searching for the same solution.  What I did (by searching, trial and error) is:</p>

<pre><code>from matplotlib.ticker import MultipleLocator, FormatStrFormatter
majorFormatter = FormatStrFormatter('%d') # shows 1 instead of 10^0 etc
</code></pre>

<p>and later, in the plot creating process:</p>

<pre><code>ax = subplot(111) # ie one plot, but need to refer to it as 'ax'
semilogy(x,y)
</code></pre>

<p>and just before show(),</p>

<pre><code>ax.yaxis.set_major_formatter(majorFormatter)
</code></pre>

<p>There may be unnecessary bits here, as I'm a rank Python newbie.</p>
"
11899573,403310,2012-08-10T10:13:43Z,1358003,130,FALSE,"<p>I use the <a href=""http://datatable.r-forge.r-project.org/"" rel=""noreferrer"">data.table</a> package.  With its <code>:=</code> operator you can :</p>

<ul>
<li>Add columns by reference</li>
<li>Modify subsets of existing columns by reference, and by group by reference</li>
<li>Delete columns by reference</li>
</ul>

<p>None of these operations copy the (potentially large) <code>data.table</code> at all, not even once.</p>

<ul>
<li>Aggregation is also particularly fast because <code>data.table</code> uses much less working memory.</li>
</ul>

<p>Related links :</p>

<ul>
<li><a href=""http://datatable.r-forge.r-project.org/LondonR_2012.pdf"" rel=""noreferrer"">News from data.table, London R presentation, 2012</a></li>
<li><a href=""https://stackoverflow.com/questions/7029944/when-should-i-use-the-operator-in-data-table"">When should I use the <code>:=</code> operator in data.table?</a></li>
</ul>
"
12064463,1615419,2012-08-21T23:25:30Z,1169539,3,FALSE,"<p>The <code>lm()</code> function above is an simple example. By the way, I imagine that your database has the columns as in the following form:</p>

<p>year        state        var1       var2      y...</p>

<p>In my point of view, you can to use the following code:</p>

<pre><code>require(base) 
library(base) 
attach(data) # data = your data base
             #state is your label for the states column
modell&lt;-by(data, data$state, function(data) lm(y~I(1/var1)+I(1/var2)))
summary(modell)
</code></pre>
"
12094057,1587518,2012-08-23T14:34:07Z,2600640,1,FALSE,"<p>I'm having massive problems using <code>sqlSave</code> with an IBM DB2 databank. I'm trying to avoid it by using <code>sqlQuery</code> instead to create the table with the correct formatting and then use <code>sqlSave</code> with <code>append=T</code> to force my R table into the database table. This resolve a lot of problems such as date formats and floating point numbers (instead of doubles).</p>
"
12167339,1631449,2012-08-28T20:50:59Z,736541,0,FALSE,"<pre><code>plot(anything, main=NULL)
</code></pre>

<p>Still works.</p>
"
12323663,1655522,2012-09-07T18:42:07Z,1355355,1,FALSE,"<p>I would use the following unlist()-based method:</p>

<pre><code>&gt; t &lt;- c(""bob_smith"",""mary_jane"",""jose_chung"",""michael_marx"",""charlie_ivan"")
&gt; tsplit &lt;- strsplit(t,""_"")
&gt; 
&gt; x &lt;- matrix(unlist(tsplit), 2)
&gt; x[1,]
[1] ""bob""     ""mary""    ""jose""    ""michael"" ""charlie""
</code></pre>

<p>The big advantage of this method is that it solves the equivalent problem for surnames at the same time:</p>

<pre><code>&gt; x[2,]
[1] ""smith"" ""jane""  ""chung"" ""marx""  ""ivan"" 
</code></pre>

<p>The downside is that you'll need to be certain that all of the names conform to the <code>firstname_lastname</code> structure; if any don't then this method will break.</p>
"
12324312,1655608,2012-09-07T19:35:04Z,1395117,37,FALSE,"<p>Change the tzone attribute of a 'POSIXct' object:</p>

<pre><code>&gt; pb.txt &lt;- ""2009-06-03 19:30""  
&gt; pb.date &lt;- as.POSIXct(pb.txt, tz=""Europe/London"")  
&gt; attributes(pb.date)$tzone &lt;- ""America/Los_Angeles""  
&gt; pb.date  
[1] ""2009-06-03 11:30:00 PDT""
</code></pre>

<p>Note that this is still a POSIXct object, tzone has changed, and correct offset has been applied:</p>

<pre><code>&gt; attributes(pb.date)
$class
[1] ""POSIXct"" ""POSIXt"" 

$tzone
[1] ""America/Los_Angeles""
</code></pre>
"
12489838,1385941,2012-09-19T06:54:59Z,2626567,10,FALSE,"<p>Here is a <code>data.table</code> solution which will be time and memory efficient for large data sets</p>

<pre><code>library(data.table)
DT &lt;- as.data.table(d)           # convert to data.table
setkey(DT, x)                    # set key to allow binary search using `J()`
DT[J(unique(x)), mult ='last']   # subset out the last row for each x
DT[J(unique(x)), mult ='first']  # if you wanted the first row for each x
</code></pre>
"
12614723,1385941,2012-09-27T05:36:40Z,2643939,60,FALSE,"<p>The two approaches offered thus far fail with large data sets as (amongst other memory issues) they create <code>is.na(df)</code>, which will be an object the same size as <code>df</code>.</p>

<p>Here are two approaches that are more memory and time efficient</p>

<p>An approach using <code>Filter</code></p>

<pre><code>Filter(function(x)!all(is.na(x)), df)
</code></pre>

<p>and an approach using data.table (for general time and memory efficiency)</p>

<pre><code>library(data.table)
DT &lt;- as.data.table(df)
DT[,which(unlist(lapply(DT, function(x)!all(is.na(x))))),with=F]
</code></pre>

<h1>examples using large data (30 columns, 1e6 rows)</h1>

<pre><code>big_data &lt;- replicate(10, data.frame(rep(NA, 1e6), sample(c(1:8,NA),1e6,T), sample(250,1e6,T)),simplify=F)
bd &lt;- do.call(data.frame,big_data)
names(bd) &lt;- paste0('X',seq_len(30))
DT &lt;- as.data.table(bd)

system.time({df1 &lt;- bd[,colSums(is.na(bd) &lt; nrow(bd))]})
# error -- can't allocate vector of size ...
system.time({df2 &lt;- bd[, !apply(is.na(bd), 2, all)]})
# error -- can't allocate vector of size ...
system.time({df3 &lt;- Filter(function(x)!all(is.na(x)), bd)})
## user  system elapsed 
## 0.26    0.03    0.29 
system.time({DT1 &lt;- DT[,which(unlist(lapply(DT, function(x)!all(is.na(x))))),with=F]})
## user  system elapsed 
## 0.14    0.03    0.18 
</code></pre>
"
12670176,203420,2012-10-01T08:56:07Z,1395309,15,FALSE,"<p>As of version 2.15, R now comes with native support for multi-core computations. Just load the  parallel package</p>

<pre><code>library(""parallel"")
</code></pre>

<p>and check out the associated vignette</p>

<pre><code>vignette(""parallel"")
</code></pre>
"
12811688,180892,2012-10-10T03:51:36Z,1848331,3,FALSE,"<p><code>XLConnect</code> works well. It is cross-platform. It can read and write xls and xlsx files. See <a href=""https://stackoverflow.com/a/6100369/180892"">this previous answer</a></p>
"
12812107,474349,2012-10-10T04:46:44Z,1848331,2,FALSE,"<p>This question is 3 years old, but I'll throw this in: if you want to write to the spreadsheet, remember to add <code>readOnly=FALSE</code> as an argument to <code>odbcConnectExcel</code> and <code>odbcConnectExcel2007</code>.</p>
"
12839552,1738003,2012-10-11T12:28:41Z,1735540,6,FALSE,"<p>A traditional Pareto chart in ggplot2.......</p>

<p>Developed after reading
Cano, E. L., Moguerza, J. M., &amp; Redchuk, A. (2012). Six Sigma with R. (G. Robert, K. Hornik, &amp; G. Parmigiani, Eds.) Springer. </p>

<pre><code>library(ggplot2);library(grid)

counts  &lt;- c(80, 27, 66, 94, 33)
defects &lt;- c(""price code"", ""schedule date"", ""supplier code"", ""contact num."", ""part num."")
dat &lt;- data.frame(count = counts, defect = defects, stringsAsFactors=FALSE )
dat &lt;- dat[order(dat$count, decreasing=TRUE),]
dat$defect &lt;- factor(dat$defect, levels=dat$defect)
dat$cum &lt;- cumsum(dat$count)
count.sum&lt;-sum(dat$count)
dat$cum_perc&lt;-100*dat$cum/count.sum

p1&lt;-ggplot(dat, aes(x=defect, y=cum_perc, group=1))
p1&lt;-p1 + geom_point(aes(colour=defect), size=4) + geom_path()

p1&lt;-p1+ ggtitle('Pareto Chart')+ theme(axis.ticks.x = element_blank(), axis.title.x = element_blank(),axis.text.x = element_blank())
p1&lt;-p1+theme(legend.position=""none"")

p2&lt;-ggplot(dat, aes(x=defect, y=count,colour=defect, fill=defect))
p2&lt;- p2 + geom_bar()

p2&lt;-p2+theme(legend.position=""none"")

plot.new()
grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 1)))
print(p1, vp = viewport(layout.pos.row = 1,layout.pos.col = 1))
print(p2, vp = viewport(layout.pos.row = 2,layout.pos.col = 1))
</code></pre>
"
12956923,1301710,2012-10-18T14:25:29Z,2682144,29,TRUE,"<p><a href=""http://pandas.pydata.org/""><code>Pandas</code></a> has a built in function <code>scatter_matrix</code> (<a href=""https://github.com/pydata/pandas/blob/master/pandas/tools/plotting.py#L28"">source code</a>) which is something like this.</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 

df = pd.DataFrame(np.random.randn(1000, 4), columns=['A','B','C','D'])
axes = pd.tools.plotting.scatter_matrix(df, alpha=0.2)
plt.tight_layout()
plt.savefig('scatter_matrix.png')
</code></pre>

<p><img src=""https://i.stack.imgur.com/V5Ke2.png"" alt=""scatter_matrix.png""></p>

<p>However it is <code>pandas</code> specific (but could be used as a starting point).</p>

<p>There are some more <code>R</code> like plots in pandas. Have a look at the <a href=""http://pandas.pydata.org/pandas-docs/stable/visualization.html"">docs</a>. </p>
"
12966019,1757985,2012-10-19T01:09:54Z,2573132,4,FALSE,"<p>PypeR it's an option if you're trying to use R with recent versions of Python (like 3.1)</p>

<p>More info at:</p>

<p><a href=""http://rinpy.sourceforge.net/"" rel=""nofollow"">http://rinpy.sourceforge.net/</a></p>
"
12970328,1116842,2012-10-19T08:35:50Z,2751065,23,FALSE,"<p>Nowadays the usage of <code>opts</code> and <code>theme_text</code> seems to be deprecated. R suggests to use <code>theme</code> and <code>element_text</code>. A solution to the answer can be found here: <a href=""http://wiki.stdout.org/rcookbook/Graphs/Facets%20%28ggplot2%29/#modifying-facet-label-text"">http://wiki.stdout.org/rcookbook/Graphs/Facets%20%28ggplot2%29/#modifying-facet-label-text</a></p>

<pre><code>qplot(hwy, cty, data = mpg) + 
      facet_grid(. ~ manufacturer) + 
      theme(strip.text.x = element_text(size = 8, colour = ""red"", angle = 90))
</code></pre>
"
12994156,1512317,2012-10-21T01:29:01Z,2564258,8,FALSE,"<p>You can use points for the overplot, that is.</p>

<pre><code>plot(x1, y1,col='red')

points(x2,y2,col='blue')
</code></pre>
"
13004554,800044,2012-10-22T03:12:17Z,2684966,2,FALSE,"<p>The answers above will not work in ggplot2 version 0.9.2.1 and above. Fortunately, there is now an easier way to do this, as described in response to a different question: <a href=""https://stackoverflow.com/a/8992102/800044"">https://stackoverflow.com/a/8992102/800044</a>.</p>
"
13020992,1724060,2012-10-22T22:41:22Z,1395147,2,FALSE,"<p>I am no expert in R. But I use:</p>

<pre><code>xyplot(y ~ x, groups= f, data= Dat, type= c('p','r'), 
   grid= T, lwd= 3, auto.key= T,)
</code></pre>

<p>This is also an option:</p>

<pre><code>interaction.plot(f,x,y, type=""b"", col=c(1:3), 
             leg.bty=""0"", leg.bg=""beige"", lwd=1, pch=c(18,24), 
             xlab="""", 
             ylab="""",
             trace.label="""",
             main=""Interaction Plot"")
</code></pre>
"
13023005,212731,2012-10-23T03:09:14Z,1445964,10,FALSE,"<p>Doing <code>options(error=traceback)</code> provides a little more information about the content of the lines leading up to the error.  It causes a traceback to appear if there is an error, and for some errors it has the line number, prefixed by <code>#</code>.  But it's hit or miss, many errors won't get line numbers.  </p>
"
13085482,1759974,2012-10-26T10:49:19Z,2436688,2,FALSE,"<p>try this function lappend</p>

<pre><code>lappend &lt;- function (lst, ...){
  lst &lt;- c(lst, list(...))
  return(lst)
}
</code></pre>

<p>and other suggestions from this page <a href=""https://stackoverflow.com/questions/9031819/add-named-vector-to-a-list/12978667#12978667"">Add named vector to a list</a></p>

<p>Bye.</p>
"
13108227,576974,2012-10-28T10:57:22Z,2061897,19,FALSE,"<p>Here is the missing example</p>

<pre><code>library(rjson)
url &lt;- 'http://someurl/data.json'
document &lt;- fromJSON(file=url, method='C')
</code></pre>
"
13155157,684229,2012-10-31T09:48:21Z,2602583,5,FALSE,"<p>I use exactly what Mark says. This way, even with tapply, you can use the built-in <code>mean</code> function, no need to define yours! For example, to compute per-group geometric means of data$value:</p>

<pre><code>exp(tapply(log(data$value), data$group, mean))
</code></pre>
"
13217538,1797801,2012-11-04T09:46:05Z,1231195,7,FALSE,"<p>If find it incredible that any language would not cater for this.</p>

<p>This is probably the cleanest workaround:</p>

<pre><code>anything=""
first comment line
second comment line
""
</code></pre>
"
13419048,1264199,2012-11-16T14:56:36Z,652136,-2,FALSE,"<p>You can use <code>which</code>.</p>

<pre><code>x&lt;-c(1:5)
x
#[1] 1 2 3 4 5
x&lt;-x[-which(x==4)]
x
#[1] 1 2 3 5
</code></pre>
"
13504690,173985,2012-11-22T01:31:04Z,1231195,19,FALSE,"<p>[Update] Based on comments.</p>

<pre><code># An empty function for Comments
Comment &lt;- function(`@Comments`) {invisible()}

#### Comments ####
Comment( `

  # Put anything in here except back-ticks.

  api_idea &lt;- function() {
    return TRUE
  }

  # Just to show api_idea isn't really there...
  print( api_idea )

`)
####

#### Code. ####
foo &lt;- function() {
  print( ""The above did not evaluate!"")
}
foo()
</code></pre>

<hr>

<p>[Original Answer]</p>

<p>Here's another way...  check out the pic at the bottom.  Cut and paste the code block into RStudio.</p>

<p>Multiline comments that make using an IDE <em>more</em> effective are a ""Good Thing"", most IDEs or simple editors don't have highlighting of text within simple commented -out blocks; though some authors have taken the time to ensure parsing within here-strings.  With R we don't have multi-line comments or here-strings either, but using invisible expressions in RStudio gives all that goodness.</p>

<p>As long as there aren't any backticks in the section desired to be used for a multiline comments, here-strings, or non-executed comment blocks then this might be something worth-while.</p>

<pre><code>#### Intro Notes &amp; Comments ####
invisible( expression( `
{ &lt;= put the brace here to reset the auto indenting...

  Base &lt;- function()
  {      &lt;^~~~~~~~~~~~~~~~ Use the function as a header and nesting marker for the comments
         that show up in the jump-menu.
         ---&gt;8---
  }

  External &lt;- function()
  {
    If we used a function similar to:
      api_idea &lt;- function() {

        some_api_example &lt;- function( nested ) {
          stopifnot( some required check here )
        }

        print(""Cut and paste this into RStudio to see the code-chunk quick-jump structure."")
        return converted object
      }

    #### Code. ####
    ^~~~~~~~~~~~~~~~~~~~~~~~~~ &lt;= Notice that this comment section isnt in the jump menu!
                                  Putting an apostrophe in isn't causes RStudio to parse as text
                                  and needs to be matched prior to nested structure working again.
    api_idea2 &lt;- function() {

    } # That isn't in the jump-menu, but the one below is...

    api_idea3 &lt;- function() {

    }

  }

    # Just to show api_idea isn't really there...
    print( api_idea )
    }`) )
####

#### Code. ####
foo &lt;- function() {
  print( ""The above did not evaluate and cause an error!"")
}

foo()

## [1] ""The above did not evaluate and cause an error!""
</code></pre>

<p>And here's the pic...</p>

<p><img src=""https://i.stack.imgur.com/PyqYn.png"" alt=""Structured Comments""></p>
"
13508639,915961,2012-11-22T08:36:46Z,1154242,3,FALSE,"<p>you can also put labels inside plot:</p>

<pre><code>plot(spline(sub$day, sub$counts), type ='l', labels = FALSE)
</code></pre>

<p>you'll get a warning. i think this is because labels is actually a parameter that's being passed down to a subroutine that plot runs (axes?). the warning will pop up because it wasn't directly a parameter of the plot function.</p>
"
13588711,200234,2012-11-27T16:31:05Z,1395323,6,FALSE,"<p>You can use system fonts with <a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/cairo.html"" rel=""noreferrer""><code>cairo_pdf</code></a>. On Ubuntu (and many other types of Linux, I guess), the <code>family</code> argument takes any font name you see in <a href=""http://manpages.ubuntu.com/manpages/precise/en/man1/fc-list.1.html"" rel=""noreferrer""><code>fc-list</code></a>.</p>

<p>Alternatively, you can use the <a href=""http://cran.r-project.org/web/packages/extrafont/"" rel=""noreferrer""><code>extrafont</code></a> package. This will allow you to use any system font with the regular <a href=""http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/pdf.html"" rel=""noreferrer""><code>pdf</code></a> device.</p>
"
13679210,1340207,2012-12-03T07:57:45Z,1714280,3,FALSE,"<p>Don't Know if this functionality was available when you first asked this question but this is easily available in base R now with the arima function; just specify your external regressors with the xreg argument within the function. Try <code>?arima</code> and when you read the documentation pay special attention to the xreg argument. This has been made very easy, good luck.</p>
"
13705159,125663,2012-12-04T14:29:14Z,2547402,1,FALSE,"<p>Another simple option that gives all values ordered by frequency is to use <code>rle</code>:</p>

<pre><code>df = as.data.frame(unclass(rle(sort(mySamples))))
df = df[order(-df$lengths),]
head(df)
</code></pre>
"
13745744,1247080,2012-12-06T14:30:36Z,1407238,1,FALSE,"<p>This is <a href=""http://www.autoitscript.com/site/autoit/"" rel=""nofollow"">AutoIt</a> code which does the same thing (replaces <code>\</code> with <code>/</code>).</p>

<pre><code>Local $text1 = ClipGet()
$text2=StringReplace($text1,""\"",""/"")
ClipPut($text2)
</code></pre>
"
13776712,1495093,2012-12-08T10:44:15Z,2602583,10,FALSE,"<p>you can use <code>psych</code> package and call <code>geometric.mean</code> function in that.</p>
"
13835535,804384,2012-12-12T08:20:19Z,1484307,0,FALSE,"<p>As I understand the question, the original poster wanted to get rid of the scientific notation of the labels. I had the same problem and found out this one works for that purpose (without using package sfsmisc from Kevin's answer, which I did not try):</p>

<pre><code>plot((1:100)^3, log = ""y"", yaxt = ""n"")
axis(2, at=axTicks(2,log=TRUE), labels=format(axTicks(2, log=TRUE), scientific=FALSE))
</code></pre>
"
13868991,1614947,2012-12-13T21:43:28Z,1923273,28,FALSE,"<p>My preferred solution uses <code>rle</code>, which will return a value (the label, <code>x</code> in your example) and a length, which represents how many times that value appeared in sequence.</p>

<p>By combining <code>rle</code> with <code>sort</code>, you have an extremely fast way to count the number of times any value appeared. This can be helpful with more complex problems.</p>

<p>Example:</p>

<pre><code>&gt; numbers &lt;- c(4,23,4,23,5,43,54,56,657,67,67,435,453,435,324,34,456,56,567,65,34,435)
&gt; a &lt;- rle(sort(numbers))
&gt; a
  Run Length Encoding
    lengths: int [1:15] 2 1 2 2 1 1 2 1 2 1 ...
    values : num [1:15] 4 5 23 34 43 54 56 65 67 324 ...
</code></pre>

<p>If the value you want doesn't show up, or you need to store that value for later, make <code>a</code> a <code>data.frame</code>.</p>

<pre><code>&gt; b &lt;- data.frame(number=a$values, n=a$lengths)
&gt; b
    values n
 1       4 2
 2       5 1
 3      23 2
 4      34 2
 5      43 1
 6      54 1
 7      56 2
 8      65 1
 9      67 2
 10    324 1
 11    435 3
 12    453 1
 13    456 1
 14    567 1
 15    657 1
</code></pre>

<p>I find it is rare that I want to know the frequency of one value and not all of the values, and rle seems to be the quickest way to get count and store them all.</p>
"
13874750,1001848,2012-12-14T08:00:22Z,2547402,24,FALSE,"<p>A quick and dirty way of estimating the mode of a vector of numbers you believe come from a continous univariate distribution (e.g. a normal distribution) is defining and using the following function:</p>

<pre><code>estimate_mode &lt;- function(x) {
  d &lt;- density(x)
  d$x[which.max(d$y)]
}
</code></pre>

<p>Then to get the mode estimate:</p>

<pre><code>x &lt;- c(5.8, 5.6, 6.2, 4.1, 4.9, 2.4, 3.9, 1.8, 5.7, 3.2)
estimate_mode(x)
## 5.439788
</code></pre>
"
13889028,1096991,2012-12-15T03:31:09Z,2258784,1,FALSE,"<p>Best list I've found for version 0.9.2.1 is <a href=""http://docs.ggplot2.org/0.9.2.1/theme.html"" rel=""nofollow"">here</a>.</p>
"
13895582,1906816,2012-12-15T19:47:01Z,1395105,4,FALSE,"<p>Here's something from my own Lab Reports.</p>

<ul>
<li><code>tickzDevice</code> exports <code>tikz</code> images for <code>LaTeX</code></li>
<li><p>Note, that in certain cases <code>""\\""</code> becomes <code>""\""</code> and <code>""$""</code> becomes <code>""$\""</code> as in the following R code: <code>""$z\\frac{a}{b}$"" -&gt; ""$\z\frac{a}{b}$\""</code></p></li>
<li><p>Also xtable exports tables to latex code</p></li>
</ul>

<p>The code:</p>

<pre><code>library(reshape2)
library(plyr)
library(ggplot2)
library(systemfit)
library(xtable)
require(graphics)
require(tikzDevice)

setwd(""~/DataFolder/"")
Lab5p9 &lt;- read.csv (file=""~/DataFolder/Lab5part9.csv"", comment.char=""#"")

AR &lt;- subset(Lab5p9,Region == ""Forward.Active"")

# make sure the data names aren't already in latex format, it interferes with the ggplot ~  # tikzDecice combo
colnames(AR) &lt;- c(""$V_{BB}[V]$"", ""$V_{RB}[V]$"" ,  ""$V_{RC}[V]$"" , ""$I_B[\\mu A]$"" , ""IC"" , ""$V_{BE}[V]$"" , ""$V_{CE}[V]$"" , ""beta"" , ""$I_E[mA]$"")

# make sure the working directory is where you want your tikz file to go
setwd(""~/TexImageFolder/"")

# export plot as a .tex file in the tikz format
tikz('betaplot.tex', width = 6,height = 3.5,pointsize = 12) #define plot name size and font size

#define plot margin widths
par(mar=c(3,5,3,5)) # The syntax is mar=c(bottom, left, top, right).

ggplot(AR, aes(x=IC, y=beta)) +                                # define data set 
    geom_point(colour=""#000000"",size=1.5) +                # use points
    geom_smooth(method=loess,span=2) +                     # use smooth
    theme_bw() +                    # no grey background
    xlab(""$I_C[mA]$"") +                 # x axis label in latex format
    ylab (""$\\beta$"") +                 # y axis label in latex format
    theme(axis.title.y=element_text(angle=0)) + # rotate y axis label
    theme(axis.title.x=element_text(vjust=-0.5)) +  # adjust x axis label down
    theme(axis.title.y=element_text(hjust=-0.5)) +  # adjust y axis lable left
    theme(panel.grid.major=element_line(colour=""grey80"", size=0.5)) +# major grid color
    theme(panel.grid.minor=element_line(colour=""grey95"", size=0.4)) +# minor grid color 
    scale_x_continuous(minor_breaks=seq(0,9.5,by=0.5)) +# adjust x minor grid spacing
    scale_y_continuous(minor_breaks=seq(170,185,by=0.5)) + # adjust y minor grid spacing
    theme(panel.border=element_rect(colour=""black"",size=.75))# border color and size

dev.off() # export file and exit tikzDevice function
</code></pre>
"
13965103,1031175,2012-12-20T04:01:40Z,1727772,65,FALSE,"<p>Strangely, no one answered the bottom part of the question for years even though this is an important one -- <code>data.frame</code>s are simply lists with the right attributes, so if you have large data you don't want to use <code>as.data.frame</code> or similar for a list. It's much faster to simply ""turn"" a list into a data frame in-place:</p>

<pre><code>attr(df, ""row.names"") &lt;- .set_row_names(length(df[[1]]))
class(df) &lt;- ""data.frame""
</code></pre>

<p>This makes no copy of the data so it's immediate (unlike all other methods). It assumes that you have already set <code>names()</code> on the list accordingly.</p>

<p>[As for loading large data into R -- personally, I dump them by column into binary files and use <code>readBin()</code> - that is by far the fastest method (other than mmapping) and is only limited by the disk speed. Parsing ASCII files is inherently slow (even in C) compared to binary data.]</p>
"
14026150,190277,2012-12-24T22:30:06Z,1411599,1,TRUE,"<p>The way to do this without disaggregating is to use <code>stat=""identity""</code> in <code>geom_bar</code>.</p>

<p>It helps to have the data frame containing numeric values rather than character strings to start:</p>

<pre><code>dat &lt;- data.frame(Zoo = ""Omaha"",
               Animals = 50, Bears = 10, `Polar Bears` = 3)
</code></pre>

<p>We do need <code>reshape2::melt</code> to get the data organized properly:</p>

<pre><code>library(reshape2)
d3 &lt;- melt(dat,id.var=1)
</code></pre>

<p>Now create the plot (identical to the other answer):</p>

<pre><code>library(ggplot2)
ggplot(d3, aes(x = variable, y = value)) +
    geom_bar(width = 1, colour = ""black"",stat=""identity"") +
    coord_polar()
</code></pre>
"
14026220,471093,2012-12-24T22:42:14Z,1411599,2,FALSE,"<p>You can use <code>inverse.rle</code> to recreate the data,</p>

<pre><code>dd = list(lengths = unlist(dat[-1]), values = names(dat)[-1])
class(dd) = ""rle""
inverse.rle(dd)
</code></pre>

<p>If you have multiple Zoos (rows), you can try</p>

<pre><code>l = plyr::dlply(dat, ""Zoo"", function(z)  
      structure(list(lengths = unlist(z[-1]), values = names(z)[-1]), class = ""rle""))

reshape2::melt(llply(l, inverse.rle))
</code></pre>
"
14030964,1928239,2012-12-25T13:14:08Z,1934751,2,FALSE,"<p>Another easy way would be:</p>

<pre><code>x &lt;- c(1,1,2,1,2,3,3,3,4,4)
myhist &lt;- hist(x)
myhistX &lt;- myhist$mids
myhistY &lt;- myhist$density
</code></pre>

<p>And now you can plot it in any way you like: </p>

<pre><code>plot(myhistY~myhistX)
</code></pre>

<p>This way you can change the stacking options when building a ""histogram"" object.  </p>
"
14056695,627042,2012-12-27T15:01:48Z,2151212,11,FALSE,"<p>Try library(getopt) ... if you want things to be nicer. For example:</p>

<pre><code>spec &lt;- matrix(c(
        'in'     , 'i', 1, ""character"", ""file from fastq-stats -x (required)"",
        'gc'     , 'g', 1, ""character"", ""input gc content file (optional)"",
        'out'    , 'o', 1, ""character"", ""output filename (optional)"",
        'help'   , 'h', 0, ""logical"",   ""this help""
),ncol=5,byrow=T)

opt = getopt(spec);

if (!is.null(opt$help) || is.null(opt$in)) {
    cat(paste(getopt(spec, usage=T),""\n""));
    q();
}
</code></pre>
"
14087041,812575,2012-12-29T23:55:23Z,2359723,28,FALSE,"<p>Opts is being deprecated. One option is to use labs()</p>

<pre><code>myTitle &lt;- ""My title""
qplot(mpg, wt, data = mtcars) + labs(title = myTitle)
</code></pre>

<p>Pretty much the same.</p>
"
14174547,1627152,2013-01-05T17:44:50Z,2286085,3,TRUE,"<p>There are some methods around for multiple comparisons in GLMs</p>

<p><a href=""http://www.r-bloggers.com/multiple-comparisons-for-glmms-using-glmer-glht/"" rel=""nofollow"">http://www.r-bloggers.com/multiple-comparisons-for-glmms-using-glmer-glht/</a></p>

<p>There is an article about simultaneous inference from the R-Project Handbook of Statistical Analyses (website) ...</p>

<p><a href=""http://cran.r-project.org/web/packages/HSAUR2/vignettes/Ch_simultaneous_inference.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/HSAUR2/vignettes/Ch_simultaneous_inference.pdf</a></p>

<p>plotmeans() from the gplot package. That includes confidence intervals.</p>

<p>Then there is a error.bars.by() function of the package ""psych"". Plots the means and SDs groupwise from a dataframe.</p>

<p>Some use density plots for visualization.</p>

<pre><code># Compare MPG distributions for cars with 
# 4,6, or 8 cylinders
library(sm)
attach(mtcars)

# create value labels 
cyl.f &lt;- factor(cyl, levels= c(4,6,8),
  labels = c(""4 cylinder"", ""6 cylinder"", ""8 cylinder"")) 

# plot densities 
sm.density.compare(mpg, cyl, xlab=""Miles Per Gallon"")
title(main=""MPG Distribution by Car Cylinders"")

# add legend via mouse click
colfill&lt;-c(2:(2+length(levels(cyl.f)))) 
legend(locator(1), levels(cyl.f), fill=colfill)
</code></pre>
"
14179446,1764155,2013-01-06T04:55:30Z,359438,4,FALSE,"<p>Try <a href=""http://lpsolve.sourceforge.net/5.5/R.htm"" rel=""nofollow"">lpSolve</a> with R. </p>

<p>A simple example:</p>

<pre><code># Maximize 
#   x1 + 9 x2 +   x3 
# Subject to: 
#   x1 + 2 x2 + 3 x3 &lt;= 9
# 3 x1 + 2 x2 + 2 x3 &lt;= 15
f.obj &lt;- c(1, 9, 3)
f.con &lt;- matrix(c(1, 2, 3, 3, 2, 2), nrow = 2, byrow = TRUE)
f.dir &lt;- c(""&lt;="", ""&lt;="")
f.rhs &lt;- c(9, 15)

lp(""max"", f.obj, f.con, f.dir, f.rhs)
lp(""max"", f.obj, f.con, f.dir, f.rhs)$solution
</code></pre>
"
14179534,1764155,2013-01-06T05:13:54Z,359438,5,FALSE,"<p>you should also try the <a href=""http://cran.r-project.org/web/packages/Rglpk/index.html"">Rglpk</a> package solve LP problems with <a href=""http://www.gnu.org/software/glpk/"">GLPK (GNU Linear Programming Kit)</a>.</p>

<p>An example:</p>

<pre><code>## Simple linear program.
## maximize:   2 x_1 + 4 x_2 + 3 x_3
## subject to: 3 x_1 + 4 x_2 + 2 x_3 &lt;= 60
##             2 x_1 +   x_2 +   x_3 &lt;= 40
##               x_1 + 3 x_2 + 2 x_3 &lt;= 80
##               x_1, x_2, x_3 are non-negative real numbers

obj &lt;- c(2, 4, 3)
mat &lt;- matrix(c(3, 2, 1, 4, 1, 3, 2, 2, 2), nrow = 3)
dir &lt;- c(""&lt;="", ""&lt;="", ""&lt;="")
rhs &lt;- c(60, 40, 80)
max &lt;- TRUE

Rglpk_solve_LP(obj, mat, dir, rhs, max = max)
</code></pre>

<p>R output:<br>
(Note that <code>$status</code> an integer with status information about the solution returned. If the control parameter canonicalize_status is set (the default) then it will return 0 for the optimal solution being found, and non-zero otherwise. If the control parameter is set to FALSE it will return the GLPK status codes). </p>

<pre><code>$optimum
[1] 76.66667

$solution
[1]  0.000000  6.666667 16.666667

$status
[1] 0
</code></pre>
"
14323064,17523,2013-01-14T17:02:26Z,1088639,4,FALSE,"<p>Here is another approach. This one requires less typing and (in my opinion) more readable:</p>

<pre><code>f &lt;- function(x) {
    y &lt;- attr(f, ""sum"")
    if (is.null(y)) {
        y &lt;- 0
    }
    y &lt;- x + y
    attr(f, ""sum"") &lt;&lt;- y
    return(y)
}
</code></pre>

<p>This snippet, as well as more complex example of the concept can by found <a href=""http://www.r-bloggers.com/emulating-local-static-variables-in-r/"" rel=""nofollow"">in this R-Bloggers article</a></p>
"
14387457,1988435,2013-01-17T20:21:42Z,2462708,6,FALSE,"<p>Try adding this line to the file <code>filetype.vim</code>. <code>filetype.vim</code> is generally located in the folder <code>~/.vim</code>. Create the file if it doesn't exist.</p>

<pre><code>au! BufNewFile,BufRead *.R,*.Rout,*.r,*.Rhistory,*.Rt,*.Rout.save,*.Rout.fail setf r
</code></pre>

<p>On startup, vim links all the file extensions listed above to the filetype r, which is likely used by your syntax file. Your syntax file will not define the file extension for you typically.</p>

<p>See this <a href=""http://vim.wikia.com/wiki/Filetype.vim"" rel=""noreferrer"">reference</a> for more information. For syntax highlighting, this <a href=""http://vim.wikia.com/wiki/Creating_your_own_syntax_files"" rel=""noreferrer"">tutorial</a> might be helpful.</p>
"
14471700,2002379,2013-01-23T02:55:58Z,2161152,7,FALSE,"<p>This is a bit late, but for future reference you might consider my <a href=""https://github.com/gpoore/pythontex"">PythonTeX</a> package for LaTeX.  PythonTeX allows you enter Python code in a LaTeX document, run it, and bring back the output. But unlike Sweave, the document you actually edit is a valid .tex document (not .Snw or .Rnw), so editing the non-code part of the document is fast and convenient.</p>

<p>PythonTeX provides many features, including the following:</p>

<ul>
<li>The document can be compiled without running any Python code; code only needs to be executed when it is modified.</li>
<li>All Python output is saved or cached.</li>
<li>Code runs in user-defined sessions.  If there are multiple sessions, sessions automatically run in parallel using all available cores.</li>
<li>Errors and warnings are synchronized with the line numbers of the .tex document, so you know exactly where they came from.</li>
<li>Code can be executed, typeset, or typeset and executed.  Syntax highlighting is provided by Pygments.</li>
<li>Anything printed by Python is automatically brought into the .tex document.</li>
<li>You can customize when code is re-executed (modified, errors, warnings, etc.).</li>
<li>The PythonTeX utilities class is available in all code that is executed.  It allows you to automatically track dependencies and specify created files that should be cleaned up.  For example, you can set the document to detect when the data it depends on is modified, so that code will be re-executed.</li>
</ul>

<p>A basic PythonTeX file looks like this:</p>

<pre><code>\documentclass{article}
\usepackage{pythontex}

\begin{document}

\begin{pycode}
#Whatever you want here!
\end{pycode}

\end{document}
</code></pre>
"
14511699,2009032,2013-01-24T22:08:26Z,1169456,10,FALSE,"<p>To help newbies navigate through the manual fog, it might be helpful to see the <code>[[ ... ]]</code> notation as a <em>collapsing</em> function - in other words, it is when you just want to 'get the data' from a named vector, list or data frame. It is good to do this if you want to use data from these objects for calculations. These simple examples will illustrate.</p>

<pre><code>(x &lt;- c(x=1, y=2)); x[1]; x[[1]]
(x &lt;- list(x=1, y=2, z=3)); x[1]; x[[1]]
(x &lt;- data.frame(x=1, y=2, z=3)); x[1]; x[[1]]
</code></pre>

<p>So from the third example:</p>

<pre><code>&gt; 2 * x[1]
  x
1 2
&gt; 2 * x[[1]]
[1] 2
</code></pre>
"
14553602,1796914,2013-01-27T23:42:19Z,2564258,69,FALSE,"<p>When constructing multilayer plots one should consider <code>ggplot</code> package. The idea is to   create a graphical object with basic aesthetics and enhance it incrementally.</p>

<p><code>ggplot</code> style requires data to be packed in <code>data.frame</code>.</p>

<pre><code># Data generation
x  &lt;- seq(-2, 2, 0.05)
y1 &lt;- pnorm(x)
y2 &lt;- pnorm(x,1,1)
df &lt;- data.frame(x,y1,y2)
</code></pre>

<p>Basic solution:</p>

<pre><code>require(ggplot2)

ggplot(df, aes(x)) +                    # basic graphical object
  geom_line(aes(y=y1), colour=""red"") +  # first layer
  geom_line(aes(y=y2), colour=""green"")  # second layer
</code></pre>

<p>Here <code>+ operator</code> is used to add extra layers to basic object.</p>

<p>With <code>ggplot</code> you have access to graphical object on every stage of plotting. Say, usual step-by-step setup can look like this:</p>

<pre><code>g &lt;- ggplot(df, aes(x))
g &lt;- g + geom_line(aes(y=y1), colour=""red"")
g &lt;- g + geom_line(aes(y=y2), colour=""green"")
g
</code></pre>

<p><code>g</code> produces the plot, and you can see it at every stage (well, after creation of at least one layer). Further enchantments of the plot are also made with created object. For example, we can add labels for axises:</p>

<pre><code>g &lt;- g + ylab(""Y"") + xlab(""X"")
g
</code></pre>

<p>Final <code>g</code> looks like:</p>

<p><img src=""https://i.stack.imgur.com/FsPps.png"" alt=""enter image description here""></p>

<p><strong>UPDATE (2013-11-08):</strong></p>

<p>As pointed out in comments, <code>ggplot</code>'s philosophy suggests using data in long format.
You can refer to this answer <a href=""https://stackoverflow.com/a/19039094/1796914"">https://stackoverflow.com/a/19039094/1796914</a> in order to see corresponding code.</p>
"
14558026,1270695,2013-01-28T08:20:18Z,2370094,2,TRUE,"<p>I know this is an OLD question, and not likely to generate a lot of interest. I also can't quite figure out <em>why</em> you're doing what you demonstrate in your example. Nevertheless, to summarize the answer, either:</p>

<ol>
<li>Wrap your <code>df.cast</code> in <code>as.data.frame</code> before ""melting"" again.</li>
<li>Ditch ""reshape"" and update to ""reshape2"". That wasn't applicable when you posted this question, since your question predates version 1 of ""reshape2"" by about half a year.</li>
</ol>

<p>Here's a lengthier walktrhough:</p>

<p>First, we'll load ""reshape"" and ""reshape2"", perform your ""casting"", and rename your ""n"" variable. Obviously, objects appended with ""R2"" are those from ""reshape2"", and ""R1"", from ""reshape"".</p>

<pre><code>library(reshape)
library(reshape2)
df.cast.R2 &lt;- dcast(df, type~., sum)
df.cast.R1 &lt;- cast(df, type~., sum)
names(df.cast.R1)[2] &lt;- ""n""
names(df.cast.R2)[2] &lt;- ""n""
</code></pre>

<p>Second, let's just have a quick look at what we've got now:</p>

<pre><code>class(df.cast.R1)
# [1] ""cast_df""    ""data.frame""
class(df.cast.R2) 
[1] ""data.frame""
str(df.cast.R1) 
# List of 2
#  $ type: num [1:3] 1 2 3
#  $ n   : num [1:3] 143 148 41
#  - attr(*, ""row.names"")= int [1:3] 1 2 3
#  - attr(*, ""idvars"")= chr ""type""
#  - attr(*, ""rdimnames"")=List of 2
#   ..$ :'data.frame':  3 obs. of  1 variable:
#   .. ..$ type: num [1:3] 1 2 3
#   ..$ :'data.frame':  1 obs. of  1 variable:
#   .. ..$ value: Factor w/ 1 level ""(all)"": 1
str(df.cast.R2)
# 'data.frame':  3 obs. of  2 variables:
#  $ type: num  1 2 3
#  $ n   : num  143 148 41
</code></pre>

<p>A few observations are obvious:</p>

<ul>
<li>By looking at the output of <code>class</code>, you can <em>guess</em> that you won't have any problems doing what you're trying to do if you're using ""reshape2""</li>
<li>Whoa. That output of <code>str(df.cast.R1)</code> is the strangest looking <code>data.frame</code> I've ever seen! It actually looks like there are two single variable <code>data.frame</code>s in there.</li>
</ul>

<p>With this new knowledge, and with the prerequisite that we do not want to change the <code>class</code> of your casted <code>data.frame</code>, let's proceed:</p>

<pre><code># You don't want this
melt(df.cast.R1, id=""type"", measure=""n"") 
#          type value value
# X.all.      1   143 (all)
# X.all..1    2   148 (all)
# X.all..2    3    41 (all)

# You *do* want this
melt(as.data.frame(df.cast.R1), id=""type"", measure=""n"")
#   type variable value
# 1    1        n   143
# 2    2        n   148
# 3    3        n    41

# And the class has not bee altered
class(df.cast.R1)
# [1] ""cast_df""    ""data.frame""

# As predicted, this works too.
melt(df.cast.R2, id=""type"", measure=""n"")
#   type variable value
# 1    1        n   143
# 2    2        n   148
# 3    3        n    41
</code></pre>

<hr>

<p>If you're still working with <code>cast</code> from ""reshape"", consider upgrading to ""reshape2"", or write a convenience wrapper function around <code>melt</code>... perhaps <code>melt2</code>?</p>

<pre><code>melt2 &lt;- function(data, ...) {
  ifelse(isTRUE(""cast_df"" %in% class(data)), 
         data &lt;- as.data.frame(data), 
         data &lt;- data)
  melt(data, ...)
}
</code></pre>

<p>Try it out on <code>df.cast.R1</code>:</p>

<pre><code>melt2(df.cast.R, id=""type"", measure=""n"")
#    ype variable value
# 1    1        n   143
# 2    2        n   148
# 3    3        n    41
</code></pre>
"
14714131,2044225,2013-02-05T18:15:05Z,1395301,1,FALSE,"<p>For Ubuntu:<br>
Insert the following command into your <code>.Rprofile</code> file (usually in your home directory):</p>

<pre><code>setwd(Sys.getenv(""PWD""))
</code></pre>

<p>Now your default working directory will be whatever directory you launched R from.  Keep in mind you can also set up default workspaces in different directories by saving your workspace image as <code>.RData</code> wherever you plan to launch R (startup sources <code>.Rprofile</code> before searching for <code>.Rdata</code> in the <code>cwd</code>).</p>
"
14722001,2045554,2013-02-06T05:07:33Z,2676554,3,FALSE,"<p>The package sciplot has the built-in function se(x)</p>
"
14808127,1818122,2013-02-11T08:09:30Z,750786,8,FALSE,"<p>Miguel Sanchez's response is the way it should be. The other way executing Rscript could be 'env' command to run the system wide RScript.</p>

<pre><code>#!/usr/bin/env Rscript
</code></pre>
"
14836426,2065097,2013-02-12T15:55:13Z,2602583,10,FALSE,"<p>The </p>

<pre><code>exp(mean(log(x)))
</code></pre>

<p>will work unless there is a 0 in x. If so, the log will produce -Inf (-Infinite) which always results in a geometric mean of 0.</p>

<p>One solution is to remove the -Inf value before calculating the mean:</p>

<pre><code>geo_mean &lt;- function(data) {
    log_data &lt;- log(data)
    gm &lt;- exp(mean(log_data[is.finite(log_data)]))
    return(gm)
}
</code></pre>

<p>You can use a one-liner to do this but it means calculating the log twice which is inefficient.</p>

<pre><code>exp(mean(log(i[is.finite(log(i))])))
</code></pre>
"
14965425,1247325,2013-02-19T19:07:33Z,1716012,8,FALSE,"<p>A very simple equivalence with tic and toc that you could have:</p>

<pre><code>tic=proc.time()[3]

...code...

toc=proc.time()[3] - tic
</code></pre>

<p>Where the [3] is because we are interested in the third element in the vector returned by proc.time(), which is elapsed time.</p>
"
14990499,566575,2013-02-20T21:42:39Z,2349205,0,FALSE,"<p>Use the ""dec"" argument to set "","" as the decimal point by adding:</p>

<pre><code> ce= read.table(""file.txt"", header= T, dec="","")
</code></pre>
"
15007398,984532,2013-02-21T16:30:06Z,2261079,75,FALSE,"<p>To manipulate the white space, use str_trim() in the stringr package.
The package has manual dated Feb 15,2013 and is in CRAN.
The function can also handle string vectors.</p>

<pre><code>install.packages(""stringr"", dependencies=TRUE)
require(stringr)
example(str_trim)
d4$clean2&lt;-str_trim(d4$V2)
</code></pre>

<p>(credit goes to commenter: R. Cotton)</p>
"
15008084,1239223,2013-02-21T17:02:35Z,2258784,0,FALSE,"<p><code>?opts</code>
although, this does not display how to finetune its arguments, therefore it's better to check the options given above. If you can get a copy of the ggplot2 reference manual, it will help you a lot. </p>
"
15010947,814601,2013-02-21T19:49:14Z,1735540,0,FALSE,"<pre><code>freqplot = function(x, by = NULL, right = FALSE)
{
if(is.null(by)) stop('Valor de ""by"" precisa ser especificado.')
breaks = seq(min(x), max(x), by = by )
ecd = ecdf(x)
den = ecd(breaks)
table = table(cut(x, breaks = breaks, right = right))
table = table/sum(table)

intervs = factor(names(table), levels = names(table))
freq = as.numeric(table/sum(table))
acum = as.numeric(cumsum(table))

normalize.vec = function(x){
  (x - min(x))/(max(x) - min(x))
}

dados = data.frame(classe = intervs, freq = freq, acum = acum, acum_norm = normalize.vec(acum))
p = ggplot(dados) + 
  geom_bar(aes(classe, freq, fill = classe), stat = 'identity') +
  geom_point(aes(classe, acum_norm, group = '1'), shape = I(1), size = I(3), colour = 'gray20') +
  geom_line(aes(classe, acum_norm, group = '1'), colour = I('gray20'))

p
}
</code></pre>
"
15058684,1385941,2013-02-25T01:07:39Z,1727772,235,FALSE,"<p>Here is an example that utilizes <code>fread</code> from <code>data.table</code> 1.8.7</p>

<p>The examples come from the help page to <code>fread</code>, with the timings on my windows XP Core 2 duo E8400.</p>

<pre><code>library(data.table)
# Demo speedup
n=1e6
DT = data.table( a=sample(1:1000,n,replace=TRUE),
                 b=sample(1:1000,n,replace=TRUE),
                 c=rnorm(n),
                 d=sample(c(""foo"",""bar"",""baz"",""qux"",""quux""),n,replace=TRUE),
                 e=rnorm(n),
                 f=sample(1:1000,n,replace=TRUE) )
DT[2,b:=NA_integer_]
DT[4,c:=NA_real_]
DT[3,d:=NA_character_]
DT[5,d:=""""]
DT[2,e:=+Inf]
DT[3,e:=-Inf]
</code></pre>

<h1>standard read.table</h1>

<pre><code>write.table(DT,""test.csv"",sep="","",row.names=FALSE,quote=FALSE)
cat(""File size (MB):"",round(file.info(""test.csv"")$size/1024^2),""\n"")    
## File size (MB): 51 

system.time(DF1 &lt;- read.csv(""test.csv"",stringsAsFactors=FALSE))        
##    user  system elapsed 
##   24.71    0.15   25.42
# second run will be faster
system.time(DF1 &lt;- read.csv(""test.csv"",stringsAsFactors=FALSE))        
##    user  system elapsed 
##   17.85    0.07   17.98
</code></pre>

<h1>optimized read.table</h1>

<pre><code>system.time(DF2 &lt;- read.table(""test.csv"",header=TRUE,sep="","",quote="""",  
                          stringsAsFactors=FALSE,comment.char="""",nrows=n,                   
                          colClasses=c(""integer"",""integer"",""numeric"",                        
                                       ""character"",""numeric"",""integer"")))


##    user  system elapsed 
##   10.20    0.03   10.32
</code></pre>

<h1>fread</h1>

<pre><code>require(data.table)
system.time(DT &lt;- fread(""test.csv""))                                  
 ##    user  system elapsed 
##    3.12    0.01    3.22
</code></pre>

<h1>sqldf</h1>

<pre><code>require(sqldf)

system.time(SQLDF &lt;- read.csv.sql(""test.csv"",dbname=NULL))             

##    user  system elapsed 
##   12.49    0.09   12.69

# sqldf as on SO

f &lt;- file(""test.csv"")
system.time(SQLf &lt;- sqldf(""select * from f"", dbname = tempfile(), file.format = list(header = T, row.names = F)))

##    user  system elapsed 
##   10.21    0.47   10.73
</code></pre>

<h1>ff / ffdf</h1>

<pre><code> require(ff)

 system.time(FFDF &lt;- read.csv.ffdf(file=""test.csv"",nrows=n))   
 ##    user  system elapsed 
 ##   10.85    0.10   10.99
</code></pre>

<h1>In summary:</h1>

<pre><code>##    user  system elapsed  Method
##   24.71    0.15   25.42  read.csv (first time)
##   17.85    0.07   17.98  read.csv (second time)
##   10.20    0.03   10.32  Optimized read.table
##    3.12    0.01    3.22  fread
##   12.49    0.09   12.69  sqldf
##   10.21    0.47   10.73  sqldf on SO
##   10.85    0.10   10.99  ffdf
</code></pre>
"
15058890,1217536,2013-02-25T01:41:03Z,2315601,12,FALSE,"<p><em>(I thought it might be helpful to lay out the ideas very simply here to summarize the good material posted by @doug, &amp; linked by @duffymo; +1 to each,btw.)</em>  </p>

<p><a href=""http://stat.ethz.ch/R-manual/R-patched/library/base/html/order.html"">?order</a> tells you which element of the original vector needs to be put first, second, etc., so as to sort the original vector, whereas <a href=""http://stat.ethz.ch/R-manual/R-devel/library/base/html/rank.html"">?rank</a> tell you which element has the lowest, second lowest, etc., value.  For example:  </p>

<pre><code>&gt; a &lt;- c(45, 50, 10, 96)
&gt; order(a)  
[1] 3 1 2 4  
&gt; rank(a)  
[1] 2 3 1 4  
</code></pre>

<p>So <code>order(a)</code> is saying, 'put the third element first when you sort... ', whereas <code>rank(a)</code> is saying, 'the first element is the second lowest... '.  (Note that they both agree on which element is lowest, etc.; they just present the information differently.)  Thus we see that we can use <code>order()</code> to sort, but we can't use <code>rank()</code> that way:  </p>

<pre><code>&gt; a[order(a)]  
[1] 10 45 50 96  
&gt; sort(a)  
[1] 10 45 50 96  
&gt; a[rank(a)]  
[1] 50 10 45 96  
</code></pre>

<p>In general, <code>order()</code> will not equal <code>rank()</code> unless the vector has been sorted already:  </p>

<pre><code>&gt; b &lt;- sort(a)  
&gt; order(b)==rank(b)  
[1] TRUE TRUE TRUE TRUE  
</code></pre>

<p>Also, since <code>order()</code> is (essentially) operating over ranks of the data, you could compose them without affecting the information, but the other way around produces gibberish:  </p>

<pre><code>&gt; order(rank(a))==order(a)  
[1] TRUE TRUE TRUE TRUE  
&gt; rank(order(a))==rank(a)  
[1] FALSE FALSE FALSE  TRUE  
</code></pre>
"
15095607,2121665,2013-02-26T17:36:44Z,2061897,2,FALSE,"<p>For the record, rjson and RJSONIO do change the file type, but they don't really parse per se. For instance, I receive ugly MongoDB data in JSON format, convert it with rjson or RJSONIO, then use unlist and tons of manual correction to actually parse it into a usable matrix.</p>
"
15133636,2119266,2013-02-28T10:53:22Z,1355355,0,FALSE,"<p>from the original <code>tsplit</code> list object given at the beginning, this command will do:</p>

<pre><code>unlist(lapply(tsplit,function(x) x[1]))
</code></pre>

<p>it extracts the first element of all list elements, then transforms a list to a vector. Unlisting first to a matrix, then extracting the fist column is also ok, but then you are dependent on the fact that all list elements have the same length. Here is the output:</p>

<pre><code>&gt; tsplit

[[1]]
[1] ""bob""   ""smith""

[[2]]
[1] ""mary"" ""jane""

[[3]]
[1] ""jose""  ""chung""

[[4]]
[1] ""michael"" ""marx""   

[[5]]
[1] ""charlie"" ""ivan""   

&gt; lapply(tsplit,function(x) x[1])

[[1]]
[1] ""bob""

[[2]]
[1] ""mary""

[[3]]
[1] ""jose""

[[4]]
[1] ""michael""

[[5]]
[1] ""charlie""

&gt; unlist(lapply(tsplit,function(x) x[1]))

[1] ""bob""     ""mary""    ""jose""    ""michael"" ""charlie""
</code></pre>
"
15139645,2120262,2013-02-28T15:41:58Z,1826519,4,FALSE,"<p>Lists seem perfect for this purpose.  For example within the function you would have</p>

<pre><code>x = desired_return_value_1 # (vector, matrix, etc)

y = desired_return_value_2 # (vector, matrix, etc)

returnlist = list(x,y...)

}  # end of function
</code></pre>

<h1>main program</h1>

<pre><code>x = returnlist[[1]]

y = returnlist[[2]]
</code></pre>
"
15140507,516548,2013-02-28T16:23:36Z,1826519,130,TRUE,"<p><strong>(1) list[...]&lt;-</strong> I had posted this over a decade ago on <a href=""https://stat.ethz.ch/pipermail/r-help/2004-June/053343.html"">r-help</a>.  It does not require a special operator but does require that the left hand side be written using <code>list[...]</code> like this:</p>

<pre><code># first run source command shown below or else install and load gsubfn dev pkg (see Note)
list[a, b] &lt;- functionReturningTwoValues()
</code></pre>

<blockquote>
  <p><strong>Note:</strong> Recently <code>list</code> has been added to the development version of the <a href=""https://github.com/ggrothendieck/gsubfn"">gsubfn package</a> and can be sourced via the following:</p>

<pre><code>source(""https://raw.githubusercontent.com/ggrothendieck/gsubfn/master/R/list.R"")
</code></pre>
  
  <p>or install and load gsubfn github development package (using devtools
  package):</p>

<pre><code>devtools::install_github(""ggrothendieck/gsubfn"")
library(gsubfn)
</code></pre>
</blockquote>

<p>If you only need the first or second component these all work too:</p>

<pre><code>list[a] &lt;- functionReturningTwoValues()
list[a, ] &lt;- functionReturningTwoValues()
list[, b] &lt;- functionReturningTwoValues()
</code></pre>

<p>(Of course, if you only needed one value then <code>functionReturningTwoValues()[[1]]</code> or <code>functionReturningTwoValues()[[2]]</code> would be sufficient.)</p>

<p>See the cited r-help thread for more examples.</p>

<p><strong>(2) with</strong>  If the intent is merely to combine the multiple values subsequently and the return values are named then a simple alternative is to use <code>with</code> :</p>

<pre><code>myfun &lt;- function() list(a = 1, b = 2)

list[a, b] &lt;- myfun()
a + b

# same
with(myfun(), a + b)
</code></pre>

<p><strong>(3) attach</strong> Another alternative is attach:</p>

<pre><code>attach(myfun())
a + b
</code></pre>

<p>ADDED: <code>with</code> and <code>attach</code></p>
"
15177894,976061,2013-03-02T18:30:59Z,2748725,22,FALSE,"<p>To calculate the weighted median of a vector <code>x</code> using a same length vector of (integer) weights <code>w</code>: </p>

<pre><code>median(rep(x, times=w))
</code></pre>
"
15310665,1937437,2013-03-09T12:28:48Z,2367328,12,FALSE,"<p>Some workaround could be rather than using dev.new() R function use this function which should work across platform  :</p>

<pre><code> dev.new &lt;- function(width = 7, height = 7) 
 { platform &lt;- sessionInfo()$platform if (grepl(""linux"",platform)) 
 { x11(width=width, height=height) } 
 else if (grepl(""pc"",platform)) 
 { windows(width=width, height=height) } 
 else if (grepl(""apple"", platform)) 
 { quartz(width=width, height=height) } }
</code></pre>
"
15317976,636656,2013-03-10T01:41:44Z,2151147,3,FALSE,"<p>Here's a function to evaluate any expression you want with Stata variable labels:</p>

<pre><code>#' Function to prettify the output of another function using a `var.labels` attribute
#' This is particularly useful in combination with read.dta et al.
#' @param dat A data.frame with attr `var.labels` giving descriptions of variables
#' @param expr An expression to evaluate with pretty var.labels
#' @return The result of the expression, with variable names replaced with their labels
#' @examples
#' testDF &lt;- data.frame( a=seq(10),b=runif(10),c=rnorm(10) )
#' attr(testDF,""var.labels"") &lt;- c(""Identifier"",""Important Data"",""Lies, Damn Lies, Statistics"")
#' prettify( testDF, quote(str(dat)) )
prettify &lt;- function( dat, expr ) {
  labels &lt;- attr(dat,""var.labels"")
  for(i in seq(ncol(dat))) colnames(dat)[i] &lt;- labels[i]
  attr(dat,""var.labels"") &lt;- NULL
  eval( expr )
}
</code></pre>

<p>You can then <code>prettify(testDF, quote(table(...)))</code> or whatever you want.</p>

<p>See <a href=""https://stackoverflow.com/questions/15039599/defer-expression-evaluation-without-using-quote"">this thread</a> for more info.</p>
"
15373917,93345,2013-03-12T23:00:00Z,1815606,25,FALSE,"<p>I couldn't get Suppressingfire's solution to work when 'source'ing from the R console.<br>
I couldn't get hadley's solution to work when using Rscript.</p>

<p>Best of both worlds?</p>

<pre><code>thisFile &lt;- function() {
        cmdArgs &lt;- commandArgs(trailingOnly = FALSE)
        needle &lt;- ""--file=""
        match &lt;- grep(needle, cmdArgs)
        if (length(match) &gt; 0) {
                # Rscript
                return(normalizePath(sub(needle, """", cmdArgs[match])))
        } else {
                # 'source'd via R console
                return(normalizePath(sys.frames()[[1]]$ofile))
        }
}
</code></pre>
"
15446400,171965,2013-03-16T06:45:56Z,2665532,3,FALSE,"<p>I put up a Github project to demonstrate how to use RGoogleDocs to read from a Google Spreadsheet. I have not yet been able to write to cells, but the read path works great.</p>

<p>Check out the README at <a href=""https://github.com/hammer/google-spreadsheets-to-r-dataframe"" rel=""nofollow"">https://github.com/hammer/google-spreadsheets-to-r-dataframe</a></p>
"
15451157,1851270,2013-03-16T15:39:17Z,2388974,16,FALSE,"<p>In general you don't need to create a cookie file, unless you want to study the cookies.</p>

<p>Given this, in real word, web servers use agent data, redirecting and hidden post data, but this should help:</p>

<pre><code>library(RCurl)

#Set your browsing links 
loginurl = ""http://api.my.url/login""
dataurl  = ""http://api.my.url/data""

#Set user account data and agent
pars=list(
     username=""xxx""
     password=""yyy""
)
agent=""Mozilla/5.0"" #or whatever 

#Set RCurl pars
curl = getCurlHandle()
curlSetOpt(cookiejar=""cookies.txt"",  useragent = agent, followlocation = TRUE, curl=curl)
#Also if you do not need to read the cookies. 
#curlSetOpt(  cookiejar="""", useragent = agent, followlocation = TRUE, curl=curl)

#Post login form
html=postForm(loginurl, .params = pars, curl=curl)

#Go wherever you want
html=getURL(dataurl, curl=curl)

#Start parsing your page
matchref=gregexpr(""... my regexp ..."", html)

#... .... ...

#Clean up. This will also print the cookie file
rm(curl)
gc()
</code></pre>

<h2>Important</h2>

<p>There can often be hidden post data, beyond username and password. To capture it you may want, e.g. in Chrome, to use <code>Developer tools</code> (<kbd>Ctrl</kbd> <kbd>Shift</kbd> <kbd>I</kbd>) -> <code>Network Tab</code>, in order to show the post field names and values. </p>
"
15555793,2196571,2013-03-21T18:52:09Z,1535086,2,FALSE,"<p>I had just about the exact same thing with ubuntu 12.04 and R 2.14.1
I installed the ubuntu packages</p>

<p>xfonts-100dpi</p>

<p>xfonts-75dpi</p>

<p>Instant fix.</p>
"
15620958,2208540,2013-03-25T17:21:23Z,2547402,12,FALSE,"<p>The following function comes in three forms:</p>

<p>method = ""mode"" [default]:  calculates the mode for a unimodal vector, else returns an NA<br>
method = ""nmodes"":  calculates the number of modes in the vector<br>
method = ""modes"":  lists all the modes for a unimodal or polymodal vector</p>

<pre><code>modeav &lt;- function (x, method = ""mode"", na.rm = FALSE)
{
  x &lt;- unlist(x)
  if (na.rm)
    x &lt;- x[!is.na(x)]
  u &lt;- unique(x)
  n &lt;- length(u)
  #get frequencies of each of the unique values in the vector
  frequencies &lt;- rep(0, n)
  for (i in seq_len(n)) {
    if (is.na(u[i])) {
      frequencies[i] &lt;- sum(is.na(x))
    }
    else {
      frequencies[i] &lt;- sum(x == u[i], na.rm = TRUE)
    }
  }
  #mode if a unimodal vector, else NA
  if (method == ""mode"" | is.na(method) | method == """")
  {return(ifelse(length(frequencies[frequencies==max(frequencies)])&gt;1,NA,u[which.max(frequencies)]))}
  #number of modes
  if(method == ""nmode"" | method == ""nmodes"")
  {return(length(frequencies[frequencies==max(frequencies)]))}
  #list of all modes
  if (method == ""modes"" | method == ""modevalues"")
  {return(u[which(frequencies==max(frequencies), arr.ind = FALSE, useNames = FALSE)])}  
  #error trap the method
  warning(""Warning: method not recognised.  Valid methods are 'mode' [default], 'nmodes' and 'modes'"")
  return()
}
</code></pre>
"
15718491,562440,2013-03-30T12:25:16Z,1632772,6,FALSE,"<p>1) create the data frame with stringsAsFactor set to FALSE. This should resolve the factor-issue</p>

<p>2) afterwards don't use rbind - it messes up the column names if the data frame is empty. simply do it this way:</p>

<pre><code>df[nrow(df)+1,] &lt;- c(""d"",""gsgsgd"",4)
</code></pre>

<p>/</p>

<pre><code>&gt; df &lt;- data.frame(a = character(0), b=character(0), c=numeric(0))

&gt; df[nrow(df)+1,] &lt;- c(""d"",""gsgsgd"",4)

Warnmeldungen:
1: In `[&lt;-.factor`(`*tmp*`, iseq, value = ""d"") :
  invalid factor level, NAs generated
2: In `[&lt;-.factor`(`*tmp*`, iseq, value = ""gsgsgd"") :
  invalid factor level, NAs generated

&gt; df &lt;- data.frame(a = character(0), b=character(0), c=numeric(0), stringsAsFactors=F)

&gt; df[nrow(df)+1,] &lt;- c(""d"",""gsgsgd"",4)

&gt; df
  a      b c
1 d gsgsgd 4
</code></pre>
"
15721562,2227649,2013-03-30T17:49:11Z,2656529,0,FALSE,"<p>Did you do unloadNamespace('ggplot2')? and the library(ggplot2) and then check the version? Because, once you load a package, it stays in memory of R, even though you might have already installed a newer version of the package, R does not see, until you do the above.</p>
"
15843895,1459982,2013-04-05T21:26:59Z,1188544,0,FALSE,"<p>It was not immediately clear from the other answers how to actually make this work (without resorting to parallel processing alternatives, so here is a solution I found that works very simply on windows</p>

<p>If you have a simple r file:</p>

<pre><code>for(i in 1:10){
  ptm0 &lt;- proc.time()
  Sys.sleep(0.5)  
  ptm1=proc.time() - ptm0
  jnk=as.numeric(ptm1[3])
  cat('\n','It took ', jnk, ""seconds to do iteration"", i)
}
</code></pre>

<p>On CMD, specify the directory of where your script is and then start a new window with Rscript to run your code. Multiple lines will open multiple r instances that run your code that also reproduce the messages that the code outputs.</p>

<pre><code>cd ""C:\rcode""
START """" Rscript example_code.r /b
START """" Rscript example_code.r /b
</code></pre>

<p>If Rscript is not on the system path, just specify the full path instead:</p>

<pre><code>START """" ""C:\Program Files\R\bin\x64\Rscript.exe"" text_within_loop.r /b
</code></pre>
"
15871417,2238226,2013-04-08T04:42:45Z,2492947,0,FALSE,"<p>I also think chart.Boxplot is the best option, it gives you the position of the mean but if you have a matrix with returns all you need is one line of code to get all the boxplots in one graph. </p>

<p>Here is a small ETF portfolio example.</p>

<pre><code>library(zoo)
library(PerformanceAnalytics)
library(tseries)
library(xts)

VTI.prices = get.hist.quote(instrument = ""VTI"", start= ""2007-03-01"", end=""2013-03-01"",
                        quote = c(""AdjClose""),provider = ""yahoo"",origin =""1970-01-01"", 
                        compression = ""m"", retclass = c(""zoo""))

VEU.prices = get.hist.quote(instrument = ""VEU"", start= ""2007-03-01"", end=""2013-03-01"",
                        quote = c(""AdjClose""),provider = ""yahoo"",origin =""1970-01-01"", 
                        compression = ""m"", retclass = c(""zoo""))

VWO.prices = get.hist.quote(instrument = ""VWO"", start= ""2007-03-01"", end=""2013-03-01"",
                        quote = c(""AdjClose""),provider = ""yahoo"",origin =""1970-01-01"", 
                        compression = ""m"", retclass = c(""zoo""))


VNQ.prices = get.hist.quote(instrument = ""VNQ"", start= ""2007-03-01"", end=""2013-03-01"",
                       quote = c(""AdjClose""),provider = ""yahoo"",origin =""1970-01-01"", 
                       compression = ""m"", retclass = c(""zoo""))

TLT.prices = get.hist.quote(instrument = ""TLT"", start= ""2007-03-01"", end=""2013-03-01"",
                        quote = c(""AdjClose""),provider = ""yahoo"",origin =""1970-01-01"", 
                        compression = ""m"", retclass = c(""zoo""))

TIP.prices = get.hist.quote(instrument = ""TIP"", start= ""2007-03-01"", end=""2013-03-01"",
                         quote = c(""AdjClose""),provider = ""yahoo"",origin =""1970-01-01"", 
                         compression = ""m"", retclass = c(""zoo""))

index(VTI.prices) = as.yearmon(index(VTI.prices))
index(VEU.prices) = as.yearmon(index(VEU.prices))
index(VWO.prices) = as.yearmon(index(VWO.prices))

index(VNQ.prices) = as.yearmon(index(VNQ.prices))
index(TLT.prices) = as.yearmon(index(TLT.prices))
index(TIP.prices) = as.yearmon(index(TIP.prices))

Prices.z=merge(VTI.prices, VEU.prices, VWO.prices, VNQ.prices, 
           TLT.prices, TIP.prices)

colnames(Prices.z) = c(""VTI"", ""VEU"", ""VWO"" , ""VNQ"", ""TLT"", ""TIP"")

returnscc.z = diff(log(Prices.z))

start(returnscc.z)
end(returnscc.z)
colnames(returnscc.z) 
head(returnscc.z)
</code></pre>

<p>Return Matrix</p>

<pre><code>ret.mat = coredata(returnscc.z)
class(ret.mat)
colnames(ret.mat)
head(ret.mat)
</code></pre>

<p>Box Plot of Return Matrix </p>

<pre><code>chart.Boxplot(returnscc.z, names=T, horizontal=TRUE, colorset=""darkgreen"", as.Tufte =F,
          mean.symbol = 20, median.symbol=""|"", main=""Return Distributions Comparison"",
          element.color = ""darkgray"", outlier.symbol = 20, 
          xlab=""Continuously Compounded Returns"", sort.ascending=F)
</code></pre>

<p>You can try changing the mean.symbol, and remove or change the median.symbol.
Hope it helped. :)</p>
"
15928813,2266596,2013-04-10T14:33:52Z,2547402,0,FALSE,"<p>Sorry, I might take it too simple, but doesn't this do the job? (in 1.3 secs for 1E6 values on my machine):</p>

<pre><code>t0 &lt;- Sys.time()
summary(as.factor(round(rnorm(1e6), 2)))[1]
Sys.time()-t0
</code></pre>

<p>You just have to replace the ""round(rnorm(1e6),2)"" with your vector.</p>
"
15932934,1759974,2013-04-10T17:53:03Z,2275896,3,FALSE,"<p>When applying functions over subsets of a vector, <code>tapply</code> can be pretty faster than a for loop. Example:</p>

<pre><code>df &lt;- data.frame(id = rep(letters[1:10], 100000),
                 value = rnorm(1000000))

f1 &lt;- function(x)
  tapply(x$value, x$id, sum)

f2 &lt;- function(x){
  res &lt;- 0
  for(i in seq_along(l &lt;- unique(x$id)))
    res[i] &lt;- sum(x$value[x$id == l[i]])
  names(res) &lt;- l
  res
}            

library(microbenchmark)

&gt; microbenchmark(f1(df), f2(df), times=100)
Unit: milliseconds
   expr      min       lq   median       uq      max neval
 f1(df) 28.02612 28.28589 28.46822 29.20458 32.54656   100
 f2(df) 38.02241 41.42277 41.80008 42.05954 45.94273   100
</code></pre>

<p><code>apply</code>, however, in most situation doesn't provide any speed increase, and in some cases can be even lot slower:</p>

<pre><code>mat &lt;- matrix(rnorm(1000000), nrow=1000)

f3 &lt;- function(x)
  apply(x, 2, sum)

f4 &lt;- function(x){
  res &lt;- 0
  for(i in 1:ncol(x))
    res[i] &lt;- sum(x[,i])
  res
}

&gt; microbenchmark(f3(mat), f4(mat), times=100)
Unit: milliseconds
    expr      min       lq   median       uq      max neval
 f3(mat) 14.87594 15.44183 15.87897 17.93040 19.14975   100
 f4(mat) 12.01614 12.19718 12.40003 15.00919 40.59100   100
</code></pre>

<p>But for these situations we've got <code>colSums</code> and <code>rowSums</code>:</p>

<pre><code>f5 &lt;- function(x)
  colSums(x) 

&gt; microbenchmark(f5(mat), times=100)
Unit: milliseconds
    expr      min       lq   median       uq      max neval
 f5(mat) 1.362388 1.405203 1.413702 1.434388 1.992909   100
</code></pre>
"
15950543,1543922,2013-04-11T13:50:20Z,2751065,3,FALSE,"<p>I guess in the example of <code>mpg</code> changing the rotation angle and font size is fine, but in many cases you might find yourself with variables that have quite lengthy labels, and it can become a pain in the neck (literally) to try read rotated lengthy labels.</p>

<p>So in addition (or complement) to changing angles and sizes, I usually reformat the labels of the factors that define the <code>facet_grid</code> whenever they can be split in a way that makes sense.</p>

<p>Typically if I have a <code>dataset$variable</code> with strings that looks like </p>

<pre><code>c(""median_something"", ""aggregated_average_x"",""error"",""something_else"")
</code></pre>

<p>I simply do:</p>

<pre><code>reformat &lt;– function(x,lab=""\n""){ sapply(x, function(c){ paste(unlist(strsplit(as.character(c) , split=""_"")),collapse=lab) }) }
</code></pre>

<p>[perhaps there are better definitions of <code>reformat</code> but at least this one works fine.]</p>

<pre><code>dataset$variable &lt;- factor(dataset$variable, labels=reformat(dataset$variable, lab='\n')
</code></pre>

<p>And upon facetting, all labels will be very readable:</p>

<pre><code>ggplot(data=dataset, aes(x,y)) + geom_point() + facet_grid(. ~ variable)
</code></pre>
"
15975391,2252205,2013-04-12T15:31:36Z,2614767,3,FALSE,"<p>to read-in the financial information try this function ( I picked it up several months ago and made some small adjustments)</p>

<pre><code>require(XML)
require(plyr)

getKeyStats_xpath &lt;- function(symbol) {
  yahoo.URL &lt;- ""http://finance.yahoo.com/q/ks?s=""
  html_text &lt;- htmlParse(paste(yahoo.URL, symbol, sep = """"), encoding=""UTF-8"")

  #search for &lt;td&gt; nodes anywhere that have class 'yfnc_tablehead1'
  nodes &lt;- getNodeSet(html_text, ""/*//td[@class='yfnc_tablehead1']"")

  if(length(nodes) &gt; 0 ) {
    measures &lt;- sapply(nodes, xmlValue)

    #Clean up the column name
    measures &lt;- gsub("" *[0-9]*:"", """", gsub("" \\(.*?\\)[0-9]*:"","""", measures))   

    #Remove dups
    dups &lt;- which(duplicated(measures))
    #print(dups) 
    for(i in 1:length(dups)) 
      measures[dups[i]] = paste(measures[dups[i]], i, sep="" "")

    #use siblings function to get value
    values &lt;- sapply(nodes, function(x)  xmlValue(getSibling(x)))

    df &lt;- data.frame(t(values))
    colnames(df) &lt;- measures
    return(df)
  } else {
    break
  }
}
</code></pre>

<p>to use it, compare for example 3 companies and write the data into a csv-file do the following:</p>

<pre><code>tickers &lt;- c(""AAPL"",""GOOG"",""F"")
stats &lt;- ldply(tickers, getKeyStats_xpath)
rownames(stats) &lt;- tickers
write.csv(t(stats), ""FinancialStats_updated.csv"",row.names=TRUE) 
</code></pre>

<p>Just tried it. Still working.</p>

<p>UPDATE as Yahoo changed it’s web site layout:</p>

<p>The function above does not work anymore as Yahoo again changed its web site layout. Fortunately its still easy to get the financial infos as the tags for getting fundamental data have not been changed. 
example for downloading a file with eps and P/E ratio for MSFT, AAPL and Ford insert the following into your browser:</p>

<pre><code>http://finance.yahoo.com/d/quotes.csv?s=MSFT+AAPL+F&amp;f=ser
</code></pre>

<p>and after entering the above URL into your browser’s address bar and hitting return/enter. The CSV will be automatically downloaded to your computer and you should get the cvs file as shown below (data as 7/22/2016):</p>

<p><a href=""https://i.stack.imgur.com/UDlq6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UDlq6.png"" alt=""enter image description here""></a></p>

<p>some yahoo tags for fundamental data:</p>

<p><a href=""https://i.stack.imgur.com/9k5sg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9k5sg.png"" alt=""enter image description here""></a></p>
"
15998763,1090562,2013-04-14T11:54:55Z,1231195,41,FALSE,"<p>You can do this easily in <a href=""http://www.rstudio.com/ide/docs/using/source"" rel=""noreferrer"">RStudio</a>:</p>

<p>select the code and click <kbd>CTR</kbd>+<kbd>SHIFT</kbd>+<kbd>C</kbd>
to comment/uncomment code.</p>
"
16046056,1665868,2013-04-16T20:04:39Z,1815606,68,FALSE,"<p><a href=""http://r.789695.n4.nabble.com/Path-to-R-script-td2196648.html"" rel=""noreferrer"">Here</a> there is a simple solution for the problem. This command:</p>

<pre><code>script.dir &lt;- dirname(sys.frame(1)$ofile)
</code></pre>

<p>returns the path of the current script file. It works after the script was saved.</p>
"
16071301,216064,2013-04-17T22:24:50Z,2188414,1,FALSE,"<p>This works on Windows</p>

<pre><code>&gt; ""\u2588""
[1] ""█""
</code></pre>
"
16185516,1544304,2013-04-24T07:06:41Z,1309263,0,FALSE,"<p>Here is one that can be used when exact output is not important (for display purposes etc.)
You need totalcount and lastmedian, plus the newvalue.</p>

<pre><code>{
totalcount++;
newmedian=lastmedian+(newvalue&gt;lastmedian?1:-1)*(lastmedian==0?newvalue: lastmedian/totalcount*2);
}
</code></pre>

<p>Produces quite exact results for things like page_display_time. </p>

<p>Rules: the input stream needs to be smooth on the order of page display time, big in count (>30 etc), and have a non zero median.</p>

<p>Example: 
page load time, 800 items, 10ms...3000ms, average 90ms, real median:11ms</p>

<p>After 30 inputs, medians error is generally &lt;=20% (9ms..12ms), and gets less and less. 
After 800 inputs, the error is +-2%.</p>

<p>Another thinker with a similar solution is here: <a href=""https://stackoverflow.com/questions/11482529/median-filter-super-efficient-implementation/15150968#15150968"">Median Filter Super efficient implementation</a></p>
"
16191334,1575670,2013-04-24T11:54:34Z,2500896,1,FALSE,"<p>I used Tony Breyal's code, but the function returned <code>NA</code> values for those URLs where there was no URL redirection. Even though Tony listed ""google.com"" in his example, I think Google redirects you in any case to some sort of localized version of google.com. </p>

<p>Here is how I modified Tony's code to deal with that: </p>

<pre><code>decode.short.url &lt;- function(u) {
  x &lt;- try( getURL(u, header = TRUE, nobody = TRUE, followlocation = FALSE) )
  if(class(x) == 'try-error') {
    print(paste(""***"", u, ""--&gt; ERORR!!!!""))    
    return(u)
  } else {
    x &lt;- strsplit(x, ""Location: "")[[1]][2]
    x.2  &lt;- strsplit(x, ""\r"")[[1]][1]
    if (is.na(x.2)){
      print(paste(""***"", u, ""--&gt; No change.""))
      return(u)
    }else{
      print(paste(""***"", x.2, ""--&gt; resolved in --&gt;"", x.2))  
      return(x.2)
    }
  }
}


u &lt;- list(""http://www.amazon.com"", ""http://tinyurl.com/adcd"") 
urls &lt;- sapply(u, decode.short.url)
</code></pre>
"
16245932,2325494,2013-04-26T22:07:21Z,1395105,4,FALSE,"<p>Here's a cool function that lets you use the plotmath functionality, but with the expressions stored as objects of the character mode.  This lets you manipulate them programmatically using paste or regular expression functions.  I don't use ggplot, but it should work there as well:</p>

<pre><code>    express &lt;- function(char.expressions){
       return(parse(text=paste(char.expressions,collapse="";"")))
    }
    par(mar=c(6,6,1,1))
    plot(0,0,xlim=sym(),ylim=sym(),xaxt=""n"",yaxt=""n"",mgp=c(4,0.2,0),
       xlab=""axis(1,(-9:9)/10,tick.labels,las=2,cex.axis=0.8)"",
       ylab=""axis(2,(-9:9)/10,express(tick.labels),las=1,cex.axis=0.8)"")
    tick.labels &lt;- paste(""x &gt;="",(-9:9)/10)
    # this is what you get if you just use tick.labels the regular way:
    axis(1,(-9:9)/10,tick.labels,las=2,cex.axis=0.8)
    # but if you express() them... voila!
    axis(2,(-9:9)/10,express(tick.labels),las=1,cex.axis=0.8)
</code></pre>
"
16307235,1748000,2013-04-30T19:08:36Z,1676990,-1,FALSE,"<p>An easier way to split 1 column into 2 columns via data.table </p>

<pre><code>require(data.table)  
data_ex = data.table( a = paste( sample(1:3, size=10, replace=TRUE),""-separate"", sep="""" ))  
data_ex[, number:=  unlist( strsplit(x=a, split=""-"") )[[1]], by=a]  
data_ex[, word:= unlist( strsplit(x=a, split=""-"") )[[2]], by=a ]
</code></pre>
"
16310499,1829950,2013-04-30T23:21:55Z,1676990,8,FALSE,"<p>It depends a little bit on how closely your actual data matches the example data you've given. I you're just trying to get everything after the space, you can use <code>gsub</code>:</p>

<pre><code>gsub("".+\\s+"", """", tmp3)
[1] ""2"" ""1"" ""2"" ""1"" ""2"" ""1"" ""2"" ""1"" ""2"" ""1""
</code></pre>

<p>If you're trying to implement a rule more complicated than ""take everything after the space"", you'll need a more complicated regular expresion.</p>
"
16325086,468660,2013-05-01T19:34:15Z,596819,1,FALSE,"<p>I came across this question looking for something else, and it's old - so I'll just provide a brief answer for now (leave a comment if you'd like more explanation).</p>

<p>You can pass around environments in R which contain anywhere from 1 to all of your variables. But probably you don't need to worry about it.</p>

<p>[You might also be able to do something similar with classes. I only currently understand how to use classes for polymorphic functions - and note there's more than 1 class system kicking around.]</p>
"
16563900,1694128,2013-05-15T11:32:09Z,952275,15,FALSE,"<p>Try <code>regmatches()</code> and <code>regexec()</code>:</p>

<pre><code>regmatches(""(sometext :: 0.1231313213)"",regexec(""\\((.*?) :: (0\\.[0-9]+)\\)"",""(sometext :: 0.1231313213)""))
[[1]]
[1] ""(sometext :: 0.1231313213)"" ""sometext""                   ""0.1231313213""
</code></pre>
"
16914536,1156245,2013-06-04T09:42:32Z,1231195,23,FALSE,"<p>A neat trick for RStudio I've just discovered is to use <code>#'</code> as this creates an self-expanding comment section (when you return to new line from such a line or insert new lines into such a section it is automatically comment).</p>
"
16965258,1156245,2013-06-06T14:49:12Z,1923273,27,FALSE,"<p>There is also <code>count(numbers)</code> from <code>plyr</code> package. Much more convenient than <code>table</code> in my opinion.</p>
"
16977253,2462235,2013-06-07T06:01:09Z,1432867,8,FALSE,"<p>Coming a little late to the game, but this solution might be useful for future users. It uses the <code>diamond</code> data.frame loaded with R and takes advantage of <code>stat_summary</code> along with two (super short) custom functions.</p>

<pre><code>require(ggplot2)

# create functions to get the lower and upper bounds of the error bars
stderr &lt;- function(x){sqrt(var(x,na.rm=TRUE)/length(na.omit(x)))}
lowsd &lt;- function(x){return(mean(x)-stderr(x))}
highsd &lt;- function(x){return(mean(x)+stderr(x))}

# create a ggplot
ggplot(diamonds,aes(cut,price,fill=color))+
# first layer is barplot with means
stat_summary(fun.y=mean, geom=""bar"", position=""dodge"", colour='white')+
# second layer overlays the error bars using the functions defined above
stat_summary(fun.y=mean, fun.ymin=lowsd, fun.ymax=highsd, geom=""errorbar"", position=""dodge"",color = 'black', size=.5)
</code></pre>

<p><a href=""http://i41.tinypic.com/ief48o.png"">bar + error plot http://i41.tinypic.com/ief48o.png</a></p>
"
17017002,1629031,2013-06-10T04:44:10Z,2617600,14,FALSE,"<p>If the URL is https, like used for Amazon S3, then use getURL</p>

<pre><code>json &lt;- fromJSON(getURL('https://s3.amazonaws.com/bucket/my.json'))
</code></pre>
"
17078042,2480538,2013-06-13T01:52:48Z,2399027,3,FALSE,"<p>System information:
R version 3.0.1 (2013-05-16)
Platform: x86_64-w64-mingw32/x64 (64-bit)</p>

<p>I encountered this same problem.  I was able to solve the issue with one line of code into the command window obtained from this <a href=""http://www.r-statistics.com/2012/08/how-to-load-the-rjava-package-after-the-error-java_home-cannot-be-determined-from-the-registry/"" rel=""nofollow"">website</a>. </p>

<blockquote>
  <p>Sys.setenv(JAVA_HOME='C:\Program Files\Java\jre7')</p>
</blockquote>

<p>Note that I used this particular line because I was on a 64 bit system. See website for 32-bit example.</p>
"
17303860,516548,2013-06-25T17:34:42Z,1785320,4,FALSE,"<p>Here is a one-liner using <code>tapply</code> and <code>prop.table</code>.  It does not rely on any auxilliary packages:</p>

<pre><code>prop.table(tapply(dfm$value, dfm[1:2], sum), 1)
</code></pre>

<p>giving:</p>

<pre><code>      ID2
ID1         ID2a       ID2b       ID2c       ID2d      ID2e
  ID1a 0.6318101 0.08660638 0.06338147 0.04436700 0.1738350
  ID1b 0.5888996 0.11442735 0.04765539 0.04967784 0.1993399
  ID1c 0.5480662 0.08875735 0.04835905 0.06279786 0.2520195
</code></pre>

<p>or this which is even shorter:</p>

<pre><code>prop.table( xtabs(value ~., dfm), 1 )
</code></pre>
"
17367713,2532345,2013-06-28T14:57:17Z,2241369,1,FALSE,"<p>I faced the same issue. The most convenient way is to start R as super user.</p>

<pre>
sudo R
</pre>

<p>After that, <code>install.packages(""some package"")</code> should work.</p>
"
17686752,1888451,2013-07-16T20:51:59Z,1714280,7,FALSE,"<p>In the forecast package, try:</p>

<pre><code>arima(df[,1:4], order=(0,0,0), xreg=df[,6:8])
</code></pre>

<p>for forecasting <code>u</code>, <code>cci</code> and <code>gdp</code>.</p>

<p>To predict <code>dx</code> from that, try the VAR model. Here's a good tutorial (<a href=""http://faculty.washington.edu/ezivot/econ584/notes/varModels.pdf"" rel=""noreferrer""><strong>PDF</strong></a>). </p>
"
17727344,2596153,2013-07-18T15:11:33Z,1245273,4,FALSE,"<p>Run the hist() function without making a graph, log-transform the counts, and then draw the figure.</p>

<pre><code>hist.data = hist(my.data, plot=F)
hist.data$counts = log(hist.data$counts, 2)
plot(hist.data)
</code></pre>

<p>It should look just like the regular histogram, but the y-axis will be log2 Frequency.</p>
"
17995964,2642348,2013-08-01T13:51:07Z,1632772,0,FALSE,"<p>here's a function to take the common row names of 2 data frames and do an rbind where we basically find the fields that are factors, add the new factors then do the rbind.  This should take care of any factor issues:</p>

<p>rbindCommonCols&lt;-function(x, y){</p>

<pre><code>commonColNames = intersect(colnames(x), colnames(y))
x = x[,commonColNames]
y = y[,commonColNames]

colClassesX = sapply(x, class)
colClassesY = sapply(y, class)
classMatch = paste( colClassesX, colClassesY, sep = ""-"" )
factorColIdx = grep(""factor"", classMatch)

for(n in factorColIdx){ 
    x[,n] = as.factor(x[,n])
    y[,n] = as.factor(y[,n])
}

for(n in factorColIdx){ 
    x[,n] = factor(x[,n], levels = unique(c( levels(x[,n]), levels(y[,n]) )))
    y[,n] = factor(y[,n], levels = unique(c( levels(y[,n]), levels(x[,n]) )))  
} 

res = rbind(x,y)
res
</code></pre>

<p>}</p>
"
18042069,2640293,2013-08-04T10:45:44Z,2351204,0,FALSE,"<p>For your specific example, a one-line solution is <code>sapply(lst,function(x) rowSums(m[,x]))</code> (although you might add some more lines to check for valid input and put in the column names).</p>

<p>Do you have other, more general, applications in mind?  Or is this possibly a case of <a href=""http://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it"" rel=""nofollow"">YAGNI</a>?</p>
"
18198985,1681480,2013-08-13T00:51:00Z,1401904,1,FALSE,"<p>The accepted answer might work if you have foresight, but I had already gotten rid of the old version so wasn't able to follow these directions. 
The steps described below worked for OSX upgrading from 2.1 and 3.1.</p>

<p>UPDATED: To get the directory for your last version (instead of typing in 3.1 or 3.2) you can use the below commands. The second one converts directly to the R-variable, skipping <code>.</code> and <code>..</code> and <code>.DS_Store</code>, use:</p>

<pre><code>OLD=$(ls -d /Library/Frameworks/R.framework/Versions/*.* |tail -n 2 | head -n 1)Resources/library/
echo ""packages = c(\""`ls $OLD | tail +4| paste -s -d ',' - | sed -E 's|,|\"",\""|'g`\"")"" | tr -d ""/""
</code></pre>

<p>Then within <em>R</em> you can paste that variable that is generated (add <code>|pbcopy</code> to the end to copy it directly. Once that is defined in the new version of R, you can loop through the installed packages from the instructions above...</p>

<pre><code>for (p in setdiff(packages, installed.packages()[,""Package""]))
   install.packages(p, dependencies=TRUE, quiet=TRUE, ask=FALSE)
</code></pre>
"
18239974,2579700,2013-08-14T18:54:16Z,1395309,39,FALSE,"<p>I have a basic system I use where I parallelize my programs on the ""for"" loops. This method is simple once you understand what needs to be done. It only works for local computing, but that seems to be what you're after.</p>

<p>You'll need these libraries installed:</p>

<pre><code>library(""parallel"")
library(""foreach"")
library(""doParallel"")
</code></pre>

<p>First you need to create your computing cluster. I usually do other stuff while running parallel programs, so I like to leave one open. The ""detectCores"" function will return the number of cores in your computer.</p>

<pre><code>cl &lt;- makeCluster(detectCores() - 1)
registerDoParallel(cl, cores = detectCores() - 1)
</code></pre>

<p>Next, call your for loop with the ""foreach"" command, along with the %dopar% operator. I always use a ""try"" wrapper to make sure that any iterations where the operations fail are discarded, and don't disrupt the otherwise good data. You will need to specify the "".combine"" parameter, and pass any necessary packages into the loop. Note that ""i"" is defined with an equals sign, not an ""in"" operator!</p>

<pre><code>data = foreach(i = 1:length(filenames), .packages = c(""ncdf"",""chron"",""stats""),
               .combine = rbind) %dopar% {
  try({
       # your operations; line 1...
       # your operations; line 2...
       # your output
     })
}
</code></pre>

<p>Once you're done, clean up with:</p>

<pre><code>stopCluster(cl)
</code></pre>
"
18294808,2693062,2013-08-18T01:12:55Z,1169248,15,FALSE,"<p>Also to find the position of the element ""which"" can be used as</p>

<pre><code>pop &lt;- c(3,4,5,7,13)

which(pop==13)
</code></pre>

<p>and to find the elements which are not contained in the target vector, one may do this:</p>

<pre><code>pop &lt;- c(1,2,4,6,10)

Tset &lt;- c(2,10,7)   # Target set

pop[which(!(pop%in%Tset))]
</code></pre>
"
18325142,1855677,2013-08-20T00:30:53Z,1269624,8,FALSE,"<p>Logical indexing is very R-ish.  Try: </p>

<pre><code> x[ x$A ==5 &amp; x$B==4.25 &amp; x$C==4.5 , ] 
</code></pre>

<p>Or:</p>

<pre><code>subset( x, A ==5 &amp; B==4.25 &amp; C==4.5 )
</code></pre>
"
18330802,2295698,2013-08-20T08:51:10Z,1897704,3,FALSE,"<p>Another solution : the correlation between the two vectors is equal to the cosine of the angle between two vectors. </p>

<p>so the angle can be computed by <code>acos(cor(u,v))</code></p>

<pre><code># example u(1,2,0) ; v(0,2,1)

cor(c(1,2),c(2,1))
theta = acos(cor(c(1,2),c(2,1)))
</code></pre>
"
18363691,817778,2013-08-21T17:11:53Z,743812,2,FALSE,"<p>The <code>caTools</code> package has very fast rolling mean/min/max/sd and few other functions. I've only worked with <code>runmean</code> and <code>runsd</code> and they are the fastest of any of the other packages mentioned to date.</p>
"
18467693,796127,2013-08-27T14:04:55Z,1377003,1,FALSE,"<p>Using Perl's Statistics::Distributions, you can achieve this with: </p>

<pre><code>#!/usr/bin/perl

use strict; use warnings;
use Statistics::Distributions qw(uprob);

my $x       = 0;
my $mean    = 4;
my $stdev   = 10;

print ""Height of probablility distribution at point $x = ""
    . (1-uprob(($x-$mean)/$stdev)).""\n"";
</code></pre>

<p>Results with ""Height of probablility distribution at point 0 = 0.34458""</p>
"
18492446,986817,2013-08-28T15:41:35Z,1431657,1,FALSE,"<p>There is also <code>barplot.xts</code> in the <code>xtsExtra</code> package.</p>
"
18579692,1184072,2013-09-02T19:28:56Z,1296646,13,FALSE,"<p>I learned about <code>order</code> with the following example which then confused me for a long time:</p>

<pre><code>set.seed(1234)

ID        = 1:10
Age       = round(rnorm(10, 50, 1))
diag      = c(""Depression"", ""Bipolar"")
Diagnosis = sample(diag, 10, replace=TRUE)

data = data.frame(ID, Age, Diagnosis)

databyAge = data[order(Age),]
databyAge
</code></pre>

<p>The only reason this example works is because <code>order</code> is sorting by the <code>vector Age</code>, not by the column named <code>Age</code> in the <code>data frame data</code>.</p>

<p>To see this create an identical data frame using <code>read.table</code> with slightly different column names and without making use of any of the above vectors:</p>

<pre><code>my.data &lt;- read.table(text = '

  id age  diagnosis
   1  49 Depression
   2  50 Depression
   3  51 Depression
   4  48 Depression
   5  50 Depression
   6  51    Bipolar
   7  49    Bipolar
   8  49    Bipolar
   9  49    Bipolar
  10  49 Depression

', header = TRUE)
</code></pre>

<p>The above line structure for <code>order</code> no longer works because there is no vector named <code>age</code>:</p>

<pre><code>databyage = my.data[order(age),]
</code></pre>

<p>The following line works because <code>order</code> sorts on the column <code>age</code> in <code>my.data</code>.</p>

<pre><code>databyage = my.data[order(my.data$age),]
</code></pre>

<p>I thought this was worth posting given how confused I was by this example for so long.  If this post is not deemed appropriate for the thread I can remove it.</p>

<p><strong>EDIT: May 13, 2014</strong></p>

<p>Below is a generalized way of sorting a data frame by every column without specifying column names.  The code below shows how to sort from left to right or by right to left.  This works if every column is numeric.  I have not tried with a character column added.  </p>

<p>I found the <code>do.call</code> code a month or two ago in an old post on a different site, but only after extensive and difficult searching.  I am not sure I could relocate that post now.  The present thread is the first hit for ordering a <code>data.frame</code> in <code>R</code>.  So, I thought my expanded version of that original <code>do.call</code> code might be useful.</p>

<pre><code>set.seed(1234)

v1  &lt;- c(0,0,0,0, 0,0,0,0, 1,1,1,1, 1,1,1,1)
v2  &lt;- c(0,0,0,0, 1,1,1,1, 0,0,0,0, 1,1,1,1)
v3  &lt;- c(0,0,1,1, 0,0,1,1, 0,0,1,1, 0,0,1,1)
v4  &lt;- c(0,1,0,1, 0,1,0,1, 0,1,0,1, 0,1,0,1)

df.1 &lt;- data.frame(v1, v2, v3, v4) 
df.1

rdf.1 &lt;- df.1[sample(nrow(df.1), nrow(df.1), replace = FALSE),]
rdf.1

order.rdf.1 &lt;- rdf.1[do.call(order, as.list(rdf.1)),]
order.rdf.1

order.rdf.2 &lt;- rdf.1[do.call(order, rev(as.list(rdf.1))),]
order.rdf.2

rdf.3 &lt;- data.frame(rdf.1$v2, rdf.1$v4, rdf.1$v3, rdf.1$v1) 
rdf.3

order.rdf.3 &lt;- rdf.1[do.call(order, as.list(rdf.3)),]
order.rdf.3
</code></pre>
"
18616866,1042613,2013-09-04T14:41:11Z,1439348,0,FALSE,"<p>For another case not covered in the answers here see the answers reported in ""<a href=""https://stackoverflow.com/questions/3485228/view-source-code-in-r"">view source code in R</a>""</p>

<p>which deals with the case of functions that show up as ""Non-visible"" when you run methods(function.I.am.looking.for)</p>

<p>In short you can then use a command of the form: package:::function.class
to see the code you want.</p>
"
18686783,2482776,2013-09-08T17:50:16Z,1660124,42,FALSE,"<p>The answer provided by rcs works and is simple. However, if you are handling larger datasets and need a performance boost there is a faster alternative:</p>

<pre><code>library(data.table)
data = data.table(Category=c(""First"",""First"",""First"",""Second"",""Third"", ""Third"", ""Second""), 
                  Frequency=c(10,15,5,2,14,20,3))
data[, sum(Frequency), by = Category]
#    Category V1
# 1:    First 30
# 2:   Second  5
# 3:    Third 34
system.time(data[, sum(Frequency), by = Category] )
# user    system   elapsed 
# 0.008     0.001     0.009 
</code></pre>

<p>Let's compare that to the same thing using data.frame and the above above:</p>

<pre><code>data = data.frame(Category=c(""First"",""First"",""First"",""Second"",""Third"", ""Third"", ""Second""),
                  Frequency=c(10,15,5,2,14,20,3))
system.time(aggregate(data$Frequency, by=list(Category=data$Category), FUN=sum))
# user    system   elapsed 
# 0.008     0.000     0.015 
</code></pre>

<p>And if you want to keep the column this is the syntax:</p>

<pre><code>data[,list(Frequency=sum(Frequency)),by=Category]
#    Category Frequency
# 1:    First        30
# 2:   Second         5
# 3:    Third        34
</code></pre>

<p>The difference will become more noticeable with larger datasets, as the code below demonstrates:</p>

<pre><code>data = data.table(Category=rep(c(""First"", ""Second"", ""Third""), 100000),
                  Frequency=rnorm(100000))
system.time( data[,sum(Frequency),by=Category] )
# user    system   elapsed 
# 0.055     0.004     0.059 
data = data.frame(Category=rep(c(""First"", ""Second"", ""Third""), 100000), 
                  Frequency=rnorm(100000))
system.time( aggregate(data$Frequency, by=list(Category=data$Category), FUN=sum) )
# user    system   elapsed 
# 0.287     0.010     0.296 
</code></pre>

<hr>

<p>For multiple aggregations, you can combine <code>lapply</code> and <code>.SD</code> as follows</p>

<pre><code>data[, lapply(.SD, sum), by = Category]
#    Category Frequency
# 1:    First        30
# 2:   Second         5
# 3:    Third        34
</code></pre>
"
18763500,2772473,2013-09-12T11:50:04Z,2547402,8,FALSE,"<p>I can't vote yet but Rasmus Bååth's answer is what I was looking for. 
However, I would modify it a bit allowing to contrain the distribution for example fro values only between 0 and 1. </p>

<pre><code>estimate_mode &lt;- function(x,from=min(x), to=max(x)) {
  d &lt;- density(x, from=from, to=to)
  d$x[which.max(d$y)]
}
</code></pre>

<p>We aware that you may not want to constrain at all your distribution, then set from=-""BIG NUMBER"", to=""BIG NUMBER""</p>
"
18895310,2795533,2013-09-19T12:59:39Z,2310913,4,FALSE,"<p>Completely manual... not recommended for large datasets.</p>

<pre><code>tree&lt;-list();
attributes(tree)&lt;-list(members=18,height=3);
class(tree)&lt;-""dendrogram"";

tree[[1]]&lt;-list();
attributes(tree[[1]])&lt;-list(members=4,height=2,edgetext=""1"");
 tree[[1]][[1]]&lt;-list();
 attributes(tree[[1]][[1]])&lt;-list(members=2,height=1,edgetext=""H"");
  tree[[1]][[1]][[1]]&lt;-list();
  attributes(tree[[1]][[1]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""1HH"",leaf=TRUE);
  tree[[1]][[1]][[2]]&lt;-list();
  attributes(tree[[1]][[1]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""1HT"",leaf=TRUE);
 tree[[1]][[2]]&lt;-list();
 attributes(tree[[1]][[2]])&lt;-list(members=2,height=1,edgetext=""T"");
  tree[[1]][[2]][[1]]&lt;-list();
  attributes(tree[[1]][[2]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""1TH"",leaf=TRUE);
  tree[[1]][[2]][[2]]&lt;-list();
  attributes(tree[[1]][[2]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""1TT"",leaf=TRUE);
tree[[2]]&lt;-list();
attributes(tree[[2]])&lt;-list(members=2,height=2,edgetext=""2"");
 tree[[2]][[1]]&lt;-list();
 attributes(tree[[2]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""2H"",leaf=TRUE);
 tree[[2]][[2]]&lt;-list();
 attributes(tree[[2]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""2T"",leaf=TRUE);
tree[[3]]&lt;-list();
attributes(tree[[3]])&lt;-list(members=4,height=2,edgetext=""3"");
 tree[[3]][[1]]&lt;-list();
 attributes(tree[[3]][[1]])&lt;-list(members=2,height=1,edgetext=""H"");
  tree[[3]][[1]][[1]]&lt;-list();
  attributes(tree[[3]][[1]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""3HH"",leaf=TRUE);
  tree[[3]][[1]][[2]]&lt;-list();
  attributes(tree[[3]][[1]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""3HT"",leaf=TRUE);
 tree[[3]][[2]]&lt;-list();
 attributes(tree[[3]][[2]])&lt;-list(members=2,height=1,edgetext=""T"");
  tree[[3]][[2]][[1]]&lt;-list();
  attributes(tree[[3]][[2]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""3TH"",leaf=TRUE);
  tree[[3]][[2]][[2]]&lt;-list();
  attributes(tree[[3]][[2]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""3TT"",leaf=TRUE);
tree[[4]]&lt;-list();
attributes(tree[[4]])&lt;-list(members=2,height=2,edgetext=""4"");
 tree[[4]][[1]]&lt;-list();
 attributes(tree[[4]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""4H"",leaf=TRUE);
 tree[[4]][[2]]&lt;-list();
 attributes(tree[[4]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""4T"",leaf=TRUE);
tree[[5]]&lt;-list();
attributes(tree[[5]])&lt;-list(members=4,height=2,edgetext=""5"");
 tree[[5]][[1]]&lt;-list();
 attributes(tree[[5]][[1]])&lt;-list(members=2,height=1,edgetext=""H"");
  tree[[5]][[1]][[1]]&lt;-list();
  attributes(tree[[5]][[1]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""5HH"",leaf=TRUE);
  tree[[5]][[1]][[2]]&lt;-list();
  attributes(tree[[5]][[1]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""5HT"",leaf=TRUE);
 tree[[5]][[2]]&lt;-list();
 attributes(tree[[5]][[2]])&lt;-list(members=2,height=1,edgetext=""T"");
  tree[[5]][[2]][[1]]&lt;-list();
  attributes(tree[[5]][[2]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""5TH"",leaf=TRUE);
  tree[[5]][[2]][[2]]&lt;-list();
  attributes(tree[[5]][[2]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""5TT"",leaf=TRUE);
tree[[6]]&lt;-list();
attributes(tree[[6]])&lt;-list(members=2,height=2,edgetext=""6"");
 tree[[6]][[1]]&lt;-list();
 attributes(tree[[6]][[1]])&lt;-list(members=1,height=0,edgetext=""H"",label=""6H"",leaf=TRUE);
 tree[[6]][[2]]&lt;-list();
 attributes(tree[[6]][[2]])&lt;-list(members=1,height=0,edgetext=""T"",label=""6T"",leaf=TRUE);


windows(width=3,rescale=""fixed"");
par(ps=8);
plot(rev(tree),center=TRUE,horiz=TRUE);
</code></pre>
"
19017814,2817561,2013-09-26T01:23:21Z,750786,0,FALSE,"<p>Try smallR for writing quick R scripts in the command line:</p>

<p><a href=""http://code.google.com/p/simple-r/"" rel=""nofollow"">http://code.google.com/p/simple-r/</a></p>

<p>(<code>r</code> command in the directory)</p>

<p>Plotting from the command line using smallR would look like this:</p>

<pre><code>r -p file.txt
</code></pre>
"
19031577,1451882,2013-09-26T14:52:34Z,2453326,10,FALSE,"<p>Here is an easy way to find the indices of N smallest/largest values in a vector(Example for N = 3):</p>

<pre><code>N &lt;- 3
</code></pre>

<p>N Smallest:</p>

<pre><code>ndx &lt;- order(x)[1:N]
</code></pre>

<p>N Largest:</p>

<pre><code>ndx &lt;- order(x, decreasing = T)[1:N]
</code></pre>

<p>So you can extract the values as:</p>

<pre><code>x[ndx]
</code></pre>
"
19039094,1851712,2013-09-26T21:32:01Z,2564258,12,FALSE,"<p>As described by @redmode, you may plot the two lines in the same graphical device using <code>ggplot</code>. However, the data in that answer was in a 'wide' format, whereas in <code>ggplot</code> it is generally most convenient to keep the data in a data frame in a 'long' format. Then, by using different 'grouping variables' in the <code>aes</code>thetics arguments, properties of the line, such as linetype or colour, will vary according to the grouping variable, and corresponding legends will appear. In this case we can use the <code>colour</code> <code>aes</code>sthetics, which matches colour of the lines to different levels of a variable in the data set (here: y1 vs y2). But first we need to melt the data from wide to long format, using the function 'melt' from <code>reshape2</code> package. </p>

<pre><code>library(ggplot2)
library(reshape2)

# original data in a 'wide' format
x  &lt;- seq(-2, 2, 0.05)
y1 &lt;- pnorm(x)
y2 &lt;- pnorm(x, 1, 1)
df &lt;- data.frame(x, y1, y2)

# melt the data to a long format
df2 &lt;- melt(data = df, id.vars = ""x"")

# plot, using the aesthetics argument 'colour'
ggplot(data = df2, aes(x = x, y = value, colour = variable)) + geom_line()
</code></pre>

<p><img src=""https://i.stack.imgur.com/zZldE.png"" alt=""enter image description here""></p>
"
19107966,2817561,2013-10-01T04:26:39Z,876711,0,FALSE,"<p>There is a simple-r way of plotting it:</p>

<p><a href=""https://code.google.com/p/simple-r/"" rel=""nofollow"">https://code.google.com/p/simple-r/</a></p>

<p>Using that script, you just have to type:</p>

<pre><code>r -cdps, -k1:2 foo.csv
</code></pre>

<p>To get the plot you want. Put it in the verbose mode (-v) to see the corresponding R script. </p>
"
19136456,1055339,2013-10-02T12:03:47Z,1169248,129,FALSE,"<p><code>is.element()</code> makes for more readable code, and is identical to <code>%in%</code></p>

<pre><code>v &lt;- c('a','b','c','e')

is.element('b', v)
'b' %in% v
## both return TRUE

is.element('f', v)
'f' %in% v
## both return FALSE

subv &lt;- c('a', 'f')
subv %in% v
## returns a vector TRUE FALSE
is.element(subv, v)
## returns a vector TRUE FALSE
</code></pre>
"
19187368,1901172,2013-10-04T17:30:40Z,743812,1,FALSE,"<p>all the options listed here are causal moving averages. if a non causal version is required, then the package <strong>signal</strong> has some options.</p>
"
19226029,1901172,2013-10-07T13:21:11Z,2679193,9,FALSE,"<p>Don't make data frames. Keep the list, name its elements but do not attach it.</p>

<p>The biggest reason for this is that if you make variables on the go, almost always you will later on have to iterate through each one of them to perform something useful. There you will again be forced to iterate through each one of the names that you have created on the fly.</p>

<p>It is far easier to name the elements of the list and iterate through the names. </p>

<p>As far as attach is concerned, its really bad programming practice in R and can lead to a lot of trouble if you are not careful.</p>
"
19291410,1831980,2013-10-10T09:01:18Z,2258784,3,FALSE,"<p><strong>Theme templates:</strong></p>

<p><a href=""https://github.com/jrnold/ggthemes"" rel=""nofollow"">https://github.com/jrnold/ggthemes</a></p>

<p>like ""The economist"", ""Stata"", ""tufte"" and more..</p>

<p>I know the answer is not exactly what was asked, but it was what I was looking for when I found this question, so others might too.</p>
"
19316523,1156245,2013-10-11T10:59:54Z,1358003,15,FALSE,"<p>Just to note that <code>data.table</code> package's <code>tables()</code> seems to be a pretty good replacement for Dirk's <code>.ls.objects()</code> custom function (detailed in earlier answers), although just for data.frames/tables and not e.g. matrices, arrays, lists.</p>
"
19550228,437441,2013-10-23T19:03:04Z,2453326,2,FALSE,"<p>I found that removing the max element first and then do another max runs in comparable speed:</p>

<pre><code>system.time({a=runif(1000000);m=max(a);i=which.max(a);b=a[-i];max(b)})
   user  system elapsed 
  0.092   0.000   0.659 

system.time({a=runif(1000000);n=length(a);sort(a,partial=n-1)[n-1]})
   user  system elapsed 
  0.096   0.000   0.653 
</code></pre>
"
19550737,2912901,2013-10-23T19:32:07Z,2535234,10,FALSE,"<p>Taking the comment from Jonathan Chang I wrote this function to mimic dist. No extra packages to load.</p>

<pre><code>cosineDist &lt;- function(x){
  as.dist(1 - x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2))))) 
}
</code></pre>
"
19648473,1003565,2013-10-29T02:14:35Z,1448600,1,FALSE,"<p>This is quite old but alternatively you could write your document using the <a href=""http://yihui.name/knitr/"" rel=""nofollow"">knitr</a> package.  This creates output that you can just copy/paste into an R session.</p>
"
19732053,2945838,2013-11-01T17:31:26Z,2676554,-10,FALSE,"<pre><code>y &lt;- mean(x, na.rm=TRUE)
</code></pre>

<p><code>sd(y)</code> for standard deviation <code>var(y)</code> for variance.</p>

<p>Both derivations use <code>n-1</code> in the denominator so they are based on sample data.</p>
"
19737664,2297736,2013-11-02T00:39:41Z,876711,0,FALSE,"<pre><code>data &lt;- read.table(...)
plot(data$scale,data$serial)
</code></pre>
"
19795764,1178441,2013-11-05T18:07:54Z,1395191,0,FALSE,"<p>You could use the <code>split</code> function
If you opened your data as:</p>

<pre><code>data &lt;- read.table('your_data.txt', header=T)
blocks &lt;- split(data, data$site)
</code></pre>

<p>After that, blocks contains data from each block, that you can access as other data.frame:</p>

<pre><code>plot(blocks$ALBEN$year, blocks$ALBEN$peak)
</code></pre>

<p>And so on for each plot.</p>
"
19797278,1539634,2013-11-05T19:31:44Z,1395105,0,FALSE,"<p>I just have a workaround. One may first generate an eps file, then convert it back to pgf using the tool eps2pgf. See <a href=""http://www.texample.net/tikz/examples/eps2pgf/"" rel=""nofollow"">http://www.texample.net/tikz/examples/eps2pgf/</a></p>
"
19844891,2457883,2013-11-07T19:27:24Z,2280724,1,FALSE,"<p>With every new version Tinn-R tries to preserve (as much as possible) the user's preferences related to old version.</p>

<p>These preferences are stored in a temporary file (INI). This file is never removed when the program is uninstalled.</p>

<p>Farrel is correct:
In the user guide it Clearly states que Should one uninstall the prior Tinn-R and then install the new version!</p>

<p>In time, it was released two new versions this week (with many new features):</p>

<p>3.0.1.8: <a href=""http://sourceforge.net/p/tinn-r/news/2013/11/tinn-r-3018-released/"" rel=""nofollow"">http://sourceforge.net/p/tinn-r/news/2013/11/tinn-r-3018-released/</a></p>

<p>3.0.1.9: <a href=""http://sourceforge.net/p/tinn-r/news/2013/11/tinn-r-3019-released/"" rel=""nofollow"">http://sourceforge.net/p/tinn-r/news/2013/11/tinn-r-3019-released/</a></p>
"
19896661,1838509,2013-11-10T23:38:56Z,1402001,2,FALSE,"<p>Now You can use  <code>dbBulkCopy</code>  from the new <a href=""https://github.com/agstudy/rsqlserver"" rel=""nofollow"">rsqlserver</a>  package:</p>

<p>A typical scenario:</p>

<ol>
<li>You create a matrix </li>
<li>you save it as a csv file</li>
<li>You call <code>dbBulkCopy</code> to read fil and insert it using internally <code>bcp</code> tool of MS Sql server.</li>
</ol>

<p>This assume that your table is already created in the data base:</p>

<pre><code>dat &lt;- matrix(round(rnorm(nrow*ncol),nrow,ncol)
id.file = ""temp_file.csv""                      
write.csv(dat,file=id.file,row.names=FALSE)
dbBulkCopy(conn,'NEW_BP_TABLE',value=id.file)
</code></pre>
"
20004385,2996802,2013-11-15T15:21:20Z,2098368,4,FALSE,"<p>For <code>sdata</code>:</p>

<pre><code>gsub("", "","""",toString(sdata))
</code></pre>

<p>For a vector of integers:</p>

<pre><code>gsub("", "","""",toString(c(1:10)))
</code></pre>
"
20271729,1527403,2013-11-28T17:20:55Z,1727772,5,FALSE,"<p>A minor additional points worth mentioning. If you have a very large file you can on the fly calculate the number of rows (if no header) using (where <code>bedGraph</code> is the name of your file in your working directory):</p>

<pre><code>&gt;numRow=as.integer(system(paste(""wc -l"", bedGraph, ""| sed 's/[^0-9.]*\\([0-9.]*\\).*/\\1/'""), intern=T))
</code></pre>

<p>You can then use that either in <code>read.csv</code> , <code>read.table</code> ...</p>

<pre><code>&gt;system.time((BG=read.table(bedGraph, nrows=numRow, col.names=c('chr', 'start', 'end', 'score'),colClasses=c('character', rep('integer',3)))))
   user  system elapsed 
 25.877   0.887  26.752 
&gt;object.size(BG)
203949432 bytes
</code></pre>
"
20282846,946850,2013-11-29T09:52:28Z,1815606,4,FALSE,"<p>I have wrapped up and extended the answers to this question into a new function <code>thisfile()</code> in <a href=""https://cran.r-project.org/web/packages/kimisc/index.html"" rel=""nofollow"">my misc package</a>. Also works for knitting with <code>knitr</code>.</p>
"
20359705,1169676,2013-12-03T19:16:38Z,2547402,0,FALSE,"<p>You could also calculate the number of times an instance has happened in your set and find the max number. e.g.</p>

<pre><code>&gt; temp &lt;- table(as.vector(x))
&gt; names (temp)[temp==max(temp)]
[1] ""1""
&gt; as.data.frame(table(x))
r5050 Freq
1     0   13
2     1   15
3     2    6
&gt; 
</code></pre>
"
20560566,214728,2013-12-13T06:39:57Z,1231195,5,FALSE,"<p>Apart from using the overkilled way to comment multi-line codes just by installing RStudio, you can use <a href=""http://notepad-plus-plus.org/%E2%80%8E"" rel=""nofollow noreferrer"">Notepad++</a> as it supports the syntax highlighting of R</p>

<p>(Select multi-lines) -> Edit -> Comment/Uncomment -> Toggle Block Comment</p>

<p>Note that you need to save the code as a .R source first (highlighted in red)</p>

<p><img src=""https://i.stack.imgur.com/DC4PI.png"" alt=""Note that you need to save the code as a .R source first (highlighted in red)""></p>
"
20935710,2235908,2014-01-05T16:03:38Z,2557863,2,FALSE,"<p>Stumbled across this page today, as I was looking for an implementation of kendall tau-b in R<br>
For anyone else looking for the same thing:<br>
tau-b is in fact part of the stats package.</p>

<p>See this link for more details:
<a href=""https://stat.ethz.ch/pipermail/r-help//2012-August/333656.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help//2012-August/333656.html</a></p>

<p>I tried it and it works:
library(stats)</p>

<pre><code>x &lt;- c(1,1,2)
y&lt;-c(1,2,3)
cor.test(x, y, method = ""kendall"", alternative = ""greater"")
</code></pre>

<p>this is the output:</p>

<pre><code>data:  x and y
z = 1.2247, p-value = 0.1103
alternative hypothesis: true tau is greater than 0
sample estimates:
      tau 
0.8164966 

Warning message:
In cor.test.default(x, y, method = ""kendall"", alternative = ""greater"") :
  Cannot compute exact p-value with ties
</code></pre>

<p>Just ignore the warning messege. The tau is in fact tau b !!!</p>
"
21005136,345660,2014-01-08T19:47:39Z,2453326,20,FALSE,"<p>I wrapped Rob's answer up into a slightly more general function, which can be used to find the 2nd, 3rd, 4th (etc.) max:</p>

<pre><code>maxN &lt;- function(x, N=2){
  len &lt;- length(x)
  if(N&gt;len){
    warning('N greater than length(x).  Setting N=length(x)')
    N &lt;- length(x)
  }
  sort(x,partial=len-N+1)[len-N+1]
}

maxN(1:10)
</code></pre>
"
21012300,819544,2014-01-09T05:33:17Z,2535234,15,FALSE,"<p>It looks like a few options are already available, but I just stumbled across an idiomatic solution I like so I thought I'd add it to the list.</p>

<pre><code>install.packages('proxy') # Let's be honest, you've never heard of this before.
library('proxy') # Library of similarity/dissimilarity measures for 'dist()'
dist(m, method=""cosine"")
</code></pre>
"
21084446,3189041,2014-01-13T05:28:34Z,2050790,5,FALSE,"<pre><code>x = list(1, 2, 3, 4)
x2 = list(1:4)
all.equal(x,x2)
</code></pre>

<p>is not the same because 1:4 is the same as c(1,2,3,4).
If you want them to be the same then:</p>

<pre><code>x = list(c(1,2,3,4))
x2 = list(1:4)
all.equal(x,x2)
</code></pre>
"
21188796,2783516,2014-01-17T14:53:12Z,2053397,2,FALSE,"<p>I was trying to find a workaround for this issue from last two days and finally I found it today. We have 19 digits long ids in our SQL database and earlier I used RODBC to get bigint data from the server. I tried int64 and bit64, also defined options(digits=19), but RODBC kept on giving issues. I replaced RODBC with RJDBC, and while retrieving bigint data from SQL server, I manipulated SQL query by using casting bigint data to string.</p>

<p>So here is sample code:</p>

<pre><code>#Include stats package
require(stats);
library(RJDBC);
#set the working directory
setwd(""W:/Users/dev/Apps/R/Data/201401_2"");

#Getting JDBC Driver
driver &lt;- JDBC(""com.microsoft.sqlserver.jdbc.SQLServerDriver"", ""W:/Users/dev/Apps/R/Data/sqljdbc/enu/sqljdbc4.jar"");

#Connect with DB
connection &lt;- dbConnect(driver, ""jdbc:sqlserver://DBServer;DatabaseName=DB;"", ""BS_User"", ""BS_Password"");
#Query string


  sqlText &lt;- paste(""SELECT DISTINCT Convert(varchar(19), ID) as ID
 FROM tbl_Sample"", sep="""");

#Execute query
queryResults &lt;- dbGetQuery(connection, sqlText);
</code></pre>

<p>With this solution, I got bigint data without any modification but it didn't work with RODBC. Now the speed of SQL server interaction with R has affected because RJDBC is slower than RODBC but its not too bad.</p>
"
21244906,3216883,2014-01-20T21:52:53Z,2653035,0,FALSE,"<p>The second code block of George Dontas a really good example. Solved a big issue for me. But it took me forever to figure out that the names of the CA-objects are actually:</p>

<pre><code>[YOUR_CA-CLASS-TABLE]$colcoord[,1]
</code></pre>

<p>and</p>

<pre><code>[YOUR_CA-CLASS-TABLE]$rowcoord[,1]
</code></pre>
"
21261034,3215940,2014-01-21T14:34:05Z,1497539,1,FALSE,"<p>I had the same problem but Dirk's solution didn't seem to work. 
I was getting this warning messege every time</p>

<pre><code>""prob"" is not a graphical parameter
</code></pre>

<p>I read through ?hist and found about <code>freq</code>: a logical vector set TRUE by default.</p>

<p>the code that worked for me is</p>

<pre><code>hist(x,freq=FALSE)
lines(density(x),na.rm=TRUE)
</code></pre>
"
21263718,586704,2014-01-21T16:27:19Z,1484904,1,FALSE,"<p>Don't forget plyr's wonderful ""revalue"" and ""rename"" command!</p>
"
21274406,2741380,2014-01-22T04:48:30Z,2564258,2,FALSE,"<p>You could use the <a href=""https://plot.ly/api/r"" rel=""nofollow noreferrer"">Plotly R API</a> to style this. Below is the code to do so, and the live version of this graph is <a href=""https://plot.ly/~MattSundquist/792/"" rel=""nofollow noreferrer"">here</a>.</p>

<pre><code># call Plotly and enter username and key
library(plotly)
p &lt;- plotly(username=""Username"", key=""API_KEY"")

# enter data
x  &lt;- seq(-2, 2, 0.05)
y1 &lt;- pnorm(x)
y2 &lt;- pnorm(x,1,1)

# format, listing y1 as your y.
First &lt;- list(
x = x,
y = y1,
type = 'scatter',
mode = 'lines',
marker = list(
    color = 'rgb(0, 0, 255)',
    opacity = 0.5
 )
)

# format again, listing y2 as your y.
Second &lt;- list(
x = x,
y = y2,
type = 'scatter',
mode = 'lines',
opacity = 0.8, 
marker = list(
    color = 'rgb(255, 0, 0)'
 )
)

# style background color
plot_bgcolor = 'rgb(245,245,247)'

# and structure the response. Plotly returns a URL when you make the call. 
response&lt;-p$plotly(list(First,Second), kwargs = list(layout=layout))
</code></pre>

<p>Full disclosure: I'm on the Plotly team.</p>

<p><img src=""https://i.stack.imgur.com/h0Wd1.png"" alt=""Graph""></p>
"
21359089,1436187,2014-01-26T02:58:14Z,1358003,4,FALSE,"<p>The <code>ll</code>function in <code>gData</code> package can show the memory usage of each object as well.</p>

<pre><code>gdata::ll(unit='MB')
</code></pre>
"
21365610,1550270,2014-01-26T15:53:39Z,1329940,5,FALSE,"<pre><code>t(sapply(a, '[', 1:max(sapply(a, length))))
</code></pre>

<p>where 'a' is a list. 
Would work for unequal row size </p>
"
21438584,2491006,2014-01-29T17:43:26Z,1299871,55,FALSE,"<p>New in 2014: </p>

<p>Especially if you're also interested in data manipulation in general (including sorting, filtering, subsetting, summarizing etc.), you should definitely take a look at <code>dplyr</code>, which comes with a variety of functions all designed to facilitate your work specifically with data frames and certain other database types. It even offers quite an elaborate SQL interface, and even a function to convert (most) SQL code directly into R.</p>

<p>The four joining-related functions in the dplyr package are (to quote):</p>

<ul>
<li><code>inner_join(x, y, by = NULL, copy = FALSE, ...)</code>: return all rows from
x where there are matching values in y, and all columns from x and y </li>
<li><code>left_join(x, y, by = NULL, copy = FALSE, ...)</code>: return all rows from x, and all columns from x and y </li>
<li><code>semi_join(x, y, by = NULL, copy = FALSE, ...)</code>: return all rows from x where there are matching values in
y, keeping just columns from x.  </li>
<li><code>anti_join(x, y, by = NULL, copy = FALSE, ...)</code>: return all rows from x
where there are not matching values in y, keeping just columns from x</li>
</ul>

<p>It's all <a href=""http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html"" rel=""noreferrer"">here</a> in great detail.</p>

<p>Selecting columns can be done by <code>select(df,""column"")</code>. If that's not SQL-ish enough for you, then there's the <code>sql()</code> function, into which you can enter SQL code as-is, and it will do the operation you specified just like you were writing in R all along (for more information, please refer to the <a href=""http://cran.r-project.org/web/packages/dplyr/vignettes/databases.html"" rel=""noreferrer"">dplyr/databases vignette</a>). For example, if applied correctly, <code>sql(""SELECT * FROM hflights"")</code> will select all the columns from the ""hflights"" dplyr table (a ""tbl"").</p>
"
21471878,843367,2014-01-31T03:25:56Z,1195826,6,FALSE,"<p>here is a way of doing that</p>

<pre><code>varFactor &lt;- factor(letters[1:15])
varFactor &lt;- varFactor[1:5]
varFactor &lt;- varFactor[drop=T]
</code></pre>
"
21481387,1144966,2014-01-31T13:34:29Z,2470248,30,FALSE,"<p>What's about a simple <code>writeLines()</code>?</p>

<pre><code>txt &lt;- ""Hallo\nWorld""
writeLines(txt, ""outfile.txt"")
</code></pre>

<p>or</p>

<pre><code>txt &lt;- c(""Hallo"", ""World"")
writeLines(txt, ""outfile.txt"")
</code></pre>
"
21600687,1860946,2014-02-06T10:47:59Z,1358003,8,FALSE,"<p>The use of environments instead of lists to handle collections of objects which occupy a significant amount of working memory.</p>

<p>The reason: each time an element of a <code>list</code> structure is modified, the whole list is temporarily duplicated. This becomes an issue if the storage requirement of the list is about half the available working memory, because then data has to be swapped to the slow hard disk. Environments, on the other hand, aren't subject to this behaviour and they can be treated similar to lists.</p>

<p>Here is an example:</p>

<pre><code>get.data &lt;- function(x)
{
  # get some data based on x
  return(paste(""data from"",x))
}

collect.data &lt;- function(i,x,env)
{
  # get some data
  data &lt;- get.data(x[[i]])
  # store data into environment
  element.name &lt;- paste(""V"",i,sep="""")
  env[[element.name]] &lt;- data
  return(NULL)  
}

better.list &lt;- new.env()
filenames &lt;- c(""file1"",""file2"",""file3"")
lapply(seq_along(filenames),collect.data,x=filenames,env=better.list)

# read/write access
print(better.list[[""V1""]])
better.list[[""V2""]] &lt;- ""testdata""
# number of list elements
length(ls(better.list))
</code></pre>

<p>In conjunction with structures such as <code>big.matrix</code> or <code>data.table</code> which allow for altering their content in-place, very efficient memory usage can be achieved.</p>
"
21614548,1041922,2014-02-06T21:35:06Z,1299871,104,FALSE,"<p>You can do joins as well using Hadley Wickham's awesome <a href=""http://blog.rstudio.org/2014/01/17/introducing-dplyr/"">dplyr</a> package.  </p>

<pre><code>library(dplyr)

#make sure that CustomerId cols are both type numeric
#they ARE not using the provided code in question and dplyr will complain
df1$CustomerId &lt;- as.numeric(df1$CustomerId)
df2$CustomerId &lt;- as.numeric(df2$CustomerId)
</code></pre>

<h2>Mutating joins: add columns to df1 using matches in df2</h2>

<pre><code>#inner
inner_join(df1, df2)

#left outer
left_join(df1, df2)

#right outer
right_join(df1, df2)

#alternate right outer
left_join(df2, df1)

#full join
full_join(df1, df2)
</code></pre>

<h2>Filtering joins: filter out rows in df1, don't modify columns</h2>

<pre><code>semi_join(df1, df2) #keep only observations in df1 that match in df2.
anti_join(df1, df2) #drops all observations in df1 that match in df2.
</code></pre>
"
21619234,2283467,2014-02-07T04:16:37Z,2547402,2,FALSE,"<p>This works pretty fine</p>

<pre><code>&gt; a&lt;-c(1,1,2,2,3,3,4,4,5)
&gt; names(table(a))[table(a)==max(table(a))]
</code></pre>
"
21630368,3284265,2014-02-07T14:34:47Z,1523126,3,FALSE,"<p>If number is separated by ""."" and decimals by "","" (1.200.000,00) in calling <code>gsub</code> you must <code>set fixed=TRUE as.numeric(gsub(""."","""",y,fixed=TRUE))</code></p>
"
21658368,3069851,2014-02-09T11:16:47Z,2471075,0,FALSE,"<p>I've tried @George Donats answer, but couldn't make it work. So I wanted to suggest another possibility for future reference.</p>

<p>I couldn't find the file online, so I've recreated a txt file like your using TAB as a seperator. You can load it into R with the Hebrew text using a connection. It is demonstrated below:</p>

<pre><code>con&lt;-file(""aa.txt"",open=""r"",encoding=""iso8859-8"") ##Open a read-only connection with encoding fit for Hebrew (iso8859-8)
</code></pre>

<p>Than you can load it into R with your code, using con variable as the file input, code described here:</p>

<pre><code>data&lt;-read.table(con,sep=""\t"",header=TRUE)
</code></pre>

<p>Browsing into the data variable gives the following results:</p>

<pre><code>str(data)

'data.frame':   3 obs. of  3 variables:
 $ אחת  : int  6 44 3
 $ שתיים: int  97 354 1
 $ שלוש : int  12 123 6

&gt; data$אחת
[1]  6 44  3
</code></pre>
"
21675813,3292623,2014-02-10T11:22:21Z,2602583,2,FALSE,"<p>In case there is missing values in your data, this is not a rare case.
you need to add one more argument. 
You may try following codes.</p>

<pre><code>exp(mean(log(i[is.finite(log(i))]),na.rm=T))
</code></pre>
"
21706190,269476,2014-02-11T15:36:09Z,77434,14,FALSE,"<p>Another way is to take the first element of the reversed vector:</p>

<pre><code>rev(dat$vect1$vec2)[1]
</code></pre>
"
21753157,2427707,2014-02-13T11:39:52Z,1395117,12,FALSE,"<p>Package <code>lubridate</code> holds two functions to convert timezones. According to the help pages:</p>

<p><br></p>

<p><code>force_tz</code> returns a date-time that has the same clock time as <code>x</code> in the new time zone.</p>

<pre><code>force_tz(time, tzone = ""America/Los_Angeles"")
</code></pre>

<p><br></p>

<p><code>with_tz</code> changes the time zone in which an instant is displayed. The clock time displayed for the instant changes, but the moment of time described remains the same.</p>

<pre><code>with_tz(time, tzone = ""America/Los_Angeles"")
</code></pre>
"
21754778,2427707,2014-02-13T12:50:46Z,1395499,0,FALSE,"<p>The package <code>lubridate</code> provides two additional timespan classes that specify what is somewhat unclear in the base package. From tha manual:</p>

<p>Durations</p>

<p>Durations measure the exact amount of time that occurs between two instants. This can create unexpected results in relation to clock times if a leap second, leap year, or change in daylight savings time (DST) occurs in the interval.</p>

<p>Periods</p>

<p>Periods measure the change in clock time that occurs between two instants. Periods provide robust predictions of clock time in the presence of leap seconds, leap years, and changes in DST.</p>
"
21840648,2102591,2014-02-17T22:34:21Z,2619618,3,FALSE,"<p>Old thread, but for anyone who searches for this topic:</p>

<pre><code>analysis = by(...)
data.frame(t(vapply(analysis,unlist,unlist(analysis[[1]]))))
</code></pre>

<p><code>unlist()</code> will take an element of a <code>by()</code> output (in this case, <code>analysis</code>) and express it as a named vector.
<code>vapply()</code> does unlist to all the elemnts of <code>analysis</code> and outputs the result. It requires a dummy argument to know the output type, which is what <code>analysis[[1]]</code> is there for. You may need to add a check that analysis is not empty if that will be possible.
Each output will be a column, so <code>t()</code> transposes it to the desired orientation where each analysis entry becomes a row.</p>
"
21865778,1036500,2014-02-18T21:29:25Z,1296646,90,FALSE,"<p>There are a lot of excellent answers here, but <a href=""http://github.com/hadley/dplyr"">dplyr</a> gives the only syntax that I can quickly and easily remember (and so now use very often):</p>

<pre><code>library(dplyr)
# sort mtcars by mpg, ascending... use desc(mpg) for descending
arrange(mtcars, mpg)
# sort mtcars first by mpg, then by cyl, then by wt)
arrange(mtcars , mpg, cyl, wt)
</code></pre>

<p>For the OP's problem:</p>

<pre><code>arrange(dd, desc(z),  b)

    b x y z
1 Low C 9 2
2 Med D 3 1
3  Hi A 8 1
4  Hi A 9 1
</code></pre>
"
21882152,218681,2014-02-19T13:37:24Z,2261079,18,FALSE,"<p>A simple <strong>function</strong> to remove leading and trailing whitespace:</p>

<pre><code>trim &lt;- function( x ) {
  gsub(""(^[[:space:]]+|[[:space:]]+$)"", """", x)
}
</code></pre>

<p><strong>Usage:</strong></p>

<pre><code>&gt; text = ""   foo bar  baz 3 ""
&gt; trim(text)
[1] ""foo bar  baz 3""
</code></pre>
"
21900637,2525396,2014-02-20T07:06:34Z,2603184,2,FALSE,"<p>In addition to the other answers here that actually <em>pass</em> your object by reference (<code>environment</code> objects and Reference Classes), if you're purely interested in call-by-reference for syntactic convenience (i.e. you don't mind your data copied inside), you could emulate that by assigning the final value back to the outside variable while returning:</p>

<pre><code>byRef &lt;- function(..., envir=parent.frame(), inherits=TRUE) {
  cl &lt;- match.call(expand.dots = TRUE)
  cl[c(1, match(c(""envir"", ""inherits""), names(cl), 0L))] &lt;- NULL
  for (x in as.list(cl)) {
    s &lt;- substitute(x)
    sx &lt;- do.call(substitute, list(s), envir=envir)
    dx &lt;- deparse(sx)
    expr &lt;- substitute(assign(dx, s, envir=parent.frame(), inherits=inherits))
    do.call(on.exit, list(expr, add=TRUE), envir=envir)
  }
}
</code></pre>

<p>Then we can declare ""call-by-reference"" arguments:</p>

<pre><code>f &lt;- function(z1, z2, z3) {
  byRef(z1, z3)

  z1 &lt;- z1 + 1
  z2 &lt;- z2 + 2
  z3 &lt;- z3 + 3

  c(z1, z2, z3)
}

x1 &lt;- 10
x2 &lt;- 20
x3 &lt;- 30

# Values inside:
print(f(x1, x2, x3))
# [1] 11 22 33

# Values outside:
print(c(x1, x2, x3))
# [1] 11 20 33
</code></pre>

<p>Note that if you access the ""by-reference"" variables by their outside names (<code>x1</code>, <code>x3</code>)  anywhere inside the function, you'll get their yet-unmodified values from the outside. Also, this implementation only handles simple variable names as arguments, so indexed arguments such as <code>f(x[1], ...)</code> will not work (though you could probably implement that with a bit more involved expression manipulation to sidestep the limited <code>assign</code>).</p>
"
22027529,359786,2014-02-25T22:22:07Z,750786,5,FALSE,"<p>This works, </p>

<pre><code>#!/usr/bin/Rscript
</code></pre>

<p>but I don't know what happens if you have more than 1 version of R installed on your machine.</p>

<p>If you do it like this</p>

<pre><code>#!/usr/bin/env Rscript
</code></pre>

<p>it tells the interpreter to just use whatever R appears first on your path.</p>
"
22041187,2195555,2014-02-26T12:20:10Z,652136,10,FALSE,"<p>If you have a named list and want to remove a specific element you can try:</p>

<pre><code>lst &lt;- list(a = 1:4, b = 4:8, c = 8:10)

if(""b"" %in% names(lst)) lst &lt;- lst[ - which(names(lst) == ""b"")]
</code></pre>

<p>This will make a list <code>lst</code> with elements <code>a</code>, <code>b</code>, <code>c</code>. The second line removes element <code>b</code> after it checks that it exists (to avoid the problem @hjv mentioned).</p>

<p>or better:</p>

<pre><code>lst$b &lt;- NULL
</code></pre>

<p>This way it is not a problem to try to delete a non-existent element (e.g. <code>lst$g &lt;- NULL</code>)</p>
"
22402022,2125442,2014-03-14T10:13:13Z,2449083,3,FALSE,"<p>Try this function from <code>stringi</code> package</p>

<pre><code>&gt; stri_count_fixed(""GCCCAAAATTTTCCGG"",c(""G"",""C""))
[1] 3 5
</code></pre>

<p>or you can use regex version to count g and G</p>

<pre><code>&gt; stri_count_regex(""GCCCAAAATTTTCCGGggcc"",c(""G|g|C|c""))
[1] 12
</code></pre>

<p>or you can use tolower function first and then stri_count</p>

<pre><code>&gt; stri_trans_tolower(""GCCCAAAATTTTCCGGggcc"")
[1] ""gcccaaaattttccggggcc""
</code></pre>

<p>time performance</p>

<pre><code>    &gt; microbenchmark(gcCount(x,1,40),gcCount2(x,1,40), stri_count_regex(x,c(""[GgCc]"")))
Unit: microseconds
                             expr     min     lq  median      uq     max neval
                gcCount(x, 1, 40) 109.568 112.42 113.771 116.473 146.492   100
               gcCount2(x, 1, 40)  15.010  16.51  18.312  19.213  40.826   100
 stri_count_regex(x, c(""[GgCc]""))  15.610  16.51  18.912  20.112  61.239   100
</code></pre>

<p>another example for longer string. stri_dup replicates string n-times</p>

<pre><code>&gt; stri_dup(""abc"",3)
[1] ""abcabcabc""
</code></pre>

<p>As you can see, for longer sequence stri_count is faster :)</p>

<pre><code>&gt; y &lt;- stri_dup(""GCCCAAAATTTTCCGGatttaagcagacataaattcgagg"",100)
    &gt; microbenchmark(gcCount(y,1,40*100),gcCount2(y,1,40*100), stri_count_regex(y,c(""[GgCc]"")))
    Unit: microseconds
                                 expr       min         lq     median        uq       max neval
              gcCount(y, 1, 40 * 100) 10367.880 10597.5235 10744.4655 11655.685 12523.828   100
             gcCount2(y, 1, 40 * 100)   360.225   369.5315   383.6400   399.100   438.274   100
     stri_count_regex(y, c(""[GgCc]""))   131.483   137.9370   151.8955   176.511   221.839   100
</code></pre>
"
22431934,3424650,2014-03-16T01:32:56Z,2169118,2,FALSE,"<p>type this command and then all packages will be installed automatically:</p>

<pre><code>     &gt;install.packages(available.packages()[,1])
</code></pre>
"
22493231,1613005,2014-03-18T23:27:47Z,2288485,10,FALSE,"<p>With the following code you can convert all data frame columns to numeric (X is the data frame that we want to convert it's columns):</p>

<pre><code>as.data.frame(lapply(X, as.numeric))
</code></pre>

<p>and for converting whole matrix into numeric you have two ways:
Either:</p>

<pre><code>mode(X) &lt;- ""numeric""
</code></pre>

<p>or:</p>

<pre><code>X &lt;- apply(X, 2, as.numeric)
</code></pre>

<p>Alternatively you can use <code>data.matrix</code> function to convert everything into numeric, although be aware that the factors might not get converted correctly, so it is safer to convert everything to <code>character</code> first:</p>

<pre><code>X &lt;- sapply(X, as.character)
X &lt;- data.matrix(X)
</code></pre>

<p>I usually use <strong>this last one</strong> if I want <strong>to convert to matrix and numeric simultaneously</strong></p>
"
22598293,16632,2014-03-23T22:59:25Z,2310409,19,FALSE,"<p>As of roxygen2 >4.0.0, you can document the data object defined
elsewhere by documenting the name of the object defined as a string:</p>

<pre><code>#' This is data to be included in my package
#'
#' @author My Name \email{blahblah@@roxygen.org}
#' @references \url{data_blah.com}
""data-name""
</code></pre>
"
22773050,3465204,2014-03-31T21:59:46Z,2614767,0,FALSE,"<p>I actually do this in Google Sheets. I thought it to be the easiest way to do it as well and because it can pull real live data was another bonus point. Lastly it doesn't consume any of my space to save these statements.</p>

<p>=importhtml(""<a href=""http://investing.money.msn.com/investments/stock-income-statement/?symbol=US%3A"" rel=""nofollow"">http://investing.money.msn.com/investments/stock-income-statement/?symbol=US%3A</a>""&amp;B1&amp;""&amp;stmtView=Ann"", ""table"",0)</p>

<p>where B1 cell contains the ticker.</p>

<p>You can do the same thing for balance sheet, and cash flow as well.</p>
"
22877996,3500497,2014-04-05T07:36:49Z,2547402,0,FALSE,"<p>Could try the following function:</p>

<ol>
<li>transform numeric values into factor
<li>use summary() to gain the frequency table
<li>return mode the index whose frequency is the largest
<li>transform factor back to numeric even there are more than 1 mode, this function works well! 
</ol>

<pre><code>mode &lt;- function(x){
  y &lt;- as.factor(x)
  freq &lt;- summary(y)
  mode &lt;- names(freq)[freq[names(freq)] == max(freq)]
  as.numeric(mode)
}
</code></pre>
"
22989094,3365190,2014-04-10T13:11:55Z,1401904,29,FALSE,"<p>This is an old question of course but there might be a new easy answer, which I just found.</p>

<pre><code>install.packages(""installr"")
require(installr)
updateR()
</code></pre>

<p>The best way of doing this is from the RGui system. All your packages will be transfered to the new folder and the old ones will be deleted or saved (you can pick either).
Then once you open RStudio again, it immediately recognises that you are using an updated version. For me this worked like a charm,</p>

<p><a href=""http://www.r-statistics.com/2013/03/updating-r-from-r-on-windows-using-the-installr-package/"">More info on {installr} here</a>.</p>

<p>Simon</p>
"
23041682,202229,2014-04-13T10:39:03Z,1299871,23,FALSE,"<p>dplyr is very good and performant. In addition to the other answers on it, here was/is its status as of </p>

<p><strong><em>v0.1.3</em></strong> (4/2014)</p>

<ul>
<li>has <strong>inner_join, left_join, semi_join, anti_join</strong></li>
<li><strong>outer_join</strong> not implemented yet, fallback is use base::merge() (or plyr::join())</li>
<li><a href=""https://groups.google.com/forum/#!topic/manipulatr/OuAPC4VyfIc"">Hadley mentioning other advantages here</a></li>
<li>one minor feature merge currently has that dplyr doesn't is <a href=""https://github.com/hadley/dplyr/issues/177"">the ability to have separate by.x,by.y columns</a> as e.g. Python pandas does.</li>
<li><a href=""https://github.com/hadley/dplyr/issues/96"">Implement right_join and <strong>outer_join</strong></a> is tagged for v0.3 (presumably at least 2015 or beyond)</li>
</ul>

<p>Per hadley's comments in that issue:</p>

<ul>
<li><strong>right_join</strong>(x,y) is the same as left_join(y,x) in terms of the rows, just the columns will be different orders. Easily worked around with select(new_column_order)</li>
<li><strong>outer_join</strong> is basically union(left_join(x, y), right_join(x, y)) - i.e. preserve all rows in both data frames.</li>
</ul>
"
23094857,1533118,2014-04-15T21:25:53Z,2040026,3,FALSE,"<p>While the data is ill-formed it still can be parsed given the following assumptions:</p>

<ul>
<li>The header defines how many variables there are (columns in the resultant table)</li>
<li>The data itself is complete - e.g. there are no missing values</li>
<li>The data is of a uniform type (e.g. <code>numeric()</code>)</li>
</ul>

<p>The following is code that parses the provided sample data as if it were read in from a text file called <code>data.txt</code>:</p>

<pre><code># read in the header and split on "",""
header = strsplit(readLines('data.txt', n=1), ',')[[1]]

# the length of the header determines how many variables there are

# read in the data which appears to have the pattern
#   &lt;numbers&gt;&lt;whitespace&gt;&lt;numbers&gt;...
# skipping the first line since it was already parsed as the header
data = scan('data.txt', skip=1, what=numeric())

# reform the data (which is read in as a 1D numeric vector) into a 2D matrix
# with the same number of columns as there are headers (filling by rows).
# header names are assigned via the `dimnames=` argument
data = matrix(data, ncol=length(header), byrow=T, dimnames=list(NULL, header))
</code></pre>

<p>producing the following output:</p>

<pre><code>       x1  x2     x3     x4   x5   x6   x7   x8   x9  x10  x11
[1,] 1953 7.4 159565 16.668 8883 47.2 26.7 16.8 37.7 29.7 19.4
[2,] 1954 7.8 162391 17.029 8685 46.5 22.7 18.0 36.8 29.7 20.0
</code></pre>
"
23187993,1779888,2014-04-20T21:53:27Z,2192316,2,FALSE,"<p>Use capturing parentheses in the regular expression and group references in the replacement. Anything in parentheses gets remembered. Then they're accessed by \2, the first item. The first backslash escapes the backslash's interpretation in R so that it gets passed to the regular expression parser.</p>

<pre><code>gsub('([[:alpha:]]+)([0-9]+)([[:alpha:]]+)', '\\2', ""aaa12xxx"")
</code></pre>
"
23425493,1824842,2014-05-02T10:03:41Z,2547402,1,FALSE,"<p>I would use the density() function to identify a smoothed maximum of a (possibly continuous) distribution :</p>

<pre><code>function(x) density(x, 2)$x[density(x, 2)$y == max(density(x, 2)$y)]
</code></pre>

<p>where x is the data collection. Pay attention to the <em>adjust</em> paremeter of the density function which regulate the smoothing.</p>
"
23568720,2253047,2014-05-09T15:40:41Z,2190756,0,FALSE,"<p>I've just had a particular problem where I had to count the number of true statements from a logical vector and this worked best for me...</p>

<pre><code>length(grep(TRUE, (gene.rep.matrix[i,1:6] &gt; 1))) &gt; 5
</code></pre>

<p>So This takes a subset of the gene.rep.matrix object, and applies a logical test, returning a logical vector. This vector is put as an argument to grep, which returns the locations of any TRUE entries. Length then calculates how many entries grep finds, thus giving the number of TRUE entries. </p>
"
23638765,1748028,2014-05-13T18:20:29Z,77434,17,FALSE,"<p>I just benchmarked these two approaches on data frame with 663,552 rows using the following code:</p>

<pre><code>system.time(
  resultsByLevel$subject &lt;- sapply(resultsByLevel$variable, function(x) {
    s &lt;- strsplit(x, ""."", fixed=TRUE)[[1]]
    s[length(s)]
  })
  )

 user  system elapsed 
  3.722   0.000   3.594 
</code></pre>

<p>and</p>

<pre><code>system.time(
  resultsByLevel$subject &lt;- sapply(resultsByLevel$variable, function(x) {
    s &lt;- strsplit(x, ""."", fixed=TRUE)[[1]]
    tail(s, n=1)
  })
  )

   user  system elapsed 
 28.174   0.000  27.662 
</code></pre>

<p>So, assuming you're working with vectors, accessing the length position is significantly faster.</p>
"
23638852,1700987,2014-05-13T18:25:01Z,2613420,7,FALSE,"<p>The function <code>na.exclude()</code> sounds like what you want, although it's only an option for some (important) functions.</p>

<p>In the context of fitting and working with models, R has a family of generic functions for dealing with NAs: <code>na.fail()</code>, <code>na.pass()</code>, <code>na.omit()</code>, and <code>na.exclude()</code>.  These are, in turn, arguments for some of R's key modeling functions, such as <code>lm()</code>, <code>glm()</code>, and <code>nls()</code> as well as functions in MASS, rpart, and survival packages. </p>

<p>All four generic functions basically act as filters.  <code>na.fail()</code> will only pass the data through if there are no NAs, otherwise it fails. <code>na.pass()</code> passes all cases through.  <code>na.omit()</code> and <code>na.exclude()</code> will both leave out cases with NAs and pass the other cases through. But <code>na.exclude()</code> has a different attribute that tells functions processing the resulting object to take into account the NAs. You could see this attribute if you did <code>attributes(na.exclude(some_data_frame))</code>.  Here's a demonstration of how <code>na.exclude()</code> alters the behavior of <code>predict()</code> in the context of a linear model. </p>

<pre><code>fakedata &lt;- data.frame(x = c(1, 2, 3, 4), y = c(0, 10, NA, 40))

## We can tell the modeling function how to handle the NAs
r_omitted &lt;- lm(x~y, na.action=""na.omit"", data=fakedata) 
r_excluded &lt;- lm(x~y, na.action=""na.exclude"", data=fakedata)

predict(r_omitted)
#        1        2        4 
# 1.115385 1.846154 4.038462 
predict(r_excluded)
#        1        2        3        4 
# 1.115385 1.846154       NA 4.038462 
</code></pre>

<p>Your default na.action, by the way, is determined by <code>options(""na.action"")</code> and begins as <code>na.omit()</code> but you can set it.</p>
"
23664370,641955,2014-05-14T20:20:36Z,2018480,1,FALSE,"<p>Try <code>deparse</code>, for example:</p>

<pre><code>&gt; deparse(1:3)
[1] ""1:3""
&gt; deparse(c(5,6))
[1] ""c(5, 6)""
&gt; deparse(data.frame(name=c('jack', 'mike')))
[1] ""structure(list(name = structure(1:2, .Label = c(\""jack\"", \""mike\""""
[2] ""), class = \""factor\"")), .Names = \""name\"", row.names = c(NA, -2L"" 
[3] ""), class = \""data.frame\"")"" 
</code></pre>

<p>It's better than <code>dump</code>, because <code>dump</code> requires a variable <em>name</em>, and it creates a dump file.</p>

<p>If you don't want to print it directly, but for example put it inside a string with <code>sprintf(fmt, ...)</code> or a variable to use later, then it's better than <code>dput</code>, because <code>dput</code> prints directly.</p>
"
23701383,3645665,2014-05-16T17:36:09Z,2502485,4,FALSE,"<p>You want to use <a href=""http://docs.ggplot2.org/0.9.3.1/geom_dotplot.html"" rel=""nofollow"">geom_dotplot</a> from ggplot2</p>

<p>you will probably want to use:</p>

<pre><code>ggplot(insert your arguments here) + geom_dotplot(binaxis = ""y"", stackdir = ""center"")
</code></pre>

<p>Hope this helps. The results will look really clean which is what I think you want.</p>
"
23724328,1165814,2014-05-18T16:36:51Z,1753299,1,FALSE,"<p>Steve Lianoglou is right. </p>

<p>In kernlab it is a bit wired, and when predicting it requires the input kernel matrix between each test example and the support vectors. You need to find this matrix yourself.</p>

<p>For example, a test matrix [n x m], where n is the number of test samples and m is the number of support vectors in the learned model (ordered in the sequence of SVindex(model)).</p>

<p>Example code</p>

<pre><code>trmat &lt;- as.kernelMatrix(kernels[trainidx,trainidx])
tsmat &lt;- as.kernelMatrix(kernels[testidx,trainidx])

#training
model = ksvm(x=trmat, y=trlabels, type = ""C-svc"", C = 1)

#testing
thistsmat = as.kernelMatrix(tsmat[,SVindex(model)])
tsprediction = predict(model, thistsmat, type = ""decision"")
</code></pre>

<p>kernels is the input kernel matrix. trainidx and testidx are ids for training and test.</p>
"
23724571,1072091,2014-05-18T17:02:56Z,1815606,1,FALSE,"<p>See <code>findSourceTraceback()</code> of the <a href=""http://cran.r-project.org/web/packages/R.utils"" rel=""nofollow"">R.utils</a> package, which</p>

<blockquote>
  <p>Finds all 'srcfile' objects generated by source() in all call frames. 
  This makes it possible to find out which files are currently scripted by source().</p>
</blockquote>
"
23746540,316644,2014-05-19T20:21:48Z,1782704,2,FALSE,"<p>Just for fun (since it's slower than <code>fillInTheBlanks</code>), here's a version of <code>na.locf</code> relying on <code>rle</code> function:</p>

<pre><code>my.na.locf &lt;- function(v,fromLast=F){
  if(fromLast){
    return(rev(my.na.locf(rev(v))))
  }
  nas &lt;- is.na(v)
  e &lt;- rle(nas)
  v[nas] &lt;- rep.int(c(NA,v[head(cumsum(e$lengths),-1)]),e$lengths)[nas]
  return(v)
}
</code></pre>

<p>e.g. </p>

<pre><code>v1 &lt;- c(3,NA,NA,NA,1,2,NA,NA,5)
v2 &lt;- c(NA,NA,NA,1,7,NA,NA,5,NA)

my.na.locf(v1)
#[1] 3 3 3 3 1 2 2 2 5

my.na.locf(v2)
#[1] NA NA NA  1  7  7  7  5  5

my.na.locf(v1,fromLast=T)
#[1] 3 1 1 1 1 2 5 5 5

my.na.locf(v2,fromLast=T)
#[1]  1  1  1  1  7  5  5  5 NA
</code></pre>
"
23764220,3657334,2014-05-20T15:33:46Z,2151212,0,FALSE,"<p>If you need to specify options with flags, (like -h, --help, --number=42, etc) you can use the R package optparse (inspired from Python):
<a href=""http://cran.r-project.org/web/packages/optparse/vignettes/optparse.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/optparse/vignettes/optparse.pdf</a>.</p>

<p>At least this how I understand your question, because I found this post when looking for an equivalent of the bash getopt, or perl Getopt, or python argparse and optparse.</p>
"
23901600,496803,2014-05-28T01:44:04Z,2192316,42,FALSE,"<p>It is probably a bit hasty to say '<em>ignore the standard functions</em>' - the help file for <code>?gsub</code> even specifically references in 'See also':</p>

<blockquote>
  <p>‘regmatches’ for extracting matched substrings based on the results of
  ‘regexpr’, ‘gregexpr’ and ‘regexec’.</p>
</blockquote>

<p>So this will work, and is fairly simple:</p>

<pre><code>txt &lt;- ""aaa12xxx""
regmatches(txt,regexpr(""[0-9]+"",txt))
#[1] ""12""
</code></pre>
"
24070958,189175,2014-06-05T22:04:59Z,2394902,8,FALSE,"<p>Here's how I get rid of the labels altogether. Similar to Jyotirmoy's solution but works for a vector as well as a data.frame. (Partial credits to Frank Harrell)</p>

<pre><code>clear.labels &lt;- function(x) {
  if(is.list(x)) {
    for(i in 1 : length(x)) class(x[[i]]) &lt;- setdiff(class(x[[i]]), 'labelled') 
    for(i in 1 : length(x)) attr(x[[i]],""label"") &lt;- NULL
  }
  else {
    class(x) &lt;- setdiff(class(x), ""labelled"")
    attr(x, ""label"") &lt;- NULL
  }
  return(x)
}
</code></pre>

<p>Use as follows:</p>

<pre><code>my.unlabelled.df &lt;- clear.labels(my.labelled.df)
</code></pre>
"
24206655,3309529,2014-06-13T13:35:38Z,1400937,1,FALSE,"<p>The <code>stri_replace_*_regex</code> functions from the <a href=""http://stringi.rexamine.com"" rel=""nofollow"">stringi</a> package do not have such limitations:</p>

<pre><code>library(""stringi"")
stri_replace_all_regex(""abcdefghijkl"", ""(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)"", ""$10$1$11$12"")
## [1] ""jakl""
</code></pre>

<p>If you want to follow the 1st capture group with 1, use e.g.</p>

<pre><code>stri_replace_all_regex(""abcdefghijkl"", ""(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)(.)"", ""$10$1$1\\1$12"")
## [1] ""jaa1l""
</code></pre>
"
24270979,3749764,2014-06-17T18:34:45Z,2564258,22,FALSE,"<p>I think that the answer you are looking for is:</p>

<pre><code>plot(first thing to plot)
plot(second thing to plot,add=TRUE)
</code></pre>
"
24395200,1160516,2014-06-24T19:56:47Z,2436688,2,FALSE,"<p>in fact there is a subtelty with the <code>c()</code> function. If you do:</p>

<pre><code>x &lt;- list()
x &lt;- c(x,2)
x = c(x,""foo"")
</code></pre>

<p>you will obtain as expected:</p>

<pre><code>[[1]]
[1]

[[2]]
[1] ""foo""
</code></pre>

<p>but if you add a matrix with <code>x &lt;- c(x, matrix(5,2,2)</code>, your list will have another 4 elements of value <code>5</code> !
You would better do:</p>

<pre><code>x &lt;- c(x, list(matrix(5,2,2))
</code></pre>

<p>It works for any other object and you will obtain as expected:</p>

<pre><code>[[1]]
[1]

[[2]]
[1] ""foo""

[[3]]
     [,1] [,2]
[1,]    5    5
[2,]    5    5
</code></pre>

<p>Finally, your function becomes:</p>

<pre><code>push &lt;- function(l, ...) c(l, list(...))
</code></pre>

<p>and it works for any type of object. You can be smarter and do:</p>

<pre><code>push_back &lt;- function(l, ...) c(l, list(...))
push_front &lt;- function(l, ...) c(list(...), l)
</code></pre>
"
24579230,7255,2014-07-04T18:17:19Z,1826519,1,FALSE,"<p>How about using assign?</p>

<pre><code>functionReturningTwoValues &lt;- function(a, b) {
  assign(a, 1, pos=1)
  assign(b, 2, pos=1)
}
</code></pre>

<p>You can pass the names of the variable you want to be passed by reference.</p>

<pre><code>&gt; functionReturningTwoValues('a', 'b')
&gt; a
[1] 1
&gt; b
[1] 2
</code></pre>

<p>If you need to access the existing values, the converse of <code>assign</code> is <code>get</code>.</p>
"
24593678,3386991,2014-07-06T07:32:59Z,2180235,3,FALSE,"<p>There is something new called <a href=""http://www.renjin.org/"" rel=""nofollow"">http://www.renjin.org/</a></p>

<p>One thing i like it over <code>JRI</code> is deployment, While <code>jri</code> required that your application users will download R, <code>renjin</code> does not, and it uses only the <code>JVM</code> to run.</p>
"
24755562,3840401,2014-07-15T10:14:36Z,1685181,1,FALSE,"<p>If the package is rather simple, the following function works for me on Windows for a package ""MY_PACKAGE_1.0.tar.gz"" generated with R (OS: Ubuntu with the command <code>R CMD build MY_PACKAGE</code>)</p>

<pre><code>install.packages(""MY_PACKAGE_1.0.tar.gz"", repos=NULL, type=""source"")
</code></pre>

<p>The option <code>type=""source""</code> is necessary, otherwise it does not work.</p>
"
24795140,210945,2014-07-17T04:54:40Z,1478532,14,FALSE,"<p>To complement Harlan's answer, here is a references for the available shapes - start from 0 at bottom left and read right then up (10y + x):</p>

<pre><code>df &lt;- data.frame(x=c(0:129))
ggplot(df, aes(x=x%%10, y=floor(x/10), shape=factor(x), colour=x, size=10)) +
  geom_point() +
  scale_shape_manual(values=df$x) + theme(legend.position='none') +
  scale_x_continuous(breaks=0:10) + scale_y_continuous(breaks=0:12) +
  scale_colour_hue() + scale_colour_gradientn(colours=rainbow(3))
</code></pre>

<p><img src=""https://i.stack.imgur.com/wUiNK.png"" alt=""Shapes available in ggplot2""></p>
"
24815218,718903,2014-07-17T23:34:36Z,1899243,2,FALSE,"<p>There are several ways to do this in R. Here are two examples using the <a href=""http://stat.ethz.ch/R-manual/R-patched/library/datasets/html/UKDriverDeaths.html"" rel=""nofollow"">""Seatbelts"" time series dataset in the datasets package</a> that comes with R.</p>

<p>The <code>arima()</code> function comes in package:stats that is included with R. The function takes an argument of the form <code>order=c(p, d, q)</code> where you you can specify the order of the auto-regressive, integrated, and the moving average component. In your question, you suggest that you want to create a AR(1) model to correct for first-order autocorrelation in the errors and that's it. We can do that with the following command:</p>

<pre class=""lang-r prettyprint-override""><code>arima(Seatbelts[,""drivers""], order=c(1,0,0),
      xreg=Seatbelts[,c(""kms"", ""PetrolPrice"", ""law"")])
</code></pre>

<p>The value for  order specifies that we want an AR(1) model. The xreg compontent should be a series of other Xs we want to add as part of a regression. The output looks a little bit like the output of <code>summary.lm()</code> turned on its side.</p>

<p>Another alternative process might be more familiar to the way you've fit regression models is to use <code>gls()</code> in the <a href=""http://cran.r-project.org/web/packages/nlme/index.html"" rel=""nofollow"">nlme package</a>. The following code turns the Seatbelt time series object into a dataframe and then extracts and adds a new column (<em>t</em>) that is just a counter in the sorted time series object:</p>

<pre><code>Seatbelts.df &lt;- data.frame(Seatbelts)
Seatbelts.df$t &lt;- 1:(dim(Seatbelts.df)[1])
</code></pre>

<p>The two lines above are only getting the data in shape. Since the <code>arima()</code> function is designed for time series, it can read time series objects more easily. To fit the model with nlme you would then run:</p>

<pre><code>library(nlme)
m &lt;- gls(drivers ~ kms + PetrolPrice + law,
         data=Seatbelts.df,
         correlation=corARMA(p=1, q=0, form=~t))
summary(m)
</code></pre>

<p>The line that begins with ""correlation"" is the way you pass in the ARMA correlation structure to GLS. The results won't be exactly the same because <code>arima()</code> uses maximum likelihood to estimate models and <code>gls()</code> uses restricted maximum likelihood by default. If you add <code>method=""ML""</code> to the call to <code>gls()</code> you will get identical estimates you got with the ARIMA function above.</p>
"
24847754,1905431,2014-07-20T05:43:52Z,1358003,3,FALSE,"<p>If you really want to avoid the leaks, you should avoid creating any big objects in the global environment.</p>

<p>What I usually do is to have a function that does the job and returns <code>NULL</code> — all data is read and manipulated in this function or others that it calls.</p>
"
24884348,125663,2014-07-22T10:01:33Z,1975110,7,TRUE,"<p>I ended up writing a general-purpose logger that produces Java-like logging messages when the standard R ""message"", ""warning"" and ""stop"" methods are called. It includes timestamps, and stack traces for warnings and above.</p>

<p>Many thanks to <a href=""http://www.man.com/"">Man Group</a> for permission to distribute this! Thanks also to Bob Albright, whose answer gave me a leg-up to what I was looking for.</p>

<pre><code>withJavaLogging = function(expr, silentSuccess=FALSE, stopIsFatal=TRUE) {
    hasFailed = FALSE
    messages = list()
    warnings = list()
    logger = function(obj) {
        # Change behaviour based on type of message
        level = sapply(class(obj), switch, debug=""DEBUG"", message=""INFO"", warning=""WARN"", caughtError = ""ERROR"",
                error=if (stopIsFatal) ""FATAL"" else ""ERROR"", """")
        level = c(level[level != """"], ""ERROR"")[1]
        simpleMessage = switch(level, DEBUG=,INFO=TRUE, FALSE)
        quashable = switch(level, DEBUG=,INFO=,WARN=TRUE, FALSE)

        # Format message
        time  = format(Sys.time(), ""%Y-%m-%d %H:%M:%OS3"")
        txt   = conditionMessage(obj)
        if (!simpleMessage) txt = paste(txt, ""\n"", sep="""")
        msg = paste(time, level, txt, sep="" "")
        calls = sys.calls()
        calls = calls[1:length(calls)-1]
        trace = limitedLabels(c(calls, attr(obj, ""calls"")))
        if (!simpleMessage &amp;&amp; length(trace) &gt; 0) {
            trace = trace[length(trace):1]
            msg = paste(msg, ""  "", paste(""at"", trace, collapse=""\n  ""), ""\n"", sep="""")
        }

        # Output message
        if (silentSuccess &amp;&amp; !hasFailed &amp;&amp; quashable) {
            messages &lt;&lt;- append(messages, msg)
            if (level == ""WARN"") warnings &lt;&lt;- append(warnings, msg)
        } else {
            if (silentSuccess &amp;&amp; !hasFailed) {
                cat(paste(messages, collapse=""""))
                hasFailed &lt;&lt;- TRUE
            }
            cat(msg)
        }

        # Muffle any redundant output of the same message
        optionalRestart = function(r) { res = findRestart(r); if (!is.null(res)) invokeRestart(res) }
        optionalRestart(""muffleMessage"")
        optionalRestart(""muffleWarning"")
    }
    vexpr = withCallingHandlers(withVisible(expr),
            debug=logger, message=logger, warning=logger, caughtError=logger, error=logger)
    if (silentSuccess &amp;&amp; !hasFailed) {
        cat(paste(warnings, collapse=""""))
    }
    if (vexpr$visible) vexpr$value else invisible(vexpr$value)
}
</code></pre>

<p>To use it, just wrap it around your code:</p>

<pre><code>withJavaLogging({
  // Your code here...
})
</code></pre>

<p>For a quieter output in the absence of errors (useful for tests!), set the silentSuccess flag. Messages will only be output if an error occurs, to give context to the failure.</p>

<p>To achieve the original goal (dump stack trace + carry on), just use try:</p>

<pre><code>try(withJavaLogging({
  // Your code here...
}, stopIsFatal=FALSE))
</code></pre>
"
24900744,967840,2014-07-23T02:09:24Z,2247045,16,FALSE,"<p>Here is a fast solution that splits the string into characters, then pastes together the even elements and the odd elements.</p>

<pre><code>x &lt;- ""xxyyxyxy""
sst &lt;- strsplit(x, """")[[1]]
paste0(sst[c(TRUE, FALSE)], sst[c(FALSE, TRUE)])
</code></pre>

<p><strong>Benchmark Setup:</strong></p>

<pre><code>library(microbenchmark)

GSee &lt;- function(x) {
  sst &lt;- strsplit(x, """")[[1]]
  paste0(sst[c(TRUE, FALSE)], sst[c(FALSE, TRUE)])
}

Shane1 &lt;- function(x) {
  substring(x, seq(1,nchar(x),2), seq(2,nchar(x),2))
}

library(""plyr"")
Shane2 &lt;- function(x) {
  laply(seq(1,nchar(x),2), function(i) substr(x, i, i+1))
}

seth &lt;- function(x) {
  strsplit(gsub(""([[:alnum:]]{2})"", ""\\1 "", x), "" "")[[1]]
}

geoffjentry &lt;- function(x) {
  idx &lt;- 1:nchar(x)  
  odds &lt;- idx[(idx %% 2) == 1]  
  evens &lt;- idx[(idx %% 2) == 0]  
  substring(x, odds, evens)  
}

drewconway &lt;- function(x) {
  c&lt;-strsplit(x,"""")[[1]]
  sapply(seq(2,nchar(x),by=2),function(y) paste(c[y-1],c[y],sep=""""))
}

KenWilliams &lt;- function(x) {
  n &lt;- 2
  sapply(seq(1,nchar(x),by=n), function(xx) substr(x, xx, xx+n-1))
}

RichardScriven &lt;- function(x) {
  regmatches(x, gregexpr(""(.{2})"", x))[[1]]
}
</code></pre>

<p><strong>Benchmark 1:</strong></p>

<pre><code>x &lt;- ""xxyyxyxy""

microbenchmark(
  GSee(x),
  Shane1(x),
  Shane2(x),
  seth(x),
  geoffjentry(x),
  drewconway(x),
  KenWilliams(x),
  RichardScriven(x)
)

# Unit: microseconds
#               expr      min        lq    median        uq      max neval
#            GSee(x)    8.032   12.7460   13.4800   14.1430   17.600   100
#          Shane1(x)   74.520   80.0025   84.8210   88.1385  102.246   100
#          Shane2(x) 1271.156 1288.7185 1316.6205 1358.5220 3839.300   100
#            seth(x)   36.318   43.3710   45.3270   47.5960   67.536   100
#     geoffjentry(x)    9.150   13.5500   15.3655   16.3080   41.066   100
#      drewconway(x)   92.329   98.1255  102.2115  105.6335  115.027   100
#     KenWilliams(x)   77.802   83.0395   87.4400   92.1540  163.705   100
#  RichardScriven(x)   55.034   63.1360   65.7545   68.4785  108.043   100
</code></pre>

<p><strong>Benchmark 2:</strong></p>

<p>Now, with bigger data.  </p>

<pre><code>x &lt;- paste(sample(c(""xx"", ""yy"", ""xy""), 1e5, replace=TRUE), collapse="""")

microbenchmark(
  GSee(x),
  Shane1(x),
  Shane2(x),
  seth(x),
  geoffjentry(x),
  drewconway(x),
  KenWilliams(x),
  RichardScriven(x),
  times=3
)

# Unit: milliseconds
#               expr          min            lq       median            uq          max neval
#            GSee(x)    29.029226    31.3162690    33.603312    35.7046155    37.805919     3
#          Shane1(x) 11754.522290 11866.0042600 11977.486230 12065.3277955 12153.169361     3
#          Shane2(x) 13246.723591 13279.2927180 13311.861845 13371.2202695 13430.578694     3
#            seth(x)    86.668439    89.6322615    92.596084    92.8162885    93.036493     3
#     geoffjentry(x) 11670.845728 11681.3830375 11691.920347 11965.3890110 12238.857675     3
#      drewconway(x)   384.863713   438.7293075   492.594902   515.5538020   538.512702     3
#     KenWilliams(x) 12213.514508 12277.5285215 12341.542535 12403.2315015 12464.920468     3
#  RichardScriven(x) 11549.934241 11730.5723030 11911.210365 11989.4930080 12067.775651     3
</code></pre>
"
24910354,1265067,2014-07-23T12:20:39Z,1358003,2,FALSE,"<p>I really appreciate some of the answers above, following @hadley and @Dirk that suggest closing R and issuing <code>source</code> and using command line I come up with a solution that worked very well for me. I had to deal with hundreds of mass spectras, each occupies around 20 Mb of memory so I used two R scripts, as follows:</p>

<p>First a wrapper:</p>

<pre><code>#!/usr/bin/Rscript --vanilla --default-packages=utils

for(l in 1:length(fdir)) {

   for(k in 1:length(fds)) {
     system(paste(""Rscript runConsensus.r"", l, k))
   }
}
</code></pre>

<p>with this script I basically control what my main script do <code>runConsensus.r</code>, and I write the data answer for the output. With this, each time the wrapper calls the script it seems the R is reopened and the memory is freed.</p>

<p>Hope it helps.   </p>
"
24911585,1270695,2014-07-23T13:18:37Z,1842736,2,FALSE,"<p>It turns out that @Shane had originally posted (but quickly deleted) what <em>is</em> a correct answer with more recent versions of R.</p>

<p>Somewhere along the way, an <code>as.matrix</code> method was added for <code>ftable</code> (I haven't found it in the NEWS files I read through though. </p>

<p>The <code>as.matrix</code> method for <code>ftable</code> lets you deal fairly nicely with ""nested"" frequency tables (which is what <code>ftable</code> creates quite nicely). Consider the following:</p>

<pre><code>temp &lt;- read.ftable(textConnection(""breathless yes no
coughed yes no
age
20-24  9  7  95 1841
25-29 23  9 108 1654
30-34 54 19 177 1863""))

class(temp)
# [1] ""ftable""
</code></pre>

<p>The <code>head(as.table(...), Inf)</code> trick doesn't work with such <code>ftables</code> because <code>as.table</code> would convert the result into a multi-dimensional array.</p>

<pre><code>head(as.table(temp), Inf)
#  [1]    9   23   54   95  108  177    7    9   19 1841 1654 1863
</code></pre>

<p>For the same reason, the second suggestion also doesn't work:</p>

<pre><code>t &lt;- as.table(temp)
class(t) &lt;- ""matrix""
# Error in class(t) &lt;- ""matrix"" : 
#   invalid to set the class to matrix unless the dimension attribute is of length 2 (was 3)
</code></pre>

<hr>

<p>However, with more recent versions of R, simply using <code>as.matrix</code> would be fine:</p>

<pre><code>as.matrix(temp)
#        breathless_coughed
# age     yes_yes yes_no no_yes no_no
#   20-24       9      7     95  1841
#   25-29      23      9    108  1654
#   30-34      54     19    177  1863

class(.Last.value)
# [1] ""matrix""
</code></pre>

<hr>

<p>If you prefer a <code>data.frame</code> to a <code>matrix</code>, check out <code>table2df</code> from my <a href=""https://github.com/mrdwab/mrdwabmisc"" rel=""nofollow"">""mrdwabmisc"" package on GitHub</a>.</p>
"
24999820,2024186,2014-07-28T16:29:41Z,1897704,13,FALSE,"<p>My answer consists of two parts. Part 1 is the math - to give clarity to all readers of the thread and to make the R code that follows understandable. Part 2 is the R programming. </p>

<h2>Part 1 - Math</h2>

<p>The dot product of two vectors <em>x</em> and <em>y</em> can be defined as:</p>

<p><img src=""https://i.stack.imgur.com/f3o3y.gif"" alt=""enter image description here""></p>

<p>where ||<em>x</em>|| is the Euclidean norm (also known as the L<sub>2</sub> norm) of the vector <em>x</em>.</p>

<p>Manipulating the definition of the dot product, we can obtain:</p>

<p><img src=""https://i.stack.imgur.com/asQEm.gif"" alt=""enter image description here""></p>

<p>where theta is the angle between the vectors <em>x</em> and <em>y</em> expressed in radians. Note that theta can take on a value that lies on the closed interval from 0 to pi.</p>

<p>Solving for theta itself, we get:</p>

<p><img src=""https://i.stack.imgur.com/UYlVy.gif"" alt=""enter image description here""></p>

<h2>Part 2 - R Code</h2>

<p>To translate the mathematics into R code, we need to know how to perform two matrix (vector) calculations; dot product and Euclidean norm (which is a specific type of norm, known as the L<sub>2</sub> norm). We also need to know the R equivalent of the inverse cosine function, cos<sup>-1</sup>.</p>

<p>Starting from the top. By reference to <code>?""%*%""</code>, the dot product (also referred to as the inner product) can be calculated using the <code>%*%</code> operator. With reference to <code>?norm</code>, the <code>norm()</code> function (base package) returns <em>a</em> norm of a vector. The norm of interest here is the L<sub>2</sub> norm or, in the parlance of the R help documentation, the ""spectral"" or ""2""-norm. This means that the <code>type</code> argument of the <code>norm()</code> function ought to be set equal to <code>""2""</code>. Lastly, the inverse cosine function in R is represented by the <code>acos()</code> function.</p>

<p><strong>Solution</strong></p>

<p>Equipped with both the mathematics and the relevant R functions, a prototype function (that is, not production standard) can be put together - using Base package functions - as shown below. If the above information makes sense then the <code>angle()</code> function that follows should be clear without further comment.</p>

<pre><code>angle &lt;- function(x,y){
  dot.prod &lt;- x%*%y 
  norm.x &lt;- norm(x,type=""2"")
  norm.y &lt;- norm(y,type=""2"")
  theta &lt;- acos(dot.prod / (norm.x * norm.y))
  as.numeric(theta)
}
</code></pre>

<p><strong>Test the function</strong></p>

<p>A test to verify that the function works. Let <em>x</em> = (2,1) and <em>y</em> = (1,2). Dot product between <em>x</em> and <em>y</em> is 4. Euclidean norm of <em>x</em> is sqrt(5). Euclidean norm of <em>y</em> is also sqrt(5). cos theta = 4/5. Theta is approximately 0.643 radians.</p>

<pre><code>x &lt;- as.matrix(c(2,1))
y &lt;- as.matrix(c(1,2))
angle(t(x),y)          # Use of transpose to make vectors (matrices) conformable.
[1] 0.6435011
</code></pre>

<p>I hope this helps!</p>
"
25108190,1855677,2014-08-03T19:17:00Z,1751540,1,FALSE,"<p>The setNames function will suffice:</p>

<pre><code>&gt; foo &lt;- function(a,nm) {
+     answer &lt;- data.frame(a=1:5)
+     setNames(answer, nm)
+     }
&gt; foo(1:10, 'bar')
  bar
1   1
2   2
3   3
4   4
5   5
</code></pre>
"
25269100,1133275,2014-08-12T16:07:50Z,1975110,1,FALSE,"<p>no line numbers but this is the closest I found so far:</p>

<pre><code>run = function() {
    // Your code here...
}
withCallingHandlers(run(), error=function(e)cat(conditionMessage(e), sapply(sys.calls(),function(sc)deparse(sc)[1]), sep=""\n   "")) 
</code></pre>
"
25297195,1000343,2014-08-13T22:22:36Z,1454211,9,FALSE,"<p>This adds <code>\donttest{}</code> and is taken (verbatim) from @hadley's <a href=""http://r-pkgs.had.co.nz/"">R Packages</a>.</p>

<blockquote>
  <p>However for the purpose of illustration, it’s often useful to include code that causes an error. <strong><code>\dontrun{}</code></strong> allows you to include code in the example that is never used. There are two other special commands. <strong><code>\dontshow{}</code></strong> is run, but not shown in the help page: this can be useful for informal tests. <strong><code>\donttest{}</code></strong> is run in examples, but not run automatically in R CMD check. This is useful if you have examples that take a long time to run. The options are summarised below.</p>
</blockquote>

<pre><code>Command      example    help       R CMD check
\dontrun{}                 x
\dontshow{}       x                          x
\donttest{}       x        x
</code></pre>
"
25353201,707145,2014-08-17T20:11:06Z,2219626,0,FALSE,"<p>How about this one?</p>

<hr>

<h2>Code</h2>

<hr>

<pre><code>set.seed(12345)
dat &lt;- ts(data=runif(n=10, min=50, max=100), frequency = 4, start = c(1959, 2))
df &lt;- data.frame(date=as.Date(time(dat)), Y=as.matrix(dat))
library(ggplot2)
ggplot(data=df, mapping=aes(x=date, y=Y))+geom_point()
</code></pre>

<hr>

<h2>Output</h2>

<hr>

<p><img src=""https://i.stack.imgur.com/UcN8n.png"" alt=""enter image description here""></p>
"
25361607,211116,2014-08-18T10:53:24Z,2564258,15,FALSE,"<p>Use the <code>matplot</code> function:</p>

<pre><code>matplot(x, cbind(y1,y2),type=""l"",col=c(""red"",""green""),lty=c(1,1))
</code></pre>

<p>use this if <code>y1</code> and <code>y2</code> are evaluated at the same <code>x</code> points. It scales the Y-axis to fit whichever is bigger (<code>y1</code> or <code>y2</code>), unlike some of the other answers here that will clip <code>y2</code> if it gets bigger than <code>y1</code> (ggplot solutions mostly are okay with this).</p>

<p>Alternatively, and if the two lines don't have the same x-coordinates, set the axis limits on the first plot and add:</p>

<pre><code>x1  &lt;- seq(-2, 2, 0.05)
x2  &lt;- seq(-3, 3, 0.05)
y1 &lt;- pnorm(x1)
y2 &lt;- pnorm(x2,1,1)

plot(x1,y1,ylim=range(c(y1,y2)),xlim=range(c(x1,x2)), type=""l"",col=""red"")
lines(x2,y2,col=""green"")
</code></pre>

<p>Am astonished this Q is 4 years old and nobody has mentioned <code>matplot</code> or <code>x/ylim</code>...</p>
"
25399055,180892,2014-08-20T07:28:07Z,2310409,12,FALSE,"<p>I found it useful to study the examples in the ggplot2 package.</p>

<p>See <a href=""https://github.com/hadley/ggplot2/blob/4bb9270ef4d5d5062353438fd99d17b6f6de98a2/R/ggplot2.r"" rel=""noreferrer"">ggplot2.r on github</a></p>

<p>A few things of note:</p>

<ul>
<li>All the Roxygen code for datasets can be included in a single <code>.r</code> file in the <code>R</code> directory of the package.</li>
</ul>

<p>See for examples, the <code>diamonds</code> dataset:</p>

<pre><code>#' Prices of 50,000 round cut diamonds
#'
#' A dataset containing the prices and other attributes of almost 54,000
#'  diamonds. The variables are as follows:
#'
#' \itemize{
#'   \item price. price in US dollars (\$326--\$18,823)
#'   \item carat. weight of the diamond (0.2--5.01)
#'   \item cut. quality of the cut (Fair, Good, Very Good, Premium, Ideal)
#'   \item colour. diamond colour, from J (worst) to D (best)
#'   \item clarity. a measurement of how clear the diamond is (I1 (worst), SI1, SI2, VS1, VS2, VVS1, VVS2, IF (best))
#'   \item x. length in mm (0--10.74)
#'   \item y. width in mm (0--58.9)
#'   \item z. depth in mm (0--31.8)
#'   \item depth. total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)
#'   \item table. width of top of diamond relative to widest point (43--95)
#' }
#'
#' @docType data
#' @keywords datasets
#' @name diamonds
#' @usage data(diamonds)
#' @format A data frame with 53940 rows and 10 variables
NULL
</code></pre>

<p>This results in a help file that looks like this:</p>

<p><img src=""https://i.stack.imgur.com/uxB7e.png"" alt=""roxygen documentation example""></p>
"
25451357,305675,2014-08-22T16:13:43Z,1727772,4,FALSE,"<p>Often times I think it is just good practice to keep larger databases inside a database (e.g. Postgres). I don't use anything too much larger than (nrow * ncol) ncell = 10M, which is pretty small; but I often find I want R to create and hold memory intensive graphs only while I query from multiple databases. In the future of 32 GB laptops, some of these types of memory problems will disappear. But the allure of using a database to hold the data and then using R's memory for the resulting query results and graphs still may be useful. Some advantages are: </p>

<p>(1) The data stays loaded in your database. You simply reconnect in pgadmin to the databases you want when you turn your laptop back on.</p>

<p>(2) It is true R can do many more nifty statistical and graphing operations than SQL. But I think SQL is better designed to query large amounts of data than R.</p>

<pre><code># Looking at Voter/Registrant Age by Decade

library(RPostgreSQL);library(lattice)

con &lt;- dbConnect(PostgreSQL(), user= ""postgres"", password=""password"",
                 port=""2345"", host=""localhost"", dbname=""WC2014_08_01_2014"")

Decade_BD_1980_42 &lt;- dbGetQuery(con,""Select PrecinctID,Count(PrecinctID),extract(DECADE from Birthdate) from voterdb where extract(DECADE from Birthdate)::numeric &gt; 198 and PrecinctID in (Select * from LD42) Group By PrecinctID,date_part Order by Count DESC;"")

Decade_RD_1980_42 &lt;- dbGetQuery(con,""Select PrecinctID,Count(PrecinctID),extract(DECADE from RegistrationDate) from voterdb where extract(DECADE from RegistrationDate)::numeric &gt; 198 and PrecinctID in (Select * from LD42) Group By PrecinctID,date_part Order by Count DESC;"")

with(Decade_BD_1980_42,(barchart(~count | as.factor(precinctid))));
mtext(""42LD Birthdays later than 1980 by Precinct"",side=1,line=0)

with(Decade_RD_1980_42,(barchart(~count | as.factor(precinctid))));
mtext(""42LD Registration Dates later than 1980 by Precinct"",side=1,line=0)
</code></pre>
"
25555105,935950,2014-08-28T17:55:51Z,2602583,35,TRUE,"<p>Here is a vectorized, zero- and NA-tolerant function for calculating geometric mean in R. The verbose <code>mean</code> calculation involving <code>length(x)</code> is necessary for the cases where <code>x</code> contains non-positive values.</p>

<pre><code>gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x &gt; 0]), na.rm=na.rm) / length(x))
}
</code></pre>

<p>Thanks to @ben-bolker for noting the <code>na.rm</code> pass-through and @Gregor for making sure it works correctly.</p>

<p>I think some of the comments are related to a false-equivalency of <code>NA</code> values in the data and zeros. In the application I had in mind they are the same, but of course this is not generally true. Thus, if you want to include optional propagation of zeros, and treat the <code>length(x)</code> differently in the case of <code>NA</code> removal, the following is a slightly longer alternative to the function above.</p>

<pre><code>gm_mean = function(x, na.rm=TRUE, zero.propagate = FALSE){
  if(any(x &lt; 0, na.rm = TRUE)){
    return(NaN)
  }
  if(zero.propagate){
    if(any(x == 0, na.rm = TRUE)){
      return(0)
    }
    exp(mean(log(x), na.rm = na.rm))
  } else {
    exp(sum(log(x[x &gt; 0]), na.rm=na.rm) / length(x))
  }
}
</code></pre>

<p>Note that it also checks for any negative values, and returns a more informative and appropriate <code>NaN</code> respecting that geometric mean is not defined for negative values (but is for zeros). Thanks to commenters who stayed on my case about this.</p>
"
25583046,3667574,2014-08-30T13:48:27Z,1169456,12,FALSE,"<p>Both of them are ways of subsetting.
The single bracket will will return a subset of the list, which in itself will be a list. ie:It may or may not contain more than one elements.
On the other hand a double bracket will return just a single element from the list.</p>

<p>-Single bracket will give us a list. We can also use single bracket if we wish to return multiple elements from the list.
consider the following list:-</p>

<pre><code>&gt;r&lt;-list(c(1:10),foo=1,far=2);
</code></pre>

<p>Now please note the way the list is returned when I try to display it.
I type r and press enter</p>

<pre><code>&gt;r

#the result is:-

[[1]]

 [1]  1  2  3  4  5  6  7  8  9 10

$foo

[1] 1

$far

[1] 2
</code></pre>

<p>Now we will see the magic of single bracket:-</p>

<pre><code>&gt;r[c(1,2,3)]

#the above command will return a list with all three elements of the actual list r as below

[[1]]

 [1]  1  2  3  4  5  6  7  8  9 10

$foo

[1] 1


$far

[1] 2
</code></pre>

<p>which is exactly the same as when we tried to display value of r on screen, which means the usage of single bracket has returned a list, where at index 1 we have a vector of 10 elements, then we have two more elements with names foo and far.
We may also choose to give a single index or element name as input to the single bracket.
eg:</p>

<pre><code>&gt; r[1]

[[1]]

 [1]  1  2  3  4  5  6  7  8  9 10
</code></pre>

<p>In this example we gave one index ""1"" and in return got a list with one element(which is an array of 10 numbers)</p>

<pre><code>&gt; r[2]

$foo

[1] 1
</code></pre>

<p>In the above example we gave one index ""2"" and in return got a list with one element</p>

<pre><code>&gt; r[""foo""];

$foo

[1] 1
</code></pre>

<p>In this example we passed the name of one element and in return a list was returned with one element.</p>

<p>You may also pass a vector of element names like:-</p>

<pre><code>&gt; x&lt;-c(""foo"",""far"")

&gt; r[x];

$foo

[1] 1

$far
[1] 2
</code></pre>

<p>In this example we passed an vector with two element names ""foo"" and ""far""</p>

<p>In return we got a list with two elements.</p>

<p>In short single bracket will always return you another list with number of elements equal to the number of elements or number of indices you pass into the single bracket.</p>

<p>In contrast, a double bracket will always return only one element.
Before moving to double bracket a note to be kept in mind.
<strong><em><code>NOTE:THE MAJOR DIFFERENCE BETWEEN THE TWO IS THAT SINGLE BRACKET RETURNS YOU A LIST WITH AS MANY ELEMENTS AS YOU WISH WHILE A DOUBLE BRACKET WILL NEVER RETURN A LIST. RATHER A DOUBLE BRACKET WILL RETURN ONLY A SINGLE ELEMENT FROM THE LIST.</code></em></strong></p>

<p>I will site a few examples. Please keep a note of the words in bold and come back to it after you are done with the examples below:</p>

<p>Double bracket will return you the actual value at the index.(It will <strong>NOT</strong> return a list)</p>

<pre><code>  &gt; r[[1]]

     [1]  1  2  3  4  5  6  7  8  9 10


  &gt;r[[""foo""]]

    [1] 1
</code></pre>

<p><strong>for double brackets if we try to view more than one elements by passing a vector it will result in an error just because it was not built to cater to that need, but just to return a single element.</strong></p>

<p>Consider the following</p>

<pre><code>&gt; r[[c(1:3)]]
Error in r[[c(1:3)]] : recursive indexing failed at level 2
&gt; r[[c(1,2,3)]]
Error in r[[c(1, 2, 3)]] : recursive indexing failed at level 2
&gt; r[[c(""foo"",""far"")]]
Error in r[[c(""foo"", ""far"")]] : subscript out of bounds
</code></pre>
"
25619783,1073104,2014-09-02T09:10:34Z,1351937,1,FALSE,"<p>Concerning your package installation, I think the problem is ""/home/rama/R/i486-pc-linux-gnu-library/2.9/00LOCK"". Just rm this dir and the installation will work. </p>

<p>a ref in Chinese: <a href=""http://cos.name/cn/topic/108555#post-239310"" rel=""nofollow"">http://cos.name/cn/topic/108555#post-239310</a></p>
"
25635740,1502898,2014-09-03T03:21:55Z,2547402,25,FALSE,"<p>I found Ken Williams post above to be great, I added a few lines to account for NA values and made it a function for ease. </p>

<pre><code>Mode &lt;- function(x, na.rm = FALSE) {
  if(na.rm){
    x = x[!is.na(x)]
  }

  ux &lt;- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}
</code></pre>
"
25666466,3759418,2014-09-04T13:12:25Z,1686569,13,FALSE,"<p>You could use the <code>dplyr</code> package:</p>

<pre><code>library(dplyr)
filter(expr, cell_type == ""hesc"")
filter(expr, cell_type == ""hesc"" | cell_type == ""bj fibroblast"")
</code></pre>
"
25675537,120731,2014-09-04T21:51:32Z,2617600,42,FALSE,"<p><code>jsonlite</code> will import the JSON into a data frame.  It can optionally flatten nested objects.  Nested arrays will be data frames.</p>

<pre><code>&gt; library(jsonlite)
&gt; winners &lt;- fromJSON(""winners.json"", flatten=TRUE)
&gt; colnames(winners)
[1] ""winner"" ""votes"" ""startPrice"" ""lastVote.timestamp"" ""lastVote.user.name"" ""lastVote.user.user_id""
&gt; winners[,c(""winner"",""startPrice"",""lastVote.user.name"")]
    winner startPrice lastVote.user.name
1 68694999          0              Lamur
&gt; winners[,c(""votes"")]
[[1]]
                            ts user.name user.user_id
1 Thu Mar 25 03:13:01 UTC 2010     Lamur     68694999
2 Thu Mar 25 03:13:08 UTC 2010     Lamur     68694999
</code></pre>
"
25753739,7255,2014-09-09T21:21:01Z,1741820,21,FALSE,"<p><code>x = y = 5</code> is equivalent to <code>x = (y = 5)</code>, because the assignment operators ""group"" right to left, which works. </p>

<p>Meaning: assign 5 to y, leaving the number 5; and then assign that 5 to x.</p>

<p>This is not the same as <code>(x = y) = 5</code>, which doesn't work!</p>

<p>Meaning: assign the value of y to x, leaving the value of y; and then assign 5 to, umm, what exactly?</p>

<p>When you mix the different kinds of assignment operator, <code>&lt;-</code> binds tighter than <code>=</code></p>

<p>So <code>x = y &lt;- 5</code> is interpreted as <code>x = (y &lt;- 5)</code>, which is the case that makes sense.</p>

<p>Unfortunately, <code>x &lt;- y = 5</code> is interpreted as <code>(x &lt;- y) = 5</code>, which is the case that doesn't work!</p>

<p>See <code>?Syntax</code> and <code>?assignOps</code> for the precedence (binding) and grouping rules.</p>
"
25800165,1714987,2014-09-12T03:13:16Z,2218395,2,FALSE,"<p>have a look at <a href=""http://r.789695.n4.nabble.com/How-to-Compare-Two-dendrograms-Hierarchical-Clusterings-td1563467.html"" rel=""nofollow noreferrer"">this page</a>: </p>

<p>I also have similar question asked <a href=""https://stackoverflow.com/questions/25799921/which-r-function-to-calculate-cophenetic-correlation"">here</a></p>

<p>It seems we can use cophenetic correlation to measure the similarity between two dendrograms. But there seems no function for this purpose in R currently.</p>

<p><strong>EDIT at 2014,9,18:</strong>
 The <code>cophenetic</code> function in <code>stats</code> package is capable to calculating the cophenetic dissimilarity matrix. and the correlation can be calculated using <code>cor</code> function. as @Tal has pointed the <code>as.dendrogram</code> function returned the tree with different order, which will cause wrong results if we calculate the correlation based on the dendrogram results. As showed in the example of function <code>cor_cophenetic</code> function in <code>dendextend</code> package:</p>

<pre><code>set.seed(23235)
ss &lt;- sample(1:150, 10 )
hc1 &lt;- iris[ss,-5] %&gt;% dist %&gt;% hclust(""com"")
hc2 &lt;- iris[ss,-5] %&gt;% dist %&gt;% hclust(""single"")
dend1 &lt;- as.dendrogram(hc1)
dend2 &lt;- as.dendrogram(hc2)
# cutree(dend1)
cophenetic(hc1)
cophenetic(hc2)
# notice how the dist matrix for the dendrograms have different orders:
cophenetic(dend1)
cophenetic(dend2)
cor(cophenetic(hc1), cophenetic(hc2)) # 0.874
cor(cophenetic(dend1), cophenetic(dend2)) # 0.16
# the difference is becasue the order of the distance table in the case of
# stats:::cophenetic.dendrogram will change between dendrograms!
</code></pre>
"
25856135,2204410,2014-09-15T20:09:50Z,2185252,49,FALSE,"<p>Three alternative solutions:</p>

<p><strong>1: With <code>reshape2</code></strong></p>

<pre><code>library(reshape2)
long &lt;- melt(wide, id.vars = c(""Code"", ""Country""))
</code></pre>

<p>giving:</p>

<pre><code>   Code     Country variable  value
1   AFG Afghanistan     1950 20,249
2   ALB     Albania     1950  8,097
3   AFG Afghanistan     1951 21,352
4   ALB     Albania     1951  8,986
5   AFG Afghanistan     1952 22,532
6   ALB     Albania     1952 10,058
7   AFG Afghanistan     1953 23,557
8   ALB     Albania     1953 11,123
9   AFG Afghanistan     1954 24,555
10  ALB     Albania     1954 12,246
</code></pre>

<p>Some alternative notations that give the same result:</p>

<pre><code># you can also define the id-variables by column number
melt(wide, id.vars = 1:2)

# as an alternative you can also specify the measure-variables
# all other variables will then be used as id-variables
melt(wide, measure.vars = 3:7)
melt(wide, measure.vars = as.character(1950:1954))
</code></pre>

<p><strong>2: With <code>data.table</code></strong></p>

<p>You can use the same <code>melt</code> function as in the <code>reshape2</code> package (which is an extended &amp; improved implementation). <code>melt</code> from <code>data.table</code> has also more parameters that the <code>melt</code> from <code>reshape2</code>. You can for exaple also specify the name of the variable-column:</p>

<pre><code>library(data.table)
long &lt;- melt(setDT(wide), id.vars=c(""Code"",""Country""), variable.name=""year"")
</code></pre>

<p>Some alternative notations:</p>

<pre><code>melt(setDT(wide), id.vars = 1:2, variable.name = ""year"")
melt(setDT(wide), measure.vars = 3:7, variable.name = ""year"")
melt(setDT(wide), measure.vars = as.character(1950:1954), variable.name = ""year"")
</code></pre>

<p><strong>3: With <code>tidyr</code></strong></p>

<pre><code>library(tidyr)
long &lt;- wide %&gt;% gather(year, value, -c(Code, Country))
</code></pre>

<p>Some alternative notations:</p>

<pre><code>wide %&gt;% gather(year, value, -Code, -Country)
wide %&gt;% gather(year, value, -1:-2)
wide %&gt;% gather(year, value, -1, -2)
wide %&gt;% gather(year, value, 3:7)
wide %&gt;% gather(year, value, `1950`:`1954`)
</code></pre>

<hr>

<p>If you want to exclude <code>NA</code> values, you can add <code>na.rm = TRUE</code> to the <code>melt</code> as well as the <code>gather</code> functions.</p>

<hr>

<p>Another problem with the data is that the values will be read by R as character-values (as a result of the <code>,</code> in the numbers). You can repair that with <code>gsub</code> and <code>as.numeric</code>:</p>

<pre><code>long$value &lt;- as.numeric(gsub("","", """", long$value))
</code></pre>

<p>Or directly with <code>data.table</code> or <code>dplyr</code>:</p>

<pre><code># data.table
long &lt;- melt(setDT(wide),
             id.vars = c(""Code"",""Country""),
             variable.name = ""year"")[, value := as.numeric(gsub("","", """", value))]

# tidyr and dplyr
long &lt;- wide %&gt;% gather(year, value, -c(Code,Country)) %&gt;% 
  mutate(value = as.numeric(gsub("","", """", value)))
</code></pre>

<hr>

<p><strong>Data:</strong></p>

<pre><code>wide &lt;- read.table(text=""Code Country        1950    1951    1952    1953    1954
AFG  Afghanistan    20,249  21,352  22,532  23,557  24,555
ALB  Albania        8,097   8,986   10,058  11,123  12,246"", header=TRUE, check.names=FALSE)
</code></pre>
"
26028089,4009071,2014-09-24T23:23:19Z,1734896,1,FALSE,"<p>you do not need to export to a file, just use summary(x)[1] for min, summary(x)[2] for first quarter value.</p>
"
26089367,2592962,2014-09-28T20:33:57Z,1816200,1,FALSE,"<p>As suggested in the comments, just use <code>print</code> to ensure the results of the test are printed to the console. Besides the line with the chisquare test, the rest of your code should remain the same.</p>

<pre><code>total &lt;- dim(crs$dataset_init)[1]
expected.fr &lt;- cl.popul / total

for (i in 1:dim(cl.vs.Onerall)[1] ) {
        if (cl.vs.Onerall[i,1] &gt; 0) {
             observed &lt;- cl.vs.Onerall[i,2:(clust_no + 1)]

             print(rownames(cl.vs.Onerall)[i])
             print(chisq.test(observed, p=expected.fr)) # edited line
             print(""------------------------------"")
    }
}
</code></pre>
"
26098290,134830,2014-09-29T10:58:56Z,1395115,9,FALSE,"<p>An example R variable, that's fairly complex:</p>

<pre><code>library(nlme)
model &lt;- lme(uptake ~ conc + Treatment, CO2, random = ~ 1 | Plant / Type)
</code></pre>

<p>The best storage database method for R variables depends upon how you want to use it.</p>

<p><strong>I need to do in-database analytics on the values</strong></p>

<p>In this case, you need to break the object down into values that the database can handle natively.  This usually means converting it into one or more data frames.  The easiest way to do this is to use the <code>broom</code> package.</p>

<pre><code>library(broom)
coefficients_etc &lt;- tidy(model)
model_level_stats &lt;- glance(model)
row_level_stats &lt;- augment(model)
</code></pre>

<hr>

<p><strong>I just want storage</strong></p>

<p>In this case you want to serialize your R variables.  That is, converting them to be a string or a binary blob.  There are several methods for this.</p>

<hr>

<p><strong>My data has to be accessible by programs other than R, and needs to be human-readable</strong></p>

<p>You should store your data in a cross-platform text format; probably JSON or YAML. JSON doesn't support some important concepts like <code>Inf</code>; YAML is more general but the support in R isn't as mature.  XML is also possible, but is too verbose to be useful for storing large arrays.</p>

<pre><code>library(RJSONIO)
model_as_json &lt;- toJSON(model)
nchar(model_as_json) # 17916

library(yaml)
# yaml package doesn't yet support conversion of language objects,
# so preprocessing is needed
model2 &lt;- within(
  model,
  {
     call &lt;- as.character(call)
     terms &lt;- as.character(terms)
  }
)
model_as_yaml &lt;- as.yaml(model2) 
nchar(model_as_yaml) # 14493
</code></pre>

<hr>

<p><strong>My data has to be accessible by programs other than R, and doesn't need to be human-readable</strong></p>

<p>You could write your data to an open, cross-platform binary format like HFD5.  Currently support for HFD5 files (via <a href=""http://www.bioconductor.org/packages/release/bioc/html/rhdf5.html"" rel=""nofollow""><code>rhdf5</code></a>) is limited, so complex objects are not supported.  (You'll probably need to <a href=""http://www.inside-r.org/r-doc/base/unclass"" rel=""nofollow""><code>unclass</code></a> everything.)</p>

<pre><code>library(rhdf5)
h5save(rapply(model2, unclass, how = ""replace""), file = ""model.h5"")
bin_h5 &lt;- readBin(""model.h5"", ""raw"", 1e6)
length(bin_h5) # 88291 not very efficient in this case
</code></pre>

<p>The <code>feather</code> package let's you save data frames in a format readable by both R and Python.  To use this, you would first have to convert the model object into data frames, as described in the broom section earlier in the answer.</p>

<pre><code>library(feather)
library(broom)
write_feather(augment(model), ""co2_row.feather"")  # 5474 bytes
write_feather(tidy(model), ""co2_coeff.feather"")   # 2093 bytes
write_feather(glance(model), ""co2_model.feather"") #  562 bytes
</code></pre>

<p>Another alternative is to save a text version of the variable (see previous section) to a zipped file and store its bytes in the database.</p>

<pre><code>writeLines(model_as_json)
tar(""model.tar.bz"", ""model.txt"", compression = ""bzip2"")
bin_bzip &lt;- readBin(""model.tar.bz"", ""raw"", 1e6)
length(bin_bzip) # only 42 bytes!
</code></pre>

<hr>

<p><strong>My data only needs to be accessible by R, and needs to be human-readable</strong></p>

<p>There are two options for turning a variable into a string: <a href=""http://www.inside-r.org/r-doc/base/serialize"" rel=""nofollow""><code>serialize</code></a> and <a href=""http://www.inside-r.org/r-doc/base/deparse"" rel=""nofollow""><code>deparse</code></a>.</p>

<pre><code>p &lt;- function(x)
{
  paste0(x, collapse = ""\n"")
}
</code></pre>

<p><code>serialize</code> needs to be sent to a text connection, and rather than writing to file, you can write to the console and capture it.</p>

<pre><code> model_serialized &lt;- p(capture.output(serialize(model, stdout())))
 nchar(model_serialized) # 23830
</code></pre>

<p>Use <code>deparse</code> with <code>control = ""all""</code> to maximise the reversibility when re-parsing later.</p>

<pre><code>model_deparsed &lt;- p(deparse(model, control = ""all""))
nchar(model_deparsed) # 22036
</code></pre>

<hr>

<p><strong>My data only needs to be accessible by R, and doesn't need to be human-readable</strong></p>

<p>The same sorts of techniques shown in the previous sections can be applied here.  You can zip a serialized or deparsed variable and re-read it as a raw vector.</p>

<p><code>serialize</code> can also write variables in a binary format. In this case, it is most easily used with its wrapper <code>saveRDS</code>.</p>

<pre><code>saveRDS(model, ""model.rds"")
bin_rds &lt;- readBin(""model.rds"", ""raw"", 1e6)
length(bin_rds) # 6350
</code></pre>
"
26129498,146041,2014-09-30T20:27:54Z,2228544,1,FALSE,"<p>A more complex approach is required if you want the 'names' of the variables to pass through accurately.</p>

<p>For example, if you do <code>plot(rnorm(1000),rnorm(1000))</code> then you will get nice labels on your x- and y- axes. Another example of this is <code>data.frame</code></p>

<pre><code>&gt; data.frame( rnorm(5), rnorm(5), first=rpois(5,1), second=rbinom(5,1,0.5) )
    rnorm.5. rnorm.5..1 first second
1  0.1964190 -0.2949770     0      0
2  0.4750665  0.8849750     1      0
3 -0.7829424  0.4174636     2      0
4  1.6551403  1.3547863     0      1
5  1.4044107 -0.4216046     0      0
</code></pre>

<p>Not that the data.frame has assigned useful names to the columns.</p>

<p>Some implementations of Curry may not do this properly, leading to unreadable column names and plot labels. Instead, I now use something like this:</p>

<pre><code>Curry &lt;- function(FUN,...) {
    .orig = match.call()
    .orig[[1]] &lt;- NULL
    .orig[[1]] &lt;- NULL # Yes, a second NULL assignment to [[1]]
    function(...) {
        .inner = match.call()
        .inner[[1]]&lt;-NULL
        do.call(FUN,c(.orig,.inner),envir=parent.frame())
    }
}
</code></pre>

<p>This is quite complex, but I think it's correct. <code>match.call</code> will catch all args, fully remembering what expressions defined the args (this is necessary for nice labels). The problem is that it catches too many args -- not just the <code>...</code> but also the <code>FUN</code>. It also remembers the name of the function that's being called (<code>Curry</code>). </p>

<p>Therefore, we want to delete these first two entries in <code>.orig</code> so that <code>.orig</code> really just corresponds to the <code>...</code> arguments. That's why we do <code>.orig[[1]]&lt;-NULL</code> twice - each time deletes an entry and shifts everything else to the left.</p>

<p>This completes the definition and we can now do the following to get <em>exactly</em> the same as above</p>

<pre><code>Curry(data.frame, rnorm(5), rnorm(5) )( first=rpois(5,1) , second=rbinom(5,1,0.5) )
</code></pre>

<p>A final note on <code>envir=parent.frame()</code>. I used this to ensure that there won't be a problem if you have external variables called '.inner' or '.orig'. Now, all variables are evaluated in the place where the curry is called.</p>
"
26131009,3613700,2014-09-30T22:12:44Z,2658752,0,FALSE,"<p>Another vote for <code>jblas</code>. all the methods are like you'd expect them to be.</p>
"
26351648,78187,2014-10-14T02:14:06Z,2614767,1,FALSE,"<p>I recently found this R package on CRAN.  Which does exactly what you are asking I believe.</p>

<p><a href=""http://cran.r-project.org/web/packages/XBRL/index.html"" rel=""nofollow"">XBRL: Extraction of business financial information from XBRL documents</a></p>
"
26488156,2169324,2014-10-21T13:39:51Z,2564258,10,FALSE,"<p>if you want to split the screen, you can do it like this:</p>

<p>(for example for 2 plots next together)</p>

<pre><code>par(mfrow=c(1,2))

plot(x)

plot(y) 
</code></pre>

<p><a href=""http://www.statmethods.net/advgraphs/layout.html"" rel=""noreferrer"">Reference Link</a></p>
"
26500661,3435769,2014-10-22T05:09:15Z,1474081,14,FALSE,"<p>A supplementarily handy (but trivial) tip  for installing older version of packages from source.</p>

<p>First, if you call ""install.packages"", it always installs the latest package from repo. If you want to install the older version of packages, say for compatibility, you can call install.packages(""url_to_source"", repo=NULL, type=""source""). For example:</p>

<pre><code>install.packages(""http://cran.r-project.org/src/contrib/Archive/RNetLogo/RNetLogo_0.9-6.tar.gz"", repo=NULL, type=""source"")
</code></pre>

<p>Without manually downloading packages to the local disk and switching to the command line or installing from local disk, I found it is very convenient and simplify the call (one-step). </p>

<p>Plus: you can use this trick with devtools library's dev_mode, in order to manage different versions of packages:</p>

<p>Reference: <a href=""https://github.com/hadley/devtools"">doc devtools</a></p>
"
26544557,366256,2014-10-24T08:53:03Z,2288485,2,FALSE,"<p>Though others have covered the topic pretty well, I'd like to add this additional quick thought/hint.  You could use regexp to check in advance whether characters potentially consist only of numerics. </p>

<pre><code>for(i in seq_along(names(df)){
     potential_numcol[i] &lt;- all(!grepl(""[a-zA-Z]"",d[,i]))
}
# and now just convert only the numeric ones
d &lt;- sapply(d[,potential_numcol],as.numeric)
</code></pre>

<p>For more sophisticated regular expressions and a neat why to learn/experience their power see this really nice website: <a href=""http://regexr.com/"" rel=""nofollow"">http://regexr.com/</a></p>
"
26564088,2030503,2014-10-25T15:34:22Z,652136,5,FALSE,"<p>There's the rlist package (<a href=""http://cran.r-project.org/web/packages/rlist/index.html"" rel=""noreferrer"">http://cran.r-project.org/web/packages/rlist/index.html</a>) to deal with various kinds of list operations.</p>

<p>Example (<a href=""http://cran.r-project.org/web/packages/rlist/vignettes/Filtering.html"" rel=""noreferrer"">http://cran.r-project.org/web/packages/rlist/vignettes/Filtering.html</a>):</p>

<pre><code>library(rlist)
devs &lt;- 
  list(
    p1=list(name=""Ken"",age=24,
      interest=c(""reading"",""music"",""movies""),
      lang=list(r=2,csharp=4,python=3)),
    p2=list(name=""James"",age=25,
      interest=c(""sports"",""music""),
      lang=list(r=3,java=2,cpp=5)),
    p3=list(name=""Penny"",age=24,
      interest=c(""movies"",""reading""),
      lang=list(r=1,cpp=4,python=2)))

list.remove(devs, c(""p1"",""p2""))
</code></pre>

<p>Results in:</p>

<pre><code># $p3
# $p3$name
# [1] ""Penny""
# 
# $p3$age
# [1] 24
# 
# $p3$interest
# [1] ""movies""  ""reading""
# 
# $p3$lang
# $p3$lang$r
# [1] 1
# 
# $p3$lang$cpp
# [1] 4
# 
# $p3$lang$python
# [1] 2
</code></pre>
"
26618922,1255931,2014-10-28T21:36:31Z,1329940,6,FALSE,"<p>The built-in <code>matrix</code> function has the nice option to enter data <code>byrow</code>. Combine that with an <code>unlist</code> on your source list will give you a matrix. We also need to specify the number of rows so it can break up the unlisted data. That is:</p>

<pre><code>&gt; matrix(unlist(a), byrow=TRUE, nrow=length(a) )
      [,1] [,2] [,3] [,4] [,5] [,6]
 [1,]    1    1    2    3    4    5
 [2,]    2    1    2    3    4    5
 [3,]    3    1    2    3    4    5
 [4,]    4    1    2    3    4    5
 [5,]    5    1    2    3    4    5
 [6,]    6    1    2    3    4    5
 [7,]    7    1    2    3    4    5
 [8,]    8    1    2    3    4    5
 [9,]    9    1    2    3    4    5
[10,]   10    1    2    3    4    5
</code></pre>
"
26619036,1255931,2014-10-28T21:44:24Z,1329940,11,FALSE,"<p><code>simplify2array</code> is a base function that is fairly intuitive. However, since R's default is to fill in data by columns first, you will need to transpose the output. (<code>sapply</code> uses <code>simplify2array</code>, as documented in <code>help(sapply)</code>.)</p>

<pre><code>&gt; t(simplify2array(a))
      [,1] [,2] [,3] [,4] [,5] [,6]
 [1,]    1    1    2    3    4    5
 [2,]    2    1    2    3    4    5
 [3,]    3    1    2    3    4    5
 [4,]    4    1    2    3    4    5
 [5,]    5    1    2    3    4    5
 [6,]    6    1    2    3    4    5
 [7,]    7    1    2    3    4    5
 [8,]    8    1    2    3    4    5
 [9,]    9    1    2    3    4    5
[10,]   10    1    2    3    4    5
</code></pre>
"
26640698,2195752,2014-10-29T21:07:40Z,1995933,26,FALSE,"<p>A simple function...</p>

<pre><code>elapsed_months &lt;- function(end_date, start_date) {
    ed &lt;- as.POSIXlt(end_date)
    sd &lt;- as.POSIXlt(start_date)
    12 * (ed$year - sd$year) + (ed$mon - sd$mon)
}
</code></pre>

<p>Example...</p>

<pre><code>&gt;Sys.time()
[1] ""2014-10-29 15:45:44 CDT""
&gt;elapsed_months(Sys.time(), as.Date(""2012-07-15""))
[1] 27
&gt;elapsed_months(""2002-06-30"", c(""2002-03-31"", ""2002-04-30"", ""2002-05-31""))
[1] 3 2 1
</code></pre>

<p>To me it makes sense to think about this problem as simply subtracting two dates, and since <a href=""http://en.wikipedia.org/wiki/Subtraction"" rel=""noreferrer""><code>minuend − subtrahend = difference</code> (wikipedia)</a>, I put the later date first in the parameter list.</p>

<p>Note that it works fine for dates preceeding 1900 despite those dates having internal representations of year as negative, thanks to the rules for subtracting negative numbers...</p>

<pre><code>&gt; elapsed_months(""1791-01-10"", ""1776-07-01"")
[1] 174
</code></pre>
"
26684240,2058131,2014-10-31T22:17:43Z,2731299,0,FALSE,"<p>I believe the assign() function is your answer:</p>

<pre><code>cols &lt;- c('Col1','Col2','Col3')
data.frame(assign(cols[1], rnorm(10)))
</code></pre>

<p>Returns:</p>

<pre><code>   assign.cols.1...rnorm.10..
1                 -0.02056822
2                 -0.03675639
3                  1.06249599
4                  0.41763399
5                  0.38873118
6                  1.01779018
7                  1.01379963
8                  1.86119518
9                  0.35760039
10                 1.14742560
</code></pre>

<p>With the lapply() or sapply() function, you should be able to loop the cbind() process. Something like:</p>

<pre><code>operation &lt;- sapply(cols, function(x) data.frame(assign(x, rnorm(10))))
final     &lt;- data.frame(lapply(operation, cbind))
</code></pre>

<p>Returns:</p>

<pre><code>   Col1.assign.x..rnorm.10.. Col2.assign.x..rnorm.10.. Col3.assign.x..rnorm.10..
1                0.001962187                -0.3561499               -0.22783816
2               -0.706804781                -0.4452781               -1.09950505
3               -0.604417525                -0.8425018               -0.73287079
4               -1.287038060                 0.2545236               -1.18795684
5                0.232084366                -1.0831463                0.40799046
6               -0.148594144                 0.4963714               -1.34938144
7                0.442054119                 0.2856748                0.05933736
8                0.984615916                -0.0795147               -1.91165189
9                1.222310749                -0.1743313                0.18256877
10              -0.231885977                -0.2273724               -0.43247570
</code></pre>

<p>Then, to clean up the column names:</p>

<pre><code>colnames(final) &lt;- cols
</code></pre>

<p>Returns:</p>

<pre><code>          Col1       Col2        Col3
1   0.19473248  0.2864232  0.93115072
2  -1.08473526 -1.5653469  0.09967827
3  -1.90968422 -0.9678024 -1.02167873
4  -1.11962371  0.4549290  0.76692067
5  -2.13776949  3.0360777 -1.48515698
6   0.64240694  1.3441656  0.47676056
7  -0.53590163  1.2696336 -1.19845723
8   0.09158526 -1.0966833  0.91856639
9  -0.05018762  1.0472368  0.15475583
10  0.27152070 -0.2148181 -1.00551111
</code></pre>

<p>Cheers,</p>

<p>Adam</p>
"
26703757,2490497,2014-11-02T20:03:42Z,1487320,3,FALSE,"<p>As of November 2014 there is easy method to blog from R to your blog hosted on github pages. No databases, no local environment, no new admin panels. Only web browser, github and R are required.</p>

<ol>
<li>Fork <a href=""https://github.com/barryclark/jekyll-now"" rel=""nofollow"">Jekyll Now</a> to deploy pre-configuerd <a href=""https://github.com/jekyll/jekyll"" rel=""nofollow"">Jekyll</a> (a static site generator) into your github repo. Change new repo name.</li>
<li>Edit <code>_config.yml</code> to set some global variables (here you can setup RSS, Disqus, Google Analytics, etc.).</li>
<li>Your blog posts will be located in <code>_posts</code> directory.</li>
<li>Use R packages <code>rmarkdown</code> or <code>knitr</code> to render your <code>Rmd</code> file to <code>md</code>.</li>
<li>Upload/copy&amp;paste your <code>YYYY-MM-DD-my-first_post.md</code> to <code>_posts</code> directory.</li>
</ol>

<p>As example my minimalist blog at: <a href=""http://jangorecki.github.io/"" rel=""nofollow"">jangorecki.github.io</a><br>
It's repo at <a href=""https://github.com/jangorecki/jangorecki.github.io"" rel=""nofollow"">github.com/jangorecki/jangorecki.github.io</a>  </p>

<p>Also storing <code>Rmd</code> files in your repo gives ability to reproduce the post (+R chunks of course) locally in R by anybody.</p>
"
26724451,819544,2014-11-03T22:27:59Z,1745622,3,FALSE,"<p>According to <a href=""http://www.r-bloggers.com/pitfall-did-you-really-mean-to-use-matrixnrow-ncol/"" rel=""nofollow"">this article</a> we can do better than preallocating with <code>NA</code> by preallocating with <code>NA_real_</code>. From the article:</p>

<blockquote>
  <p>as soon as you assign a numeric value to any of the cells in 'x', the matrix will first have to be coerced to numeric when a new value is assigned. The originally allocated logical matrix was allocated in vain and just adds an unnecessary memory footprint and extra work for the garbage collector.
  Instead allocate it using NA_real_ (or NA_integer_ for integers)</p>
</blockquote>

<p>As recommended: let's test it.</p>

<pre><code>testfloat = function(mat){
  n=nrow(mat)
  for(i in 1:n){
    mat[i,] = 1.2
  }
}

&gt;system.time(testfloat(matrix(data=NA,nrow=1e4,ncol=1e4)))
user  system elapsed 
3.08    0.24    3.32 
&gt; system.time(testfloat(matrix(data=NA_real_,nrow=1e4,ncol=1e4)))
user  system elapsed 
2.91    0.23    3.14 
</code></pre>

<p>And for integers:</p>

<pre><code>testint = function(mat){
  n=nrow(mat)
  for(i in 1:n){
    mat[i,] = 3
  }
}

&gt; system.time(testint(matrix(data=NA,nrow=1e4,ncol=1e4)))
user  system elapsed 
2.96    0.29    3.31 
&gt; system.time(testint(matrix(data=NA_integer_,nrow=1e4,ncol=1e4)))
user  system elapsed 
2.92    0.35    3.28 
</code></pre>

<p>The difference is small in my test cases, but it's there.</p>
"
26804584,563329,2014-11-07T15:30:36Z,2050790,1,FALSE,"<p>Regarding vectors and the hash/array concept from other languages:</p>

<ol>
<li><p>Vectors are the atoms of R. Eg, <code>rpois(1e4,5)</code> (5 random numbers), <code>numeric(55)</code> (length-55 zero vector over doubles), and <code>character(12)</code> (12 empty strings), are all ""basic"".</p></li>
<li><p>Either lists or vectors can have <code>names</code>.</p>

<pre><code>&gt; n = numeric(10)
&gt; n
 [1] 0 0 0 0 0 0 0 0 0 0
&gt; names(n)
NULL
&gt; names(n) = LETTERS[1:10]
&gt; n
A B C D E F G H I J 
0 0 0 0 0 0 0 0 0 0
</code></pre></li>
<li><p>Vectors require everything to be the same data type. Watch this:</p>

<pre><code>&gt; i = integer(5)
&gt; v = c(n,i)
&gt; v
A B C D E F G H I J           
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
&gt; class(v)
[1] ""numeric""
&gt; i = complex(5)
&gt; v = c(n,i)
&gt; class(v)
[1] ""complex""
&gt; v
   A    B    C    D    E    F    G    H    I    J                          
0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i 0+0i
</code></pre></li>
<li><p>Lists can contain varying data types, as seen in other answers and the OP's question itself.</p></li>
</ol>

<p>I've seen languages (ruby, javascript) in which ""arrays"" may contain variable datatypes, but for example in C++ ""arrays"" must be all the same datatype. I believe this is a speed/efficiency thing: if you have a <code>numeric(1e6)</code> you know its size and the location of every element <em>a priori</em>; if the thing might contain <code>""Flying Purple People Eaters""</code> in some unknown slice, then you have to actually parse stuff to know basic facts about it.</p>

<p>Certain standard R operations also make more sense when the type is guaranteed. For example <code>cumsum(1:9)</code> makes sense whereas <code>cumsum(list(1,2,3,4,5,'a',6,7,8,9))</code> does not, without the type being guaranteed to be double.</p>

<hr>

<p>As to your second question:</p>

<blockquote>
  <p>Lists can be returned from functions even though you never passed in a List when you called the function</p>
</blockquote>

<p>Functions return different data types than they're input all the time. <code>plot</code> returns a plot even though it doesn't take a plot as an input. <code>Arg</code> returns a <code>numeric</code> even though it accepted a <code>complex</code>. Etc.</p>

<p>(And as for <code>strsplit</code>: the source code is <a href=""https://github.com/wch/r-source/blob/trunk/src/main/grep.c#L136"" rel=""nofollow"">here</a>.)</p>
"
26845518,26575,2014-11-10T13:59:21Z,2682144,2,FALSE,"<p>The <code>subplots</code> function in recent versions of matplotlib (at least 1.4) makes this a little bit easier:</p>

<pre><code>def pairs(data, names):
    ""Quick&amp;dirty scatterplot matrix""
    d = len(data)
    fig, axes = plt.subplots(nrows=d, ncols=d, sharex='col', sharey='row')
    for i in range(d):
        for j in range(d):
            ax = axes[i,j]
            if i == j:
                ax.text(0.5, 0.5, names[i], transform=ax.transAxes,
                        horizontalalignment='center', verticalalignment='center',
                        fontsize=16)
            else:
                ax.scatter(data[j], data[i], s=10)
</code></pre>
"
26980513,NA,2014-11-17T19:35:02Z,476726,2,FALSE,"<pre><code>a &lt;- data.frame(a=c(1,2,0,1),b=c(NA,1,NA,1), c=c(3,4,5,1))

na.omit(a)
  a b c
2 2 1 4
4 1 1 1

a[rowSums(is.na(a))==0,]
  a b c
2 2 1 4
4 1 1 1

a[complete.cases(a),]
  a b c
2 2 1 4
4 1 1 1
</code></pre>
"
27090046,1456930,2014-11-23T14:32:57Z,2061897,38,FALSE,"<p>The <a href=""http://cran.r-project.org/web/packages/jsonlite/index.html"">jsonlite</a> package is easy to use and tries to convert json into data frames.</p>

<p>Example:</p>

<pre><code>library(jsonlite)

# url with some information about project in Andalussia
url &lt;- 'http://www.juntadeandalucia.es/export/drupaljda/ayudas.json'

# read url and convert to data.frame
document &lt;- fromJSON(txt=url)
</code></pre>
"
27090068,1679903,2014-11-23T14:35:01Z,1815606,1,FALSE,"<pre><code>#!/usr/bin/env Rscript
print(""Hello"")

# sad workaround but works :(
programDir &lt;- dirname(sys.frame(1)$ofile)
source(paste(programDir,""other.R"",sep='/'))
source(paste(programDir,""other-than-other.R"",sep='/'))
</code></pre>
"
27266488,3521006,2014-12-03T08:02:19Z,1660124,93,FALSE,"<p>More recently, you can also use the <strong>dplyr</strong> package for that purpose:</p>

<pre><code>library(dplyr)
x %&gt;% 
  group_by(Category) %&gt;% 
  summarise(Frequency = sum(Frequency))

#Source: local data frame [3 x 2]
#
#  Category Frequency
#1    First        30
#2   Second         5
#3    Third        34
</code></pre>

<p>Or, for <strong>multiple summary columns</strong> (works with one column too):</p>

<pre><code>x %&gt;% 
  group_by(Category) %&gt;% 
  summarise_each(funs(sum))
</code></pre>

<p><strong>Update for dplyr >= 0.5:</strong> <code>summarise_each</code> has been replaced by <code>summarise_all</code>, <code>summarise_at</code> and <code>summarise_if</code> family of functions in dplyr.</p>

<p>Or, if you have <strong>multiple columns to group by,</strong> you can specify all of them in the <code>group_by</code> separated with commas:</p>

<pre><code>mtcars %&gt;% 
  group_by(cyl, gear) %&gt;%                            # multiple group columns
  summarise(max_hp = max(hp), mean_mpg = mean(mpg))  # multiple summary columns
</code></pre>

<p>For more information, including the <code>%&gt;%</code> operator, see the <a href=""http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html"" rel=""noreferrer"">introduction to dplyr</a>. </p>
"
27328812,2496554,2014-12-06T05:58:06Z,2288485,15,FALSE,"<p>I would have added a comment (cant low rating)</p>

<p>Just to add on user276042 and pangratz</p>

<pre><code>dat$x = as.numeric(as.character(dat$x))
</code></pre>

<p>This will override the values of existing column x</p>
"
27429290,455826,2014-12-11T17:53:03Z,1376967,0,FALSE,"<p>If you don't want to generate the normal distribution line-graph ""by hand"", still use stat_function, and show graphs side-by-side -- then you could consider using the ""multiplot"" function published on ""Cookbook for R"" as an alternative to facet_wrap. You can copy the multiplot code to your project <a href=""http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/"" rel=""nofollow"">from here</a>.</p>

<p>After you copy the code, do the following:</p>

<pre><code># Some fake data (copied from hadley's answer)
dd &lt;- data.frame(
  predicted = rnorm(72, mean = 2, sd = 2),
  state = rep(c(""A"", ""B"", ""C""), each = 24)
) 

# Split the data by state, apply a function on each member that converts it into a 
# plot object, and return the result as a vector.
plots &lt;- lapply(split(dd,dd$state),FUN=function(state_slice){ 
  # The code here is the plot code generation. You can do anything you would 
  # normally do for a single plot, such as calling stat_function, and you do this 
  # one slice at a time.
  ggplot(state_slice, aes(predicted)) + 
    geom_density() + 
    stat_function(fun=dnorm, 
                  args=list(mean=mean(state_slice$predicted), 
                            sd=sd(state_slice$predicted)),
                  color=""red"")
})

# Finally, present the plots on 2 columns.
multiplot(plotlist = plots, cols=2)
</code></pre>
"
27431904,2334254,2014-12-11T20:34:57Z,2470248,-1,FALSE,"<h2>The ugly system option</h2>

<pre><code>ptf &lt;- function (txtToPrint,outFile){system(paste(paste(paste(""echo '"",cat(txtToPrint),sep = """",collapse = NULL),""'&gt;"",sep = """",collapse = NULL),outFile))}
#Prints txtToPrint to outFile in cwd. #!/bin/bash echo txtToPrint &gt; outFile
</code></pre>
"
27446561,4214855,2014-12-12T15:12:29Z,2531372,2,FALSE,"<p>From <a href=""http://www.r-bloggers.com/a-small-customization-of-ess/"" rel=""nofollow noreferrer"">http://www.r-bloggers.com/a-small-customization-of-ess/</a> and
<a href=""https://stackoverflow.com/questions/25404278/how-to-change-smart-assign-key-to-binding-in-ess"">How to change smart assign key (&quot;_&quot; to &quot;&lt;-&quot;) binding in ESS</a></p>

<p>To assign "":"" to ""&lt;-"" and to stop the assignment of underscore (underbar) ""_"" to  ""&lt;-"" put the following in .emacs (yes, the repeated line is correct)</p>

<pre><code>(setq ess-smart-S-assign-key "":"")
(ess-toggle-S-assign nil)
(ess-toggle-S-assign nil)
(ess-toggle-underscore nil) ; leave underscore key alone!
</code></pre>
"
27498588,4365116,2014-12-16T06:09:50Z,2723034,5,FALSE,"<p>you can use 'capture.output' like below. This allows you to use the data later:</p>

<pre><code>log &lt;- capture.output({
  test &lt;- CensReg.SMN(cc=cc,x=x,y=y, nu=NULL, type=""Normal"")
})

test$betas
</code></pre>
"
27515342,3632729,2014-12-16T22:43:56Z,1815606,2,FALSE,"<p>I like this approach:</p>

<pre><code>this.file &lt;- sys.frame(tail(grep('source',sys.calls()),n=1))$ofile
this.dir &lt;- dirname(this.file)
</code></pre>
"
27555344,1296044,2014-12-18T20:43:35Z,1109017,7,FALSE,"<p>Here's a more flexible version for debugging/verbose use in Rscript. Not only it prints to <code>stderr</code> as you ask, but it also allows you to pass variable number of arguments, types etc, like <code>printf</code> does.</p>

<pre><code>v &lt;- function(...) cat(sprintf(...), sep='', file=stderr())
</code></pre>

<p>Now one can do things like:</p>

<pre><code>v(""name: %s  age: %d\n"", name, age)
</code></pre>

<p>etc.</p>
"
27639512,3078062,2014-12-24T16:08:02Z,2547402,2,FALSE,"<p>While I like Ken Williams simple function, I would like to retrieve the multiple modes if they exist.  With that in mind, I use the following function which returns a list of the modes if multiple or the single.</p>

<pre><code>rmode &lt;- function(x) {
  x &lt;- sort(x)  
  u &lt;- unique(x)
  y &lt;- lapply(u, function(y) length(x[x==y]))
  u[which( unlist(y) == max(unlist(y)) )]
} 
</code></pre>
"
27653991,2682018,2014-12-26T07:11:31Z,1923273,1,FALSE,"<p>One more way i find convenient is:</p>

<pre><code>numbers &lt;- c(4,23,4,23,5,43,54,56,657,67,67,435,453,435,324,34,456,56,567,65,34,435)
(s&lt;-summary (as.factor(numbers)))
</code></pre>

<p>This converts the dataset to factor, and then summary() gives us the control totals (counts of the unique values).</p>

<p>Output is:</p>

<pre><code>4   5  23  34  43  54  56  65  67 324 435 453 456 567 657 
2   1   2   2   1   1   2   1   2   1   3   1   1   1   1 
</code></pre>

<p>This can be stored as dataframe if preferred.</p>

<blockquote>
  <p>as.data.frame(cbind(Number = names(s),Freq = s), stringsAsFactors=F, row.names = 1:length(s))</p>
</blockquote>

<p>here row.names has been used to rename row names.
without using row.names, column names in s are used as row names in new dataframe</p>

<p>Output is:</p>

<pre><code>     Number Freq
1       4    2
2       5    1
3      23    2
4      34    2
5      43    1
6      54    1
7      56    2
8      65    1
9      67    2
10    324    1
11    435    3
12    453    1
13    456    1
14    567    1
15    657    1
</code></pre>
"
27659805,1325646,2014-12-26T17:06:41Z,1923273,1,FALSE,"<p>Using table but without comparing with <code>names</code>:</p>

<pre><code>numbers &lt;- c(4,23,4,23,5,43,54,56,657,67,67,435)
x &lt;- 67
numbertable &lt;- table(numbers)
numbertable[as.character(x)]
#67 
# 2 
</code></pre>

<p><code>table</code> is useful when you are using the counts of different elements several times. If you need only one count, use <code>sum(numbers == x)</code></p>
"
27954964,4455582,2015-01-15T00:29:05Z,2261079,3,FALSE,"<p>I'd prefer to add the answer as comment to user56 but yet unable so writing as an independent answer.
Removing leading and trailing blanks might be achieved through trim() function from gdata package as well:</p>

<pre><code>require(gdata)
example(trim)
</code></pre>

<p>Usage example:</p>

<pre><code>&gt; trim(""   Remove leading and trailing blanks    "")
[1] ""Remove leading and trailing blanks""
</code></pre>
"
27956817,4435021,2015-01-15T04:28:25Z,1296646,1,FALSE,"<p>Just like the mechanical card sorters of long ago, first sort by the least significant key, then the next most significant, etc.  No library required, works with any number of keys and any combination of ascending and descending keys.</p>

<pre><code> dd &lt;- dd[order(dd$b, decreasing = FALSE),]
</code></pre>

<p>Now we're ready to do the most significant key.  The sort is stable, and any ties in the most significant key have already been resolved.</p>

<pre><code>dd &lt;- dd[order(dd$z, decreasing = TRUE),]
</code></pre>

<p>This may not be the fastest, but it is certainly simple and reliable</p>
"
27961541,4457130,2015-01-15T10:38:30Z,750786,4,FALSE,"<p>Just a note to add to this post. Later versions of <code>R</code> seem to have buried <code>Rscript</code> somewhat.  For R 3.1.2-1 on OSX downloaded Jan 2015 I found <code>Rscript</code> in </p>

<pre><code>/sw/Library/Frameworks/R.framework/Versions/3.1/Resources/bin/Rscript
</code></pre>

<p>So, instead of something like <code>#! /sw/bin/Rscript</code>, I needed to use the following at the top of my script.</p>

<pre><code>#! /sw/Library/Frameworks/R.framework/Versions/3.1/Resources/bin/Rscript
</code></pre>

<p>The <code>locate Rscript</code> might be helpful to you.</p>
"
27986562,1600821,2015-01-16T14:49:35Z,743812,7,FALSE,"<p>You could use <code>RcppRoll</code> for very quick moving averages written in C++. Just call the <code>roll_mean</code> function. Docs can be found <a href=""http://cran.r-project.org/web/packages/RcppRoll/RcppRoll.pdf"">here</a>.  </p>

<p>Otherwisem, this (slower) for loop should do the trick. </p>

<pre><code>ma &lt;- function(arr, n=15){
  res = arr
  for(i in n:length(arr)){
    res[i] = mean(arr[(i-n):i])
  }
  res
}
</code></pre>
"
27992356,2682018,2015-01-16T20:35:49Z,77434,6,FALSE,"<p>I have another method for finding the last element in a vector.
Say the vector is <code>a</code>.</p>

<pre><code>&gt; a&lt;-c(1:100,555)
&gt; end(a)      #Gives indices of last and first positions
[1] 101   1
&gt; a[end(a)[1]]   #Gives last element in a vector
[1] 555
</code></pre>

<p>There you go!</p>
"
27992812,2521252,2015-01-16T21:12:01Z,2464680,1,FALSE,"<p>See the <a href=""http://cran.r-project.org/web/packages/fifer/index.html"" rel=""nofollow"">fifer</a> package for function chisq.post.hoc()</p>
"
28102021,319741,2015-01-23T02:15:12Z,2545879,3,FALSE,"<p>If you give the function a name rather than making it anonymous, you can pass arguments more easily.   We can use <code>nrow</code> to get the number of rows and pass a vector of the row numbers in as a parameter, along with the frame to be indexed this way.  </p>

<p>For clarity I used a different example function; this example multiplies column x by column y for a 2 column matrix: </p>

<pre><code>test &lt;- data.frame(x=c(26,21,20),y=c(34,29,28))
myfun &lt;- function(position, df) {
    print(df[position,1] * df[position,2])
}

positions &lt;- 1:nrow(test)
lapply(positions, myfun, test)
</code></pre>
"
28139279,1719517,2015-01-25T17:29:24Z,1828742,13,FALSE,"<p>As <strong>Maciej Jończyk</strong> mentioned, you may also need to increase margins</p>

<pre><code>par(las=2)
par(mar=c(8,8,1,1)) # adjust as needed
plot(...)
</code></pre>
"
28220204,1079292,2015-01-29T16:53:12Z,952275,3,FALSE,"<p>I like perl compatible regular expressions. Probably someone else does too...</p>

<p>Here is a function that does perl compatible regular expressions and matches the functionality of functions in other languages that I am used to:</p>

<pre><code>regexpr_perl &lt;- function(expr, str) {
  match &lt;- regexpr(expr, str, perl=T)
  matches &lt;- character(0)
  if (attr(match, 'match.length') &gt;= 0) {
    capture_start &lt;- attr(match, 'capture.start')
    capture_length &lt;- attr(match, 'capture.length')
    total_matches &lt;- 1 + length(capture_start)
    matches &lt;- character(total_matches)
    matches[1] &lt;- substr(str, match, match + attr(match, 'match.length') - 1)
    if (length(capture_start) &gt; 1) {
      for (i in 1:length(capture_start)) {
        matches[i + 1] &lt;- substr(str, capture_start[[i]], capture_start[[i]] + capture_length[[i]] - 1)
      }
    }
  }
  matches
}
</code></pre>
"
28354885,4534936,2015-02-05T22:00:54Z,2619543,2,FALSE,"<p>I know this thread is 5 years old, but I just wanted to point out that these are standard values for the IEEE single-precision and double-precision standard, which is used by the vast majority of consumer-level statistics packages.</p>
"
28356213,2399597,2015-02-05T23:48:34Z,2453326,1,FALSE,"<p>When I was recently looking for an <strong>R</strong> function returning indexes of top N max/min numbers in a given vector, I was surprised there is no such a function. </p>

<p>And this is something very similar. </p>

<p>The brute force solution using <strong>base::order</strong> function seems to be the easiest one.</p>

<pre><code>topMaxUsingFullSort &lt;- function(x, N) {
  sort(x, decreasing = TRUE)[1:min(N, length(x))]
}
</code></pre>

<p>But it is not the fastest one in case your <strong>N</strong> value is relatively small compared to length of the vector <strong>x</strong>.</p>

<p>On the other side if the <strong>N</strong> is really small, you can use <strong>base::whichMax</strong> function iteratively and in each iteration you can replace found value by <strong>-Inf</strong></p>

<pre><code># the input vector 'x' must not contain -Inf value 
topMaxUsingWhichMax &lt;- function(x, N) {
  vals &lt;- c()
  for(i in 1:min(N, length(x))) {
    idx      &lt;- which.max(x)
    vals     &lt;- c(vals, x[idx]) # copy-on-modify (this is not an issue because idxs is relative small vector)
    x[idx]   &lt;- -Inf            # copy-on-modify (this is the issue because data vector could be huge)
  }
  vals
}
</code></pre>

<p>I believe you see the problem - the copy-on-modify nature of R. So this will perform better for very very very small N (1,2,3) but it will rapidly slow down for larger N values. And you are iterating over all elements in vector <strong>x</strong> <strong>N</strong> times.</p>

<p>I think the best solution in clean <strong>R</strong> is to use partial <strong>base::sort</strong>.</p>

<pre><code>topMaxUsingPartialSort &lt;- function(x, N) {
  N &lt;- min(N, length(x))
  x[x &gt;= -sort(-x, partial=N)[N]][1:N]
}
</code></pre>

<p>Then you can select the last (<strong>N</strong>th) item from the result of functions defiend above.</p>

<p>Note: functions defined above are just examples - if you want to use them, you have to check/sanity inputs (eg. <strong>N > length(x)</strong>).</p>

<p>I wrote a small article about something very similar (get indexes of top N max/min values of a vector) at <a href=""http://palusga.cz/?p=18"" rel=""nofollow"">http://palusga.cz/?p=18</a> - you can find here some benchmarks of similar functions I defined above.</p>
"
28491301,2668831,2015-02-13T01:55:19Z,2151212,0,FALSE,"<p>I just put together a nice data structure and chain of processing to generate this switching behaviour, no libraries needed. I'm sure it will have been implemented numerous times over, and came across this thread looking for examples - thought I'd chip in.</p>

<p>I didn't even particularly need flags (the only flag here is a debug mode, creating a variable which I check for as a condition of starting a downstream function <code>if (!exists(debug.mode)) {...} else {print(variables)})</code>. The flag checking <code>lapply</code> statements below produce the same as:</p>

<pre><code>if (""--debug"" %in% args) debug.mode &lt;- T
if (""-h"" %in% args || ""--help"" %in% args) 
</code></pre>

<p>where <code>args</code> is the variable read in from command line arguments (a character vector, equivalent to <code>c('--debug','--help')</code> when you supply these on for instance)</p>

<p>It's reusable for any other flag and you avoid all the repetition, and no libraries so no dependencies:</p>

<pre><code>args &lt;- commandArgs(TRUE)

flag.details &lt;- list(
""debug"" = list(
  def = ""Print variables rather than executing function XYZ..."",
  flag = ""--debug"",
  output = ""debug.mode &lt;- T""),
""help"" = list(
  def = ""Display flag definitions"",
  flag = c(""-h"",""--help""),
  output = ""cat(help.prompt)"") )

flag.conditions &lt;- lapply(flag.details, function(x) {
  paste0(paste0('""',x$flag,'""'), sep = "" %in% args"", collapse = "" || "")
})
flag.truth.table &lt;- unlist(lapply(flag.conditions, function(x) {
  if (eval(parse(text = x))) {
    return(T)
  } else return(F)
}))

help.prompts &lt;- lapply(names(flag.truth.table), function(x){
# joins 2-space-separatated flags with a tab-space to the flag description
  paste0(c(paste0(flag.details[x][[1]][['flag']], collapse=""  ""),
  flag.details[x][[1]][['def']]), collapse=""\t"")
} )

help.prompt &lt;- paste(c(unlist(help.prompts),''),collapse=""\n\n"")

# The following lines handle the flags, running the corresponding 'output' entry in flag.details for any supplied
flag.output &lt;- unlist(lapply(names(flag.truth.table), function(x){
  if (flag.truth.table[x]) return(flag.details[x][[1]][['output']])
}))
eval(parse(text = flag.output))
</code></pre>

<p>Note that in <code>flag.details</code> here the commands are stored as strings, then evaluated with <code>eval(parse(text = '...'))</code>. Optparse is obviously desirable for any serious script, but minimal-functionality code is good too sometimes.</p>

<p>Sample output:</p>

<pre><strong>$</strong> Rscript <a href=""https://github.com/lmmx/ScholarDaemon/blob/master/check_mail.Rscript"" rel=""nofollow"">check_mail.Rscript</a> --help
--debug Print  variables rather than executing function XYZ...

-h  --help  Display flag definitions</pre>
"
28502446,4497472,2015-02-13T15:04:03Z,1699046,2,FALSE,"<p>Well, since you asked for R equivalent to other languages, I tried to do this. Seems to work though I haven't really looked at which technique is more efficient in R.</p>

<pre><code>&gt; myDf &lt;- head(iris)
&gt; myDf
Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
&gt; nRowsDf &lt;- nrow(myDf)
&gt; for(i in 1:nRowsDf){
+ print(myDf[i,4])
+ }
[1] 0.2
[1] 0.2
[1] 0.2
[1] 0.2
[1] 0.2
[1] 0.4
</code></pre>

<p>For the categorical columns though, it would fetch you a Data Frame which you could typecast using as.character() if needed.</p>
"
28628919,1350852,2015-02-20T12:22:39Z,2436688,3,FALSE,"<p>I have made a small comparison of methods mentioned here.</p>

<pre><code>n = 1e+4
library(microbenchmark)
### Using environment as a container
lPtrAppend &lt;- function(lstptr, lab, obj) {lstptr[[deparse(substitute(lab))]] &lt;- obj}
### Store list inside new environment
envAppendList &lt;- function(lstptr, obj) {lstptr$list[[length(lstptr$list)+1]] &lt;- obj} 

microbenchmark(times = 5,  
        env_with_list_ = {
            listptr &lt;- new.env(parent=globalenv())
            listptr$list &lt;- NULL
            for(i in 1:n) {envAppendList(listptr, i)}
            listptr$list
        },
        c_ = {
            a &lt;- list(0)
            for(i in 1:n) {a = c(a, list(i))}
        },
        list_ = {
            a &lt;- list(0)
            for(i in 1:n) {a &lt;- list(a, list(i))}
        },
        by_index = {
            a &lt;- list(0)
            for(i in 1:n) {a[length(a) + 1] &lt;- i}
            a
        },
        append_ = { 
            a &lt;- list(0)    
            for(i in 1:n) {a &lt;- append(a, i)} 
            a
        },
        env_as_container_ = {
            listptr &lt;- new.env(parent=globalenv())
            for(i in 1:n) {lPtrAppend(listptr, i, i)} 
            listptr
        }   
)
</code></pre>

<p>Results:</p>

<pre><code>Unit: milliseconds
              expr       min        lq       mean    median        uq       max neval cld
    env_with_list_  188.9023  198.7560  224.57632  223.2520  229.3854  282.5859     5  a 
                c_ 1275.3424 1869.1064 2022.20984 2191.7745 2283.1199 2491.7060     5   b
             list_   17.4916   18.1142   22.56752   19.8546   20.8191   36.5581     5  a 
          by_index  445.2970  479.9670  540.20398  576.9037  591.2366  607.6156     5  a 
           append_ 1140.8975 1316.3031 1794.10472 1620.1212 1855.3602 3037.8416     5   b
 env_as_container_  355.9655  360.1738  399.69186  376.8588  391.7945  513.6667     5  a 
</code></pre>
"
28749730,3735948,2015-02-26T18:11:13Z,1299871,16,FALSE,"<p>In joining two data frames with ~1 million rows each, one with 2 columns and the other with ~20, I've surprisingly found <code>merge(..., all.x = TRUE, all.y = TRUE)</code> to be faster then <code>dplyr::full_join()</code>. This is with dplyr v0.4 </p>

<p>Merge takes ~17 seconds, full_join takes ~65 seconds.  </p>

<p>Some food for though, since I generally default to dplyr for manipulation tasks.</p>
"
28828423,4627021,2015-03-03T09:39:24Z,2050790,1,FALSE,"<p>If it helps, I tend to conceive ""lists"" in R as ""records"" in other pre-OO languages:</p>

<ul>
<li>they do not make any assumptions about an overarching type (or rather the type of all possible records of any arity and field names is available).</li>
<li>their fields can be anonymous (then you access them by strict definition order).</li>
</ul>

<p>The name ""record"" would clash with the standard meaning of ""records"" (aka rows) in database parlance, and may be this is why their name suggested itself: as lists (of fields).</p>
"
28828479,4536595,2015-03-03T09:42:13Z,1995933,20,FALSE,"<p>There may be a simpler way. It's not a function but it is only one line.</p>

<pre><code>length(seq(from=date1, to=date2, by='month')) - 1
</code></pre>

<p>e.g.</p>

<pre><code>&gt; length(seq(from=Sys.Date(), to=as.Date(""2020-12-31""), by='month')) - 1
</code></pre>

<p>Produces:</p>

<pre><code>[1] 69
</code></pre>

<p>This calculates the number of whole months between the two dates. Remove the -1 if you want to include the current month/ remainder that isn't a whole month.</p>
"
28847488,3349904,2015-03-04T05:52:33Z,1753299,0,FALSE,"<p>Build the labels yourself from the elements of the solution. Use this alternate predictor method which takes ksvm model (m) and data in original training format (d)</p>

<pre><code>predict.alt &lt;- function(m, d){
  sign(d[, m@SVindex] %*% m@coef[[1]] - m@b)
}
</code></pre>

<p>K is a <code>kernelMatrix</code> for training. For validation's sake, if you run <code>predict.alt</code> on the training data you will notice that the alternate predictor method switches values alongside the fitted values returned by ksvm. The native predictor behaves in an unexpected way:</p>

<pre><code>aux &lt;- data.frame(fit=kout@fitted, native=predict(kout, K), alt=predict.alt(m=kout, d=as.matrix(K))) 
sample_n(aux, 10)
    fit  native alt
1     0       0  -1
100   1       0   1
218   1       0   1
200   1       0   1
182   1       0   1
87    0       0  -1
183   1       0   1
174   1       0   1
94    1       0   1
165   1       0   1
</code></pre>
"
28986190,4658270,2015-03-11T12:04:47Z,2665532,1,FALSE,"<p>I just wrote another package to download Google Docs spreadsheets. Its much simpler than the alternatives, since it just requires the URL (and that 'share by link' is enabled).</p>

<p>Try it:</p>

<pre><code>install.packages('gsheet')
library(gsheet)
gsheet2tbl('docs.google.com/spreadsheets/d/1I9mJsS5QnXF2TNNntTy-HrcdHmIF9wJ8ONYvEJTXSNo')
</code></pre>

<p>More detail is here: <a href=""https://github.com/maxconway/gsheet"" rel=""nofollow"">https://github.com/maxconway/gsheet</a></p>
"
29050376,563329,2015-03-14T15:04:22Z,2564258,13,FALSE,"<p><strong>tl;dr:</strong> You want to use <code>curve</code> (with <code>add=TRUE</code>) or <code>lines</code>.</p>

<hr>

<p>I disagree with <code>par(new=TRUE)</code> because that will double-print tick-marks and axis labels. Eg</p>

<p><img src=""https://i.stack.imgur.com/xx8e1.png"" alt=""sine and parabola ""></p>

<p><em>The output of <code>plot(sin); par(new=T); plot( function(x) x**2 )</code>.</em></p>

<p>Look how messed up the vertical axis labels are! Since the ranges are different you would need to set <code>ylim=c(lowest point between the two functions, highest point between the two functions)</code>, which is less easy than what I'm about to show you---and <em>way</em> less easy if you want to add not just two curves, but many.</p>

<hr>

<p>What always confused me about plotting is the difference between <code>curve</code> and <code>lines</code>. <em>(If you can't remember that these are the names of the two important plotting commands, just <a href=""http://www.tubechop.com/watch/5461987"" rel=""noreferrer"">sing</a> it.)</em></p>

<h3>Here's the big difference between <code>curve</code> and <code>lines</code>.</h3>

<p><code>curve</code> will plot a function, like <code>curve(sin)</code>. <code>lines</code> plots points with x and y values, like: <code>lines( x=0:10, y=sin(0:10) )</code>.</p>

<p>And here's a minor difference: <code>curve</code> needs to be called with <code>add=TRUE</code> for what you're trying to do, while <code>lines</code> already assumes you're adding to an existing plot.</p>

<p><img src=""https://i.stack.imgur.com/VD8Cu.png"" alt=""id &amp; sine""></p>

<p><em>Here's the result of calling <code>plot(0:2); curve(sin)</code>.</em></p>

<hr>

<p>Behind the scenes, check out <code>methods(plot)</code>. And check <code>body( plot.function )[[5]]</code>. When you call <code>plot(sin)</code> R figures out that <code>sin</code> is a function (not y values) and uses the <code>plot.function</code> method, which ends up calling <code>curve</code>. So <code>curve</code> is the tool meant to handle functions.</p>
"
29104586,2523362,2015-03-17T16:30:26Z,2453326,0,FALSE,"<p><code>head(sort(x),..)</code> or <code>tail(sort(x),...)</code> should work</p>
"
29219741,4521956,2015-03-23T20:28:31Z,2545879,0,FALSE,"<p>I'm a little confuse so excuse me if I get this wrong  but you want work out n-th root of the numbers in each row of a matrix where n = the row number. If this this the case then its really simple create a new array with the same dimensions as the original with each column having the same values as the corresponding row number:</p>

<pre><code>test_row_order = array(seq(1:length(test[,1]), dim = dim(test))
</code></pre>

<p>Then simply apply a function (the n-th root in this case):</p>

<pre><code>n_root = test^(1/test_row_order)
</code></pre>
"
29256556,4470365,2015-03-25T13:07:08Z,2665532,5,FALSE,"<p>As of 2015, there is now the <a href=""https://github.com/jennybc/googlesheets"" rel=""nofollow"">googlesheets</a> package.  It is the best option out there for analyzing and editing Google Sheets data in R.  Not only can it pull data from Google Sheets, but you can edit the data in Google Sheets, create new sheets, etc.</p>

<p>The GitHub link above has a readme with usage details; there's also a <a href=""http://htmlpreview.github.io/?https://raw.githubusercontent.com/jennybc/googlesheets/master/vignettes/basic-usage.html"" rel=""nofollow"">vignette</a> for getting started, or you can find the official documentation on <a href=""https://cran.r-project.org/web/packages/googlesheets/googlesheets.pdf"" rel=""nofollow"">CRAN</a>.</p>
"
29256600,2963704,2015-03-25T13:08:51Z,1816719,1,FALSE,"<p>Subsetting outside of the ggplot function:</p>

<pre><code>library(ggplot2)
set.seed(1)
x &lt;- data.frame(a = 1:10, b = rnorm(10))
x$lab &lt;- letters[1:10]
x$lab[!(abs(x$b) &gt; 0.5)] &lt;- NA
ggplot(data = x, aes(a, b, label = lab)) + 
  geom_point() + 
  geom_text(vjust = 0) 
</code></pre>

<p>Using qplot:</p>

<pre><code>qplot(a, b, data = x, label = lab, geom = c('point','text'))
</code></pre>
"
29264250,856624,2015-03-25T18:59:23Z,1716012,2,FALSE,"<p>As of the date 2015-03-25,
and possibly earlier,
the <a href=""http://cran.r-project.org/web/packages/pracma/index.html"" rel=""nofollow"">pracma</a>
package contains the functions <code>tic()</code> and <code>toc()</code>.</p>

<p>Example:</p>

<pre><code>&gt; library(pracma)
&gt; tic()
&gt; for(i in 1:10000) mad(runif(10000))    # kill time
&gt; toc()
elapsed time is 18.610000 seconds 
</code></pre>
"
29314036,3217870,2015-03-28T05:31:18Z,1523126,3,FALSE,"<h3>a <code>dplyr</code> solution using <code>mutate_each</code> and pipes</h3>

<p>say you have the following:</p>

<pre><code>&gt; dft
Source: local data frame [11 x 5]

   Bureau.Name Account.Code   X2014   X2015   X2016
1       Senate          110 158,000 211,000 186,000
2       Senate          115       0       0       0
3       Senate          123  15,000  71,000  21,000
4       Senate          126   6,000  14,000   8,000
5       Senate          127 110,000 234,000 134,000
6       Senate          128 120,000 159,000 134,000
7       Senate          129       0       0       0
8       Senate          130 368,000 465,000 441,000
9       Senate          132       0       0       0
10      Senate          140       0       0       0
11      Senate          140       0       0       0
</code></pre>

<p>and want to remove commas from the year variables X2014-X2016, and
convert them to numeric. also, let's say X2014-X2016 are read in as
factors (default)</p>

<pre><code>dft %&gt;%
    mutate_each(funs(as.character(.)), X2014:X2016) %&gt;%
    mutate_each(funs(gsub("","", """", .)), X2014:X2016) %&gt;%
    mutate_each(funs(as.numeric(.)), X2014:X2016)
</code></pre>

<p><code>mutate_each</code> applies the function(s) inside <code>funs</code> to the specified columns</p>

<p>I did it sequentially, one function at a time (if you use multiple
functions inside <code>funs</code> then you create additional, unnecessary columns)</p>
"
29327303,4466255,2015-03-29T08:56:33Z,2531372,2,FALSE,"<p>Since the feature is useful. You can assign it to other key which is less used by you in R it will automatically unassign it from underscore. I personally assign it to <code>"";""</code> by adding following line in .emacs file.</p>

<pre><code>(setq ess-smart-S-assign-key "";"")
</code></pre>

<p>My version of emacs is 24.3 All-in-one installation file by Vincent Goulet.(Installed on windows 7)</p>

<p>hope this helps</p>

<p><strong>Edit</strong>
In emacs 25.2 above do not work instead add following in the .emacs file</p>

<pre><code>(setq ess-smart-S-assign-key "";"")
(ess-toggle-S-assign nil)
(ess-toggle-S-assign nil)
</code></pre>
"
29331287,559784,2015-03-29T15:52:00Z,1296646,58,FALSE,"<p>The R package <code>data.table</code> provides both <em>fast</em> and <em>memory efficient</em> ordering of <em>data.tables</em> with a straightforward syntax (a part of which Matt has highlighted quite nicely <a href=""https://stackoverflow.com/a/10758086/559784"">in his answer</a>). There has been quite a lot of improvements and also a new function <code>setorder()</code> since then. From <code>v1.9.5+</code>, <code>setorder()</code> also works with <em>data.frames</em>.</p>

<p>First, we'll create a dataset big enough and benchmark the different methods mentioned from other answers and then list the features of <em>data.table</em>.</p>

<h3>Data:</h3>

<pre><code>require(plyr)
require(doBy)
require(data.table)
require(dplyr)
require(taRifx)

set.seed(45L)
dat = data.frame(b = as.factor(sample(c(""Hi"", ""Med"", ""Low""), 1e8, TRUE)),
                 x = sample(c(""A"", ""D"", ""C""), 1e8, TRUE),
                 y = sample(100, 1e8, TRUE),
                 z = sample(5, 1e8, TRUE), 
                 stringsAsFactors = FALSE)
</code></pre>

<h3>Benchmarks:</h3>

<p>The timings reported are from running <code>system.time(...)</code> on these functions shown below. The timings are tabulated below (in the order of slowest to fastest).</p>

<pre><code>orderBy( ~ -z + b, data = dat)     ## doBy
plyr::arrange(dat, desc(z), b)     ## plyr
arrange(dat, desc(z), b)           ## dplyr
sort(dat, f = ~ -z + b)            ## taRifx
dat[with(dat, order(-z, b)), ]     ## base R

# convert to data.table, by reference
setDT(dat)

dat[order(-z, b)]                  ## data.table, base R like syntax
setorder(dat, -z, b)               ## data.table, using setorder()
                                   ## setorder() now also works with data.frames 

# R-session memory usage (BEFORE) = ~2GB (size of 'dat')
# ------------------------------------------------------------
# Package      function    Time (s)  Peak memory   Memory used
# ------------------------------------------------------------
# doBy          orderBy      409.7        6.7 GB        4.7 GB
# taRifx           sort      400.8        6.7 GB        4.7 GB
# plyr          arrange      318.8        5.6 GB        3.6 GB 
# base R          order      299.0        5.6 GB        3.6 GB
# dplyr         arrange       62.7        4.2 GB        2.2 GB
# ------------------------------------------------------------
# data.table      order        6.2        4.2 GB        2.2 GB
# data.table   setorder        4.5        2.4 GB        0.4 GB
# ------------------------------------------------------------
</code></pre>

<ul>
<li><p><code>data.table</code>'s <code>DT[order(...)]</code> syntax was <strong>~10x</strong> faster than the fastest of other methods (<code>dplyr</code>), while consuming the same amount of memory as <code>dplyr</code>.</p></li>
<li><p><code>data.table</code>'s <code>setorder()</code> was <strong>~14x</strong> faster than the fastest of other methods (<code>dplyr</code>), while taking <strong>just 0.4GB extra memory</strong>. <code>dat</code> is now in the order we require (as it is updated by reference).</p></li>
</ul>

<h3>data.table features:</h3>

<p><strong>Speed:</strong></p>

<ul>
<li><p><em>data.table</em>'s ordering is extremely fast because it implements <a href=""http://en.wikipedia.org/wiki/Radix_sort"" rel=""noreferrer"">radix ordering</a>. </p></li>
<li><p>The syntax <code>DT[order(...)]</code> is optimised internally to use <em>data.table</em>'s fast ordering as well. You can keep using the familiar base R syntax but speed up the process (and use less memory).</p></li>
</ul>

<p><strong>Memory:</strong></p>

<ul>
<li><p>Most of the times, we don't require the original <em>data.frame</em> or <em>data.table</em> after reordering. That is, we usually assign the result back to the same object, for example:</p>

<pre><code>DF &lt;- DF[order(...)]
</code></pre>

<p>The issue is that this requires at least twice (2x) the memory of the original object. To be <em>memory efficient</em>, <em>data.table</em> therefore also provides a function <code>setorder()</code>.</p>

<p><code>setorder()</code> reorders <em>data.tables</em> <code>by reference</code> (<em>in-place</em>), without making any additional copies. It only uses extra memory equal to the size of one column.</p></li>
</ul>

<p><strong>Other features:</strong></p>

<ol>
<li><p>It supports <code>integer</code>, <code>logical</code>, <code>numeric</code>, <code>character</code> and even <code>bit64::integer64</code> types.</p>

<blockquote>
  <p>Note that <code>factor</code>, <code>Date</code>, <code>POSIXct</code> etc.. classes are all <code>integer</code>/<code>numeric</code> types underneath with additional attributes and are therefore supported as well. </p>
</blockquote></li>
<li><p>In base R, we can not use <code>-</code> on a character vector to sort by that column in decreasing order. Instead we have to use <code>-xtfrm(.)</code>. </p>

<p>However, in <em>data.table</em>, we can just do, for example, <code>dat[order(-x)]</code> or <code>setorder(dat, -x)</code>.</p></li>
</ol>
"
29711740,1758727,2015-04-18T01:10:13Z,750786,-1,FALSE,"<p>Do you ever know you can use your browser to use RStudio on the server?</p>

<p><a href=""http://www.rstudio.com/products/rstudio/download-server/"" rel=""nofollow"">http://www.rstudio.com/products/rstudio/download-server/</a></p>
"
29714278,1981787,2015-04-18T07:22:01Z,1727772,0,FALSE,"<p>Instead of the conventional read.table I feel fread is a faster function. 
Specifying additional attributes like select only the required columns, specifying colclasses and string as factors will reduce the time take to import the file.</p>

<pre><code>data_frame &lt;- fread(""filename.csv"",sep="","",header=FALSE,stringsAsFactors=FALSE,select=c(1,4,5,6,7),colClasses=c(""as.numeric"",""as.character"",""as.numeric"",""as.Date"",""as.Factor""))
</code></pre>
"
29714304,1981787,2015-04-18T07:25:01Z,2288485,3,FALSE,"<p>To convert a data frame column to numeric you just have to do:-</p>

<p>factor to numeric:- </p>

<pre><code>data_frame$column &lt;- as.numeric(as.character(data_frame$column))
</code></pre>
"
29847221,1651408,2015-04-24T12:10:38Z,2351744,13,FALSE,"<p>For the sake of completeness, Karsten W.'s comment points at <code>strwrap</code>, which is the easiest function to remember:</p>

<pre><code>strwrap(""Lorem ipsum... you know the routine"", width=10)
</code></pre>

<p>and to match exactly the solution proposed in the question, the string has to be pasted afterwards:</p>

<pre><code>paste(strwrap(s,90), collapse=""\n"")
</code></pre>

<p><sub>This post is deliberately made community wiki since the honor of finding the function isn't mine.</sub></p>
"
29870770,1245420,2015-04-25T21:09:01Z,2436688,82,FALSE,"<p>The OP (in the April 2012 updated revision of the question) is interested in knowing if there's a way to add to a list in amortized constant time, such as can be done, for example, with a C++ <code>vector&lt;&gt;</code> container. The best answer(s?) here so far only show the relative execution times for various solutions given a fixed-size problem, but do not address any of the various solutions' <a href=""http://en.wikipedia.org/wiki/Algorithmic_efficiency"">algorithmic efficiency</a> directly. Comments below many of the answers discuss the algorithmic efficiency of some of the solutions, but <strong><em>in every case to date</em></strong> (as of April 2015) <strong><em>they come to the wrong conclusion.</em></strong></p>

<p>Algorithmic efficiency captures the growth characteristics, either in time (execution time) or space (amount of memory consumed) <strong><em>as a problem size grows</em></strong>. Running a performance test for various solutions given a fixed-size problem does not address the various solutions' growth rate. The OP is interested in knowing if there is a way to append objects to an R list in ""amortized constant time"". What does that mean? To explain, first let me describe ""constant time"":</p>

<ul>
<li><p><strong>Constant</strong> or <strong>O(1)</strong> growth:</p>

<p>If the time required to perform a given task <em>remains the same</em> as the size of the problem <em>doubles</em>, then we say the algorithm exhibits <em>constant time</em> growth, or stated in ""Big O"" notation, exhibits O(1) time growth. When the OP says ""amortized"" constant time, he simply means ""in the long run""... i.e., if performing a single operation occasionally takes much longer than normal (e.g. if a preallocated buffer is exhausted and occasionally requires resizing to a larger buffer size), as long as the long-term average performance is constant time, we'll still call it O(1).</p>

<p>For comparison, I will also describe ""linear time"" and ""quadratic time"":</p></li>
<li><p><strong>Linear</strong> or <strong>O(n)</strong> growth:</p>

<p>If the time required to perform a given task <em>doubles</em> as the size of the problem <em>doubles</em>, then we say the algorithm exhibits <em>linear time</em>, or <em>O(n)</em> growth.</p></li>
<li><p><strong>Quadratic</strong> or <strong>O(n<sup>2</sup>)</strong> growth:</p>

<p>If the time required to perform a given task <em>increases by the square of the problem size</em>, them we say the algorithm exhibits <em>quadratic time</em>, or <em>O(n<sup>2</sup>)</em> growth.</p></li>
</ul>

<p>There are many other efficiency classes of algorithms; I defer to the <a href=""http://en.wikipedia.org/wiki/Algorithmic_efficiency"">Wikipedia article</a> for further discussion.</p>

<p>I thank @CronAcronis for his answer, as I am new to R and it was nice to have a fully-constructed block of code for doing a performance analysis of the various solutions presented on this page. I am borrowing his code for my analysis, which I duplicate (wrapped in a function) below:</p>

<pre><code>library(microbenchmark)
### Using environment as a container
lPtrAppend &lt;- function(lstptr, lab, obj) {lstptr[[deparse(substitute(lab))]] &lt;- obj}
### Store list inside new environment
envAppendList &lt;- function(lstptr, obj) {lstptr$list[[length(lstptr$list)+1]] &lt;- obj} 
runBenchmark &lt;- function(n) {
    microbenchmark(times = 5,  
        env_with_list_ = {
            listptr &lt;- new.env(parent=globalenv())
            listptr$list &lt;- NULL
            for(i in 1:n) {envAppendList(listptr, i)}
            listptr$list
        },
        c_ = {
            a &lt;- list(0)
            for(i in 1:n) {a = c(a, list(i))}
        },
        list_ = {
            a &lt;- list(0)
            for(i in 1:n) {a &lt;- list(a, list(i))}
        },
        by_index = {
            a &lt;- list(0)
            for(i in 1:n) {a[length(a) + 1] &lt;- i}
            a
        },
        append_ = { 
            a &lt;- list(0)    
            for(i in 1:n) {a &lt;- append(a, i)} 
            a
        },
        env_as_container_ = {
            listptr &lt;- new.env(parent=globalenv())
            for(i in 1:n) {lPtrAppend(listptr, i, i)} 
            listptr
        }   
    )
}
</code></pre>

<p>The results posted by @CronAcronis definitely seem to suggest that the <code>a &lt;- list(a, list(i))</code> method is fastest, at least for a problem size of 10000, but the results for a single problem size do not address the growth of the solution. For that, we need to run a minimum of two profiling tests, with differing problem sizes:</p>

<pre><code>&gt; runBenchmark(2e+3)
Unit: microseconds
              expr       min        lq      mean    median       uq       max neval
    env_with_list_  8712.146  9138.250 10185.533 10257.678 10761.33 12058.264     5
                c_ 13407.657 13413.739 13620.976 13605.696 13790.05 13887.738     5
             list_   854.110   913.407  1064.463   914.167  1301.50  1339.132     5
          by_index 11656.866 11705.140 12182.104 11997.446 12741.70 12809.363     5
           append_ 15986.712 16817.635 17409.391 17458.502 17480.55 19303.560     5
 env_as_container_ 19777.559 20401.702 20589.856 20606.961 20939.56 21223.502     5
&gt; runBenchmark(2e+4)
Unit: milliseconds
              expr         min         lq        mean    median          uq         max neval
    env_with_list_  534.955014  550.57150  550.329366  553.5288  553.955246  558.636313     5
                c_ 1448.014870 1536.78905 1527.104276 1545.6449 1546.462877 1558.609706     5
             list_    8.746356    8.79615    9.162577    8.8315    9.601226    9.837655     5
          by_index  953.989076 1038.47864 1037.859367 1064.3942 1065.291678 1067.143200     5
           append_ 1634.151839 1682.94746 1681.948374 1689.7598 1696.198890 1706.683874     5
 env_as_container_  204.134468  205.35348  208.011525  206.4490  208.279580  215.841129     5
&gt; 
</code></pre>

<p>First of all, a word about the min/lq/mean/median/uq/max values: Since we are performing the exact same task for each of 5 runs, in an ideal world, we could expect that it would take exactly the same amount of time for each run. But the first run is normally biased toward longer times due to the fact that the code we are testing is not yet loaded into the CPU's cache. Following the first run, we would expect the times to be fairly consistent, but occasionally our code may be evicted from the cache due to timer tick interrupts or other hardware interrupts that are unrelated to the code we are testing. By testing the code snippets 5 times, we are allowing the code to be loaded into the cache during the first run and then giving each snippet 4 chances to run to completion without interference from outside events. For this reason, and because we are really running the exact same code under the exact same input conditions each time, we will consider only the 'min' times to be sufficient for the best comparison between the various code options.</p>

<p>Note that I chose to first run with a problem size of 2000 and then 20000, so my problem size increased by a factor of 10 from the first run to the second.</p>

<p><strong>Performance of the <code>list</code> solution: O(1) (constant time)</strong></p>

<p>Let's first look at the growth of the <code>list</code> solution, since we can tell right away that it's the fastest solution in both profiling runs: In the first run, it took 854 <strong>micro</strong>seconds (0.854 <strong>milli</strong>seconds) to perform 2000 ""append"" tasks. In the second run, it took 8.746 milliseconds to perform 20000 ""append"" tasks. A naïve observer would say, <em>""Ah, the <code>list</code> solution exhibits O(n) growth, since as the problem size grew by a factor of ten, so did the time required to execute the test.""</em> The problem with that analysis is that what the OP wants is the growth rate of <strong>a single object insertion</strong>, not the growth rate of the overall problem. Knowing that, it's clear then that the <code>list</code> solution provides exactly what the OP wants: a method of appending objects to a list in O(1) time.</p>

<p><strong>Performance of the other solutions</strong></p>

<p>None of the other solutions come even close to the speed of the <code>list</code> solution, but it is informative to examine them anyway:</p>

<p>Most of the other solutions appear to be O(n) in performance. For example, the <code>by_index</code> solution, a very popular solution based on the frequency with which I find it in other SO posts, took 11.6 milliseconds to append 2000 objects, and 953 milliseconds to append ten times that many objects. The overall problem's time grew by a factor of 100, so a naïve observer might say <em>""Ah, the <code>by_index</code> solution exhibits O(n<sup>2</sup>) growth, since as the problem size grew by a factor of ten, the time required to execute the test grew by a factor of 100.""</em> As before, this analysis is flawed, since the OP is interested in the growth of a single object insertion. If we divide the overall time growth by the problem's size growth, we find that the time growth of appending objects increased by a factor of only 10, not a factor of 100, which matches the growth of the problem size, so the <code>by_index</code> solution is O(n). There are no solutions listed which exhibit O(n<sup>2</sup>) growth for appending a single object.</p>
"
29931698,1217536,2015-04-28T23:29:29Z,2470248,2,FALSE,"<p>To round out the possibilities, you can use <code>writeLines()</code> with <code>sink()</code>, if you want:  </p>

<pre><code>&gt; sink(""tempsink"", type=""output"")
&gt; writeLines(""Hello\nWorld"")
&gt; sink()
&gt; file.show(""tempsink"", delete.file=TRUE)
Hello
World
</code></pre>

<p>To me, it always seems most intuitive to use <code>print()</code>, but if you do that the output won't be what you want:  </p>

<pre><code>...
&gt; print(""Hello\nWorld"")
...
[1] ""Hello\nWorld""
</code></pre>
"
30017804,3097109,2015-05-03T18:35:25Z,2478352,0,FALSE,"<p>I revised a simple function from @mnel, which adds flexibility by using connections. Here is the function:</p>

<pre><code>my.write &lt;- function(x, file, header, f = write.csv, ...){
# create and open the file connection
datafile &lt;- file(file, open = 'wt')
# close on exit 
on.exit(close(datafile))
# if a header is defined, write it to the file (@CarlWitthoft's suggestion)
if(!missing(header)) {
writeLines(header,con=datafile, sep='\t')
writeLines('', con=datafile, sep='\n')
}
# write the file using the defined function and required addition arguments  
f(x, datafile,...)
}
</code></pre>

<p>You can specify the function to be 'write.table', 'write.csv', 'write.delim' etc.</p>

<p>Cheers!</p>
"
30114446,2763246,2015-05-08T01:42:10Z,1716012,4,FALSE,"<p>There is a relatively new package tictoc that replicates the features exactly as you would use them in Matlab.</p>

<p><a href=""http://cran.r-project.org/web/packages/tictoc/index.html"" rel=""nofollow"">http://cran.r-project.org/web/packages/tictoc/index.html</a></p>

<pre><code>## Basic use case
tic()
print(""Do something..."")
Sys.sleep(1)
toc()
# 1.034 sec elapsed
</code></pre>
"
30145350,4883061,2015-05-09T21:46:48Z,2190756,4,FALSE,"<p>Another option is to use summary function. It gives a summary of the Ts, Fs and NAs.</p>

<pre><code>&gt; summary(hival)
   Mode   FALSE    TRUE    NA's 
logical    4367      53    2076 
&gt; 
</code></pre>
"
30210713,392445,2015-05-13T09:26:21Z,2261079,404,FALSE,"<p>As of R 3.2.0 a new function was introduced for removing leading/trailing whitespaces:</p>

<pre><code>trimws()
</code></pre>

<p>See: <a href=""http://stat.ethz.ch/R-manual/R-patched/library/base/html/trimws.html"">http://stat.ethz.ch/R-manual/R-patched/library/base/html/trimws.html</a></p>

<p>(now the only issue is getting on top as the best answer... :) )</p>
"
30217650,2591234,2015-05-13T14:20:35Z,1395233,0,FALSE,"<p>You can use a <code>substitute</code>, <code>eval</code> combintation. </p>

<pre><code>foo &lt;- function(x, y = min(m)) {
  y &lt;- substitute(y)
  m &lt;- 1:10
  x + eval(y)
}

foo(1)
## [1] 2
foo(1, y = max(m))
## [1] 11
</code></pre>
"
30220130,819544,2015-05-13T16:09:14Z,2182337,1,FALSE,"<p>The accepted answer gives bad advice which leaves your application vulnerable to SQL injection. You should always use bind variables instead of concatenating values directly into your query. Use the <code>dbGetPreparedQUery</code> method as described in this answer: <a href=""https://stackoverflow.com/questions/2186015/bind-variables-in-r-dbi"">Bind variables in R DBI</a></p>
"
30259691,4903853,2015-05-15T12:35:40Z,1923273,4,FALSE,"<p>If you want to count the number of appearances subsequently, you can make use of the <code>sapply</code> function:</p>

<pre><code>index&lt;-sapply(1:length(numbers),function(x)sum(numbers[1:x]==numbers[x]))
cbind(numbers, index)
</code></pre>

<p>Output:</p>

<pre><code>        numbers index
 [1,]       4     1
 [2,]      23     1
 [3,]       4     2
 [4,]      23     2
 [5,]       5     1
 [6,]      43     1
 [7,]      54     1
 [8,]      56     1
 [9,]     657     1
[10,]      67     1
[11,]      67     2
[12,]     435     1
[13,]     453     1
[14,]     435     2
[15,]     324     1
[16,]      34     1
[17,]     456     1
[18,]      56     2
[19,]     567     1
[20,]      65     1
[21,]      34     2
[22,]     435     3
</code></pre>
"
30269593,3097865,2015-05-15T22:20:48Z,520810,3,FALSE,"<p>Even simpler:</p>

<pre><code>qw &lt;- function(...){
as.character(substitute(list(...)))[-1]
}
</code></pre>
"
30332804,1332108,2015-05-19T17:55:44Z,1309263,1,FALSE,"<p>It is maybe worth pointing out that there is a special case which has a simple exact solution:  when all the values in the stream are integers within a (relatively) small defined range.  For instance, assume they must all lie between 0 and 1023.  In this case just define an array of 1024 elements and a count, and clear all of these values.  For each value in the stream increment the corresponding bin and the count.  After the stream ends find the bin which contains the count/2 highest value - easily accomplished by adding successive bins starting from 0.  Using the same method the value of an arbitrary rank order may be found.  (There is a minor complication if detecting bin saturation and ""upgrading"" the size of the storage bins to a larger type during a run will be needed.) </p>

<p>This special case may seem artificial, but in practice it is very common.  It can also be applied as an approximation for real numbers if they lie within a range and a ""good enough"" level of precision is known.  This would hold for pretty much any set of measurements on a group of ""real world"" objects.  For instance, the heights or weights of a group of people.  Not a big enough set?  It would work just as well for the lengths or weights of all the (individual) bacteria on the planet - assuming somebody could supply the data!</p>

<p>It looks like I misread the original - which seems like it wants a sliding window median instead of the just the median of a very long stream.  This approach still works for that.  Load the first N stream values for the initial window, then for the N+1th stream value increment the corresponding bin while decrementing the bin corresponding to the 0th stream value.  It is necessary in this case to retain the last N values to allow the decrement, which can be done efficiently by cyclically addressing an array of size N.  Since the position of the median can only change by -2,-1,0,1,2 on each step of the sliding window, it isn't necessary to sum all the bins up to the median on each step, just adjust the ""median pointer"" depending upon which side(s) bins were modified.   For instance, if both the new value and the one being removed fall below the current median then it doesn't change (offset = 0). The method breaks down when N becomes too large to hold conveniently in memory.</p>
"
30392068,1487468,2015-05-22T08:39:07Z,1716012,3,FALSE,"<h1>A Closure Approach</h1>

<p>
A very clean and simple way to do this is by using a closure (which just means having a function within a function):</p>

<pre class=""lang-r prettyprint-override""><code>tic &lt;- function () { 
    now &lt;- proc.time()
    function () { 
        proc.time() - now 
    }
}
</code></pre>

<p>You start the timer like so:</p>

<pre class=""lang-r prettyprint-override""><code>toc &lt;- tic()
</code></pre>

<p>And then you get the time back like this:</p>

<pre class=""lang-r prettyprint-override""><code>toc()
</code></pre>

<p>Which outputs a named vector that gets printed like so:</p>

<pre class=""lang-r prettyprint-override""><code> user  system elapsed 
0.008   0.004   2.055 
</code></pre>

<p>Even with the simplicity of this version you also get all the functionality of the Matlab and Richie Cotton's versions plus the added feature of being able to run multiple timers:</p>

<pre class=""lang-r prettyprint-override""><code>toc1 &lt;- tic()
toc2 &lt;- tic()
</code></pre>
"
30438040,3126969,2015-05-25T12:08:08Z,1195826,3,FALSE,"<p>Very interesting thread, I especially liked idea to just factor subselection again. I had the similar problem before and I just converted to character and then back to factor.</p>

<pre><code>   df &lt;- data.frame(letters=letters[1:5],numbers=seq(1:5))
   levels(df$letters)
   ## [1] ""a"" ""b"" ""c"" ""d"" ""e""
   subdf &lt;- df[df$numbers &lt;= 3]
   subdf$letters&lt;-factor(as.character(subdf$letters))
</code></pre>
"
30454264,2508473,2015-05-26T09:10:15Z,2557863,4,FALSE,"<p>Quite a while, but the 3 functions are implemented in DescTools.</p>

<pre><code>library(DescTools)
# example in: 
# http://support.sas.com/documentation/cdl/en/statugfreq/63124/PDF/default/statugfreq.pdf
# pp. S. 1821
tab &lt;- as.table(rbind(c(26,26,23,18,9),c(6,7,9,14,23)))

# tau-a
KendallTauA(tab, conf.level=0.95)
tau_a    lwr.ci    ups.ci 
0.2068323 0.1771300 0.2365346 

# tau-b
KendallTauB(tab, conf.level=0.95)
    tau_b    lwr.ci    ups.ci 
0.3372567 0.2114009 0.4631126 

# tau-c
&gt; StuartTauC(tab, conf.level=0.95)
     tauc    lwr.ci    ups.ci 
0.4110953 0.2546754 0.5675151 

# alternative for tau-b:
d.frm &lt;- Untable(tab, dimnames = list(1:2, 1:5))
cor(as.numeric(d.frm$Var1), as.numeric(d.frm$Var2),method=""kendall"")
[1] 0.3372567

# but no confidence intervalls for tau-b! Check:
unclass(cor.test(as.numeric(d.frm$Var1), as.numeric(d.frm$Var2), method=""kendall""))
</code></pre>
"
30470623,2322456,2015-05-26T23:22:23Z,2686320,2,FALSE,"<p>@Sharpie 's technique did not work for me, as pandoc failed with error 43 on conversion to pdf. Therefore, here is what I did:</p>

<p>moved the <code>\\centering</code> marker:</p>

<pre><code>names(calqc_table)=c(rep(""\\multicolumn{1}{p{0.75in}}{\\centering Identifier of the Run within the Study}"", 6))
</code></pre>

<p>(here applied to all 6 columns of the table)</p>

<p>and disabled sanitization in xtable printing:</p>

<pre><code>print(calqc_table, sanitize.colnames.function=function(x){x})
</code></pre>
"
30474946,3691003,2015-05-27T06:40:29Z,1815606,1,FALSE,"<p>I had issues with the implementations above as my script is operated from a symlinked directory, or at least that's why I think the above solutions didn't work for me. Along the lines of @ennuikiller's answer, I wrapped my Rscript in bash. I set the path variable using <code>pwd -P</code>, which resolves symlinked directory structures. Then pass the path into the Rscript.</p>

<p><strong><em>Bash.sh</em></strong></p>

<pre><code>#!/bin/bash

# set path variable
path=`pwd -P`

#Run Rscript with path argument
Rscript foo.R $path
</code></pre>

<p><strong><em>foo.R</em></strong></p>

<pre><code>args &lt;- commandArgs(trailingOnly=TRUE)
setwd(args[1])
source(other.R)
</code></pre>
"
30550054,3851145,2015-05-30T18:51:44Z,2557863,0,FALSE,"<p>According to this r-tutor page <a href=""http://www.r-tutor.com/gpu-computing/correlation/kendall-tau-b"" rel=""nofollow"">http://www.r-tutor.com/gpu-computing/correlation/kendall-tau-b</a> tau-b is in fact computed by the base r function. </p>
"
30551944,4594021,2015-05-30T22:21:49Z,2003663,7,FALSE,"<p>Below is an R function that reads in a multiline SQL query (from a text file) and converts it into a single-line string. The function removes formatting and whole-line comments.</p>

<p>To use it, run the code to define the functions, and your single-line string will be the result of running
ONELINEQ(""querytextfile.sql"",""~/path/to/thefile"").  </p>

<p>How it works: Inline comments detail this; it reads each line of the query and deletes (replaces with nothing) whatever isn't needed to write out a single-line version of the query (as asked for in the question). The result is a list of lines, some of which are blank and get filtered out; the last step is to paste this (unlisted) list together and return the single line. </p>

#

<pre><code># This set of functions allows us to read in formatted, commented SQL queries
# Comments must be entire-line comments, not on same line as SQL code, and begun with ""--""
# The parsing function, to be applied to each line:
LINECLEAN &lt;- function(x) {
  x = gsub(""\t+"", """", x, perl=TRUE); # remove all tabs
  x = gsub(""^\\s+"", """", x, perl=TRUE); # remove leading whitespace
  x = gsub(""\\s+$"", """", x, perl=TRUE); # remove trailing whitespace
  x = gsub(""[ ]+"", "" "", x, perl=TRUE); # collapse multiple spaces to a single space
  x = gsub(""^[--]+.*$"", """", x, perl=TRUE); # destroy any comments
  return(x)
}
# PRETTYQUERY is the filename of your formatted query in quotes, eg ""myquery.sql""
# DIRPATH is the path to that file, eg ""~/Documents/queries""
ONELINEQ &lt;- function(PRETTYQUERY,DIRPATH) { 
  A &lt;- readLines(paste0(DIRPATH,""/"",PRETTYQUERY)) # read in the query to a list of lines
  B &lt;- lapply(A,LINECLEAN) # process each line
  C &lt;- Filter(function(x) x != """",B) # remove blank and/or comment lines
  D &lt;- paste(unlist(C),collapse="" "") # paste lines together into one-line string, spaces between.
  return(D)
}
# TODO: add eof newline automatically to remove warning
#############################################################################################
</code></pre>
"
30625115,3917,2015-06-03T15:58:08Z,1172485,5,FALSE,"<p>I use this:</p>

<pre><code>wideScreen &lt;- function(howWide=as.numeric(strsplit(system('stty size', intern=T), ' ')[[1]])[2]) {
   options(width=as.integer(howWide))
}
</code></pre>

<p>Because the <code>COLUMNS</code> environment variable, and <code>tset</code>, aren't updated when the window is resized, but <code>stty size</code> is.</p>
"
30632007,2741380,2015-06-03T22:35:14Z,2196985,1,FALSE,"<p>A few years later, there are options available for dashboards and layouts with R.</p>

<ul>
<li>For making grids in R, ggplot2's <a href=""http://docs.ggplot2.org/0.9.3.1/facet_wrap.html"" rel=""nofollow noreferrer"">facet_wrap</a> and <a href=""http://docs.ggplot2.org/0.9.3.1/facet_grid.html"" rel=""nofollow noreferrer"">facet_grid</a> are excellent.</li>
<li><a href=""http://shiny.rstudio.com/"" rel=""nofollow noreferrer"">Shiny</a> allows you to make web apps and dashboards with R. Shiny handles the HTML, CSS, and JavaScript for you. It's on <a href=""http://cran.r-project.org/web/packages/shiny/index.html"" rel=""nofollow noreferrer"">CRAN</a>. </li>
<li>To use ggplot2 to make an interactive Shiny dashboard with d3.js, you can connect to Plotly's <a href=""https://plot.ly/ggplot2"" rel=""nofollow noreferrer"">ggplot2 figure converter</a>. Here is <a href=""http://moderndata.plot.ly/dashboards-in-r-with-shiny-plotly/"" rel=""nofollow noreferrer"">a tutorial</a> with code examples. Your published apps let you zoom, toggle, filter, pan, and see data on the hover, e.g.:
<br>
<br>
<img src=""https://i.stack.imgur.com/NfwKG.gif"" alt=""enter image description here"">
<br>
<br>
Disclaimer: I'm on the Plotly team.</li>
</ul>
"
30714805,4280826,2015-06-08T16:46:42Z,2470248,9,FALSE,"<p>You could do that in a single statement</p>

<pre><code>cat(""hello"",""world"",file=""output.txt"",sep=""\n"",append=TRUE)
</code></pre>
"
30759423,2035799,2015-06-10T14:31:31Z,652136,4,FALSE,"<p>Don't know if you still need an answer to this but I found from my limited (3 weeks worth of self teaching R) experience with R that, using the NULL assignment is actually wrong or sub-optimal especially if you're dynamically updating a list in something like a for-loop.</p>

<p>To be more precise, using
myList[[5]] &lt;- NULL
will throw the error ""myList[[5]] &lt;- NULL : replacement has length zero"" or ""more elements supplied there there are to replace"".</p>

<p>What I found to work more consistently is 
myList &lt;- myList[[-5]];</p>
"
30787000,5000206,2015-06-11T16:59:49Z,1395105,21,FALSE,"<p><a href=""https://github.com/stefano-meschiari/latex2exp"">This script</a> contains a function <code>latex2exp</code> to approximately translate LaTeX formulas to R's plotmath expressions. You can use it anywhere you could enter mathematical annotations, such as axis labels, legend labels, and general text.</p>

<p>For example:</p>

<pre><code>x &lt;- seq(0, 4, length.out=100)
alpha &lt;- 1:5
plot(x, xlim=c(0, 4), ylim=c(0, 10), xlab='x', ylab=latex2exp('\\alpha
     x^\\alpha\\text{, where }\\alpha \\in \\text{1:5}'), type='n')
for (a in alpha)
    lines(x, a*x^a, col=a)
legend('topleft', legend=latex2exp(sprintf(""\\alpha = %d"", alpha)), lwd=1, col=alpha)
</code></pre>

<p>produces <a href=""http://i.stack.imgur.com/0eBkt.png"">this plot</a>.</p>
"
30848434,2092205,2015-06-15T14:58:15Z,2628621,2,FALSE,"<p>On this subject I'd like to point out that the &lt;&lt; operator will behave strangely when applied (incorrectly) within a for loop (there may be other cases too). Given the following code:</p>

<p><code>fortest &lt;- function() {
    mySum &lt;- 0
    for (i in c(1, 2, 3)) {
        mySum &lt;&lt;- mySum + i
    }
    mySum
}</code></p>

<p>you might expect that the function would return the expected sum, 6, but instead it returns 0, with a global variable <code>mySum</code> being created and assigned the value 3. I can't fully explain what is going on here but certainly the body of a for loop is <strong>not</strong> a new scope 'level'. Instead, it seems that R looks outside of the <code>fortest</code> function, can't find a <code>mySum</code> variable to assign to, so creates one and assigns the value 1, the first time through the loop. On subsequent iterations, the RHS in the assignment must be referring to the (unchanged) inner <code>mySum</code> variable whereas the LHS refers to the global variable. Therefore each iteration overwrites the value of the global variable to that iteration's value of <code>i</code>, hence it has the value 3 on exit from the function.</p>

<p>Hope this helps someone - this stumped me for a couple of hours today! (BTW, just replace &lt;&lt; with &lt; and the function works as expected).</p>
"
30848758,3216713,2015-06-15T15:12:24Z,2628621,2,FALSE,"<p>The <code>&lt;&lt;-</code> operator can also be useful for <a href=""https://stat.ethz.ch/R-manual/R-devel/library/methods/html/refClass.html"" rel=""nofollow"">Reference Classes when writing Reference Methods</a>. For example:</p>

<pre><code>myRFclass &lt;- setRefClass(Class = ""RF"",
                         fields = list(A = ""numeric"",
                                       B = ""numeric"",
                                       C = function() A + B))
myRFclass$methods(show = function() cat(""A ="", A, ""B ="", B, ""C ="",C))
myRFclass$methods(changeA = function() A &lt;&lt;- A*B) # note the &lt;&lt;-
obj1 &lt;- myRFclass(A = 2, B = 3)
obj1
# A = 2 B = 3 C = 5
obj1$changeA()
obj1
# A = 6 B = 3 C = 9
</code></pre>
"
30899262,5021015,2015-06-17T18:15:01Z,2619618,2,FALSE,"<p>This expands upon Shane's solution of using rbind() but also adds columns identifying groups and removes NULL groups - two features which were requested in the question. By using base package functions, no other dependencies are required, e.g., plyr.</p>

<pre><code>simplify_by_output = function(by_output) {
    null_ind = unlist(lapply(by_output, is.null))  # by() returns NULL for combinations of grouping variables for which there are no data. rbind() ignores those, so you have to keep track of them.
    by_df = do.call(rbind, by_output)  # Combine the results into a data frame.
    return(cbind(expand.grid(dimnames(by_output))[!null_ind, ], by_df))  # Add columns identifying groups, discarding names of groups for which no data exist.
}
</code></pre>
"
31123140,856624,2015-06-29T18:41:20Z,1311920,1,FALSE,"<p>The method that works best for me
is to use the <code>lag</code> function from the <code>dplyr</code> package.</p>

<p>Example:</p>

<pre><code>&gt; require(dplyr)
&gt; lag(1:10, 1)
 [1] NA  1  2  3  4  5  6  7  8  9
&gt; lag(1:10, 2)
 [1] NA NA  1  2  3  4  5  6  7  8
</code></pre>
"
31167238,5070595,2015-07-01T16:47:37Z,652136,-1,FALSE,"<p>How about this? Again, using indices</p>

<pre><code>&gt; m &lt;- c(1:5)
&gt; m
[1] 1 2 3 4 5

&gt; m[1:length(m)-1]
[1] 1 2 3 4
</code></pre>

<p>or</p>

<pre><code>&gt; m[-(length(m))]
[1] 1 2 3 4
</code></pre>
"
31223588,4975218,2015-07-04T17:53:48Z,1249548,53,FALSE,"<p>I've never liked the solutions based on <code>grid.arrange</code>, because they make it difficult to label the plots with letters (A, B, etc.), as most journals require. It's also difficult to align plots with <code>grid.arrange</code>.</p>

<p>I wrote the <a href=""http://cran.r-project.org/web/packages/cowplot/index.html"" rel=""noreferrer"">cowplot</a> package to solve these issues, specifically the function <code>plot_grid()</code>: </p>

<pre><code>require(cowplot)
plot1 &lt;- qplot(1)
plot2 &lt;- qplot(1)
plot_grid(plot1, plot2, align='h', labels=c('A', 'B'))
</code></pre>

<p>For a more in-depth description of <code>plot_grid()</code>, see <a href=""http://cran.r-project.org/web/packages/cowplot/vignettes/plot_grid.html"" rel=""noreferrer"">this vignette.</a></p>
"
31227890,4794438,2015-07-05T06:56:07Z,1716012,2,FALSE,"<p>No, but here is a one line solution.</p>

<pre><code>time.it&lt;-function(f) { a&lt;-proc.time(); out&lt;-f(); print(proc.time()-a); out }
</code></pre>

<p>And an example for usage:</p>

<pre><code>result&lt;-time.it(function(){ A&lt;-matrix(runif(5000^2),nrow=5000); b&lt;-runif(5000); solve(A,b) } )
user  system elapsed 
12.788  12.268   8.623 
</code></pre>

<p>Otherwise, <a href=""http://cran.r-project.org/web/packages/microbenchmark/index.html"" rel=""nofollow"">microbenchmark</a> is my favorite in terms of packages.</p>
"
31298923,181638,2015-07-08T16:53:00Z,1105659,5,FALSE,"<p>The <code>setNames()</code> built-in function makes it easy to create a hash from given key and value lists.  <em>(Thanks to Nick K for the better suggestion.)</em></p>

<p>Usage: <code>hh &lt;- setNames(as.list(values), keys)</code></p>

<p>Example:</p>

<pre><code>players &lt;- c(""bob"", ""tom"", ""tim"", ""tony"", ""tiny"", ""hubert"", ""herbert"")
rankings &lt;- c(0.2027, 0.2187, 0.0378, 0.3334, 0.0161, 0.0555, 0.1357)
league &lt;- setNames(as.list(rankings), players)
</code></pre>

<p>Then accessing the values through the keys is easy:</p>

<blockquote>
<pre><code>league$bob
 [1] 0.2027
league$hubert
 [1] 0.0555
</code></pre>
</blockquote>
"
31312205,4011719,2015-07-09T08:33:29Z,1535086,0,FALSE,"<p>You should try ""xset fp rehash"" to tell X to rescan and rehash installed fonts. (worked for me)</p>
"
31407655,2831559,2015-07-14T13:12:18Z,1699046,8,FALSE,"<p>I was curious about the time performance of the non-vectorised options.
For this purpose, I have used the function f defined by knguyen</p>

<pre><code>f &lt;- function(x, output) {
  wellName &lt;- x[1]
  plateName &lt;- x[2]
  wellID &lt;- 1
  print(paste(wellID, x[3], x[4], sep="",""))
  cat(paste(wellID, x[3], x[4], sep="",""), file= output, append = T, fill = T)
}
</code></pre>

<p>and a dataframe like the one in his example:</p>

<pre><code>n = 100; #number of rows for the data frame
d &lt;- data.frame( name = LETTERS[ sample.int( 25, n, replace=T ) ],
                  plate = paste0( ""P"", 1:n ),
                  value1 = 1:n,
                  value2 = (1:n)*10 )
</code></pre>

<p>I included two vectorised functions (for sure quicker than the others) in order to compare the cat() approach with a write.table() one...</p>

<pre><code>library(""ggplot2"")
library( ""microbenchmark"" )
library( foreach )
library( iterators )

tm &lt;- microbenchmark(S1 =
                       apply(d, 1, f, output = 'outputfile1'),
                     S2 = 
                       for(i in 1:nrow(d)) {
                         row &lt;- d[i,]
                         # do stuff with row
                         f(row, 'outputfile2')
                       },
                     S3 = 
                       foreach(d1=iter(d, by='row'), .combine=rbind) %dopar% f(d1,""outputfile3""),
                     S4= {
                       print( paste(wellID=rep(1,n), d[,3], d[,4], sep="","") )
                       cat( paste(wellID=rep(1,n), d[,3], d[,4], sep="",""), file= 'outputfile4', sep='\n',append=T, fill = F)                           
                     },
                     S5 = {
                       print( (paste(wellID=rep(1,n), d[,3], d[,4], sep="","")) )
                       write.table(data.frame(rep(1,n), d[,3], d[,4]), file='outputfile5', row.names=F, col.names=F, sep="","", append=T )
                     },
                     times=100L)
autoplot(tm)
</code></pre>

<p>The resulting image shows that apply gives the best performance for a non-vectorised version, whereas write.table() seems to outperform cat().
<img src=""https://i.stack.imgur.com/hHQ3M.png"" alt=""ForEachRunningTime""></p>
"
31428875,4836511,2015-07-15T11:14:54Z,1195826,9,FALSE,"<p>Another way of doing the same but with <code>dplyr</code></p>

<pre><code>library(dplyr)
subdf &lt;- df %&gt;% filter(numbers &lt;= 3) %&gt;% droplevels()
str(subdf)
</code></pre>

<p><strong>Edit:</strong> </p>

<p>Also Works ! Thanks to <a href=""https://stackoverflow.com/users/3871924/agenis"">agenis</a></p>

<pre><code>subdf &lt;- df %&gt;% filter(numbers &lt;= 3) %&gt;% droplevels
levels(subdf$letters)
</code></pre>
"
31432851,1873521,2015-07-15T14:12:13Z,2288485,7,FALSE,"<p>If you run into problems with:</p>

<pre><code>as.numeric(as.character(dat$x))
</code></pre>

<p>Take a look to your decimal marks. If they are "","" instead of ""."" (e.g. ""5,3"") the above won't work.</p>

<p>A potential solution is:</p>

<pre><code>as.numeric(gsub("","", ""."", dat$x))
</code></pre>

<p>I believe this is quite common in some non English speaking countries.</p>
"
31539675,5139211,2015-07-21T12:57:44Z,2643939,0,FALSE,"<p>I hope this may also help. It could be made into a single command, but I found it easier for me to read by dividing it in two commands. I made a function with the following instruction and worked lightning fast.</p>

<p><code>naColsRemoval = function (DataTable) {
     na.cols = DataTable [ , .( which ( apply ( is.na ( .SD ) , 2 , all ) ) )]
     DataTable [ , unlist (na.cols) := NULL , with = F]
     }</code></p>

<p>.SD will allow to limit the verification to part of the table, if you wish, but it will take the whole table as</p>
"
31586042,1457051,2015-07-23T11:28:20Z,2351744,5,FALSE,"<p>For further completeness, there's:</p>

<ul>
<li><code>stringi::stri_wrap</code></li>
<li><code>stringr::str_wrap</code> (which just ultimately calls <code>stringi::stri_wrap</code></li>
</ul>

<p>The <code>stringi</code> version will deal with character sets better (it's built on the ICU library) and it's in C/C++ so it'll ultimately be faster than <code>base::strwrap</code>. It's also vectorized over the <code>str</code> parameter.</p>
"
31599402,4721576,2015-07-23T22:53:48Z,1219480,1,FALSE,"<p>This <a href=""https://cran.r-project.org/web/packages/mediation/vignettes/mediation.pdf"" rel=""nofollow"">vignette</a> is what you are looking for, if the above answer isn't enough.</p>

<p>It's about as hand-holdy as you can get with this sort of thing.</p>
"
31642376,1100107,2015-07-26T22:01:42Z,1897704,4,FALSE,"<p>For 2D-vectors, the way given in the accepted answer and other ones does not take into account the orientation (the sign) of the angle (<code>angle(M,N)</code> is the same as <code>angle(N,M)</code>) and it returns a correct value only for an angle between <code>0</code> and <code>pi</code>.</p>

<p>Use the <code>atan2</code> function to get an oriented angle and a correct value (modulo <code>2pi</code>).</p>

<pre><code>angle &lt;- function(M,N){
     acos( sum(M*N) / ( sqrt(sum(M*M)) * sqrt(sum(N*N)) ) )
   }
angle2 &lt;- function(M,N){
  atan2(N[2],N[1]) - atan2(M[2],M[1]) 
}
</code></pre>

<p>Check that <code>angle2</code> gives the correct value:</p>

<pre><code>&gt; theta &lt;- seq(-2*pi, 2*pi, length.out=10)
&gt; O &lt;- c(1,0)
&gt; test1 &lt;- sapply(theta, function(theta) angle(M=O, N=c(cos(theta),sin(theta))))
&gt; all.equal(test1 %% (2*pi), theta %% (2*pi))
[1] ""Mean relative difference: 1""
&gt; test2 &lt;- sapply(theta, function(theta) angle2(M=O, N=c(cos(theta),sin(theta))))
&gt; all.equal(test2 %% (2*pi), theta %% (2*pi))
[1] TRUE
</code></pre>
"
31680856,4589639,2015-07-28T15:29:57Z,2557863,3,FALSE,"<p>Doug's answer here is incorrect. <strong>Package Kendall can be used to calculate Tau b.</strong> </p>

<p>The Kendall package function Kendall (and it would also seem cor(x,y,method=""kendall"")) calculate ties using the formula for Tau-b. However, for vectors with ties, the Kendall package has the more correct p-value. See page 4 of the documentation for Kendall, from <a href=""https://cran.r-project.org/web/packages/Kendall/Kendall.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/Kendall/Kendall.pdf</a> page 4, with D referencing the denominator of the Kendall calculation:</p>

<blockquote>
  <p>and D = n(n − 1)/2. S is called the score and D, the denominator, is the maximum possible value of S. When there are ties, the formula for D is more complicated (Kendall, 1974, Ch. 3) and this general forumla for ties in both reankings is implemented in our function.The p-value of tau under the null hypothesis of no association is computed by in the case of no ties using an exact algorithm given by Best and Gipps (1974). When ties are present, a normal approximation with continuity correction is used by taking S as normally distributed with mean zero and variance var(S), where var(S) is given byKendall (1976, eqn 4.4, p.55). Unless ties are very extensive and/or the data is very short, this approximation is adequate. If extensive ties are present then the bootstrap provides an expedient solution (Davis and Hinkley, 1997). Alternatively an exact method based on exhaustive enumeration is also available (Valz and Thompson, 1994) but this is not implemented in this package.</p>
</blockquote>

<p>I originally made an edit to Doug's answer regarding this, but it was rejected for 'being directed at the author and more appropriate as an answer or a comment'. I would have left this as a comment on the answer, but my reputation is not yet high enough to comment.</p>
"
31709168,3063910,2015-07-29T19:11:52Z,1676990,1,FALSE,"<p>Another option is <code>scan()</code>. To get the second value, we can use a logical subset.</p>

<pre><code>scan(text = tmp3)[c(FALSE, TRUE)]
#  [1] 2 1 2 1 2 1 2 1 2 1
</code></pre>
"
31869522,1172002,2015-08-07T04:03:34Z,1296646,2,FALSE,"<p>For the sake of completeness: you can also use the <code>sortByCol()</code> function from the <code>BBmisc</code> package:</p>

<pre><code>library(BBmisc)
sortByCol(dd, c(""z"", ""b""), asc = c(FALSE, TRUE))
    b x y z
4 Low C 9 2
2 Med D 3 1
1  Hi A 8 1
3  Hi A 9 1
</code></pre>

<p>Performance comparison:</p>

<pre><code>library(microbenchmark)
microbenchmark(sortByCol(dd, c(""z"", ""b""), asc = c(FALSE, TRUE)), times = 100000)
median 202.878

library(plyr)
microbenchmark(arrange(dd,desc(z),b),times=100000)
median 148.758

microbenchmark(dd[with(dd, order(-z, b)), ], times = 100000)
median 115.872
</code></pre>
"
31974751,3006624,2015-08-12T20:26:21Z,743812,16,FALSE,"<p>Using <code>cumsum</code> should be sufficient and efficient. Assuming you have a vector <em>x</em> and you want a running sum of <em>n</em> numbers</p>

<pre><code>cx &lt;- cumsum(x)
rsum &lt;- (cx[(n+1):length(x)] - cx[1:(length(x) - n)]) / n
</code></pre>
"
31986610,1289801,2015-08-13T11:13:59Z,2261079,2,FALSE,"<p>Another related problem occurs if you have multiple spaces inbetween inputs:</p>

<pre><code>&gt; a &lt;- ""  a string         with lots   of starting, inter   mediate and trailing   whitespace     ""
</code></pre>

<p>You can then easily split this string into ""real"" tokens using a regular expression to the <code>split</code> argument:</p>

<pre><code>&gt; strsplit(a, split="" +"")
[[1]]
 [1] """"           ""a""          ""string""     ""with""       ""lots""      
 [6] ""of""         ""starting,""  ""inter""      ""mediate""    ""and""       
[11] ""trailing""   ""whitespace""
</code></pre>

<p>Note that if there is a match at the beginning of
a (non-empty) string, the first element of the output is ‘""""’, but
if there is a match at the end of the string, the output is the
same as with the match removed.</p>
"
32016824,4535465,2015-08-14T18:46:17Z,1815606,5,FALSE,"<p>I liked steamer25's solution as it seems the most robust for my purposes. However, when debugging in RStudio (in windows), the path would not get set properly. The reason being that if a breakpoint is set in RStudio, sourcing the file uses an alternate ""debug source"" command which sets the script path a little differently. Here is the final version which I am currently using which accounts for this alternate behavior within RStudio when debugging: </p>

<pre><code># @return full path to this script
get_script_path &lt;- function() {
    cmdArgs = commandArgs(trailingOnly = FALSE)
    needle = ""--file=""
    match = grep(needle, cmdArgs)
    if (length(match) &gt; 0) {
        # Rscript
        return(normalizePath(sub(needle, """", cmdArgs[match])))
    } else {
        ls_vars = ls(sys.frames()[[1]])
        if (""fileName"" %in% ls_vars) {
            # Source'd via RStudio
            return(normalizePath(sys.frames()[[1]]$fileName)) 
        } else {
            # Source'd via R console
            return(normalizePath(sys.frames()[[1]]$ofile))
        }
    }
}
</code></pre>
"
32097810,3846421,2015-08-19T14:05:56Z,1402001,1,FALSE,"<p>From everything I can find, there is NO solution for bulk insert to MySQL and nothing that works with SSIS which is why Microsoft is including in-database analytics with SQL Server 2016 after buying Revolution R Analytics.</p>

<p>I tried to comment on the previous answer but don't have the reputation to do it.</p>

<p>The <code>rsqlserver</code> package needs to run with <code>rClr</code> and neither of those packages are well-behaved, especially because <code>rsqlserver</code>'s INSERT functions have poor data type handling. So if you use it, you'll have no idea what you're looking at in the SQL table as much of the information in your data.frame will have been transformed.</p>

<p>Considering the <code>RODBC</code> package has been around for 15 years, I'm pretty disappointed that no one has created a bulk insert function... </p>
"
32108585,4346952,2015-08-20T02:23:18Z,1576201,0,FALSE,"<p>Using the argument <code>arr.ind=TRUE</code> with <code>which</code> is a great way for finding the row (or column) numbers where a condition is <code>TRUE</code>,</p>

<pre><code>df &lt;- matrix(c(0.6,0.2,0.1,0.25,0.11,0.13,0.23,0.18,0.21,0.29,0.23,0.51), nrow=4)

#      [,1] [,2] [,3]
# [1,] 0.60 0.11 0.21
# [2,] 0.20 0.13 0.29
# [3,] 0.10 0.23 0.23
# [4,] 0.25 0.18 0.51
</code></pre>

<p><code>which</code> with <code>arr.ind=TRUE</code> returns the array indices where the condition is <code>TRUE</code></p>

<pre><code>which(df &gt; 0.5, arr.ind=TRUE)
     row col
[1,]   1   1
[2,]   4   3
</code></pre>

<p>so the subset becomes</p>

<pre><code>df[-which(df &gt; 0.5, arr.ind=TRUE)[, ""row""], ]

#      [,1] [,2] [,3]
# [1,]  0.2 0.13 0.29
# [2,]  0.1 0.23 0.23
</code></pre>
"
32122548,2081427,2015-08-20T15:38:23Z,2710442,0,FALSE,"<p>Have a look at <a href=""https://www.gnu.org/software/emacs/manual/html_node/emacs/Auto-Scrolling.html#Auto-Scrolling"" rel=""nofollow"">auto-scrolling</a> in emacs doc. For my part:</p>

<p><code>customize-variable RET scroll-down-aggressively RET</code></p>

<p>scroll-down-aggressively set to 1 did the job.</p>
"
32126197,2788076,2015-08-20T18:56:42Z,1395301,0,FALSE,"<p>To set the R work directory like the current directory of the R script that I'm working, I always use a combination of the commands <code>getwd()</code> and <code>setwd()</code>, like this:</p>

<p><code>path &lt;- getwd()
setwd(path)</code></p>

<p>or</p>

<p><code>setwd(getwd())</code></p>

<p>If you want learn more about it, see this <a href=""http://rfunction.com/archives/1001"" rel=""nofollow"">article</a>.</p>

<p>Cheers,</p>

<p>[]'s</p>
"
32223455,1720405,2015-08-26T09:57:46Z,1299871,3,FALSE,"<ol>
<li>Using Merge function we can select the variable of left table or right table, same way like we all familiar with select statement in SQL (EX : Select a.* ...or Select b.* from .....)</li>
<li><p>We have to add extra code which will subset from the newly joined table .</p>

<ul>
<li><p><strong>SQL :- select a.* from df1 a inner join df2 b on<br>
a.CustomerId=b.CustomerId</strong></p></li>
<li><p><strong>R :- merge(df1, df2, by.x = ""CustomerId"", by.y =<br>
""CustomerId"")[,names(df1)]</strong></p></li>
</ul></li>
</ol>

<p>Same way </p>

<ul>
<li><p><strong>SQL :- select b.* from df1 a inner join df2 b on<br>
a.CustomerId=b.CustomerId</strong></p></li>
<li><p><strong>R :- merge(df1, df2, by.x = ""CustomerId"", by.y =
""CustomerId"")[,names(df2)]</strong></p></li>
</ul>
"
32257168,1186143,2015-08-27T18:44:18Z,1699046,12,FALSE,"<p>I use this simple utility function:</p>

<pre><code>rows = function(tab) lapply(
  seq_len(nrow(tab)),
  function(i) unclass(tab[i,,drop=F])
)
</code></pre>

<p>Or a faster, less clear form:</p>

<pre><code>rows = function(x) lapply(seq_len(nrow(x)), function(i) lapply(x,""["",i))
</code></pre>

<p>This function just splits a data.frame to a list of rows. Then you can make a normal ""for"" over this list:</p>

<pre><code>tab = data.frame(x = 1:3, y=2:4, z=3:5)
for (A in rows(tab)) {
    print(A$x + A$y * A$z)
}        
</code></pre>

<p>Your code from the question will work with a minimal modification:</p>

<pre><code>for (well in rows(dataFrame)) {
  wellName &lt;- well$name    # string like ""H1""
  plateName &lt;- well$plate  # string like ""plate67""
  wellID &lt;- getWellID(wellName, plateName)
  cat(paste(wellID, well$value1, well$value2, sep="",""), file=outputFile)
}
</code></pre>
"
32354427,1156498,2015-09-02T13:23:13Z,1815606,1,FALSE,"<p>I would use a variant of @steamer25 's approach. The point is that I prefer to obtain the last sourced script even when my session was started through Rscript. The following snippet, when included on a file, will provided a variable <code>thisScript</code> containing the normalized path of the script.
I confess the (ab)use of source'ing, so sometimes I invoke Rscript and the script provided in the <code>--file</code> argument sources another script that sources another one... Someday I will invest in making my messy code turns into a package.</p>

<pre><code>thisScript &lt;- (function() {
  lastScriptSourced &lt;- tail(unlist(lapply(sys.frames(), function(env) env$ofile)), 1)

  if (is.null(lastScriptSourced)) {
    # No script sourced, checking invocation through Rscript
    cmdArgs &lt;- commandArgs(trailingOnly = FALSE)
    needle &lt;- ""--file=""
    match &lt;- grep(needle, cmdArgs)
    if (length(match) &gt; 0) {
      return(normalizePath(sub(needle, """", cmdArgs[match]), winslash=.Platform$file.sep, mustWork=TRUE))
    }
  } else {
    # 'source'd via R console
    return(normalizePath(lastScriptSourced, winslash=.Platform$file.sep, mustWork=TRUE))
  }
})()
</code></pre>
"
32421846,4998352,2015-09-06T09:09:18Z,2547402,3,FALSE,"<p>Here is a function to find the mode:</p>

<pre><code>mode &lt;- function(x) {
  unique_val &lt;- unique(x)
  counts &lt;- vector()
  for (i in 1:length(unique_val)) {
    counts[i] &lt;- length(which(x==unique_val[i]))
  }
  position &lt;- c(which(counts==max(counts)))
  if (mean(counts)==max(counts)) 
    mode_x &lt;- 'Mode does not exist'
  else 
    mode_x &lt;- unique_val[position]
  return(mode_x)
}
</code></pre>
"
32446970,3162788,2015-09-07T22:56:02Z,2129952,2,FALSE,"<p>As the accepted solution of @Shane is not supported in RStudio (see <a href=""https://support.rstudio.com/hc/communities/public/questions/200847056-controlling-graphics-device-size-creating-new-window"" rel=""nofollow"">here</a>) as of now (Sep 2015), I would like to add an advice to @James Thompson answer regarding workflow:</p>

<p>If you use <a href=""http://www.sumatrapdfreader.org/free-pdf-reader.html"" rel=""nofollow"">SumatraPDF</a> as viewer you do not need to close the PDF file before making changes to it. Sumatra does not put a opened file in read-only and thus does not prevent it from being overwritten. Therefore, once you opened your PDF file with Sumatra, changes out of RStudio (or any other R IDE) are immediately displayed in Sumatra.</p>
"
32503431,3001626,2015-09-10T13:36:01Z,1660124,10,FALSE,"<p>Several years later, just to add another simple base R solution that isn't present here for some reason- <code>xtabs</code></p>

<pre><code>xtabs(Frequency ~ Category, df)
# Category
# First Second  Third 
#    30      5     34 
</code></pre>

<p>Or if want a <code>data.frame</code> back</p>

<pre><code>as.data.frame(xtabs(Frequency ~ Category, df))
#   Category Freq
# 1    First   30
# 2   Second    5
# 3    Third   34
</code></pre>
"
32510333,4934536,2015-09-10T19:42:18Z,77434,5,FALSE,"<p>Whats about</p>

<pre><code>&gt; a &lt;- c(1:100,555)
&gt; a[NROW(a)]
[1] 555
</code></pre>
"
32638391,3609772,2015-09-17T19:21:43Z,1511431,0,FALSE,"<p>Because the accepted answer uses a deprecated feature I'll point out this alternate answer that works for <code>ggplot2 1.0.1</code></p>

<p><a href=""https://stackoverflow.com/questions/17279027/ggplot2-visualizing-counts-of-points-plotted-on-top-of-each-other-stat-bin2d-or"">ggplot2 visualizing counts of points plotted on top of each other: stat_bin2d or geom_tile or point size?</a></p>
"
32641571,534238,2015-09-17T23:31:30Z,1523126,9,FALSE,"<p>This question is several years old, but I stumbled upon it, which means maybe others will.</p>

<p>The <a href=""https://github.com/hadley/readr"" rel=""noreferrer""><code>readr</code></a> library / package has some nice features to it.  One of them is a nice way to interpret ""messy"" columns, like these.</p>

<pre><code>library(readr)
read_csv(""numbers\n800\n\""1,800\""\n\""3500\""\n6.5"",
          col_types = list(col_numeric())
        )
</code></pre>

<p>This yields</p>

<p>Source: local data frame [4 x 1]</p>

<pre><code>  numbers
    (dbl)
1   800.0
2  1800.0
3  3500.0
4     6.5
</code></pre>

<hr>

<p>An important point when reading in files:  you either have to pre-process, like the comment above regarding <code>sed</code>, or you have to process <strong>while reading</strong>.  Often, if you try to fix things after the fact, there are some dangerous assumptions made that are hard to find.  (Which is why flat files are so evil in the first place.)</p>

<p>For instance, if I had not flagged the <code>col_types</code>, I would have gotten this:</p>

<pre><code>&gt; read_csv(""numbers\n800\n\""1,800\""\n\""3500\""\n6.5"")
Source: local data frame [4 x 1]

  numbers
    (chr)
1     800
2   1,800
3    3500
4     6.5
</code></pre>

<p>(Notice that it is now a <code>chr</code> (<code>character</code>) instead of a <code>numeric</code>.)</p>

<p>Or, more dangerously, if it were long enough and most of the early elements did not contain commas:</p>

<pre><code>&gt; set.seed(1)
&gt; tmp &lt;- as.character(sample(c(1:10), 100, replace=TRUE))
&gt; tmp &lt;- c(tmp, ""1,003"")
&gt; tmp &lt;- paste(tmp, collapse=""\""\n\"""")
</code></pre>

<p>(such that the last few elements look like:)</p>

<pre><code>\""5\""\n\""9\""\n\""7\""\n\""1,003""
</code></pre>

<p>Then you'll find trouble reading that comma at all!</p>

<pre><code>&gt; tail(read_csv(tmp))
Source: local data frame [6 x 1]

     3""
  (dbl)
1 8.000
2 5.000
3 5.000
4 9.000
5 7.000
6 1.003
Warning message:
1 problems parsing literal data. See problems(...) for more details. 
</code></pre>
"
32756543,5150029,2015-09-24T08:16:32Z,2667673,6,FALSE,"<p>For at DataFrame one can simply type</p>

<pre><code>head(data, num=10L)
</code></pre>

<p>to get the first 10 for example.</p>

<p>For a data.frame one can simply type</p>

<pre><code>head(data, 10)
</code></pre>

<p>to get the first 10. </p>
"
32810902,5382156,2015-09-27T17:43:26Z,1231195,2,FALSE,"<p>I use vim to edit the R script. </p>

<p>Let's say the R script is test.R, containing say ""Line 1"", ""Line 2"", and ""Line 3"" on 3 separate lines.</p>

<p>I open test.R on the command line with Vim by typing ""vim test.R"".
Then I go to the 1st line I want to comment out, type ""Control-V"", down arrow to the last line I want to comment out, type a capital I i.e. ""I"" for insert, type ""# "", and then hit the Escape key to add ""# "" to every line that I selected by arrowing down. Save the file in Vim and then exit Vim by typing "":wq"". Changes should show up in Rstudio.</p>

<p>To delete the comments in Vim, start at the first line on top of the character ""#"" you want to delete, again do ""Control-V"", and arrow down to the last line you want to delete a ""#"" from. Then type ""dd"". The ""#"" signs should be deleted.</p>

<p>There's seconds-worth of lag time between when changes to test.R in Vim are reflected in Rstudio.</p>
"
32832865,4638884,2015-09-28T22:35:03Z,2748725,14,FALSE,"<p>Some experience using the answers from @wkmor1 and @Jaitropmange. </p>

<hr>

<p>I've checked 3 functions from 3 packages, <code>isotone</code>, <code>laeken</code>, and <code>matrixStats</code>. Only <code>matrixStats</code> works properly. Other two (just as the <code>median(rep(x, times=w)</code> solution) give integer output. As long as I calculated median age of populations, decimal places matter.</p>

<h3>Reproducible example. Calculation of the median age of a population</h3>

<pre><code>df &lt;- data.frame(age = 0:100,
                 pop = spline(c(4,7,9,8,7,6,4,3,2,1),n = 101)$y)

library(isotone)
library(laeken)
library(matrixStats)

isotone::weighted.median(df$age,df$pop)
# [1] 36
laeken::weightedMedian(df$age,df$pop)
# [1] 36
matrixStats::weightedMedian(df$age,df$pop)
# [1] 36.164
median(rep(df$age, times=df$pop))
# [1] 35
</code></pre>

<h3>Summary</h3>

<p><code>matrixStats::weightedMedian()</code> is the reliable solution</p>
"
32834125,2723734,2015-09-29T01:12:21Z,1719447,3,FALSE,"<p>Although this is an old question, here is another solution that is more in the spirit of the outer function.  The idea is to apply outer along the indices of list1 and list2:</p>

<pre><code>cor2 &lt;- Vectorize(function(x,y) {
   vec1 &lt;- list1[[x]]
   vec2 &lt;- list2[[y]]
   cor(vec1,vec2,method=""spearman"")
})
outer(1:length(list1), 1:length(list2), cor2)
</code></pre>
"
32865781,54848,2015-09-30T12:13:25Z,2228544,5,TRUE,"<p>The standard place for functional programming in R is now the <code>functional</code> library. </p>

<p>From the library:</p>

<blockquote>
  <p>functional: Curry, Compose, and other higher-order functions</p>
</blockquote>

<p>Example:</p>

<pre><code>   library(functional)
   newfunc &lt;- Curry(oldfunc,x=5)
</code></pre>

<p>CRAN:
<a href=""https://cran.r-project.org/web/packages/functional/index.html"" rel=""noreferrer"">https://cran.r-project.org/web/packages/functional/index.html</a></p>

<p>PS: This library substitutes the <code>ROxigen</code> library.</p>
"
32870310,264177,2015-09-30T15:41:10Z,2436688,32,FALSE,"<p>In the other answers, only the <code>list</code> approach results in O(1) appends, but it results in a deeply nested list structure, and not a plain single list. I have used the below datastructures, they supports O(1) (amortized) appends, and allow the result to be converted back to a plain list. </p>

<pre><code>expandingList &lt;- function(capacity = 10) {
    buffer &lt;- vector('list', capacity)
    length &lt;- 0

    methods &lt;- list()

    methods$double.size &lt;- function() {
        buffer &lt;&lt;- c(buffer, vector('list', capacity))
        capacity &lt;&lt;- capacity * 2
    }

    methods$add &lt;- function(val) {
        if(length == capacity) {
            methods$double.size()
        }

        length &lt;&lt;- length + 1
        buffer[[length]] &lt;&lt;- val
    }

    methods$as.list &lt;- function() {
        b &lt;- buffer[0:length]
        return(b)
    }

    methods
}
</code></pre>

<p>and</p>

<pre><code>linkedList &lt;- function() {
    head &lt;- list(0)
    length &lt;- 0

    methods &lt;- list()

    methods$add &lt;- function(val) {
        length &lt;&lt;- length + 1
        head &lt;&lt;- list(head, val)
    }

    methods$as.list &lt;- function() {
        b &lt;- vector('list', length)
        h &lt;- head
        for(i in length:1) {
            b[[i]] &lt;- head[[2]]
            head &lt;- head[[1]]
        }
        return(b)
    }
    methods
}
</code></pre>

<p>Use them as follows:</p>

<pre><code>&gt; l &lt;- expandingList()
&gt; l$add(""hello"")
&gt; l$add(""world"")
&gt; l$add(101)
&gt; l$as.list()
[[1]]
[1] ""hello""

[[2]]
[1] ""world""

[[3]]
[1] 101
</code></pre>

<p>These solutions could be expanded into full objects that support al list-related operations by themselves, but that will remain as an exercise for the reader. </p>

<p>Another variant for a named list:</p>

<pre><code>namedExpandingList &lt;- function(capacity = 10) {
    buffer &lt;- vector('list', capacity)
    names &lt;- character(capacity)
    length &lt;- 0

    methods &lt;- list()

    methods$double.size &lt;- function() {
        buffer &lt;&lt;- c(buffer, vector('list', capacity))
        names &lt;&lt;- c(names, character(capacity))
        capacity &lt;&lt;- capacity * 2
    }

    methods$add &lt;- function(name, val) {
        if(length == capacity) {
            methods$double.size()
        }

        length &lt;&lt;- length + 1
        buffer[[length]] &lt;&lt;- val
        names[length] &lt;&lt;- name
    }

    methods$as.list &lt;- function() {
        b &lt;- buffer[0:length]
        names(b) &lt;- names[0:length]
        return(b)
    }

    methods
}
</code></pre>

<p><strong>Benchmarks</strong></p>

<p>Performance comparison using @phonetagger's code (which is based on @Cron Arconis' code). I have also added a <code>better_env_as_container</code> and changed the <code>env_as_container_</code> a bit. The original <code>env_as_container_</code> was broken and doesn't actually store all the numbers.</p>

<pre><code>library(microbenchmark)
lPtrAppend &lt;- function(lstptr, lab, obj) {lstptr[[deparse(lab)]] &lt;- obj}
### Store list inside new environment
envAppendList &lt;- function(lstptr, obj) {lstptr$list[[length(lstptr$list)+1]] &lt;- obj} 
env2list &lt;- function(env, len) {
    l &lt;- vector('list', len)
    for (i in 1:len) {
        l[[i]] &lt;- env[[as.character(i)]]
    }
    l
}
envl2list &lt;- function(env, len) {
    l &lt;- vector('list', len)
    for (i in 1:len) {
        l[[i]] &lt;- env[[paste(as.character(i), 'L', sep='')]]
    }
    l
}
runBenchmark &lt;- function(n) {
    microbenchmark(times = 5,  
        env_with_list_ = {
            listptr &lt;- new.env(parent=globalenv())
            listptr$list &lt;- NULL
            for(i in 1:n) {envAppendList(listptr, i)}
            listptr$list
        },
        c_ = {
            a &lt;- list(0)
            for(i in 1:n) {a = c(a, list(i))}
        },
        list_ = {
            a &lt;- list(0)
            for(i in 1:n) {a &lt;- list(a, list(i))}
        },
        by_index = {
            a &lt;- list(0)
            for(i in 1:n) {a[length(a) + 1] &lt;- i}
            a
        },
        append_ = { 
            a &lt;- list(0)    
            for(i in 1:n) {a &lt;- append(a, i)} 
            a
        },
        env_as_container_ = {
            listptr &lt;- new.env(hash=TRUE, parent=globalenv())
            for(i in 1:n) {lPtrAppend(listptr, i, i)} 
            envl2list(listptr, n)
        },
        better_env_as_container = {
            env &lt;- new.env(hash=TRUE, parent=globalenv())
            for(i in 1:n) env[[as.character(i)]] &lt;- i
            env2list(env, n)
        },
        linkedList = {
            a &lt;- linkedList()
            for(i in 1:n) { a$add(i) }
            a$as.list()
        },
        inlineLinkedList = {
            a &lt;- list()
            for(i in 1:n) { a &lt;- list(a, i) }
            b &lt;- vector('list', n)
            head &lt;- a
            for(i in n:1) {
                b[[i]] &lt;- head[[2]]
                head &lt;- head[[1]]
            }                
        },
        expandingList = {
            a &lt;- expandingList()
            for(i in 1:n) { a$add(i) }
            a$as.list()
        },
        inlineExpandingList = {
            l &lt;- vector('list', 10)
            cap &lt;- 10
            len &lt;- 0
            for(i in 1:n) {
                if(len == cap) {
                    l &lt;- c(l, vector('list', cap))
                    cap &lt;- cap*2
                }
                len &lt;- len + 1
                l[[len]] &lt;- i
            }
            l[1:len]
        }
    )
}

# We need to repeatedly add an element to a list. With normal list concatenation
# or element setting this would lead to a large number of memory copies and a
# quadratic runtime. To prevent that, this function implements a bare bones
# expanding array, in which list appends are (amortized) constant time.
    expandingList &lt;- function(capacity = 10) {
        buffer &lt;- vector('list', capacity)
        length &lt;- 0

        methods &lt;- list()

        methods$double.size &lt;- function() {
            buffer &lt;&lt;- c(buffer, vector('list', capacity))
            capacity &lt;&lt;- capacity * 2
        }

        methods$add &lt;- function(val) {
            if(length == capacity) {
                methods$double.size()
            }

            length &lt;&lt;- length + 1
            buffer[[length]] &lt;&lt;- val
        }

        methods$as.list &lt;- function() {
            b &lt;- buffer[0:length]
            return(b)
        }

        methods
    }

    linkedList &lt;- function() {
        head &lt;- list(0)
        length &lt;- 0

        methods &lt;- list()

        methods$add &lt;- function(val) {
            length &lt;&lt;- length + 1
            head &lt;&lt;- list(head, val)
        }

        methods$as.list &lt;- function() {
            b &lt;- vector('list', length)
            h &lt;- head
            for(i in length:1) {
                b[[i]] &lt;- head[[2]]
                head &lt;- head[[1]]
            }
            return(b)
        }

        methods
    }

# We need to repeatedly add an element to a list. With normal list concatenation
# or element setting this would lead to a large number of memory copies and a
# quadratic runtime. To prevent that, this function implements a bare bones
# expanding array, in which list appends are (amortized) constant time.
    namedExpandingList &lt;- function(capacity = 10) {
        buffer &lt;- vector('list', capacity)
        names &lt;- character(capacity)
        length &lt;- 0

        methods &lt;- list()

        methods$double.size &lt;- function() {
            buffer &lt;&lt;- c(buffer, vector('list', capacity))
            names &lt;&lt;- c(names, character(capacity))
            capacity &lt;&lt;- capacity * 2
        }

        methods$add &lt;- function(name, val) {
            if(length == capacity) {
                methods$double.size()
            }

            length &lt;&lt;- length + 1
            buffer[[length]] &lt;&lt;- val
            names[length] &lt;&lt;- name
        }

        methods$as.list &lt;- function() {
            b &lt;- buffer[0:length]
            names(b) &lt;- names[0:length]
            return(b)
        }

        methods
    }
</code></pre>

<p>result:</p>

<pre><code>&gt; runBenchmark(1000)
Unit: microseconds
                    expr       min        lq      mean    median        uq       max neval
          env_with_list_  3128.291  3161.675  4466.726  3361.837  3362.885  9318.943     5
                      c_  3308.130  3465.830  6687.985  8578.913  8627.802  9459.252     5
                   list_   329.508   343.615   389.724   370.504   449.494   455.499     5
                by_index  3076.679  3256.588  5480.571  3395.919  8209.738  9463.931     5
                 append_  4292.321  4562.184  7911.882 10156.957 10202.773 10345.177     5
       env_as_container_ 24471.511 24795.849 25541.103 25486.362 26440.591 26511.200     5
 better_env_as_container  7671.338  7986.597  8118.163  8153.726  8335.659  8443.493     5
              linkedList  1700.754  1755.439  1829.442  1804.746  1898.752  1987.518     5
        inlineLinkedList  1109.764  1115.352  1163.751  1115.631  1206.843  1271.166     5
           expandingList  1422.440  1439.970  1486.288  1519.728  1524.268  1525.036     5
     inlineExpandingList   942.916   973.366  1002.461  1012.197  1017.784  1066.044     5
&gt; runBenchmark(10000)
Unit: milliseconds
                    expr        min         lq       mean     median         uq        max neval
          env_with_list_ 357.760419 360.277117 433.810432 411.144799 479.090688 560.779139     5
                      c_ 685.477809 734.055635 761.689936 745.957553 778.330873 864.627811     5
                   list_   3.257356   3.454166   3.505653   3.524216   3.551454   3.741071     5
                by_index 445.977967 454.321797 515.453906 483.313516 560.374763 633.281485     5
                 append_ 610.777866 629.547539 681.145751 640.936898 760.570326 763.896124     5
       env_as_container_ 281.025606 290.028380 303.885130 308.594676 314.972570 324.804419     5
 better_env_as_container  83.944855  86.927458  90.098644  91.335853  92.459026  95.826030     5
              linkedList  19.612576  24.032285  24.229808  25.461429  25.819151  26.223597     5
        inlineLinkedList  11.126970  11.768524  12.216284  12.063529  12.392199  13.730200     5
           expandingList  14.735483  15.854536  15.764204  16.073485  16.075789  16.081726     5
     inlineExpandingList  10.618393  11.179351  13.275107  12.391780  14.747914  17.438096     5
&gt; runBenchmark(20000)
Unit: milliseconds
                    expr         min          lq       mean      median          uq         max neval
          env_with_list_ 1723.899913 1915.003237 1921.23955 1938.734718 1951.649113 2076.910767     5
                      c_ 2759.769353 2768.992334 2810.40023 2820.129738 2832.350269 2870.759474     5
                   list_    6.112919    6.399964    6.63974    6.453252    6.910916    7.321647     5
                by_index 2163.585192 2194.892470 2292.61011 2209.889015 2436.620081 2458.063801     5
                 append_ 2832.504964 2872.559609 2983.17666 2992.634568 3004.625953 3213.558197     5
       env_as_container_  573.386166  588.448990  602.48829  597.645221  610.048314  642.912752     5
 better_env_as_container  154.180531  175.254307  180.26689  177.027204  188.642219  206.230191     5
              linkedList   38.401105   47.514506   46.61419   47.525192   48.677209   50.952958     5
        inlineLinkedList   25.172429   26.326681   32.33312   34.403442   34.469930   41.293126     5
           expandingList   30.776072   30.970438   34.45491   31.752790   38.062728   40.712542     5
     inlineExpandingList   21.309278   22.709159   24.64656   24.290694   25.764816   29.158849     5
</code></pre>

<p>I have added <code>linkedList</code> and <code>expandingList</code> and an inlined version of both. The <code>inlinedLinkedList</code> is basically a copy of <code>list_</code>, but it also converts the nested structure back into a plain list. Beyond that the difference between the inlined and non-inlined versions is due to the overhead of the function calls.</p>

<p>All variants of <code>expandingList</code> and <code>linkedList</code> show O(1) append performance, with the benchmark time scaling linearly with the number of items appended. <code>linkedList</code> is slower than <code>expandingList</code>, and the function call overhead is also visible. So if you really need all the speed you can get (and want to stick to R code), use an inlined version of <code>expandingList</code>.</p>

<p>I've also had a look at the C implementation of R, and both approaches should be O(1) append for any size up until you run out of memory.</p>

<p>I have also changed <code>env_as_container_</code>, the original version would store every item under index ""i"", overwriting the previously appended item. The <code>better_env_as_container</code> I have added is very similar to <code>env_as_container_</code> but without the <code>deparse</code> stuff. Both exhibit O(1) performance, but they have an overhead that is quite a bit larger than the linked/expanding lists.</p>

<p><strong>Memory overhead</strong></p>

<p>In the C R implementation there is an overhead of 4 words and 2 ints per allocated object. The <code>linkedList</code> approach allocates one list of length two per append, for a total of (4*8+4+4+2*8=) 56 bytes per appended item on 64-bit computers (excluding memory allocation overhead, so probably closer to 64 bytes). The <code>expandingList</code> approach uses one word per appended item, plus a copy when doubling the vector length, so a total memory usage of up to 16 bytes per item. Since the memory is all in one or two objects the per-object overhead is insignificant. I haven't looked deeply into the <code>env</code> memory usage, but I think it will be closer to <code>linkedList</code>. </p>
"
33050704,1863950,2015-10-10T05:35:29Z,2288485,1,FALSE,"<p>Universal way using <code>type.convert()</code> and <code>rapply()</code>:</p>

<pre><code>convert_types &lt;- function(x) {
    stopifnot(is.list(x))
    x[] &lt;- rapply(x, utils::type.convert, classes = ""character"",
                  how = ""replace"", as.is = TRUE)
    return(x)
}
d &lt;- data.frame(char = letters[1:5], 
                fake_char = as.character(1:5), 
                fac = factor(1:5), 
                char_fac = factor(letters[1:5]), 
                num = 1:5, stringsAsFactors = FALSE)
sapply(d, class)
#&gt;        char   fake_char         fac    char_fac         num 
#&gt; ""character"" ""character""    ""factor""    ""factor""   ""integer""
sapply(convert_types(d), class)
#&gt;        char   fake_char         fac    char_fac         num 
#&gt; ""character""   ""integer""    ""factor""    ""factor""   ""integer""
</code></pre>
"
33072960,564164,2015-10-12T03:46:06Z,1743698,13,FALSE,"<p>Alternatively, you can use <code>evals</code> from my <code>pander</code> package to capture output and all warnings, errors and other messages along with the raw results:</p>

<pre><code>&gt; pander::evals(""5+5"")
[[1]]
$src
[1] ""5 + 5""

$result
[1] 10

$output
[1] ""[1] 10""

$type
[1] ""numeric""

$msg
$msg$messages
NULL

$msg$warnings
NULL

$msg$errors
NULL


$stdout
NULL

attr(,""class"")
[1] ""evals""
</code></pre>
"
33087563,4900820,2015-10-12T18:10:56Z,1429476,1,FALSE,"<p>You should look at the <code>LCS</code> function of <code>qualV</code> package. It is C-implemented, therefore quite efficient.</p>
"
33103278,3837003,2015-10-13T12:54:36Z,1898101,2,FALSE,"<p>If you have Cairo installed (see the other answers), to save it as an anti-aliased PNG, just change your code to:</p>

<p><code>ggsave(file=""filename.png"", type=""cairo-png"")</code></p>

<p>as specified <a href=""https://gist.github.com/dsparks/3777731"" rel=""nofollow"">here</a>.</p>

<p>But for what purpose do you want to ""see a plot on the monitor as anti-aliased graph"" or ""anti-alias my plot windows""? If you mean like in the Plots window (tab) in RStudio, I am not sure that can be done, it serves basically just as a preview. I suggest you save the graph to a file and then use this file to display it or for any other purpose.</p>
"
33155936,2776559,2015-10-15T18:31:12Z,2151212,4,FALSE,"<p>Since <code>optparse</code> has been mentioned a couple of times in the answers, and it provides a comprehensive kit for command line processing, here's a short simplified example of how you can use it, assuming the input file exists:</p>

<p><strong><em>script.R:</em></strong></p>

<pre><code>library(optparse)

option_list &lt;- list(
  make_option(c(""-n"", ""--count_lines""), action=""store_true"", default=FALSE,
    help=""Count the line numbers [default]""),
  make_option(c(""-f"", ""--factor""), type=""integer"", default=3,
    help=""Multiply output by this number [default %default]"")
)

parser &lt;- OptionParser(usage=""%prog [options] file"", option_list=option_list)

args &lt;- parse_args(parser, positional_arguments = 1)
opt &lt;- args$options
file &lt;- args$args

if(opt$count_lines) {
  print(paste(length(readLines(file)) * opt$factor))
}
</code></pre>

<p>Given an arbitrary file <code>blah.txt</code> with 23 lines.</p>

<p>On the command line:</p>

<p><code>Rscript script.R -h</code> <em>outputs</em></p>

<pre><code>Usage: script.R [options] file


Options:
        -n, --count_lines
                Count the line numbers [default]

        -f FACTOR, --factor=FACTOR
                Multiply output by this number [default 3]

        -h, --help
                Show this help message and exit
</code></pre>

<p><code>Rscript script.R -n blah.txt</code> <em>outputs</em> <code>[1] ""69""</code></p>

<p><code>Rscript script.R -n -f 5 blah.txt</code> <em>outputs</em> <code>[1] ""115""</code></p>
"
33156676,1452257,2015-10-15T19:15:05Z,1785118,1,FALSE,"<p>You cannot extract the formula from LOESS itself. However, you could simply run another method on the points found by LOESS. If it's a simple 2D graph then it shouldn't be that hard to find a good formula. One method for doing this is symbolic regression (see <a href=""https://en.wikipedia.org/wiki/Symbolic_regression"" rel=""nofollow"">wiki</a>).</p>

<p>Be aware that this is probably not optimal and it might be better to simply use another method than LOESS.</p>
"
33332650,166686,2015-10-25T17:23:38Z,1400937,0,FALSE,"<p>This limitation to 9 backreferences is specific to the <code>sub()</code> and <code>gsub()</code>functions, not to functions like <code>grep()</code> and the like. Support for more than 9 backreferences in R implies using PCRE regular expression (i.e. the <code>perl=TRUE</code> argument);  however, even with this option, the sub() and gsub() functions do not support it. </p>

<p>The R documentation is explicit on this point:  see  <code>?regexp</code></p>

<pre><code>There can be more than 9 backreferences (but the replacement in sub can
only refer to the first 9).
</code></pre>

<p>Furthermore the idea of using named capture groups to circumvent this limitation is bound to fail since named capture groups are not supported with sub() functions.</p>

<pre><code>regexpr and gregexpr support ‘named capture’. If groups are named,
e.g., ""(?&lt;first&gt;[A-Z][a-z]+)"" then the positions of the matches are also
returned by name. (Named backreferences are not supported by sub.)
</code></pre>
"
33438722,355827,2015-10-30T14:47:52Z,2192316,1,FALSE,"<p>Another solution:</p>

<pre><code>temp = regexpr('\\d', ""aaa12xxx"");
substr(""aaa12xxx"", temp[1], temp[1]+attr(temp,""match.length"")[1])
</code></pre>
"
33614363,5543388,2015-11-09T17:07:03Z,743622,1,FALSE,"<p>How about the following, where y is the name of your matrix and you are looking for the maximum in the entire matrix: </p>

<pre><code>row(y)[y==max(y)]
</code></pre>

<p>if you want to extract the row:</p>

<pre><code>y[row(y)[y==max(y)],] # this returns unsorted rows.
</code></pre>

<p>To return sorted rows use:</p>

<pre><code>y[sort(row(y)[y==max(y)]),]
</code></pre>

<p>The advantage of this approach is that you can change the conditional inside to anything you need.  Also, using <code>col(y)</code> and location of the hanging comma you can also extract columns.</p>

<pre><code>y[,col(y)[y==max(y)]]
</code></pre>

<p>To find just the row for the max in a particular column, say column 2 you could use:</p>

<pre><code>seq(along=y[,2])[y[,2]==max(y[,2])]
</code></pre>

<p>again the conditional is flexible to look for different requirements.</p>

<p>See Phil Spector's excellent ""An introduction to S and S-Plus"" Chapter 5 for additional ideas.</p>
"
33621293,2779871,2015-11-10T01:20:32Z,2603184,2,FALSE,"<p>As other have stated, it's not possible for S4 classes. But R now provides the possibility with <strong>R6</strong> library, called <strong>reference</strong> classes. See <a href=""https://cran.r-project.org/web/packages/R6/vignettes/Introduction.html"" rel=""nofollow"">official documentation</a></p>
"
33648276,397940,2015-11-11T10:07:14Z,2749390,2,FALSE,"<p>The command <code>history()</code> will recall the last 25 used commands, whereas <code>history(max.show=Inf)</code>will get back all previous ones. If you are using RStudio on top of your R distro, <code>CTRL + UP</code> will give you the list of all previous commands. </p>
"
33672234,1328355,2015-11-12T13:12:08Z,2151147,3,FALSE,"<p>I would recommend that you use the new <a href=""https://cran.r-project.org/web/packages/haven/index.html"" rel=""nofollow noreferrer"">haven package</a> (<a href=""https://github.com/hadley/haven"" rel=""nofollow noreferrer"">GitHub</a>)
 for importing your data.</p>

<p>As <a href=""https://stackoverflow.com/users/16632/hadley"">Hadley Wickham</a> mentions in the <a href=""https://cran.r-project.org/web/packages/haven/README.html"" rel=""nofollow noreferrer"">README.md file</a>:</p>

<blockquote>
  <p>You always get a data frame, date times are converted to corresponding R classes and <strong>labelled vectors are returned as new labelled class</strong>. You can easily coerce to factors or replace labelled values with missings as appropriate. If you also use dplyr, you'll notice that large data frames are printed in a convenient way.</p>
</blockquote>

<p>(emphasis mine)</p>

<p>If you use <a href=""http://www.rstudio.com/"" rel=""nofollow noreferrer"">RStudio</a> this will automatically display the labels under variable names in the <code>View(""data.frame"")</code> viewer pane (<a href=""http://blog.rstudio.org/2015/03/04/haven-0-1-0/"" rel=""nofollow noreferrer"">source</a>).</p>

<blockquote>
  <p>Variable labels are attached as an attribute to each variable. These are not printed (because they tend to be long), but if you have a preview version of RStudio, you’ll see them in the revamped viewer pane.</p>
</blockquote>

<p>You can install the package using:</p>

<pre><code>install.packages(""haven"")
</code></pre>

<p>and import your Stata date using:</p>

<pre><code>read_dta(""path/to/file"")
</code></pre>

<p>For more info see:</p>

<pre><code>help(""read_dta"")
</code></pre>
"
33754058,1430950,2015-11-17T10:04:08Z,1169539,8,FALSE,"<p>A nice solution using <code>data.table</code> was posted <a href=""https://stats.stackexchange.com/questions/22925/regression-coefficients-by-group-in-r"">here</a> in CrossValidated by @Zach.
I'd just add that it is possible to obtain iteratively also the regression coefficient r^2:</p>

<pre><code>## make fake data
    library(data.table)
    set.seed(1)
    dat &lt;- data.table(x=runif(100), y=runif(100), grp=rep(1:2,50))

##calculate the regression coefficient r^2
    dat[,summary(lm(y~x))$r.squared,by=grp]
       grp         V1
    1:   1 0.01465726
    2:   2 0.02256595
</code></pre>

<p>as well as all the other output from <code>summary(lm)</code>:</p>

<pre><code>dat[,list(r2=summary(lm(y~x))$r.squared , f=summary(lm(y~x))$fstatistic[1] ),by=grp]
   grp         r2        f
1:   1 0.01465726 0.714014
2:   2 0.02256595 1.108173
</code></pre>
"
33870137,1033808,2015-11-23T11:37:04Z,1169539,17,FALSE,"<p>Since 2009, <code>dplyr</code> has been released which actually provides a very nice way to do this kind of grouping, closely resembling what SAS does.</p>

<pre><code>library(dplyr)

d &lt;- data.frame(state=rep(c('NY', 'CA'), c(10, 10)),
                year=rep(1:10, 2),
                response=c(rnorm(10), rnorm(10)))
fitted_models = d %&gt;% group_by(state) %&gt;% do(model = lm(response ~ year, data = .))
# Source: local data frame [2 x 2]
# Groups: &lt;by row&gt;
#
#    state   model
#   (fctr)   (chr)
# 1     CA &lt;S3:lm&gt;
# 2     NY &lt;S3:lm&gt;
fitted_models$model
# [[1]]
# 
# Call:
# lm(formula = response ~ year, data = .)
# 
# Coefficients:
# (Intercept)         year  
#    -0.06354      0.02677  
#
#
# [[2]]
# 
# Call:
# lm(formula = response ~ year, data = .)
# 
# Coefficients:
# (Intercept)         year  
#    -0.35136      0.09385  
</code></pre>

<p>To retrieve the coefficients and Rsquared/p.value, one can use the <code>broom</code> package. This package provides:</p>

<blockquote>
  <p>three S3 generics: tidy, which summarizes a model's
       statistical findings such as coefficients of a regression;
       augment, which adds columns to the original data such as
       predictions, residuals and cluster assignments; and glance, which
       provides a one-row summary of model-level statistics.</p>
</blockquote>

<pre><code>library(broom)
fitted_models %&gt;% tidy(model)
# Source: local data frame [4 x 6]
# Groups: state [2]
# 
#    state        term    estimate  std.error  statistic   p.value
#   (fctr)       (chr)       (dbl)      (dbl)      (dbl)     (dbl)
# 1     CA (Intercept) -0.06354035 0.83863054 -0.0757668 0.9414651
# 2     CA        year  0.02677048 0.13515755  0.1980687 0.8479318
# 3     NY (Intercept) -0.35135766 0.60100314 -0.5846187 0.5749166
# 4     NY        year  0.09385309 0.09686043  0.9689519 0.3609470
fitted_models %&gt;% glance(model)
# Source: local data frame [2 x 12]
# Groups: state [2]
# 
#    state   r.squared adj.r.squared     sigma statistic   p.value    df
#   (fctr)       (dbl)         (dbl)     (dbl)     (dbl)     (dbl) (int)
# 1     CA 0.004879969  -0.119510035 1.2276294 0.0392312 0.8479318     2
# 2     NY 0.105032068  -0.006838924 0.8797785 0.9388678 0.3609470     2
# Variables not shown: logLik (dbl), AIC (dbl), BIC (dbl), deviance (dbl),
#   df.residual (int)
fitted_models %&gt;% augment(model)
# Source: local data frame [20 x 10]
# Groups: state [2]
# 
#     state   response  year      .fitted   .se.fit     .resid      .hat
#    (fctr)      (dbl) (int)        (dbl)     (dbl)      (dbl)     (dbl)
# 1      CA  0.4547765     1 -0.036769875 0.7215439  0.4915464 0.3454545
# 2      CA  0.1217003     2 -0.009999399 0.6119518  0.1316997 0.2484848
# 3      CA -0.6153836     3  0.016771076 0.5146646 -0.6321546 0.1757576
# 4      CA -0.9978060     4  0.043541551 0.4379605 -1.0413476 0.1272727
# 5      CA  2.1385614     5  0.070312027 0.3940486  2.0682494 0.1030303
# 6      CA -0.3924598     6  0.097082502 0.3940486 -0.4895423 0.1030303
# 7      CA -0.5918738     7  0.123852977 0.4379605 -0.7157268 0.1272727
# 8      CA  0.4671346     8  0.150623453 0.5146646  0.3165112 0.1757576
# 9      CA -1.4958726     9  0.177393928 0.6119518 -1.6732666 0.2484848
# 10     CA  1.7481956    10  0.204164404 0.7215439  1.5440312 0.3454545
# 11     NY -0.6285230     1 -0.257504572 0.5170932 -0.3710185 0.3454545
# 12     NY  1.0566099     2 -0.163651479 0.4385542  1.2202614 0.2484848
# 13     NY -0.5274693     3 -0.069798386 0.3688335 -0.4576709 0.1757576
# 14     NY  0.6097983     4  0.024054706 0.3138637  0.5857436 0.1272727
# 15     NY -1.5511940     5  0.117907799 0.2823942 -1.6691018 0.1030303
# 16     NY  0.7440243     6  0.211760892 0.2823942  0.5322634 0.1030303
# 17     NY  0.1054719     7  0.305613984 0.3138637 -0.2001421 0.1272727
# 18     NY  0.7513057     8  0.399467077 0.3688335  0.3518387 0.1757576
# 19     NY -0.1271655     9  0.493320170 0.4385542 -0.6204857 0.2484848
# 20     NY  1.2154852    10  0.587173262 0.5170932  0.6283119 0.3454545
# Variables not shown: .sigma (dbl), .cooksd (dbl), .std.resid (dbl)
</code></pre>
"
34001635,16549,2015-11-30T15:05:22Z,2296694,1,FALSE,"<p>There are 2 that I know of.</p>

<p><a href=""http://www.renjin.org/"" rel=""nofollow"">http://www.renjin.org/</a> - Renjin</p>

<p><a href=""https://github.com/bedatadriven/renjin"" rel=""nofollow"">https://github.com/bedatadriven/renjin</a> - Renjin</p>

<p>and...</p>

<p><a href=""https://github.com/allr/purdue-fastr"" rel=""nofollow"">https://github.com/allr/purdue-fastr</a> - FastR</p>

<p><a href=""https://bitbucket.org/allr/fastr"" rel=""nofollow"">https://bitbucket.org/allr/fastr</a> - FastR</p>
"
34002138,5028253,2015-11-30T15:31:06Z,2003663,2,FALSE,"<p>This is what I use:</p>

<pre><code># Set Filename
fileName &lt;- 'Input File.txt'

doSub &lt;- function(src, dest_var_name, src_pattern, dest_pattern) {
    assign(
            x       = dest_var_name
        ,   value   = gsub(
                            pattern     = src_pattern
                        ,   replacement = dest_pattern
                        ,   x = src
                    )
        ,   envir   = .GlobalEnv
    )
}


# Read File Contents
original_text &lt;- readChar(fileName, file.info(fileName)$size)

# Convert to UNIX line ending for ease of use
doSub(src = original_text, dest_var_name = 'unix_text', src_pattern = '\r\n', dest_pattern = '\n')

# Remove Block Comments
doSub(src = unix_text, dest_var_name = 'wo_bc_text', src_pattern = '/\\*.*?\\*/', dest_pattern = '')

# Remove Line Comments
doSub(src = wo_bc_text, dest_var_name = 'wo_bc_lc_text', src_pattern = '--.*?\n', dest_pattern = '')

# Remove Line Endings to get Flat Text
doSub(src = wo_bc_lc_text, dest_var_name = 'flat_text', src_pattern = '\n', dest_pattern = ' ')

# Remove Contiguous Spaces
doSub(src = flat_text, dest_var_name = 'clean_flat_text', src_pattern = ' +', dest_pattern = ' ')
</code></pre>
"
34056066,3626521,2015-12-03T01:02:53Z,1169248,13,FALSE,"<p>I really like grep() and grepl() for this purpose.</p>

<p>grep() returns a vector of integers, which indicate where matches are.</p>

<pre><code>yo &lt;- c(""a"", ""a"", ""b"", ""b"", ""c"", ""c"")

grep(""b"", yo)
[1] 3 4
</code></pre>

<p>grepl() returns a logical vector, with ""TRUE"" at the location of matches.</p>

<pre><code>yo &lt;- c(""a"", ""a"", ""b"", ""b"", ""c"", ""c"")

grepl(""b"", yo)
[1] FALSE FALSE  TRUE  TRUE FALSE FALSE
</code></pre>

<p>These functions are case-sensitive.</p>
"
34063851,576031,2015-12-03T10:43:28Z,2061897,3,FALSE,"<p>The function fromJSON() in RJSONIO, rjson and jsonlite don't return a simple 2D data.frame for complex nested json objects. </p>

<p>To overcome this you can use <strong>tidyjson</strong>. It takes in a json and always returns a data.frame. It is currently not availble in CRAN, you can get it here: <a href=""https://github.com/sailthru/tidyjson"" rel=""nofollow"">https://github.com/sailthru/tidyjson</a></p>

<p><strong>Update:</strong>  tidyjson is now available in cran, you can install it directly using <code>install.packages(""tidyjson"")</code></p>
"
34066942,2545305,2015-12-03T13:10:40Z,1828742,6,FALSE,"<p>You need to use theme() function as follows rotating x-axis labels by 60 degrees:</p>

<pre><code>ggplot(...)+...+ theme(axis.text.x = element_text(angle=60, hjust=1))
</code></pre>
"
34181931,2490497,2015-12-09T14:56:58Z,1195826,5,FALSE,"<p>Looking at the <code>droplevels</code> methods <a href=""https://github.com/wch/r-source/blob/b156e3a711967f58131e23c1b1dc1ea90e2f0c43/src/library/base/R/factor.R#L93"" rel=""nofollow"">code in the R source you can see</a> it wraps to <code>factor</code> function. That means you can basically recreate the column with <code>factor</code> function.<br>
Below the data.table way to drop levels from all the factor columns.  </p>

<p></p>

<pre><code>library(data.table)
dt = data.table(letters=factor(letters[1:5]), numbers=seq(1:5))
levels(dt$letters)
#[1] ""a"" ""b"" ""c"" ""d"" ""e""
subdt = dt[numbers &lt;= 3]
levels(subdt$letters)
#[1] ""a"" ""b"" ""c"" ""d"" ""e""

upd.cols = sapply(subdt, is.factor)
subdt[, names(subdt)[upd.cols] := lapply(.SD, factor), .SDcols = upd.cols]
levels(subdt$letters)
#[1] ""a"" ""b"" ""c""
</code></pre>
"
34219998,2490497,2015-12-11T09:23:09Z,1299871,47,FALSE,"<p>Update on data.table methods for joining datasets. See below examples for each type of join. There are two methods, one from <code>[.data.table</code> when passing second data.table as the first argument to subset, another way is to use <code>merge</code> function which dispatched to fast data.table method.  </p>

<p><strong>Update on 2016-04-01 - and it isn't April Fools joke!</strong><br>
In 1.9.7 version of data.table joins are now capable to use existing index which tremendously reduce the timing of a join. <strong>Below code and benchmark does NOT use data.table indices on join</strong>. If you are looking for near real-time join you should use data.table indices.</p>

<p></p>

<pre><code>df1 = data.frame(CustomerId = c(1:6), Product = c(rep(""Toaster"", 3), rep(""Radio"", 3)))
df2 = data.frame(CustomerId = c(2L, 4L, 7L), State = c(rep(""Alabama"", 2), rep(""Ohio"", 1))) # one value changed to show full outer join

library(data.table)

dt1 = as.data.table(df1)
dt2 = as.data.table(df2)
setkey(dt1, CustomerId)
setkey(dt2, CustomerId)
# right outer join keyed data.tables
dt1[dt2]

setkey(dt1, NULL)
setkey(dt2, NULL)
# right outer join unkeyed data.tables - use `on` argument
dt1[dt2, on = ""CustomerId""]

# left outer join - swap dt1 with dt2
dt2[dt1, on = ""CustomerId""]

# inner join - use `nomatch` argument
dt1[dt2, nomatch=0L, on = ""CustomerId""]

# anti join - use `!` operator
dt1[!dt2, on = ""CustomerId""]

# inner join
merge(dt1, dt2, by = ""CustomerId"")

# full outer join
merge(dt1, dt2, by = ""CustomerId"", all = TRUE)

# see ?merge.data.table arguments for other cases
</code></pre>

<p>Below benchmark tests base R, sqldf, dplyr and data.table.<br>
Benchmark tests unkeyed/unindexed datasets. You can get even better performance if you are using keys on your data.tables or indexes with sqldf. Base R and dplyr does not have indexes or keys so I did not include that scenario in benchmark.<br>
Benchmark is performed on 5M-1 rows datasets, there are 5M-2 common values on join column so each scenario (left, right, full, inner) can be tested and join is still not trivial to perform.  </p>

<p></p>

<pre><code>library(microbenchmark)
library(sqldf)
library(dplyr)
library(data.table)

n = 5e6
set.seed(123)
df1 = data.frame(x=sample(n,n-1L), y1=rnorm(n-1L))
df2 = data.frame(x=sample(n,n-1L), y2=rnorm(n-1L))
dt1 = as.data.table(df1)
dt2 = as.data.table(df2)

# inner join
microbenchmark(times = 10L,
               base = merge(df1, df2, by = ""x""),
               sqldf = sqldf(""SELECT * FROM df1 INNER JOIN df2 ON df1.x = df2.x""),
               dplyr = inner_join(df1, df2, by = ""x""),
               data.table = dt1[dt2, nomatch = 0L, on = ""x""])
#Unit: milliseconds
#       expr        min         lq      mean     median        uq       max neval
#       base 15546.0097 16083.4915 16687.117 16539.0148 17388.290 18513.216    10
#      sqldf 44392.6685 44709.7128 45096.401 45067.7461 45504.376 45563.472    10
#      dplyr  4124.0068  4248.7758  4281.122  4272.3619  4342.829  4411.388    10
# data.table   937.2461   946.0227  1053.411   973.0805  1214.300  1281.958    10

# left outer join
microbenchmark(times = 10L,
               base = merge(df1, df2, by = ""x"", all.x = TRUE),
               sqldf = sqldf(""SELECT * FROM df1 LEFT OUTER JOIN df2 ON df1.x = df2.x""),
               dplyr = left_join(df1, df2, by = c(""x""=""x"")),
               data.table = dt2[dt1, on = ""x""])
#Unit: milliseconds
#       expr       min         lq       mean     median         uq       max neval
#       base 16140.791 17107.7366 17441.9538 17414.6263 17821.9035 19453.034    10
#      sqldf 43656.633 44141.9186 44777.1872 44498.7191 45288.7406 47108.900    10
#      dplyr  4062.153  4352.8021  4780.3221  4409.1186  4450.9301  8385.050    10
# data.table   823.218   823.5557   901.0383   837.9206   883.3292  1277.239    10

# right outer join
microbenchmark(times = 10L,
               base = merge(df1, df2, by = ""x"", all.y = TRUE),
               sqldf = sqldf(""SELECT * FROM df2 LEFT OUTER JOIN df1 ON df2.x = df1.x""),
               dplyr = right_join(df1, df2, by = ""x""),
               data.table = dt1[dt2, on = ""x""])
#Unit: milliseconds
#       expr        min         lq       mean     median        uq       max neval
#       base 15821.3351 15954.9927 16347.3093 16044.3500 16621.887 17604.794    10
#      sqldf 43635.5308 43761.3532 43984.3682 43969.0081 44044.461 44499.891    10
#      dplyr  3936.0329  4028.1239  4102.4167  4045.0854  4219.958  4307.350    10
# data.table   820.8535   835.9101   918.5243   887.0207  1005.721  1068.919    10

# full outer join
microbenchmark(times = 10L,
               base = merge(df1, df2, by = ""x"", all = TRUE),
               #sqldf = sqldf(""SELECT * FROM df1 FULL OUTER JOIN df2 ON df1.x = df2.x""), # not supported
               dplyr = full_join(df1, df2, by = ""x""),
               data.table = merge(dt1, dt2, by = ""x"", all = TRUE))
#Unit: seconds
#       expr       min        lq      mean    median        uq       max neval
#       base 16.176423 16.908908 17.485457 17.364857 18.271790 18.626762    10
#      dplyr  7.610498  7.666426  7.745850  7.710638  7.832125  7.951426    10
# data.table  2.052590  2.130317  2.352626  2.208913  2.470721  2.951948    10
</code></pre>
"
34284736,5681262,2015-12-15T08:54:25Z,2247045,0,FALSE,"<p>ATTENTION with substring, if string length is not a multiple of your requested length, then you will need a <strong><em>+(n-1)</em></strong> in the second sequence:</p>

<pre><code>substring(x,seq(1,nchar(x),n),seq(n,nchar(x)+n-1,n)) 
</code></pre>
"
34303118,2702219,2015-12-16T02:45:39Z,2547402,1,FALSE,"<p>Another possible solution:</p>

<pre><code>Mode &lt;- function(x) {
    if (is.numeric(x)) {
        x_table &lt;- table(x)
        return(as.numeric(names(x_table)[which.max(x_table)]))
    }
}
</code></pre>

<p>Usage:</p>

<pre><code>set.seed(100)
v &lt;- sample(x = 1:100, size = 1000000, replace = TRUE)
system.time(Mode(v))
</code></pre>

<p>Output:</p>

<pre><code>   user  system elapsed 
   0.32    0.00    0.31 
</code></pre>
"
34382518,5700340,2015-12-20T15:24:38Z,2478272,2,FALSE,"<p>You can use PMCMR package. <a href=""https://cran.r-project.org/web/packages/PMCMR/vignettes/PMCMR.pdf"" rel=""nofollow"">There</a> is more information about it. </p>

<pre><code>Spelling_Grades &lt;- c(90,87,89,90,75,88,97,99,78,85,72,76,77,79,70)
Methods &lt;- c(""A"",""A"",""A"",""A"",""B"",""B"",""B"",""B"",""B"",""B"",""C"",""C"",""C"",""C"",""C"")
kruskalmc(Spelling_Grades~Methods)

#This method doesn't accept characters that's why I've changed the methods to integer
Methods &lt;- c(1,1,1,1,2,2,2,2,2,2,3,3,3,3,3)
posthoc.kruskal.nemenyi.test(Spelling_Grades~Methods) 
</code></pre>

<p>The two methods above give same results.</p>
"
34439020,2776559,2015-12-23T15:37:15Z,952275,0,FALSE,"<p>As suggested in the <a href=""https://cran.r-project.org/web/packages/stringr/index.html"" rel=""nofollow""><code>stringr</code></a> package, this can be achieved using either <code>str_match()</code> or <code>str_extract()</code>. </p>

<p>Adapted from the manual:</p>

<pre><code>library(stringr)

strings &lt;- c("" 219 733 8965"", ""329-293-8753 "", ""banana"", 
             ""239 923 8115 and 842 566 4692"",
             ""Work: 579-499-7527"", ""$1000"",
             ""Home: 543.355.3679"")
phone &lt;- ""([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})""
</code></pre>

<p>Extracting and combining our groups:</p>

<pre><code>str_extract(strings, phone)
# [1] ""219 733 8965"" ""329-293-8753"" NA             ""239 923 8115"" ""579-499-7527"" NA            
# [7] ""543.355.3679""
</code></pre>

<p>Indicating groups with an output matrix (we're interested in columns 2+):</p>

<pre><code>str_match(strings, phone)
#      [,1]           [,2]  [,3]  [,4]  
# [1,] ""219 733 8965"" ""219"" ""733"" ""8965""
# [2,] ""329-293-8753"" ""329"" ""293"" ""8753""
# [3,] NA             NA    NA    NA    
# [4,] ""239 923 8115"" ""239"" ""923"" ""8115""
# [5,] ""579-499-7527"" ""579"" ""499"" ""7527""
# [6,] NA             NA    NA    NA    
# [7,] ""543.355.3679"" ""543"" ""355"" ""3679""
</code></pre>
"
34497727,3357059,2015-12-28T17:12:21Z,1581232,5,FALSE,"<p>You can also try using the fuction argument 'format.args' </p>

<pre><code>  ## Demonstration of additional formatC() arguments.
  print(fm1.table, format.args = list(big.mark = ""'"", decimal.mark = "",""))
</code></pre>

<p>from here</p>

<p><a href=""https://cran.rstudio.com/web/packages/xtable/xtable.pdf"" rel=""nofollow noreferrer"">https://cran.rstudio.com/web/packages/xtable/xtable.pdf</a></p>
"
34500661,168139,2015-12-28T20:56:58Z,1395391,1,TRUE,"<p>The original question was written 2009-09. At the time I was using RGoogleDocs to read google spreadsheets. In the last few months I discovered an actively created and maintained package called ""<a href=""https://cran.r-project.org/web/packages/googlesheets/index.html"" rel=""nofollow"">googlesheets: Manage Google Spreadsheets from R</a>"" by Jennifer Bryan and Joanna Zhao in British Columbia, Canada. At first it was only on GitHub and now it is in the Cran repository. It works well. It is good for reading and writing. It does not expose one's google password. All problems that I previously had in RGoogleDocs have been made irrelevant by googlesheets. </p>
"
34558755,2604954,2016-01-01T18:21:52Z,1444306,2,FALSE,"<p>Try the <code>outliers::score</code> function. I don't advise removing the so called outlier's, but knowing your extreme observations is good.</p>

<pre><code>library(outliers)
set.seed(1234)
x = rnorm(10)
[1] -1.2070657  0.2774292  1.0844412 -2.3456977  0.4291247  0.5060559 -0.5747400 -0.5466319
[9] -0.5644520 -0.8900378
outs &lt;- scores(x, type=""chisq"", prob=0.9)  # beyond 90th %ile based on chi-sq
#&gt; [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE
x[outs]  # most extreme
#&gt; [1] -2.345698
</code></pre>

<p>You'll find more help with <a href=""http://r-statistics.co/Outlier-Treatment-With-R.html#outliers%20package"" rel=""nofollow"">outlier detection here</a></p>
"
34672791,4950019,2016-01-08T08:46:11Z,2689900,0,FALSE,"<p>You could try simply saving the workspace from one session and manually loading it into the other one (or any kind of variation on this theme, like saving only the objects you share between the 2 sessions with <code>saveRDS</code> or similar). That would require some extra <code>load</code> and <code>save</code> commands but you could automatise this further by adding some lines in your <code>.RProfile</code> file that is executed at the beginning of every <code>R</code> session. <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html"" rel=""nofollow"">Here</a> is some more detailed information about <code>R</code> on startup. But I guess it all highly depends on what are you doing inside the <code>R</code> sessions. hth</p>
"
34780988,2846766,2016-01-14T03:04:12Z,2096473,2,FALSE,"<p>I run the same code from any of three Linux or Windows machines. I use the following to set up working directories:</p>

<pre><code>if(R.Version()$os == ""linux-gnu"" {
  dir.pre &lt;- ""/home""
} else {
  dir.pre &lt;- ""C:/Users""
}
</code></pre>

<p>On my debian linux server and my Ubuntu laptop:</p>

<pre><code>&gt; .Platform$OS.type
[1] ""unix""
&gt; R.Version()$os
[1] ""linux-gnu""
</code></pre>

<p>On my Windows 10 laptop, in RStudio:</p>

<pre><code>&gt; .Platform$OS.type
[1] ""windows""
&gt; R.Version()$os
[1] ""mingw32""
</code></pre>

<p>Feel free to edit and add to this list.</p>
"
34795143,2204410,2016-01-14T16:48:55Z,2261079,3,FALSE,"<p>Another option is to use the <code>stri_trim</code> function from the <code>stringi</code> package which defaults to removing leading and trailing whitespace:</p>

<pre><code>&gt; x &lt;- c(""  leading space"",""trailing space   "")
&gt; stri_trim(x)
[1] ""leading space""  ""trailing space""
</code></pre>

<p>For only removing leading whitespace, use <code>stri_trim_left</code>. For only removing trailing whitespace, use <code>stri_trim_right</code>. When you want to remove other leading or trailing characters, you have to specify that with <code>pattern =</code>.</p>

<p>See also <code>?stri_trim</code> for more info.</p>
"
34803700,3126298,2016-01-15T03:01:13Z,1439348,0,FALSE,"<p>I recently stumbled across the <a href=""https://github.com/cran"" rel=""nofollow"">CRAN Meta repository on GitHub</a>, which basically holds a read-only version of all packages in the CRAN repository. You can search by package name, after selecting the package you'Re after there is a folder called <strong>R</strong> within, which houses the source code in R.</p>

<p>For example, I searched for the package <em>mboost</em>, clicked on the in and entered the <strong>R</strong> folder, which shows this <a href=""https://github.com/cran/mboost/tree/master/R"" rel=""nofollow"">list of code</a>.</p>

<p>I haven't had a case yet where I couldn't find what I was after.
I also haven't got as far as to see if <em>ALL</em> underlying C/C++ code is shown or linked, but it was for the <em>mboost</em> example above. The c-code was in another top-level folder named <em>src</em></p>
"
34872323,2604954,2016-01-19T08:53:59Z,2258784,0,FALSE,"<p>I made this quick reference for any <a href=""http://r-statistics.co/ggplot2-cheatsheet.html"" rel=""nofollow"">theme or tasks</a> you might look for. For a more general understanding, this <a href=""http://r-statistics.co/ggplot2-Tutorial-With-R.html"" rel=""nofollow"">ggplot2 tutorial</a> should help.</p>
"
34949476,1731796,2016-01-22T14:50:45Z,2667673,4,FALSE,"<p>If you have less than 4 rows, you can use the <code>head</code> function ( <code>head(data, 4)</code> or <code>head(data, n=4)</code>) and it works like a charm. But, assume we have the following dataset with 15 rows</p>

<pre><code>&gt;data &lt;- data &lt;- read.csv(""./data.csv"", sep = "";"", header=TRUE)

&gt;data
 LungCap Age Height Smoke Gender Caesarean
1    6.475   6   62.1    no   male        no
2   10.125  18   74.7   yes female        no
3    9.550  16   69.7    no female       yes
4   11.125  14   71.0    no   male        no
5    4.800   5   56.9    no   male        no
6    6.225  11   58.7    no female        no
7    4.950   8   63.3    no   male       yes
8    7.325  11   70.4    no  male         no
9    8.875  15   70.5    no   male        no
10   6.800  11   59.2    no   male        no
11   6.900  12   59.3    no   male        no
12   6.100  13   59.4    no   male        no
13   6.110  14   59.5    no   male        no
14   6.120  15   59.6    no   male        no
15   6.130  16   59.7    no   male        no
</code></pre>

<p>Let's say, you want to select the first 10 rows. The easiest way to do it would be <code>data[1:10, ]</code>.</p>

<pre><code>&gt; data[1:10,]
   LungCap Age Height Smoke Gender Caesarean
1    6.475   6   62.1    no   male        no
2   10.125  18   74.7   yes female        no
3    9.550  16   69.7    no female       yes
4   11.125  14   71.0    no   male        no
5    4.800   5   56.9    no   male        no
6    6.225  11   58.7    no female        no
7    4.950   8   63.3    no   male       yes
8    7.325  11   70.4    no  male         no
9    8.875  15   70.5    no   male        no
10   6.800  11   59.2    no   male        no
</code></pre>

<p>However, let's say you try to retrieve the first 19 rows and see the what happens - you will have missing values</p>

<pre><code>&gt; data[1:19,]
     LungCap Age Height Smoke Gender Caesarean
1      6.475   6   62.1    no   male        no
2     10.125  18   74.7   yes female        no
3      9.550  16   69.7    no female       yes
4     11.125  14   71.0    no   male        no
5      4.800   5   56.9    no   male        no
6      6.225  11   58.7    no female        no
7      4.950   8   63.3    no   male       yes
8      7.325  11   70.4    no  male         no
9      8.875  15   70.5    no   male        no
10     6.800  11   59.2    no   male        no
11     6.900  12   59.3    no   male        no
12     6.100  13   59.4    no   male        no
13     6.110  14   59.5    no   male        no
14     6.120  15   59.6    no   male        no
15     6.130  16   59.7    no   male        no
NA        NA  NA     NA  &lt;NA&gt;   &lt;NA&gt;      &lt;NA&gt;
NA.1      NA  NA     NA  &lt;NA&gt;   &lt;NA&gt;      &lt;NA&gt;
NA.2      NA  NA     NA  &lt;NA&gt;   &lt;NA&gt;      &lt;NA&gt;
NA.3      NA  NA     NA  &lt;NA&gt;   &lt;NA&gt;      &lt;NA&gt;
</code></pre>

<p>and with the head() function, </p>

<pre><code>&gt; head(data, 19) # or head(data, n=19)
   LungCap Age Height Smoke Gender Caesarean
1    6.475   6   62.1    no   male        no
2   10.125  18   74.7   yes female        no
3    9.550  16   69.7    no female       yes
4   11.125  14   71.0    no   male        no
5    4.800   5   56.9    no   male        no
6    6.225  11   58.7    no female        no
7    4.950   8   63.3    no   male       yes
8    7.325  11   70.4    no  male         no
9    8.875  15   70.5    no   male        no
10   6.800  11   59.2    no   male        no
11   6.900  12   59.3    no   male        no
12   6.100  13   59.4    no   male        no
13   6.110  14   59.5    no   male        no
14   6.120  15   59.6    no   male        no
15   6.130  16   59.7    no   male        no
</code></pre>

<p>Hope this help!</p>
"
34961890,5414452,2016-01-23T09:40:52Z,2632441,1,FALSE,"<p>using a property of the Pascal triangle:</p>

<pre><code>x &lt;- 1
print(x)
for (i in 1:10) { x &lt;- c(0, x) + c(x, 0); print(x) }
</code></pre>

<p>I suppose this code is very fast.</p>
"
34983829,4281711,2016-01-25T01:33:58Z,2470248,14,FALSE,"<p><strong>1</strong>.Using <strong>file argument</strong> in <code>cat</code>.</p>

<pre><code> cat(""Hello World"", file=""filename"")
</code></pre>

<p><strong>2</strong>.Use <code>sink</code> function to <strong>redirect</strong> all output from both <strong>print</strong> and <strong>cat</strong> to file.</p>

<pre><code> sink(""filename"")                     # Begin writing output to file
 print(""Hello World"")
 sink()                               # Resume writing output to console
</code></pre>

<p><strong>NOTE</strong>: The print function cannot redirect its output, but the sink function can
force all output to a file. </p>

<p><strong>3</strong>.Making <strong>connection to a file</strong> and <strong>writing</strong>.</p>

<pre><code>con &lt;- file(""filename"", ""w"")
cat(""Hello World"", file=con)
close(con)
</code></pre>
"
35013835,3910157,2016-01-26T12:26:04Z,1395410,1,FALSE,"<pre><code>pdf(file = ""Location_where_you_want_the_file/name_of_file.pdf"", title=""if you want any"")
plot() # Or other graphics you want to have printed in your pdf
dev.off()
</code></pre>

<p>You can plot as many things as you want in the pdf, the plots will be added to the pdf in different pages. 
dev.off() closes the connection to the file and the pdf will be created and you will se something like</p>

<pre><code>&gt; dev.off()
null device 1
</code></pre>
"
35015218,1169233,2016-01-26T13:43:16Z,2540129,5,FALSE,"<p>This is simple to do once you read <code>?print.trellis</code>. Of particular interest is the <code>split</code> parameter. It may seem complicated at first sight, but it's quite straightforward once you understand what it means. From the documentation:</p>

<blockquote>
  <p>split: a vector of 4 integers, c(x,y,nx,ny), that says to position the current plot at the x,y position in a regular array of nx by ny plots. (Note: this has origin at top left)</p>
</blockquote>

<p>You can see a couple of implementations on <code>example(print.trellis)</code>, but here's one that I prefer:</p>

<pre><code>library(lattice)

# Data
w &lt;- as.matrix(dist(Loblolly))
x &lt;- as.matrix(dist(HairEyeColor))
y &lt;- as.matrix(dist(rock))
z &lt;- as.matrix(dist(women))

# Plot assignments
pw &lt;- levelplot(w, scales = list(draw = FALSE))  # ""scales..."" removes axes
px &lt;- levelplot(x, scales = list(draw = FALSE))
py &lt;- levelplot(y, scales = list(draw = FALSE))
pz &lt;- levelplot(z, scales = list(draw = FALSE))

# Plot prints
print(pw, split = c(1, 1, 2, 2), more = TRUE)
print(px, split = c(2, 1, 2, 2), more = TRUE)
print(py, split = c(1, 2, 2, 2), more = TRUE)
print(pz, split = c(2, 2, 2, 2), more = FALSE)  # more = FALSE is redundant
</code></pre>

<p>The code above gives you this figure:
<a href=""https://i.stack.imgur.com/c7Sjx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c7Sjx.png"" alt=""levelplots""></a></p>

<p>As you can see, <code>split</code> takes four parameters. The <em>last two</em> refer to the size of your frame (similar to what <code>mfrow</code> does), whereas the <em>first two</em> parameters position your plot into the <code>nx</code> by <code>ny</code> frame.</p>
"
35022583,5067372,2016-01-26T19:49:30Z,2754469,5,FALSE,"<p>You can now use the <code>markovchain</code> package available in CRAN. The user <a href=""https://cran.r-project.org/web/packages/markovchain/vignettes/an_introduction_to_markovchain_package.pdf"" rel=""nofollow"">manual</a>. is pretty good and has several examples.  </p>
"
35233592,1908452,2016-02-05T21:11:52Z,1296646,10,FALSE,"<p>In response to a comment added in the OP for how to sort programmatically:</p>

<p>Using <code>dplyr</code> and <code>data.table</code></p>

<pre><code>library(dplyr)
library(data.table)
</code></pre>

<h1>dplyr</h1>

<p>Just use <code>arrange_</code>, which is the Standard Evaluation version for <code>arrange</code>.</p>

<pre><code>df1 &lt;- tbl_df(iris)
#using strings or formula
arrange_(df1, c('Petal.Length', 'Petal.Width'))
arrange_(df1, ~Petal.Length, ~Petal.Width)
    Source: local data frame [150 x 5]

   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          (dbl)       (dbl)        (dbl)       (dbl)  (fctr)
1           4.6         3.6          1.0         0.2  setosa
2           4.3         3.0          1.1         0.1  setosa
3           5.8         4.0          1.2         0.2  setosa
4           5.0         3.2          1.2         0.2  setosa
5           4.7         3.2          1.3         0.2  setosa
6           5.4         3.9          1.3         0.4  setosa
7           5.5         3.5          1.3         0.2  setosa
8           4.4         3.0          1.3         0.2  setosa
9           5.0         3.5          1.3         0.3  setosa
10          4.5         2.3          1.3         0.3  setosa
..          ...         ...          ...         ...     ...


#Or using a variable
sortBy &lt;- c('Petal.Length', 'Petal.Width')
arrange_(df1, .dots = sortBy)
    Source: local data frame [150 x 5]

   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          (dbl)       (dbl)        (dbl)       (dbl)  (fctr)
1           4.6         3.6          1.0         0.2  setosa
2           4.3         3.0          1.1         0.1  setosa
3           5.8         4.0          1.2         0.2  setosa
4           5.0         3.2          1.2         0.2  setosa
5           4.7         3.2          1.3         0.2  setosa
6           5.5         3.5          1.3         0.2  setosa
7           4.4         3.0          1.3         0.2  setosa
8           4.4         3.2          1.3         0.2  setosa
9           5.0         3.5          1.3         0.3  setosa
10          4.5         2.3          1.3         0.3  setosa
..          ...         ...          ...         ...     ...

#Doing the same operation except sorting Petal.Length in descending order
sortByDesc &lt;- c('desc(Petal.Length)', 'Petal.Width')
arrange_(df1, .dots = sortByDesc)
</code></pre>

<p>more info here: <a href=""https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html"">https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html</a></p>

<p>It is better to use formula as it also captures the environment to evaluate an expression in</p>

<h1>data.table</h1>

<pre><code>dt1 &lt;- data.table(iris) #not really required, as you can work directly on your data.frame
sortBy &lt;- c('Petal.Length', 'Petal.Width')
sortType &lt;- c(-1, 1)
setorderv(dt1, sortBy, sortType)
dt1
     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
  1:          7.7         2.6          6.9         2.3 virginica
  2:          7.7         2.8          6.7         2.0 virginica
  3:          7.7         3.8          6.7         2.2 virginica
  4:          7.6         3.0          6.6         2.1 virginica
  5:          7.9         3.8          6.4         2.0 virginica
 ---                                                            
146:          5.4         3.9          1.3         0.4    setosa
147:          5.8         4.0          1.2         0.2    setosa
148:          5.0         3.2          1.2         0.2    setosa
149:          4.3         3.0          1.1         0.1    setosa
150:          4.6         3.6          1.0         0.2    setosa
</code></pre>
"
35241496,1887645,2016-02-06T13:20:47Z,1395309,0,FALSE,"<p>If you do a lot of matrix operations and you are using Windows you can install <a href=""http://revolutionanalytics.com/revolution-r-open"" rel=""nofollow"">revolutionanalytics.com/revolution-r-open</a> for free, and this one comes with the intel MKL libraries which allow you to do multithreaded matrix operations. On Windows if you take the libiomp5md.dll, Rblas.dll and Rlapack.dll files from that install and overwrite the ones in whatever R version you like to use you'll have multithreaded matrix operations (typically you get a 10-20 x speedup for matrix operations). Or you can use the Atlas Rblas.dll from <a href=""http://prs.ism.ac.jp/~nakama/SurviveGotoBLAS2/binary/windows/x64"" rel=""nofollow"">prs.ism.ac.jp/~nakama/SurviveGotoBLAS2/binary/windows/x64</a> which also work on 64 bit R and are almost as fast as the MKL ones. I found this the single easiest thing to do to drastically increase R's performance on Windows systems. Not sure why they don't come as standard in fact on R Windows installs. </p>

<p>On Windows, multithreading unfortunately is not well supported in R (unless you use <a href=""https://wbnicholson.wordpress.com/2014/07/10/parallelization-in-rcpp-via-openmp/"" rel=""nofollow"">OpenMP via Rcpp</a>) and the available <a href=""http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/"" rel=""nofollow"">SOCKET-based parallelization on Windows systems, e.g. via package parallel, is very inefficient. On POSIX systems things are better as you can use forking there.</a> (package <code>multicore</code> there is I believe the most efficient one). You could also try to use package <code>Rdsm</code> for multithreading within a shared memory model - I've got a version on my github that has unflagged -unix only flag and should work also on Windows (earlier Windows wasn't supported as dependency <code>bigmemory</code> supposedly didn't work on Windows, but now it seems it does) :</p>

<pre><code>library(devtools)
devtools::install_github('tomwenseleers/Rdsm')
library(Rdsm)
</code></pre>
"
35262327,4133676,2016-02-08T04:24:29Z,2723034,0,FALSE,"<pre><code>invisible(cat(""Dataset: "", dataset, fill = TRUE))
invisible(cat("" Width: "" ,width, fill = TRUE))
invisible(cat("" Bin1:  "" ,bin1interval, fill = TRUE))
invisible(cat("" Bin2:  "" ,bin2interval, fill = TRUE))
invisible(cat("" Bin3:  "" ,bin3interval, fill = TRUE))
</code></pre>

<p>produces output without NULL at the end of the line or on the next line</p>

<pre><code>Dataset:  17 19 26 29 31 32 34 45 47 51 52 59 60 62 63
Width:  15.33333

Bin1:   17 32.33333
Bin2:   32.33333 47.66667
Bin3:   47.66667 63
</code></pre>
"
35280360,5900637,2016-02-08T22:12:20Z,1401904,0,FALSE,"<p>for me this page is good
<a href=""https://www.r-statistics.com/2013/03/updating-r-from-r-on-windows-using-the-installr-package/"" rel=""nofollow noreferrer"">https://www.r-statistics.com/2013/03/updating-r-from-r-on-windows-using-the-installr-package/</a>
or 
another option is just install the new option and at final you put, for example in windows in my pc</p>

<p>.libPaths(c(
""D:/Documents/R/win-library/3.2"",
""C:/Program Files/R/R-3.2.3/library"",
""C:/Program Files/R/R-3.2.0/library"",
""D:/Documents/R/win-library/2.15""
)</p>

<p>every path of last version in my case i always put  the first path is  ""D:/Documents/R/win-library/3.2"" that is fixed
and then i put the other because you do not need copy or move any packages, in my sugest just call it</p>
"
35367854,1583016,2016-02-12T16:42:17Z,1407238,2,FALSE,"<p>I've adapted the following autohotkey code shared to replace all backslashes with forward slashes whenever I paste anything in RStudio. There are pros and cons to this approach.</p>

<p><strong>Pros:</strong> You don't have to think about it. The code will only run if the active window is RStudio.</p>

<p><strong>Cons:</strong> The code is called every time you paste something in R. Each time it attempts to find backslashes and replace them with forward slashes.</p>

<pre><code>GroupAdd, R, RStudio

;replaces backslashes with forward slashes in a file name that is stored on the clipboard
#IfWinActive ahk_group R
   ^v::
      StringReplace,clipboard,clipboard,\,/,All
      send %clipboard%
   return
#IfWinActive
</code></pre>
"
35463249,5931876,2016-02-17T17:10:51Z,1608130,24,FALSE,"<p>Simple example:</p>

<pre><code>f &lt;- function(a, b){ 

    if (a == 0){ 
            stop(""error message"")
    }
 }
</code></pre>
"
35477636,4819359,2016-02-18T09:34:54Z,1923273,4,FALSE,"<p>You can change the number to whatever you wish in following line </p>

<pre><code>length(which(numbers == 4))
</code></pre>
"
35907378,5658494,2016-03-10T04:05:36Z,1995933,4,FALSE,"<pre><code>library(lubridate)
</code></pre>

<p>case1: naive function</p>

<pre><code>mos&lt;-function (begin, end) {
      mos1&lt;-as.period(interval(ymd(begin),ymd(end)))
      mos&lt;-mos1@year*12+mos1@month
      mos
}
</code></pre>

<p>case2: if you need to consider only 'Month' regardless of 'Day'</p>

<pre><code>mob&lt;-function (begin, end) {
      begin&lt;-paste(substr(begin,1,6),""01"",sep="""")
      end&lt;-paste(substr(end,1,6),""01"",sep="""")
      mob1&lt;-as.period(interval(ymd(begin),ymd(end)))
      mob&lt;-mob1@year*12+mob1@month
      mob
}
</code></pre>

<p>Example : </p>

<pre><code>mos(20150101,20150228) # 1
mos(20150131,20150228) # 0
# you can use ""20150101"" instead of 20150101

mob(20150131,20150228) # 1
mob(20150131,20150228) # 1
# you can use a format of ""20150101"", 20150101, 201501
</code></pre>
"
35932083,4846979,2016-03-11T04:13:37Z,2288485,0,FALSE,"<p>In my PC (R v.3.2.3), <code>apply</code> or <code>sapply</code> give error. <code>lapply</code> works well.</p>

<pre><code>dt[,2:4] &lt;- lapply(dt[,2:4], function (x) as.factor(as.numeric(x)))
</code></pre>
"
35965611,5490241,2016-03-13T02:02:15Z,743812,3,FALSE,"<p>In fact <code>RcppRoll</code> is very good.</p>

<p>The code posted by <a href=""https://stackoverflow.com/users/1600821/cantdutchthis"">cantdutchthis</a> must be corrected in the fourth line to the window be fixed:</p>

<pre><code>ma &lt;- function(arr, n=15){
  res = arr
  for(i in n:length(arr)){
    res[i] = mean(arr[(i-n+1):i])
  }
  res
}
</code></pre>

<p>Another way, which handles missings, is given <a href=""http://www.cookbook-r.com/Manipulating_data/Calculating_a_moving_average/"" rel=""nofollow noreferrer"">here</a>.  </p>

<p>A third way, improving <a href=""https://stackoverflow.com/users/1600821/cantdutchthis"">cantdutchthis</a> code to calculate partial averages or not, follows:</p>

<pre><code>  ma &lt;- function(x, n=2,parcial=TRUE){
  res = x #set the first values

  if (parcial==TRUE){
    for(i in 1:length(x)){
      t&lt;-max(i-n+1,1)
      res[i] = mean(x[t:i])
    }
    res

  }else{
    for(i in 1:length(x)){
      t&lt;-max(i-n+1,1)
      res[i] = mean(x[t:i])
    }
    res[-c(seq(1,n-1,1))] #remove the n-1 first,i.e., res[c(-3,-4,...)]
  }
}
</code></pre>
"
35996376,6062644,2016-03-14T19:29:34Z,2307925,0,FALSE,"<p>I'm not sure that the accepted answer is correct in cases where X is sorted first, then Y is sorted by the index of (sorted) X, in that if there are duplicate values in X, Y does not always get sorted in classic 'order by x, y' style.  For example:</p>

<pre><code>&gt; x &lt;- c(3,2,2,2,1)
&gt; y &lt;- c(5,4,3,2,1)
&gt; xind &lt;- order(x)
&gt; x[xind]
[1] 1 2 2 2 3
&gt; y[xind]
[1] 1 4 3 2 5
</code></pre>

<p>Y <em>is</em> ordered by the new order of X, but not in lockstep, as not all of the X indexes changed.  A simple function to do as the OP required:</p>

<pre><code>&gt; sort.xy &lt;- function(x,y)
+ {
+ df.xy &lt;- data.frame(x,y)
+ df.xy[ order(df.xy[,1], df.xy[,2]), ]
+ }
</code></pre>

<p>In use:</p>

<pre><code>&gt; c(sort.xy(x,y))
$x
[1] 1 2 2 2 3

$y
[1] 1 2 3 4 5
</code></pre>
"
36015931,324364,2016-03-15T15:44:51Z,2641653,25,FALSE,"<p>This answer will cover many of the same elements as existing answers, but this issue (passing column names to functions) comes up often enough that I wanted there to be an answer that covered things a little more comprehensively.</p>

<p>Suppose we have a very simple data frame:</p>

<pre><code>dat &lt;- data.frame(x = 1:4,
                  y = 5:8)
</code></pre>

<p>and we'd like to write a function that creates a new column <code>z</code> that is the sum of columns <code>x</code> and <code>y</code>.</p>

<p>A very common stumbling block here is that a natural (but incorrect) attempt often looks like this:</p>

<pre><code>foo &lt;- function(df,col_name,col1,col2){
      df$col_name &lt;- df$col1 + df$col2
      df
}

#Call foo() like this:    
foo(dat,z,x,y)
</code></pre>

<p>The problem here is that <code>df$col1</code> doesn't evaluate the expression <code>col1</code>. It simply looks for a column in <code>df</code> literally called <code>col1</code>. This behavior is described in <code>?Extract</code> under the section ""Recursive (list-like) Objects"".</p>

<p>The simplest, and most often recommended solution is simply switch from <code>$</code> to <code>[[</code> and pass the function arguments as strings:</p>

<pre><code>new_column1 &lt;- function(df,col_name,col1,col2){
    #Create new column col_name as sum of col1 and col2
    df[[col_name]] &lt;- df[[col1]] + df[[col2]]
    df
}

&gt; new_column1(dat,""z"",""x"",""y"")
  x y  z
1 1 5  6
2 2 6  8
3 3 7 10
4 4 8 12
</code></pre>

<p>This is often considered ""best practice"" since it is the method that is hardest to screw up. Passing the column names as strings is about as unambiguous as you can get.</p>

<p>The following two options are more advanced. Many popular packages make use of these kinds of techniques, but using them <em>well</em> requires more care and skill, as they can introduce subtle complexities and unanticipated points of failure. <a href=""http://adv-r.had.co.nz/Computing-on-the-language.html"" rel=""noreferrer"">This</a> section of Hadley's Advanced R book is an excellent reference for some of these issues.</p>

<p>If you <em>really</em> want to save the user from typing all those quotes, one option might be to convert bare, unquoted column names to strings using <code>deparse(substitute())</code>:</p>

<pre><code>new_column2 &lt;- function(df,col_name,col1,col2){
    col_name &lt;- deparse(substitute(col_name))
    col1 &lt;- deparse(substitute(col1))
    col2 &lt;- deparse(substitute(col2))

    df[[col_name]] &lt;- df[[col1]] + df[[col2]]
    df
}

&gt; new_column2(dat,z,x,y)
  x y  z
1 1 5  6
2 2 6  8
3 3 7 10
4 4 8 12
</code></pre>

<p>This is, frankly, a bit silly probably, since we're really doing the same thing as in <code>new_column1</code>, just with a bunch of extra work bare names to strings.</p>

<p>Finally, if we want to get <em>really</em> fancy, we might decide that rather than passing in the names of two columns to add, we'd like to be more flexible and allow for other combinations of two variables. In that case we'd likely resort to using <code>eval()</code> on an expression involving the two columns:</p>

<pre><code>new_column3 &lt;- function(df,col_name,expr){
    col_name &lt;- deparse(substitute(col_name))
    df[[col_name]] &lt;- eval(substitute(expr),df,parent.frame())
    df
}
</code></pre>

<p>Just for fun, I'm still using <code>deparse(substitute())</code> for the name of the new column. Here, all of the following will work:</p>

<pre><code>&gt; new_column3(dat,z,x+y)
  x y  z
1 1 5  6
2 2 6  8
3 3 7 10
4 4 8 12
&gt; new_column3(dat,z,x-y)
  x y  z
1 1 5 -4
2 2 6 -4
3 3 7 -4
4 4 8 -4
&gt; new_column3(dat,z,x*y)
  x y  z
1 1 5  5
2 2 6 12
3 3 7 21
4 4 8 32
</code></pre>

<p>So the short answer is basically: pass data.frame column names as strings and use <code>[[</code> to select single columns. Only start delving into <code>eval</code>, <code>substitute</code>, etc. if you really know what you're doing.</p>
"
36042253,5889876,2016-03-16T17:01:09Z,1813550,2,FALSE,"<p>You can do <code>summary(santa$Believe)</code> and you will get the count for <code>TRUE</code> and <code>FALSE</code></p>
"
36089193,3507767,2016-03-18T16:11:46Z,2453326,-1,FALSE,"<p>You can identify the next higher value with <code>cummax()</code>. If you want the location of the each new higher value for example you can pass your vector of <code>cummax()</code> values to the <code>diff()</code> function to identify locations at which the <code>cummax()</code> value changed. say we have the vector </p>

<pre><code>v &lt;- c(4,6,3,2,-5,6,8,12,16)
cummax(v) will give us the vector
4  6  6  6  6  6  8 12 16
</code></pre>

<p>Now, if you want to find the location of a change in <code>cummax()</code> you have many options I tend to use <code>sign(diff(cummax(v)))</code>. You have to adjust for the lost first element because of <code>diff()</code>. The complete code for vector <code>v</code> would be:</p>

<pre><code>which(sign(diff(cummax(v)))==1)+1
</code></pre>
"
36094121,2573061,2016-03-18T21:00:06Z,2098368,1,FALSE,"<p>Matt Turner's answer is definitely the right answer.  However, in the spirit of Ken Williams' answer, you could also do:</p>

<pre><code>capture.output(cat(sdata, sep="""")) 
</code></pre>
"
36172385,471093,2016-03-23T07:45:58Z,2181902,12,FALSE,"<p>Here's a minimalist geom to display raster images instead of points,</p>

<pre><code>library(ggplot2)
library(grid)

## replace by a named list with matrices to be displayed
## by rasterGrob
.flaglist &lt;- list(""ar"" = matrix(c(""blue"", ""white"", ""blue""), 1), 
                  ""fr"" = matrix(c(""blue"", ""white"", ""red""), 1))

flagGrob &lt;- function(x, y, country, size=1, alpha=1){
  grob(x=x, y=y, country=country, size=size, cl = ""flag"")
}

drawDetails.flag &lt;- function(x, recording=FALSE){

  for(ii in seq_along(x$country)){
    grid.raster(x$x[ii], x$y[ii], 
                width = x$size[ii]*unit(1,""mm""), height = x$size[ii]*unit(0.5,""mm""),
                image = .flaglist[[x$country[[ii]]]], interpolate=FALSE)
  }
}


scale_country &lt;- function(..., guide = ""legend"") {
  sc &lt;- discrete_scale(""country"", ""identity"", scales::identity_pal(), ..., guide = guide)

  sc$super &lt;- ScaleDiscreteIdentity
  class(sc) &lt;- class(ScaleDiscreteIdentity)
  sc
}

GeomFlag &lt;- ggproto(""GeomFlag"", Geom,
                    required_aes = c(""x"", ""y"", ""country""),
                    default_aes = aes(size = 5, country=""fr""),

                    draw_key = function (data, params, size) 
                    {
                      flagGrob(0.5,0.5, country=data$country,  size=data$size)
                    },

                    draw_group = function(data, panel_scales, coord) {
                      coords &lt;- coord$transform(data, panel_scales)     
                      flagGrob(coords$x, coords$y, coords$country, coords$size)
                    }
)

geom_flag &lt;- function(mapping = NULL, data = NULL, stat = ""identity"",
                      position = ""identity"", na.rm = FALSE, show.legend = NA, 
                      inherit.aes = TRUE, ...) {
  layer(
    geom = GeomFlag, mapping = mapping,  data = data, stat = stat, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, ...)
  )
}


set.seed(1234)
d &lt;- data.frame(x=rnorm(10), y=rnorm(10), 
                country=sample(c(""ar"",""fr""), 10, TRUE), 
                stringsAsFactors = FALSE)


ggplot(d, aes(x=x, y=y, country=country, size=x)) + 
  geom_flag() + 
  scale_country()
</code></pre>

<p><a href=""https://i.stack.imgur.com/bBexE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bBexE.png"" alt=""enter image description here""></a></p>

<p>(output from the ggflags package)</p>
"
36181343,2854817,2016-03-23T14:46:36Z,1826519,6,FALSE,"<pre><code>functionReturningTwoValues &lt;- function() { 
  results &lt;- list()
  results$first &lt;- 1
  results$second &lt;-2
  return(results) 
}
a &lt;- functionReturningTwoValues()
</code></pre>

<p>I think this works. </p>
"
36276269,5907647,2016-03-29T05:39:18Z,1815606,9,FALSE,"<p>The answer of <a href=""https://stackoverflow.com/users/1021892/rakensi"">rakensi</a> from <a href=""https://stackoverflow.com/questions/3452086/getting-path-of-an-r-script"">Getting path of an R script</a> is the most correct and really brilliant IMHO. Yet, it's still a hack incorporating a dummy function. I'm quoting it here, in order to have it easier found by others.</p>

<blockquote>
  <p>sourceDir &lt;- getSrcDirectory(function(dummy) {dummy})</p>
</blockquote>

<p>This gives the directory of the file where the statement was placed (where the dummy function is defined). It can then be used to set the working direcory and use relative paths e.g.</p>

<pre><code>setwd(sourceDir)
source(""other.R"")
</code></pre>

<p>or to create absolute paths</p>

<pre><code> source(paste(sourceDir, ""/other.R"", sep=""""))
</code></pre>
"
36336922,5497983,2016-03-31T14:54:16Z,2535234,0,FALSE,"<p>If you have a dot product matrix, you can use this function to compute the cosine similarity matrix:</p>

<pre><code>get_cos = function(S){
  doc_norm = apply(as.matrix(dt),1,function(x) norm(as.matrix(x),""f"")) 
  divide_one_norm = S/doc_norm 
  cosine = t(divide_one_norm)/doc_norm
  return (cosine)
}
</code></pre>

<p>Input S is the matrix of dot product. Simply, <code>S = dt %*% t(dt)</code>, where <code>dt</code> is your dataset. </p>

<p>This function is basically to divide the dot product by the norms of vectors. </p>
"
36337557,3298423,2016-03-31T15:22:43Z,1358003,1,FALSE,"<p>As well as the more general memory management techniques given in the answers above, I always try to reduce the size of my objects as far as possible. For example, I work with very large but very sparse matrices, in other words matrices where most values are zero. Using the 'Matrix' package (capitalisation important) I was able to reduce my average object sizes from ~2GB to ~200MB as simply as:</p>

<pre><code>my.matrix &lt;- Matrix(my.matrix)
</code></pre>

<p>The Matrix package includes data formats that can be used exactly like a regular matrix (no need to change your other code) but are able to store sparse data much more efficiently, whether loaded into memory or saved to disk.</p>

<p>Additionally, the raw files I receive are in 'long' format where each data point has variables <code>x, y, z, i</code>. Much more efficient to transform the data into an <code>x * y * z</code> dimension array with only variable <code>i</code>.</p>

<p>Know your data and use a bit of common sense.</p>
"
36629143,1687200,2016-04-14T16:43:40Z,2436688,0,FALSE,"<p>This is a very interesting question and I hope my thought below could contribute an way of solution to it. This method do give a flat list without indexing, but it does have list and unlist to avoid the nesting structures. I'm not sure about the speed since I don't know how to benchmark it.</p>

<pre><code>a_list&lt;-list()
for(i in 1:3){
  a_list&lt;-list(unlist(list(unlist(a_list,recursive = FALSE),list(rnorm(2))),recursive = FALSE))
}
a_list

[[1]]
[[1]][[1]]
[1] -0.8098202  1.1035517

[[1]][[2]]
[1] 0.6804520 0.4664394

[[1]][[3]]
[1] 0.15592354 0.07424637
</code></pre>
"
36766824,142651,2016-04-21T10:28:26Z,1402001,1,FALSE,"<p>Our <code>n2khelper</code> package can use <code>bcp</code> (bulkcopy) when it is available. When not available it falls back to multiple INSERT statements.</p>

<p>You can find the package on <a href=""https://github.com/INBO-Natura2000/n2khelper"" rel=""nofollow"">https://github.com/INBO-Natura2000/n2khelper</a></p>

<p>Install it with <code>devtools::install_git(""INBO-Natura2000/n2khelper"")</code> and look for the <code>odbc_insert()</code> function.</p>
"
36771070,2638082,2016-04-21T13:27:13Z,1995933,7,FALSE,"<p>I think this is a closer answer to the question asked in terms of parity with MathWorks function</p>

<p><strong>MathWorks months function</strong></p>

<pre><code>MyMonths = months(StartDate, EndDate, EndMonthFlag)
</code></pre>

<p>My R code</p>

<pre><code>library(lubridate)
interval(mdy(10012015), today()) %/% months(1)
</code></pre>

<p>Output (as when the code was run in April 2016)</p>

<pre><code>[1] 6
</code></pre>

<p><strong>Lubridate [package]</strong> provides tools that make it easier to parse and manipulate dates. These tools are grouped below by common purpose. More information about each function can be found in its help documentation.</p>

<p><strong>interval {lubridate}</strong> creates an Interval-class object with the specified start and end dates. If the start date occurs before the end date, the interval will be positive. Otherwise, it will be negative</p>

<p><strong>today {lubridate}</strong> The current date</p>

<p><strong>months</strong> <strong>{Base}</strong> Extract the month These are generic functions: the methods for the internal date-time classes are documented here.</p>

<p><strong>%/% {base}</strong> indicates integer division AKA ( x %/% y ) (up to rounding error)</p>
"
36777602,2292993,2016-04-21T18:24:36Z,1815606,5,FALSE,"<p>My all in one!</p>

<pre><code>#' current script file (in full path)
#' @param
#' @return
#' @examples
#' works with Rscript, source() or in RStudio Run selection
#' @export
csf &lt;- function() {
    # http://stackoverflow.com/a/32016824/2292993
    cmdArgs = commandArgs(trailingOnly = FALSE)
    needle = ""--file=""
    match = grep(needle, cmdArgs)
    if (length(match) &gt; 0) {
        # Rscript via command line
        return(normalizePath(sub(needle, """", cmdArgs[match])))
    } else {
        ls_vars = ls(sys.frames()[[1]])
        if (""fileName"" %in% ls_vars) {
            # Source'd via RStudio
            return(normalizePath(sys.frames()[[1]]$fileName)) 
        } else {
            if (!is.null(sys.frames()[[1]]$ofile)) {
            # Source'd via R console
            return(normalizePath(sys.frames()[[1]]$ofile))
            } else {
                # RStudio Run Selection
                # http://stackoverflow.com/a/35842176/2292993  
                return(normalizePath(rstudioapi::getActiveDocumentContext()$path))
            }
        }
    }
}
</code></pre>
"
36864227,4904438,2016-04-26T11:55:40Z,1358003,2,FALSE,"<p>This adds nothing to the above, but is written in the simple and heavily commented style that I like. It yields a table with the objects ordered in size , but without some of the detail given in the examples above:</p>

<pre><code>#Find the objects       
MemoryObjects = ls()    
#Create an array
MemoryAssessmentTable=array(NA,dim=c(length(MemoryObjects),2))
#Name the columns
colnames(MemoryAssessmentTable)=c(""object"",""bytes"")
#Define the first column as the objects
MemoryAssessmentTable[,1]=MemoryObjects
#Define a function to determine size        
MemoryAssessmentFunction=function(x){object.size(get(x))}
#Apply the function to the objects
MemoryAssessmentTable[,2]=t(t(sapply(MemoryAssessmentTable[,1],MemoryAssessmentFunction)))
#Produce a table with the largest objects first
noquote(MemoryAssessmentTable[rev(order(as.numeric(MemoryAssessmentTable[,2]))),])
</code></pre>
"
36920083,2120262,2016-04-28T16:11:25Z,2185252,-2,FALSE,"<p>I prefer to not use additional functions if I don't need them but the sub scripting is a bit tricky. I used read.table rather than read.csv. Here you go:</p>

<pre><code>df1 = read.table(file = ""ColstoRows.txt"", sep = ""\t"", header = T, stringsAsFactors=F)
df1
d = dim(df1)[1]

v = data.frame(Country = character(),Year = numeric(),Gender = character(),Score1990 = numeric(),Score1986 = numeric(),stringsAsFactors=FALSE)
df2= data.frame(Country = character(),Year = numeric(),Gender = character(),Score1990 = numeric(),Score1986 = numeric(),stringsAsFactors=FALSE)
#print(dim(v));print(dim(df2))
gender = c(""Both"",""Male"",""Female"")
for (i in 1:d)
{
    v[1,1] = df1[i,1]
    v[1,2] = df1[i,2]

    for (j in 3:5)
    {
        k = j %% 3+1
        v[1,3]= gender[k]
        v[1,4] = df1[i,j] #starts at j=3 - then 4 and 5
        k = j+3 
        v[1,5] = df1[i,k]  #moves to j=6 then 7 and 8

        df2 = rbind(df2,v)      
    }
}
df2
</code></pre>
"
36959910,3576984,2016-04-30T20:29:31Z,1169456,8,FALSE,"<p>Just adding here that <code>[[</code> also is equipped for <em>recursive indexing</em>.</p>

<p>This was hinted at in the answer by @JijoMatthew but not explored.</p>

<p>As noted in <code>?""[[""</code>, syntax like <code>x[[y]]</code>, where <code>length(y) &gt; 1</code>, is interpreted as:</p>

<pre><code>x[[ y[1] ]][[ y[2] ]][[ y[3] ]] ... [[ y[length(y)] ]]
</code></pre>

<p>Note that this <em>doesn't</em> change what should be your main takeaway on the difference between <code>[</code> and <code>[[</code> -- namely, that the former is used for <strong>subsetting</strong>, and the latter is used for <strong>extracting</strong> single list elements.</p>

<p>For example,</p>

<pre><code>x &lt;- list(list(list(1), 2), list(list(list(3), 4), 5), 6)
x
# [[1]]
# [[1]][[1]]
# [[1]][[1]][[1]]
# [1] 1
#
# [[1]][[2]]
# [1] 2
#
# [[2]]
# [[2]][[1]]
# [[2]][[1]][[1]]
# [[2]][[1]][[1]][[1]]
# [1] 3
#
# [[2]][[1]][[2]]
# [1] 4
#
# [[2]][[2]]
# [1] 5
#
# [[3]]
# [1] 6
</code></pre>

<p>To get the value 3, we can do:</p>

<pre><code>x[[c(2, 1, 1, 1)]]
# [1] 3
</code></pre>

<p>Getting back to @JijoMatthew's answer above, recall <code>r</code>:</p>

<pre><code>r &lt;- list(1:10, foo=1, far=2)
</code></pre>

<p>In particular, this explains the errors we tend to get when mis-using <code>[[</code>, namely:</p>

<pre><code>r[[1:3]]
</code></pre>

<blockquote>
  <p>Error in <code>r[[1:3]]</code> : recursive indexing failed at level 2</p>
</blockquote>

<p>Since this code actually tried to evaluate <code>r[[1]][[2]][[3]]</code>, and the nesting of <code>r</code> stops at level one, the attempt to extract through recursive indexing failed at <code>[[2]]</code>, i.e., at level 2.</p>

<blockquote>
  <p>Error in <code>r[[c(""foo"", ""far"")]]</code> : subscript out of bounds</p>
</blockquote>

<p>Here, R was looking for <code>r[[""foo""]][[""far""]]</code>, which doesn't exist, so we get the subscript out of bounds error.</p>

<p>It probably would be a bit more helpful/consistent if both of these errors gave the same message.</p>
"
36979201,1834057,2016-05-02T09:32:35Z,1330989,11,FALSE,"<p>I'd like to provide an alternate solution, a robust solution similar to what I am about to propose was required in the latest version of <a href=""http://ggtern.com"" rel=""noreferrer"">ggtern</a>, since introducing the canvas rotation feature.</p>

<p>Basically, you need to determine the relative positions using trigonometry, by building a function which returns an <code>element_text</code> object, given angle (ie degrees) and positioning (ie one of x,y,top or right) information.</p>

<pre><code>#Load Required Libraries
library(ggplot2)
library(gridExtra)

#Build Function to Return Element Text Object
rotatedAxisElementText = function(angle,position='x'){
  angle     = angle[1]; 
  position  = position[1]
  positions = list(x=0,y=90,top=180,right=270)
  if(!position %in% names(positions))
    stop(sprintf(""'position' must be one of [%s]"",paste(names(positions),collapse="", "")),call.=FALSE)
  if(!is.numeric(angle))
    stop(""'angle' must be numeric"",call.=FALSE)
  rads  = (angle - positions[[ position ]])*pi/180
  hjust = 0.5*(1 - sin(rads))
  vjust = 0.5*(1 + cos(rads))
  element_text(angle=angle,vjust=vjust,hjust=hjust)
}
</code></pre>

<p>Frankly, in my opinion, I think that an 'auto' option should be made available in <code>ggplot2</code> for the <code>hjust</code> and <code>vjust</code> arguments, when specifying the angle, anyway, lets demonstrate how the above works.</p>

<pre><code>#Demonstrate Usage for a Variety of Rotations
df    = data.frame(x=0.5,y=0.5)
plots = lapply(seq(0,90,length.out=4),function(a){
  ggplot(df,aes(x,y)) + 
    geom_point() + 
    theme(axis.text.x = rotatedAxisElementText(a,'x'),
          axis.text.y = rotatedAxisElementText(a,'y')) +
    labs(title = sprintf(""Rotated %s"",a))
})
grid.arrange(grobs=plots)
</code></pre>

<p>Which produces the following:</p>

<p><a href=""https://i.stack.imgur.com/orUEV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/orUEV.png"" alt=""Example""></a></p>
"
37025077,4595964,2016-05-04T10:27:58Z,2261079,0,FALSE,"<p>I created a <code>trim.strings ()</code> function to trim leading and/or trailing whitespace as:</p>

<pre><code># Arguments:    x - character vector
#            side - side(s) on which to remove whitespace 
#                   default : ""both""
#                   possible values: c(""both"", ""leading"", ""trailing"")

trim.strings &lt;- function(x, side = ""both"") { 
    if (is.na(match(side, c(""both"", ""leading"", ""trailing"")))) { 
      side &lt;- ""both"" 
      } 
    if (side == ""leading"") { 
      sub(""^\\s+"", """", x)
      } else {
        if (side == ""trailing"") {
          sub(""\\s+$"", """", x)
    } else gsub(""^\\s+|\\s+$"", """", x)
    } 
} 
</code></pre>

<p>For illustration, </p>

<pre><code>a &lt;- c(""   ABC123 456    "", "" ABC123DEF          "")

# returns string without leading and trailing whitespace
trim.strings(a)
# [1] ""ABC123 456"" ""ABC123DEF"" 

# returns string without leading whitespace
trim.strings(a, side = ""leading"")
# [1] ""ABC123 456    ""      ""ABC123DEF          ""

# returns string without trailing whitespace
trim.strings(a, side = ""trailing"")
# [1] ""   ABC123 456"" "" ABC123DEF""   
</code></pre>
"
37168477,6229919,2016-05-11T16:31:37Z,1866816,0,FALSE,"<p>With the base plotting system, a quick solution could be to rotate the x-labels to be vertical, using <code>las=2</code> or <code>las=3</code>. Of course this also only works if your labels are not extremely long, but beyond a certain label length you will run into trouble with any type of plot anyway (shortening labels would be the way to go then).</p>

<p>But I agree with @doug that for more fine grained control, lattice or ggplot2 should be considered.</p>
"
37175368,766330,2016-05-12T00:31:19Z,1172485,6,FALSE,"<p>Insert this line to your <code>~/.Rprofile</code> </p>

<pre><code>options(width=system(""tput cols"", intern=TRUE))
</code></pre>
"
37199673,5792244,2016-05-13T00:55:01Z,1395528,11,FALSE,"<p>The rvest along with xml2 is another popular package for parsing html web pages.</p>

<pre><code>library(rvest)
theurl &lt;- ""http://en.wikipedia.org/wiki/Brazil_national_football_team""
file&lt;-read_html(theurl)
tables&lt;-html_nodes(file, ""table"")
table1 &lt;- html_table(tables[4], fill = TRUE)
</code></pre>

<p>The syntax is easier to use than the xml package and for most web page or xml code provides all of the options ones needs.</p>
"
37238415,179927,2016-05-15T12:39:46Z,77434,31,FALSE,"<p>To answer this not from an aesthetical but performance-oriented point of view, I've put all of the above suggestions through a <strong>benchmark</strong>. To be precise, I've considered the suggestions</p>

<ul>
<li><code>x[length(x)]</code></li>
<li><code>mylast(x)</code>, where <code>mylast</code> is a C++ function implemented through Rcpp,</li>
<li><code>tail(x, n=1)</code></li>
<li><code>dplyr::last(x)</code></li>
<li><code>x[end(x)[1]]]</code></li>
<li><code>rev(x)[1]</code></li>
</ul>

<p>and applied them to random vectors of various sizes (10^3, 10^4, 10^5, 10^6, and 10^7). Before we look at the numbers, I think it should be clear that anything that becomes noticeably slower with greater input size (i.e., anything that is not O(1)) is not an option. Here's the code that I used:</p>

<pre><code>Rcpp::cppFunction('double mylast(NumericVector x) { int n = x.size(); return x[n-1]; }')
options(width=100)
for (n in c(1e3,1e4,1e5,1e6,1e7)) {
  x &lt;- runif(n);
  print(microbenchmark::microbenchmark(x[length(x)],
                                       mylast(x),
                                       tail(x, n=1),
                                       dplyr::last(x),
                                       x[end(x)[1]],
                                       rev(x)[1]))}
</code></pre>

<p>It gives me</p>

<pre><code>Unit: nanoseconds
           expr   min      lq     mean  median      uq   max neval
   x[length(x)]   171   291.5   388.91   337.5   390.0  3233   100
      mylast(x)  1291  1832.0  2329.11  2063.0  2276.0 19053   100
 tail(x, n = 1)  7718  9589.5 11236.27 10683.0 12149.0 32711   100
 dplyr::last(x) 16341 19049.5 22080.23 21673.0 23485.5 70047   100
   x[end(x)[1]]  7688 10434.0 13288.05 11889.5 13166.5 78536   100
      rev(x)[1]  7829  8951.5 10995.59  9883.0 10890.0 45763   100
Unit: nanoseconds
           expr   min      lq     mean  median      uq    max neval
   x[length(x)]   204   323.0   475.76   386.5   459.5   6029   100
      mylast(x)  1469  2102.5  2708.50  2462.0  2995.0   9723   100
 tail(x, n = 1)  7671  9504.5 12470.82 10986.5 12748.0  62320   100
 dplyr::last(x) 15703 19933.5 26352.66 22469.5 25356.5 126314   100
   x[end(x)[1]] 13766 18800.5 27137.17 21677.5 26207.5  95982   100
      rev(x)[1] 52785 58624.0 78640.93 60213.0 72778.0 851113   100
Unit: nanoseconds
           expr     min        lq       mean    median        uq     max neval
   x[length(x)]     214     346.0     583.40     529.5     720.0    1512   100
      mylast(x)    1393    2126.0    4872.60    4905.5    7338.0    9806   100
 tail(x, n = 1)    8343   10384.0   19558.05   18121.0   25417.0   69608   100
 dplyr::last(x)   16065   22960.0   36671.13   37212.0   48071.5   75946   100
   x[end(x)[1]]  360176  404965.5  432528.84  424798.0  450996.0  710501   100
      rev(x)[1] 1060547 1140149.0 1189297.38 1180997.5 1225849.0 1383479   100
Unit: nanoseconds
           expr     min        lq        mean    median         uq      max neval
   x[length(x)]     327     584.0     1150.75     996.5     1652.5     3974   100
      mylast(x)    2060    3128.5     7541.51    8899.0     9958.0    16175   100
 tail(x, n = 1)   10484   16936.0    30250.11   34030.0    39355.0    52689   100
 dplyr::last(x)   19133   47444.5    55280.09   61205.5    66312.5   105851   100
   x[end(x)[1]] 1110956 2298408.0  3670360.45 2334753.0  4475915.0 19235341   100
      rev(x)[1] 6536063 7969103.0 11004418.46 9973664.5 12340089.5 28447454   100
Unit: nanoseconds
           expr      min         lq         mean      median          uq       max neval
   x[length(x)]      327      722.0      1644.16      1133.5      2055.5     13724   100
      mylast(x)     1962     3727.5      9578.21      9951.5     12887.5     41773   100
 tail(x, n = 1)     9829    21038.0     36623.67     43710.0     48883.0     66289   100
 dplyr::last(x)    21832    35269.0     60523.40     63726.0     75539.5    200064   100
   x[end(x)[1]] 21008128 23004594.5  37356132.43  30006737.0  47839917.0 105430564   100
      rev(x)[1] 74317382 92985054.0 108618154.55 102328667.5 112443834.0 187925942   100
</code></pre>

<p>This immediately rules out anything involving <code>rev</code> or <code>end</code> since they're clearly not <code>O(1)</code> (and the resulting expressions are evaluated in a non-lazy fashion). <code>tail</code> and <code>dplyr::last</code> are not far from being <code>O(1)</code> but they're also considerably slower than <code>mylast(x)</code> and <code>x[length(x)]</code>. Since <code>mylast(x)</code> is slower than <code>x[length(x)]</code> and provides no benefits (rather, it's custom and does not handle an empty vector gracefully), I think the answer is clear: <strong>Please use <code>x[length(x)]</code></strong>.</p>
"
37275859,2364325,2016-05-17T12:12:56Z,1660124,11,FALSE,"<p>While I have recently become a convert to <code>dplyr</code> for most of these types of operations, the <code>sqldf</code> package is still really nice (and IMHO more readable) for some things. </p>

<p>Here is an example of how this question can be answered with <code>sqldf</code></p>

<pre><code>x &lt;- data.frame(Category=factor(c(""First"", ""First"", ""First"", ""Second"",
                                  ""Third"", ""Third"", ""Second"")), 
                Frequency=c(10,15,5,2,14,20,3))

sqldf(""select 
          Category
          ,sum(Frequency) as Frequency 
       from x 
       group by 
          Category"")

##   Category Frequency
## 1    First        30
## 2   Second         5
## 3    Third        34
</code></pre>
"
37338066,5321548,2016-05-20T04:41:52Z,1358003,0,FALSE,"<p>Running</p>

<pre><code>for (i in 1:10) 
    gc(reset = T)
</code></pre>

<p>from time to time also helps R to free unused but still not released memory.</p>
"
37474054,4272725,2016-05-27T02:49:33Z,2547402,2,FALSE,"<p>I was looking through all these options and started to wonder about their relative features and performances, so I did some tests. In case anyone else are curious about the same, I'm sharing my results here.</p>

<p>Not wanting to bother about all the functions posted here, I chose to focus on a sample based on a few criteria: the function should work on both character, factor, logical and numeric vectors, it should deal with NAs and other problematic values appropriately, and output should be 'sensible', i.e. no numerics as character or other such silliness.</p>

<p>I also added a function of my own, which is based on the same <code>rle</code> idea as chrispy's, except adapted for more general use:</p>

<pre><code>library(magrittr)

Aksel &lt;- function(x, freq=FALSE) {
    z &lt;- 2
    if (freq) z &lt;- 1:2
    run &lt;- x %&gt;% as.vector %&gt;% sort %&gt;% rle %&gt;% unclass %&gt;% data.frame
    colnames(run) &lt;- c(""freq"", ""value"")
    run[which(run$freq==max(run$freq)), z] %&gt;% as.vector   
}

set.seed(2)

F &lt;- sample(c(""yes"", ""no"", ""maybe"", NA), 10, replace=TRUE) %&gt;% factor
Aksel(F)

# [1] maybe yes  

C &lt;- sample(c(""Steve"", ""Jane"", ""Jonas"", ""Petra""), 20, replace=TRUE)
Aksel(C, freq=TRUE)

# freq value
#    7 Steve
</code></pre>

<p>I ended up running five functions, on two sets of test data, through <code>microbenchmark</code>. The function names refer to their respective authors:</p>

<p><a href=""https://i.stack.imgur.com/yg3RH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yg3RH.png"" alt=""enter image description here""></a></p>

<p>Chris' function was set to <code>method=""modes""</code> and <code>na.rm=TRUE</code> by default to make it more comparable, but other than that the functions were used as presented here by their authors.</p>

<p>In matter of speed alone Kens version wins handily, but it is also the only one of these that will only report one mode, no matter how many there really are. As is often the case, there's a trade-off between speed and versatility. In <code>method=""mode""</code>, Chris' version will return a value iff there is one mode, else NA. I think that's a nice touch.
I also think it's interesting how some of the functions are affected by an increased number of unique values, while others aren't nearly as much. I haven't studied the code in detail to figure out why that is, apart from eliminating logical/numeric as a the cause.</p>
"
37659057,6430721,2016-06-06T13:41:46Z,2453326,0,FALSE,"<pre><code>topn = function(vector, n){
  maxs=c()
  ind=c()
  for (i in 1:n){
    biggest=match(max(vector), vector)
    ind[i]=biggest
    maxs[i]=max(vector)
    vector=vector[-biggest]
  }
  mat=cbind(maxs, ind)
  return(mat)
}
</code></pre>

<p>this function will return a matrix with the top n values and their indices. 
hope it helps
VDevi-Chou</p>
"
37686960,4678112,2016-06-07T18:42:49Z,77434,6,FALSE,"<p>Package <code>data.table</code> includes <code>last</code> function</p>

<pre><code>library(data.table)
last(c(1:10))
# [1] 10
</code></pre>
"
37687126,4470365,2016-06-07T18:51:23Z,77434,9,FALSE,"<p>The <a href=""https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html"" rel=""noreferrer"">dplyr</a> package includes a function <code>last()</code>:</p>

<pre><code>last(mtcars$mpg)
# [1] 21.4
</code></pre>
"
37701661,4114724,2016-06-08T11:47:45Z,1401904,4,FALSE,"<p>With respect to the solution given in the question, it might not be easy to run your older version of R if you have already installed the new version. In this case, you can still reinstall all missing packages from the previous R version as follows.</p>

<pre><code># Get names of packages in previous R version
old.packages &lt;- list.files(""/Library/Frameworks/R.framework/Versions/3.2/Resources/library"")

# Install packages in the previous version. 

# For each package p in previous version...
    for (p in old.packages) {
      # ... Only if p is not already installed
      if (!(p %in% installed.packages()[,""Package""])) {
        # Install p 
        install.packages(p) 
      }
    }
</code></pre>

<p>(Note that the argument to <code>list.files()</code> in the first line of code should be the path to the library directory for your previous R version, where all folders of packages in the previous version are. In my current case, this is <code>""/Library/Frameworks/R.framework/Versions/3.2/Resources/library""</code>. This will be different if your previous R version is not 3.2, or if you're on Windows.)</p>

<p>The <code>if</code> statement makes sure that a package is <em>not</em> installed if</p>

<ul>
<li>It's already installed in the new R version, or</li>
<li>Has been installed as a dependency from a package installed in a previous iteration of the <code>for</code> loop.</li>
</ul>
"
37809193,5276738,2016-06-14T10:16:22Z,2453326,-1,FALSE,"<p>You can use the <code>sort</code> keyword like this:</p>

<pre><code>sort(unique(c))[1:N]
</code></pre>

<p>Example: </p>

<pre><code>c &lt;- c(4,2,44,2,1,45,34,2,4,22,244)
sort(unique(c), decreasing = TRUE)[1:5]
</code></pre>

<p>will give the first 5 max numbers. </p>
"
37862546,4825783,2016-06-16T14:47:41Z,1358003,1,FALSE,"<p>You also can get some benefit using knitr and puting your script in Rmd chuncks. </p>

<p>I usually divide the code in different chunks and select which one will save a checkpoint to cache or to a RDS file, and </p>

<p>Over there you can set a chunk to be saved to ""cache"", or you can decide to run or not a particular chunk. In this way, in a first run you can process only ""part 1"", another execution you can select only ""part 2"", etc. </p>

<p>Example:</p>

<pre><code>part1
```{r corpus, warning=FALSE, cache=TRUE, message=FALSE, eval=TRUE}
corpusTw &lt;- corpus(twitter)  # build the corpus
```
part2
```{r trigrams, warning=FALSE, cache=TRUE, message=FALSE, eval=FALSE}
dfmTw &lt;- dfm(corpusTw, verbose=TRUE, removeTwitter=TRUE, ngrams=3)
```
</code></pre>

<p>As a side effect, this also could save you some headaches in terms of reproducibility :)</p>
"
37883160,6479671,2016-06-17T13:41:28Z,2615128,0,FALSE,"<p>You do not want the '='</p>

<p>Use .libPaths(""C:/R/library"") in you Rprofile.site file</p>

<p>And make sure you have correct "" symbol (Shift-2)</p>
"
37923576,5571617,2016-06-20T13:14:59Z,2192316,0,FALSE,"<p>One important difference between these approaches the the behaviour with any non-matches. For example, the regmatches method may not return a string of the same length as the input if there is not a match in all positions</p>

<pre><code>&gt; txt &lt;- c(""aaa12xxx"",""xyz"")

&gt; regmatches(txt,regexpr(""[0-9]+"",txt)) # could cause problems

[1] ""12""

&gt; gsub(""[^0-9]"", """", txt)

[1] ""12"" """"  

&gt; str_extract(txt, ""[0-9]+"")

[1] ""12"" NA  
</code></pre>
"
37967042,5124002,2016-06-22T11:42:50Z,2566128,3,FALSE,"<p>Code similar to the one in the question works for me without a problem, and generates the correct documentation. As multiple comments say, the reason must be that I am using the roxygen2 package. So just use roxygen2 and then escape ""%"" in the documentation using backslash:</p>

<pre><code>#' The percentage sign is written in the documentation like this: \%.
</code></pre>
"
38047463,2980246,2016-06-27T06:46:42Z,2218395,0,FALSE,"<p>There is a rich body of literature for tree distance metrics in the phylogenetics community that seems to have been neglected from the computer science perspective. See <a href=""http://www.inside-r.org/packages/cran/ape/docs/dist.topo"" rel=""nofollow""><code>dist.topo</code></a> of the <code>ape</code> package for two tree distance metrics and several citations (Penny and Hardy 1985, Kuhner and Felsenstein 1994) which considering the similarity of tree partitions, and also the <a href=""https://en.wikipedia.org/wiki/Robinson%E2%80%93Foulds_metric"" rel=""nofollow"">Robinson-Foulds metric</a> which has an R implementation in the <a href=""http://www.inside-r.org/packages/cran/phangorn/docs/treedist"" rel=""nofollow""><code>phangorn</code></a> package.</p>

<p>One problem is that these metrics don't have a fixed scale, so they are only useful in the cases of 1) tree comparison or 2) comparison to some generated baseline, perhaps via <a href=""https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html#correlation-measures"" rel=""nofollow"">permutation tests similar to what Tal has done with Baker's Gamma</a> in his fantastic dendextend package.</p>

<p>If you have hclust or dendrogram objects generated from <code>R</code> hierarchical clustering, using <code>as.phylo</code> from the <code>ape</code> package will convert your dendrograms to phylogenetic trees for usage in these functions.</p>
"
38097776,4398595,2016-06-29T11:05:33Z,2547402,4,FALSE,"<p>Based on @Chris's function to calculate the mode or related metrics, however using Ken Williams's method to calculate frequencies. This one provides a fix for the case of no modes at all (all elements equally frequent), and some more readable <code>method</code> names. </p>

<pre><code>Mode &lt;- function(x, method = ""one"", na.rm = FALSE) {
  x &lt;- unlist(x)
  if (na.rm) {
    x &lt;- x[!is.na(x)]
  }

  # Get unique values
  ux &lt;- unique(x)
  n &lt;- length(ux)

  # Get frequencies of all unique values
  frequencies &lt;- tabulate(match(x, ux))
  modes &lt;- frequencies == max(frequencies)

  # Determine number of modes
  nmodes &lt;- sum(modes)
  nmodes &lt;- ifelse(nmodes==n, 0L, nmodes)

  if (method %in% c(""one"", ""mode"", """") | is.na(method)) {
    # Return NA if not exactly one mode, else return the mode
    if (nmodes != 1) {
      return(NA)
    } else {
      return(ux[which(modes)])
    }
  } else if (method %in% c(""n"", ""nmodes"")) {
    # Return the number of modes
    return(nmodes)
  } else if (method %in% c(""all"", ""modes"")) {
    # Return NA if no modes exist, else return all modes
    if (nmodes &gt; 0) {
      return(ux[which(modes)])
    } else {
      return(NA)
    }
  }
  warning(""Warning: method not recognised.  Valid methods are 'one'/'mode' [default], 'n'/'nmodes' and 'all'/'modes'"")
}
</code></pre>

<p>Since it uses Ken's method to calculate frequencies the performance is also optimised, using AkselA's post I benchmarked some of the previous answers as to show how my function is close to Ken's in performance, with the conditionals for the various ouput options causing only minor overhead:
<a href=""https://i.stack.imgur.com/KGdUj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KGdUj.png"" alt=""Comparison of Mode functions""></a></p>
"
38130460,4272464,2016-06-30T18:11:26Z,1299871,13,FALSE,"<p>For the case of a left join with a <code>0..*:0..1</code> cardinality or a right join with a <code>0..1:0..*</code> cardinality it is possible to assign in-place the unilateral columns from the joiner (the <code>0..1</code> table) directly onto the joinee (the <code>0..*</code> table), and thereby avoid the creation of an entirely new table of data. This requires matching the key columns from the joinee into the joiner and indexing+ordering the joiner's rows accordingly for the assignment.</p>

<p>If the key is a single column, then we can use a single call to <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/match.html"" rel=""noreferrer""><code>match()</code></a> to do the matching. This is the case I'll cover in this answer.</p>

<p>Here's an example based on the OP, except I've added an extra row to <code>df2</code> with an id of 7 to test the case of a non-matching key in the joiner. This is effectively <code>df1</code> left join <code>df2</code>:</p>

<pre><code>df1 &lt;- data.frame(CustomerId=1:6,Product=c(rep('Toaster',3L),rep('Radio',3L)));
df2 &lt;- data.frame(CustomerId=c(2L,4L,6L,7L),State=c(rep('Alabama',2L),'Ohio','Texas'));
df1[names(df2)[-1L]] &lt;- df2[match(df1[,1L],df2[,1L]),-1L];
df1;
##   CustomerId Product   State
## 1          1 Toaster    &lt;NA&gt;
## 2          2 Toaster Alabama
## 3          3 Toaster    &lt;NA&gt;
## 4          4   Radio Alabama
## 5          5   Radio    &lt;NA&gt;
## 6          6   Radio    Ohio
</code></pre>

<p>In the above I hard-coded an assumption that the key column is the first column of both input tables. I would argue that, in general, this is not an unreasonable assumption, since, if you have a data.frame with a key column, it would be strange if it had not been set up as the first column of the data.frame from the outset. And you can always reorder the columns to make it so. An advantageous consequence of this assumption is that the name of the key column does not have to be hard-coded, although I suppose it's just replacing one assumption with another. Concision is another advantage of integer indexing, as well as speed. In the benchmarks below I'll change the implementation to use string name indexing to match the competing implementations.</p>

<p>I think this is a particularly appropriate solution if you have several tables that you want to left join against a single large table. Repeatedly rebuilding the entire table for each merge would be unnecessary and inefficient.</p>

<p>On the other hand, if you need the joinee to remain unaltered through this operation for whatever reason, then this solution cannot be used, since it modifies the joinee directly. Although in that case you could simply make a copy and perform the in-place assignment(s) on the copy.</p>

<hr>

<p>As a side note, I briefly looked into possible matching solutions for multicolumn keys. Unfortunately, the only matching solutions I found were:</p>

<ul>
<li>inefficient concatenations. e.g. <code>match(interaction(df1$a,df1$b),interaction(df2$a,df2$b))</code>, or the same idea with <code>paste()</code>.</li>
<li>inefficient cartesian conjunctions, e.g. <code>outer(df1$a,df2$a,`==`) &amp; outer(df1$b,df2$b,`==`)</code>.</li>
<li>base R <code>merge()</code> and equivalent package-based merge functions, which always allocate a new table to return the merged result, and thus are not suitable for an in-place assignment-based solution.</li>
</ul>

<p>For example, see <a href=""https://stackoverflow.com/questions/13286881/matching-multiple-columns-on-different-data-frames-and-getting-other-column-as-r"">Matching multiple columns on different data frames and getting other column as result</a>, <a href=""https://stackoverflow.com/questions/6880450/match-two-columns-with-two-other-columns"">match two columns with two other columns</a>, <a href=""https://stat.ethz.ch/pipermail/r-help/2007-January/123488.html"" rel=""noreferrer"">Matching on multiple columns</a>, and the dupe of this question where I originally came up with the in-place solution, <a href=""https://stackoverflow.com/questions/38066077/combine-two-data-frames-with-different-number-of-rows-in-r/38066281#38066281"">Combine two data frames with different number of rows in R</a>.</p>

<hr>

<h1>Benchmarking</h1>

<p>I decided to do my own benchmarking to see how the in-place assignment approach compares to the other solutions that have been offered in this question.</p>

<p>Testing code:</p>

<pre><code>library(microbenchmark);
library(data.table);
library(sqldf);
library(plyr);
library(dplyr);

solSpecs &lt;- list(
    merge=list(testFuncs=list(
        inner=function(df1,df2,key) merge(df1,df2,key),
        left =function(df1,df2,key) merge(df1,df2,key,all.x=T),
        right=function(df1,df2,key) merge(df1,df2,key,all.y=T),
        full =function(df1,df2,key) merge(df1,df2,key,all=T)
    )),
    data.table.unkeyed=list(argSpec='data.table.unkeyed',testFuncs=list(
        inner=function(dt1,dt2,key) dt1[dt2,on=key,nomatch=0L,allow.cartesian=T],
        left =function(dt1,dt2,key) dt2[dt1,on=key,allow.cartesian=T],
        right=function(dt1,dt2,key) dt1[dt2,on=key,allow.cartesian=T],
        full =function(dt1,dt2,key) merge(dt1,dt2,key,all=T,allow.cartesian=T) ## calls merge.data.table()
    )),
    data.table.keyed=list(argSpec='data.table.keyed',testFuncs=list(
        inner=function(dt1,dt2) dt1[dt2,nomatch=0L,allow.cartesian=T],
        left =function(dt1,dt2) dt2[dt1,allow.cartesian=T],
        right=function(dt1,dt2) dt1[dt2,allow.cartesian=T],
        full =function(dt1,dt2) merge(dt1,dt2,all=T,allow.cartesian=T) ## calls merge.data.table()
    )),
    sqldf.unindexed=list(testFuncs=list( ## note: must pass connection=NULL to avoid running against the live DB connection, which would result in collisions with the residual tables from the last query upload
        inner=function(df1,df2,key) sqldf(paste0('select * from df1 inner join df2 using(',paste(collapse=',',key),')'),connection=NULL),
        left =function(df1,df2,key) sqldf(paste0('select * from df1 left join df2 using(',paste(collapse=',',key),')'),connection=NULL),
        right=function(df1,df2,key) sqldf(paste0('select * from df2 left join df1 using(',paste(collapse=',',key),')'),connection=NULL) ## can't do right join proper, not yet supported; inverted left join is equivalent
        ##full =function(df1,df2,key) sqldf(paste0('select * from df1 full join df2 using(',paste(collapse=',',key),')'),connection=NULL) ## can't do full join proper, not yet supported; possible to hack it with a union of left joins, but too unreasonable to include in testing
    )),
    sqldf.indexed=list(testFuncs=list( ## important: requires an active DB connection with preindexed main.df1 and main.df2 ready to go; arguments are actually ignored
        inner=function(df1,df2,key) sqldf(paste0('select * from main.df1 inner join main.df2 using(',paste(collapse=',',key),')')),
        left =function(df1,df2,key) sqldf(paste0('select * from main.df1 left join main.df2 using(',paste(collapse=',',key),')')),
        right=function(df1,df2,key) sqldf(paste0('select * from main.df2 left join main.df1 using(',paste(collapse=',',key),')')) ## can't do right join proper, not yet supported; inverted left join is equivalent
        ##full =function(df1,df2,key) sqldf(paste0('select * from main.df1 full join main.df2 using(',paste(collapse=',',key),')')) ## can't do full join proper, not yet supported; possible to hack it with a union of left joins, but too unreasonable to include in testing
    )),
    plyr=list(testFuncs=list(
        inner=function(df1,df2,key) join(df1,df2,key,'inner'),
        left =function(df1,df2,key) join(df1,df2,key,'left'),
        right=function(df1,df2,key) join(df1,df2,key,'right'),
        full =function(df1,df2,key) join(df1,df2,key,'full')
    )),
    dplyr=list(testFuncs=list(
        inner=function(df1,df2,key) inner_join(df1,df2,key),
        left =function(df1,df2,key) left_join(df1,df2,key),
        right=function(df1,df2,key) right_join(df1,df2,key),
        full =function(df1,df2,key) full_join(df1,df2,key)
    )),
    in.place=list(testFuncs=list(
        left =function(df1,df2,key) { cns &lt;- setdiff(names(df2),key); df1[cns] &lt;- df2[match(df1[,key],df2[,key]),cns]; df1; },
        right=function(df1,df2,key) { cns &lt;- setdiff(names(df1),key); df2[cns] &lt;- df1[match(df2[,key],df1[,key]),cns]; df2; }
    ))
);

getSolTypes &lt;- function() names(solSpecs);
getJoinTypes &lt;- function() unique(unlist(lapply(solSpecs,function(x) names(x$testFuncs))));
getArgSpec &lt;- function(argSpecs,key=NULL) if (is.null(key)) argSpecs$default else argSpecs[[key]];

initSqldf &lt;- function() {
    sqldf(); ## creates sqlite connection on first run, cleans up and closes existing connection otherwise
    if (exists('sqldfInitFlag',envir=globalenv(),inherits=F) &amp;&amp; sqldfInitFlag) { ## false only on first run
        sqldf(); ## creates a new connection
    } else {
        assign('sqldfInitFlag',T,envir=globalenv()); ## set to true for the one and only time
    }; ## end if
    invisible();
}; ## end initSqldf()

setUpBenchmarkCall &lt;- function(argSpecs,joinType,solTypes=getSolTypes(),env=parent.frame()) {
    ## builds and returns a list of expressions suitable for passing to the list argument of microbenchmark(), and assigns variables to resolve symbol references in those expressions
    callExpressions &lt;- list();
    nms &lt;- character();
    for (solType in solTypes) {
        testFunc &lt;- solSpecs[[solType]]$testFuncs[[joinType]];
        if (is.null(testFunc)) next; ## this join type is not defined for this solution type
        testFuncName &lt;- paste0('tf.',solType);
        assign(testFuncName,testFunc,envir=env);
        argSpecKey &lt;- solSpecs[[solType]]$argSpec;
        argSpec &lt;- getArgSpec(argSpecs,argSpecKey);
        argList &lt;- setNames(nm=names(argSpec$args),vector('list',length(argSpec$args)));
        for (i in seq_along(argSpec$args)) {
            argName &lt;- paste0('tfa.',argSpecKey,i);
            assign(argName,argSpec$args[[i]],envir=env);
            argList[[i]] &lt;- if (i%in%argSpec$copySpec) call('copy',as.symbol(argName)) else as.symbol(argName);
        }; ## end for
        callExpressions[[length(callExpressions)+1L]] &lt;- do.call(call,c(list(testFuncName),argList),quote=T);
        nms[length(nms)+1L] &lt;- solType;
    }; ## end for
    names(callExpressions) &lt;- nms;
    callExpressions;
}; ## end setUpBenchmarkCall()

harmonize &lt;- function(res) {
    res &lt;- as.data.frame(res); ## coerce to data.frame
    for (ci in which(sapply(res,is.factor))) res[[ci]] &lt;- as.character(res[[ci]]); ## coerce factor columns to character
    for (ci in which(sapply(res,is.logical))) res[[ci]] &lt;- as.integer(res[[ci]]); ## coerce logical columns to integer (works around sqldf quirk of munging logicals to integers)
    ##for (ci in which(sapply(res,inherits,'POSIXct'))) res[[ci]] &lt;- as.double(res[[ci]]); ## coerce POSIXct columns to double (works around sqldf quirk of losing POSIXct class) ----- POSIXct doesn't work at all in sqldf.indexed
    res &lt;- res[order(names(res))]; ## order columns
    res &lt;- res[do.call(order,res),]; ## order rows
    res;
}; ## end harmonize()

checkIdentical &lt;- function(argSpecs,solTypes=getSolTypes()) {
    for (joinType in getJoinTypes()) {
        callExpressions &lt;- setUpBenchmarkCall(argSpecs,joinType,solTypes);
        if (length(callExpressions)&lt;2L) next;
        ex &lt;- harmonize(eval(callExpressions[[1L]]));
        for (i in seq(2L,len=length(callExpressions)-1L)) {
            y &lt;- harmonize(eval(callExpressions[[i]]));
            if (!isTRUE(all.equal(ex,y,check.attributes=F))) {
                ex &lt;&lt;- ex;
                y &lt;&lt;- y;
                solType &lt;- names(callExpressions)[i];
                stop(paste0('non-identical: ',solType,' ',joinType,'.'));
            }; ## end if
        }; ## end for
    }; ## end for
    invisible();
}; ## end checkIdentical()

testJoinType &lt;- function(argSpecs,joinType,solTypes=getSolTypes(),metric=NULL,times=100L) {
    callExpressions &lt;- setUpBenchmarkCall(argSpecs,joinType,solTypes);
    bm &lt;- microbenchmark(list=callExpressions,times=times);
    if (is.null(metric)) return(bm);
    bm &lt;- summary(bm);
    res &lt;- setNames(nm=names(callExpressions),bm[[metric]]);
    attr(res,'unit') &lt;- attr(bm,'unit');
    res;
}; ## end testJoinType()

testAllJoinTypes &lt;- function(argSpecs,solTypes=getSolTypes(),metric=NULL,times=100L) {
    joinTypes &lt;- getJoinTypes();
    resList &lt;- setNames(nm=joinTypes,lapply(joinTypes,function(joinType) testJoinType(argSpecs,joinType,solTypes,metric,times)));
    if (is.null(metric)) return(resList);
    units &lt;- unname(unlist(lapply(resList,attr,'unit')));
    res &lt;- do.call(data.frame,c(list(join=joinTypes),setNames(nm=solTypes,rep(list(rep(NA_real_,length(joinTypes))),length(solTypes))),list(unit=units,stringsAsFactors=F)));
    for (i in seq_along(resList)) res[i,match(names(resList[[i]]),names(res))] &lt;- resList[[i]];
    res;
}; ## end testAllJoinTypes()

testGrid &lt;- function(makeArgSpecsFunc,sizes,overlaps,solTypes=getSolTypes(),joinTypes=getJoinTypes(),metric='median',times=100L) {

    res &lt;- expand.grid(size=sizes,overlap=overlaps,joinType=joinTypes,stringsAsFactors=F);
    res[solTypes] &lt;- NA_real_;
    res$unit &lt;- NA_character_;
    for (ri in seq_len(nrow(res))) {

        size &lt;- res$size[ri];
        overlap &lt;- res$overlap[ri];
        joinType &lt;- res$joinType[ri];

        argSpecs &lt;- makeArgSpecsFunc(size,overlap);

        checkIdentical(argSpecs,solTypes);

        cur &lt;- testJoinType(argSpecs,joinType,solTypes,metric,times);
        res[ri,match(names(cur),names(res))] &lt;- cur;
        res$unit[ri] &lt;- attr(cur,'unit');

    }; ## end for

    res;

}; ## end testGrid()
</code></pre>

<hr>

<p>Here's a benchmark of the example based on the OP that I demonstrated earlier:</p>

<pre><code>## OP's example, supplemented with a non-matching row in df2
argSpecs &lt;- list(
    default=list(copySpec=1:2,args=list(
        df1 &lt;- data.frame(CustomerId=1:6,Product=c(rep('Toaster',3L),rep('Radio',3L))),
        df2 &lt;- data.frame(CustomerId=c(2L,4L,6L,7L),State=c(rep('Alabama',2L),'Ohio','Texas')),
        'CustomerId'
    )),
    data.table.unkeyed=list(copySpec=1:2,args=list(
        as.data.table(df1),
        as.data.table(df2),
        'CustomerId'
    )),
    data.table.keyed=list(copySpec=1:2,args=list(
        setkey(as.data.table(df1),CustomerId),
        setkey(as.data.table(df2),CustomerId)
    ))
);
## prepare sqldf
initSqldf();
sqldf('create index df1_key on df1(CustomerId);'); ## upload and create an sqlite index on df1
sqldf('create index df2_key on df2(CustomerId);'); ## upload and create an sqlite index on df2

checkIdentical(argSpecs);

testAllJoinTypes(argSpecs,metric='median');
##    join    merge data.table.unkeyed data.table.keyed sqldf.unindexed sqldf.indexed      plyr    dplyr in.place         unit
## 1 inner  644.259           861.9345          923.516        9157.752      1580.390  959.2250 270.9190       NA microseconds
## 2  left  713.539           888.0205          910.045        8820.334      1529.714  968.4195 270.9185 224.3045 microseconds
## 3 right 1221.804           909.1900          923.944        8930.668      1533.135 1063.7860 269.8495 218.1035 microseconds
## 4  full 1302.203          3107.5380         3184.729              NA            NA 1593.6475 270.7055       NA microseconds
</code></pre>

<hr>

<p>Here I benchmark on random input data, trying different scales and different patterns of key overlap between the two input tables. This benchmark is still restricted to the case of a single-column integer key. As well, to ensure that the in-place solution would work for both left and right joins of the same tables, all random test data uses <code>0..1:0..1</code> cardinality. This is implemented by sampling without replacement the key column of the first data.frame when generating the key column of the second data.frame.</p>

<pre><code>makeArgSpecs.singleIntegerKey.optionalOneToOne &lt;- function(size,overlap) {

    com &lt;- as.integer(size*overlap);

    argSpecs &lt;- list(
        default=list(copySpec=1:2,args=list(
            df1 &lt;- data.frame(id=sample(size),y1=rnorm(size),y2=rnorm(size)),
            df2 &lt;- data.frame(id=sample(c(if (com&gt;0L) sample(df1$id,com) else integer(),seq(size+1L,len=size-com))),y3=rnorm(size),y4=rnorm(size)),
            'id'
        )),
        data.table.unkeyed=list(copySpec=1:2,args=list(
            as.data.table(df1),
            as.data.table(df2),
            'id'
        )),
        data.table.keyed=list(copySpec=1:2,args=list(
            setkey(as.data.table(df1),id),
            setkey(as.data.table(df2),id)
        ))
    );
    ## prepare sqldf
    initSqldf();
    sqldf('create index df1_key on df1(id);'); ## upload and create an sqlite index on df1
    sqldf('create index df2_key on df2(id);'); ## upload and create an sqlite index on df2

    argSpecs;

}; ## end makeArgSpecs.singleIntegerKey.optionalOneToOne()

## cross of various input sizes and key overlaps
sizes &lt;- c(1e1L,1e3L,1e6L);
overlaps &lt;- c(0.99,0.5,0.01);
system.time({ res &lt;- testGrid(makeArgSpecs.singleIntegerKey.optionalOneToOne,sizes,overlaps); });
##     user   system  elapsed
## 22024.65 12308.63 34493.19
</code></pre>

<p>I wrote some code to create log-log plots of the above results. I generated a separate plot for each overlap percentage. It's a little bit cluttered, but I like having all the solution types and join types represented in the same plot.</p>

<p>I used spline interpolation to show a smooth curve for each solution/join type combination, drawn with individual pch symbols. The join type is captured by the pch symbol, using a dot for inner, left and right angle brackets for left and right, and a diamond for full. The solution type is captured by the color as shown in the legend.</p>

<pre><code>plotRes &lt;- function(res,titleFunc,useFloor=F) {
    solTypes &lt;- setdiff(names(res),c('size','overlap','joinType','unit')); ## derive from res
    normMult &lt;- c(microseconds=1e-3,milliseconds=1); ## normalize to milliseconds
    joinTypes &lt;- getJoinTypes();
    cols &lt;- c(merge='purple',data.table.unkeyed='blue',data.table.keyed='#00DDDD',sqldf.unindexed='brown',sqldf.indexed='orange',plyr='red',dplyr='#00BB00',in.place='magenta');
    pchs &lt;- list(inner=20L,left='&lt;',right='&gt;',full=23L);
    cexs &lt;- c(inner=0.7,left=1,right=1,full=0.7);
    NP &lt;- 60L;
    ord &lt;- order(decreasing=T,colMeans(res[res$size==max(res$size),solTypes],na.rm=T));
    ymajors &lt;- data.frame(y=c(1,1e3),label=c('1ms','1s'),stringsAsFactors=F);
    for (overlap in unique(res$overlap)) {
        x1 &lt;- res[res$overlap==overlap,];
        x1[solTypes] &lt;- x1[solTypes]*normMult[x1$unit]; x1$unit &lt;- NULL;
        xlim &lt;- c(1e1,max(x1$size));
        xticks &lt;- 10^seq(log10(xlim[1L]),log10(xlim[2L]));
        ylim &lt;- c(1e-1,10^((if (useFloor) floor else ceiling)(log10(max(x1[solTypes],na.rm=T))))); ## use floor() to zoom in a little more, only sqldf.unindexed will break above, but xpd=NA will keep it visible
        yticks &lt;- 10^seq(log10(ylim[1L]),log10(ylim[2L]));
        yticks.minor &lt;- rep(yticks[-length(yticks)],each=9L)*1:9;
        plot(NA,xlim=xlim,ylim=ylim,xaxs='i',yaxs='i',axes=F,xlab='size (rows)',ylab='time (ms)',log='xy');
        abline(v=xticks,col='lightgrey');
        abline(h=yticks.minor,col='lightgrey',lty=3L);
        abline(h=yticks,col='lightgrey');
        axis(1L,xticks,parse(text=sprintf('10^%d',as.integer(log10(xticks)))));
        axis(2L,yticks,parse(text=sprintf('10^%d',as.integer(log10(yticks)))),las=1L);
        axis(4L,ymajors$y,ymajors$label,las=1L,tick=F,cex.axis=0.7,hadj=0.5);
        for (joinType in rev(joinTypes)) { ## reverse to draw full first, since it's larger and would be more obtrusive if drawn last
            x2 &lt;- x1[x1$joinType==joinType,];
            for (solType in solTypes) {
                if (any(!is.na(x2[[solType]]))) {
                    xy &lt;- spline(x2$size,x2[[solType]],xout=10^(seq(log10(x2$size[1L]),log10(x2$size[nrow(x2)]),len=NP)));
                    points(xy$x,xy$y,pch=pchs[[joinType]],col=cols[solType],cex=cexs[joinType],xpd=NA);
                }; ## end if
            }; ## end for
        }; ## end for
        ## custom legend
        ## due to logarithmic skew, must do all distance calcs in inches, and convert to user coords afterward
        ## the bottom-left corner of the legend will be defined in normalized figure coords, although we can convert to inches immediately
        leg.cex &lt;- 0.7;
        leg.x.in &lt;- grconvertX(0.275,'nfc','in');
        leg.y.in &lt;- grconvertY(0.6,'nfc','in');
        leg.x.user &lt;- grconvertX(leg.x.in,'in');
        leg.y.user &lt;- grconvertY(leg.y.in,'in');
        leg.outpad.w.in &lt;- 0.1;
        leg.outpad.h.in &lt;- 0.1;
        leg.midpad.w.in &lt;- 0.1;
        leg.midpad.h.in &lt;- 0.1;
        leg.sol.w.in &lt;- max(strwidth(solTypes,'in',leg.cex));
        leg.sol.h.in &lt;- max(strheight(solTypes,'in',leg.cex))*1.5; ## multiplication factor for greater line height
        leg.join.w.in &lt;- max(strheight(joinTypes,'in',leg.cex))*1.5; ## ditto
        leg.join.h.in &lt;- max(strwidth(joinTypes,'in',leg.cex));
        leg.main.w.in &lt;- leg.join.w.in*length(joinTypes);
        leg.main.h.in &lt;- leg.sol.h.in*length(solTypes);
        leg.x2.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in*2+leg.main.w.in+leg.midpad.w.in+leg.sol.w.in,'in');
        leg.y2.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in*2+leg.main.h.in+leg.midpad.h.in+leg.join.h.in,'in');
        leg.cols.x.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in+leg.join.w.in*(0.5+seq(0L,length(joinTypes)-1L)),'in');
        leg.lines.y.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in+leg.main.h.in-leg.sol.h.in*(0.5+seq(0L,length(solTypes)-1L)),'in');
        leg.sol.x.user &lt;- grconvertX(leg.x.in+leg.outpad.w.in+leg.main.w.in+leg.midpad.w.in,'in');
        leg.join.y.user &lt;- grconvertY(leg.y.in+leg.outpad.h.in+leg.main.h.in+leg.midpad.h.in,'in');
        rect(leg.x.user,leg.y.user,leg.x2.user,leg.y2.user,col='white');
        text(leg.sol.x.user,leg.lines.y.user,solTypes[ord],cex=leg.cex,pos=4L,offset=0);
        text(leg.cols.x.user,leg.join.y.user,joinTypes,cex=leg.cex,pos=4L,offset=0,srt=90); ## srt rotation applies *after* pos/offset positioning
        for (i in seq_along(joinTypes)) {
            joinType &lt;- joinTypes[i];
            points(rep(leg.cols.x.user[i],length(solTypes)),ifelse(colSums(!is.na(x1[x1$joinType==joinType,solTypes[ord]]))==0L,NA,leg.lines.y.user),pch=pchs[[joinType]],col=cols[solTypes[ord]]);
        }; ## end for
        title(titleFunc(overlap));
        readline(sprintf('overlap %.02f',overlap));
    }; ## end for
}; ## end plotRes()

titleFunc &lt;- function(overlap) sprintf('R merge solutions: single-column integer key, 0..1:0..1 cardinality, %d%% overlap',as.integer(overlap*100));
plotRes(res,titleFunc,T);
</code></pre>

<p><a href=""https://i.stack.imgur.com/yvTPK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yvTPK.png"" alt=""R-merge-benchmark-single-column-integer-key-optional-one-to-one-99""></a></p>

<p><a href=""https://i.stack.imgur.com/kUyPB.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kUyPB.png"" alt=""R-merge-benchmark-single-column-integer-key-optional-one-to-one-50""></a></p>

<p><a href=""https://i.stack.imgur.com/vAuxQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vAuxQ.png"" alt=""R-merge-benchmark-single-column-integer-key-optional-one-to-one-1""></a></p>

<hr>

<p>Here's a second large-scale benchmark that's more heavy-duty, with respect to the number and types of key columns, as well as cardinality. For this benchmark I use three key columns: one character, one integer, and one logical, with no restrictions on cardinality (that is, <code>0..*:0..*</code>). (In general it's not advisable to define key columns with double or complex values due to floating-point comparison complications, and basically no one ever uses the raw type, much less for key columns, so I haven't included those types in the key columns. Also, for information's sake, I initially tried to use four key columns by including a POSIXct key column, but the POSIXct type didn't play well with the <code>sqldf.indexed</code> solution for some reason, possibly due to floating-point comparison anomalies, so I removed it.)</p>

<pre><code>makeArgSpecs.assortedKey.optionalManyToMany &lt;- function(size,overlap,uniquePct=75) {

    ## number of unique keys in df1
    u1Size &lt;- as.integer(size*uniquePct/100);

    ## (roughly) divide u1Size into bases, so we can use expand.grid() to produce the required number of unique key values with repetitions within individual key columns
    ## use ceiling() to ensure we cover u1Size; will truncate afterward
    u1SizePerKeyColumn &lt;- as.integer(ceiling(u1Size^(1/3)));

    ## generate the unique key values for df1
    keys1 &lt;- expand.grid(stringsAsFactors=F,
        idCharacter=replicate(u1SizePerKeyColumn,paste(collapse='',sample(letters,sample(4:12,1L),T))),
        idInteger=sample(u1SizePerKeyColumn),
        idLogical=sample(c(F,T),u1SizePerKeyColumn,T)
        ##idPOSIXct=as.POSIXct('2016-01-01 00:00:00','UTC')+sample(u1SizePerKeyColumn)
    )[seq_len(u1Size),];

    ## rbind some repetitions of the unique keys; this will prepare one side of the many-to-many relationship
    ## also scramble the order afterward
    keys1 &lt;- rbind(keys1,keys1[sample(nrow(keys1),size-u1Size,T),])[sample(size),];

    ## common and unilateral key counts
    com &lt;- as.integer(size*overlap);
    uni &lt;- size-com;

    ## generate some unilateral keys for df2 by synthesizing outside of the idInteger range of df1
    keys2 &lt;- data.frame(stringsAsFactors=F,
        idCharacter=replicate(uni,paste(collapse='',sample(letters,sample(4:12,1L),T))),
        idInteger=u1SizePerKeyColumn+sample(uni),
        idLogical=sample(c(F,T),uni,T)
        ##idPOSIXct=as.POSIXct('2016-01-01 00:00:00','UTC')+u1SizePerKeyColumn+sample(uni)
    );

    ## rbind random keys from df1; this will complete the many-to-many relationship
    ## also scramble the order afterward
    keys2 &lt;- rbind(keys2,keys1[sample(nrow(keys1),com,T),])[sample(size),];

    ##keyNames &lt;- c('idCharacter','idInteger','idLogical','idPOSIXct');
    keyNames &lt;- c('idCharacter','idInteger','idLogical');
    ## note: was going to use raw and complex type for two of the non-key columns, but data.table doesn't seem to fully support them
    argSpecs &lt;- list(
        default=list(copySpec=1:2,args=list(
            df1 &lt;- cbind(stringsAsFactors=F,keys1,y1=sample(c(F,T),size,T),y2=sample(size),y3=rnorm(size),y4=replicate(size,paste(collapse='',sample(letters,sample(4:12,1L),T)))),
            df2 &lt;- cbind(stringsAsFactors=F,keys2,y5=sample(c(F,T),size,T),y6=sample(size),y7=rnorm(size),y8=replicate(size,paste(collapse='',sample(letters,sample(4:12,1L),T)))),
            keyNames
        )),
        data.table.unkeyed=list(copySpec=1:2,args=list(
            as.data.table(df1),
            as.data.table(df2),
            keyNames
        )),
        data.table.keyed=list(copySpec=1:2,args=list(
            setkeyv(as.data.table(df1),keyNames),
            setkeyv(as.data.table(df2),keyNames)
        ))
    );
    ## prepare sqldf
    initSqldf();
    sqldf(paste0('create index df1_key on df1(',paste(collapse=',',keyNames),');')); ## upload and create an sqlite index on df1
    sqldf(paste0('create index df2_key on df2(',paste(collapse=',',keyNames),');')); ## upload and create an sqlite index on df2

    argSpecs;

}; ## end makeArgSpecs.assortedKey.optionalManyToMany()

sizes &lt;- c(1e1L,1e3L,1e5L); ## 1e5L instead of 1e6L to respect more heavy-duty inputs
overlaps &lt;- c(0.99,0.5,0.01);
solTypes &lt;- setdiff(getSolTypes(),'in.place');
system.time({ res &lt;- testGrid(makeArgSpecs.assortedKey.optionalManyToMany,sizes,overlaps,solTypes); });
##     user   system  elapsed
## 38895.50   784.19 39745.53
</code></pre>

<p>The resulting plots, using the same plotting code given above:</p>

<pre><code>titleFunc &lt;- function(overlap) sprintf('R merge solutions: character/integer/logical key, 0..*:0..* cardinality, %d%% overlap',as.integer(overlap*100));
plotRes(res,titleFunc,F);
</code></pre>

<p><a href=""https://i.stack.imgur.com/7gShO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7gShO.png"" alt=""R-merge-benchmark-assorted-key-optional-many-to-many-99""></a></p>

<p><a href=""https://i.stack.imgur.com/yTeRO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yTeRO.png"" alt=""R-merge-benchmark-assorted-key-optional-many-to-many-50""></a></p>

<p><a href=""https://i.stack.imgur.com/buxZ2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/buxZ2.png"" alt=""R-merge-benchmark-assorted-key-optional-many-to-many-1""></a></p>
"
38246955,3808394,2016-07-07T13:36:37Z,2231993,7,FALSE,"<p>It's highly recommended to use the <a href=""https://github.com/dgrtwo/fuzzyjoin"" rel=""noreferrer"">dgrtwo/fuzzyjoin</a> package.
<code>
stringdist_inner_join(a,b, by=""Fund.Name"")
</code></p>
"
38255415,1681468,2016-07-07T21:16:40Z,2478352,4,FALSE,"<p>A slight modification to @Marek very helpful answer WILL add a header to the rownames column: temporarily add the rownames as the first column in the data.frame, and write that, ignoring the real rownames.</p>

<pre><code>&gt; a = matrix(1:9, nrow = 3, ncol = 3, dimnames = list(LETTERS[1:3], LETTERS[1:3]))
&gt; write.table(data.frame(""H""=rownames(a),a),""a.txt"", row.names=FALSE)
</code></pre>

<p>and you get</p>

<pre><code>""H"" ""A"" ""B"" ""C""
""A"" 1 4 7
""B"" 2 5 8
""C"" 3 6 9
</code></pre>
"
38366783,6574113,2016-07-14T06:16:58Z,1405571,1,FALSE,"<p>Try the following codes:</p>

<p><code>writeLines(capture.out((summary(lm(hwy~cyl*drv,data=mpg)),con=""summary.txt"",sep=""\n"")</code></p>

<p>Then you can open the txt file ""summary.txt"" to see your results.</p>
"
38423485,1840471,2016-07-17T16:36:00Z,2427742,1,FALSE,"<p>To generalize @xiechao's solution (@hadley's doesn't work in latest ggplot), you can reverse the factor orders to achieve this:</p>

<pre><code>library(ggplot2)
data(mtcars)
mtcars$gear &lt;- factor(mtcars$gear)  # First make factor with default levels
mtcars$gear &lt;- factor(mtcars$gear, levels=rev(levels(mtcars$gear)))
qplot(cyl, data=mtcars, geom=""bar"", fill=gear)
# Or with ggplot
ggplot(mtcars, aes(factor(cyl), fill=gear)) + geom_bar()
</code></pre>

<p><a href=""https://i.stack.imgur.com/VqOXQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VqOXQ.png"" alt=""enter image description here""></a></p>
"
38429257,2600357,2016-07-18T05:37:50Z,2098368,-3,FALSE,"<p>There is one very simple method: </p>

<pre><code>oneString = c(""bhushan"", ""pant"") 
twoString = c(""is"", ""a"", ""good"", ""boy"") 
c(oneString, twoString) 

Output: ""bhushan"" ""pant"" ""is"" ""a"" ""good"" ""boy""
</code></pre>

<p>Thanks :)</p>
"
38483821,4564247,2016-07-20T14:18:12Z,2247045,1,FALSE,"<p>A helper function:</p>

<pre><code>fixed_split &lt;- function(text, n) {
  strsplit(text, paste0(""(?&lt;=.{"",n,""})""), perl=TRUE)
}

fixed_split(x, 2)
[[1]]
[1] ""xx"" ""yy"" ""xy"" ""xy""
</code></pre>
"
38650457,6652533,2016-07-29T04:21:30Z,1402001,0,FALSE,"<p><code>n2khelper</code> package is not available for <strong>R version 3.2.4</strong></p>
"
38779637,6680314,2016-08-05T01:05:42Z,1815606,4,FALSE,"<p>I just worked this out myself.  To ensure portability of your script always begin it with:</p>

<pre><code>wd &lt;- setwd(""."")
setwd(wd)
</code></pre>

<p>It works because ""."" translates like the Unix command $PWD. Assigning this string to a character object allows you to then insert that character object into setwd() and <em>Presto</em> your code will always run with its current directory as the working directory, no matter whose machine it is on or where in the file structure it is located. (Extra bonus: The wd object can be used with file.path() (ie. file.path(wd, ""output_directory"") to allow for the creation of a standard output directory regardless of the file path leading to your named directory.  This does require you to make the new directory before referencing it this way but that, too, can be aided with the wd object.</p>

<p>Alternately, the following code performs the exact same thing:</p>

<pre><code>wd &lt;- getwd()
setwd(wd)
</code></pre>

<p>or, if you don't need the file path in an object you can simply:</p>

<pre><code>setwd(""."")
</code></pre>
"
38811467,802050,2016-08-07T06:15:13Z,1309263,-1,FALSE,"<p>Here is the java implementation</p>

<pre><code>package MedianOfIntegerStream;

import java.util.Comparator;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;
import java.util.TreeSet;


public class MedianOfIntegerStream {

    public Set&lt;Integer&gt; rightMinSet;
    public Set&lt;Integer&gt; leftMaxSet;
    public int numOfElements;

    public MedianOfIntegerStream() {
        rightMinSet = new TreeSet&lt;Integer&gt;();
        leftMaxSet = new TreeSet&lt;Integer&gt;(new DescendingComparator());
        numOfElements = 0;
    }

    public void addNumberToStream(Integer num) {
        leftMaxSet.add(num);

        Iterator&lt;Integer&gt; iterMax = leftMaxSet.iterator();
        Iterator&lt;Integer&gt; iterMin = rightMinSet.iterator();
        int maxEl = iterMax.next();
        int minEl = 0;
        if (iterMin.hasNext()) {
            minEl = iterMin.next();
        }

        if (numOfElements % 2 == 0) {
            if (numOfElements == 0) {
                numOfElements++;
                return;
            } else if (maxEl &gt; minEl) {
                iterMax.remove();

                if (minEl != 0) {
                    iterMin.remove();
                }
                leftMaxSet.add(minEl);
                rightMinSet.add(maxEl);
            }
        } else {

            if (maxEl != 0) {
                iterMax.remove();
            }

            rightMinSet.add(maxEl);
        }
        numOfElements++;
    }

    public Double getMedian() {
        if (numOfElements % 2 != 0)
            return new Double(leftMaxSet.iterator().next());
        else
            return (leftMaxSet.iterator().next() + rightMinSet.iterator().next()) / 2.0;
    }

    private class DescendingComparator implements Comparator&lt;Integer&gt; {
        @Override
        public int compare(Integer o1, Integer o2) {
            return o2 - o1;
        }
    }

    public static void main(String[] args) {
        MedianOfIntegerStream streamMedian = new MedianOfIntegerStream();

        streamMedian.addNumberToStream(1);
        System.out.println(streamMedian.getMedian()); // should be 1

        streamMedian.addNumberToStream(5);
        streamMedian.addNumberToStream(10);
        streamMedian.addNumberToStream(12);
        streamMedian.addNumberToStream(2);
        System.out.println(streamMedian.getMedian()); // should be 5

        streamMedian.addNumberToStream(3);
        streamMedian.addNumberToStream(8);
        streamMedian.addNumberToStream(9);
        System.out.println(streamMedian.getMedian()); // should be 6.5
    }
}
</code></pre>
"
38944950,4955995,2016-08-14T17:49:35Z,1826519,1,FALSE,"<p>[A] 
If each of foo and bar is a single number, then there's nothing wrong with c(foo,bar); and you can also name the components: c(Foo=foo,Bar=bar). So you could access the components of the result 'res' as res[1], res[2]; or, in the named case, as res[""Foo""], res[""BAR""]. </p>

<p>[B] 
If foo and bar are vectors of the same type and length, then again there's nothing wrong with returning cbind(foo,bar) or rbind(foo,bar); likewise nameable. In the 'cbind' case, you would access foo and bar as res[,1], res[,2] or as res[,""Foo""], res[,""Bar""]. You might also prefer to return a dataframe rather than a matrix: </p>

<pre><code>data.frame(Foo=foo,Bar=bar)
</code></pre>

<p>and access them as res$Foo, res$Bar. This would also work well if foo and bar were of the same length but not of the same type (e.g. foo is a vector of numbers, bar a vector of character strings). </p>

<p>[C] 
If foo and bar are sufficiently different not to combine conveniently as above, then you shuld definitely return a list. </p>

<p>For example, your function might fit a linear model and 
also calculate predicted values, so you could have </p>

<pre><code>LM&lt;-lm(....) ; foo&lt;-summary(LM); bar&lt;-LM$fit
</code></pre>

<p>and then you would <code>return list(Foo=foo,Bar=bar)</code> and then access the summary as res$Foo, the predicted values as res$Bar</p>

<p>source: <a href=""http://r.789695.n4.nabble.com/How-to-return-multiple-values-in-a-function-td858528.html"" rel=""nofollow"">http://r.789695.n4.nabble.com/How-to-return-multiple-values-in-a-function-td858528.html</a></p>
"
39164220,6761007,2016-08-26T10:33:54Z,2050790,0,FALSE,"<p>why do these two different operators, <code>[ ]</code>, and <code>[[ ]]</code>, return the same result?</p>

<pre><code>x = list(1, 2, 3, 4)
</code></pre>

<ol>
<li><p><code>[ ]</code> provides sub setting operation. In general sub set of any object
will have the same type as the original object. Therefore, <code>x[1]</code>
provides a list. Similarly <code>x[1:2]</code> is a subset of original list,
therefore it is a list. Ex.</p>

<pre><code>x[1:2]

[[1]] [1] 1

[[2]] [1] 2
</code></pre></li>
<li><p><code>[[ ]]</code> is for extracting an element from the list. <code>x[[1]]</code> is valid
and extract the first element from the list. <code>x[[1:2]]</code> is not valid as <code>[[ ]]</code>
does not provide sub setting like <code>[ ]</code>. </p>

<pre><code> x[[2]] [1] 2 

&gt; x[[2:3]] Error in x[[2:3]] : subscript out of bounds
</code></pre></li>
</ol>
"
39178813,6764048,2016-08-27T07:54:50Z,2547402,-1,FALSE,"<p>An easy way to calculate MODE of a vector 'v' containing discrete values is:</p>

<pre><code>names(sort(table(v)))[length(sort(table(v)))]
</code></pre>
"
39184196,3564318,2016-08-27T18:03:16Z,1401904,1,FALSE,"<p>I am on Windows 8 and for some weird reason, I can never install packages using my internet connections.</p>

<p>I generally install it using the .zip file from CRAN.</p>

<p>After I went from R 3.2.5 to R 3.3.1.</p>

<p>I simply copied the packages from </p>

<p><code>C:\Path\to\packa\R\win-library\3.2</code> to <code>C:\Path\to\packa\R\win-library\3.3</code>.</p>

<p>And then I restarted the R session. Worked perfectly. 
I haven't checked if ALL the packages are functioning well. 
But, the ones I checked are working perfectly well.
Hope this hack works for everybody.</p>

<p>Cheers.</p>
"
39205077,5236611,2016-08-29T11:22:59Z,1439513,1,FALSE,"<p>for ""a"" to ""z"" its</p>

<p>paste(letters)</p>

<p>for ""A"" to ""Z"" its </p>

<p>paste(LETTERS)</p>

<p>And to print specific letters in the sequence, say if you want only j, k &amp; l</p>

<p>print(letters[10:12])</p>
"
39410542,6682076,2016-09-09T11:27:23Z,1395174,1,FALSE,"<p>You can used Shiny app into R
<a href=""http://shiny.rstudio.com/tutorial/"" rel=""nofollow"">http://shiny.rstudio.com/tutorial/</a></p>
"
39420973,3217870,2016-09-09T23:24:20Z,1581232,1,FALSE,"<p>to format some summaries from <code>dplyr</code>, here is boilerplate code:</p>

<p><code>df %&gt;%
    summarise(mu=mean(big_values),
              min=min(big_values),
              max=max(big_values)) %&gt;%
    mutate_each(funs(prettyNum(., big.mark="","")))
</code></p>
"
39450051,6682076,2016-09-12T12:13:08Z,1197434,1,FALSE,"<p>Now we can used read.csv or read.table. </p>

<p>For example</p>

<pre><code>df = read.csv(""~/data/demo.csv"",header = TRUE)
</code></pre>
"
39463984,1534917,2016-09-13T07:01:34Z,2547402,4,FALSE,"<p>This hack should work fine. Gives you the value as well as the count of mode: </p>

<pre><code>Mode &lt;- function(x){
a = table(x) # x is a vector
return(a[which.max(a)])
}
</code></pre>
"
39515378,6835986,2016-09-15T15:50:51Z,1238250,1,FALSE,"<p>A late response, but I thought I'd follow up to pageman's comment. Let's say you create an object with the vss function:</p>

<pre><code>my.vss &lt;- vss(test.data)
</code></pre>

<p>The summary function will provide the VSS and MAP criterion results, e.g.,  </p>

<pre><code>summary(my.vss)
</code></pre>

<p>but you can easily pull out the MAP results from the object as well (like I needed to when running VSS and MAP criterion tests over many datasets), like so:</p>

<pre><code>#returns the number of factors recommended by MAP
which(my.vss$map == min(my.vss$map))   

#returns the number of factors recommended by VSS for complexity 1
which(my.vss$cfit.1 == max(my.vss$cfit.1)

#returns the number of factors recommended by VSS for complexity 2
which(my.vss$cfit.2 == max(my.vss$cfit.2))
</code></pre>

<p>There are also a whole bunch of fun stats hanging out inside of the vss.stats data.frame with in the vss object, i.e.,</p>

<pre><code>class(my.vss$vss.stats)
</code></pre>
"
39572385,1090848,2016-09-19T11:44:34Z,2619618,2,FALSE,"<p>I would do</p>

<pre><code>x = by(data, list(data$x, data$y), function(d) whatever(d))
array(x, dim(x), dimnames(x))
</code></pre>
"
39624805,6337680,2016-09-21T19:15:01Z,2547402,0,FALSE,"<p>Calculating Mode is mostly in case of factor variable then we can use </p>

<pre><code>labels(table(HouseVotes84$V1)[as.numeric(labels(max(table(HouseVotes84$V1))))])
</code></pre>

<p>HouseVotes84 is dataset available in 'mlbench' package.</p>

<p>it will give max label value. it is easier to use by inbuilt functions itself without writing function.</p>
"
39669864,2911282,2016-09-23T21:33:46Z,2315601,2,FALSE,"<p>Running this little piece of code allowed me to understand the order function</p>

<pre><code>x &lt;- c(3, 22, 5, 1, 77)

cbind(
  index=1:length(x),
  rank=rank(x),
  x, 
  order=order(x), 
  sort=sort(x)
)

     index rank  x order sort
[1,]     1    2  3     4    1
[2,]     2    4 22     1    3
[3,]     3    3  5     3    5
[4,]     4    1  1     2   22
[5,]     5    5 77     5   77
</code></pre>

<p>Reference: <a href=""http://r.789695.n4.nabble.com/I-don-t-understand-the-order-function-td4664384.html"" rel=""nofollow"">http://r.789695.n4.nabble.com/I-don-t-understand-the-order-function-td4664384.html</a> </p>
"
39699347,6045390,2016-09-26T09:37:09Z,2254986,3,FALSE,"<p>There is of course a <code>lubridate</code> solution for this:</p>

<pre><code>library(lubridate)
date &lt;- ""2009-10-01""

ymd(date) - 5
# [1] ""2009-09-26""
</code></pre>

<p>is the same as</p>

<pre><code>ymd(date) - days(5)
# [1] ""2009-09-26""
</code></pre>

<p>Other time formats could be:</p>

<pre><code>ymd(date) - months(5)
# [1] ""2009-05-01""

ymd(date) - years(5)
# [1] ""2004-10-01""

ymd(date) - years(1) - months(2) - days(3)
# [1] ""2008-07-29""
</code></pre>
"
39946226,5124002,2016-10-09T17:07:22Z,1429907,0,FALSE,"<p>I also do what Josh Reich does, only I do that creating my personal R-packages, as it helps me structure my code and data, and it is also quite easy to share those with others.</p>

<ol>
<li>create my package</li>
<li>load</li>
<li>clean</li>
<li>functions</li>
<li>do</li>
</ol>

<p>creating my package: devtools::create('package_name')</p>

<p>load and clean: I create scripts in the data-raw/ subfolder of my package for loading, cleaning, and storing the resulting data objects in the package using devtools::use_data(object_name). Then I compile the package. 
From now on, calling library(package_name) makes these data available (and they are not loaded until necessary).</p>

<p>functions: I put the functions for my analyses into the R/ subfolder of my package, and export only those that need to be called from outside (and not the helper functions, which can remain invisible).</p>

<p>do: I create a script that uses the data and functions stored in my package.
(If the analyses only need to be done once, I can put this script as well into the data-raw/ subfolder, run it, and store the results in the package to make it easily accessible.)</p>
"
39965828,1110063,2016-10-10T19:57:14Z,1630724,4,FALSE,"<p>A graceful way of <em>""including""</em> a long SQL query is to keep it in a separate <code>.sql</code> file. Preferably somewhere it can  be syntax highlighted, a text file in RStudio will do the job. You can then in your main R script read the file into a string and populate it with variables using one of the many ""named"" <code>sprintf</code>-type solutions, such as <a href=""https://github.com/Bart6114/infuser"" rel=""nofollow"">infuser</a>.</p>

<p><strong>.sql</strong></p>

<pre><code>select *
from mytable
where id = {{a}} 
and somevar = {{b}}
</code></pre>

<p><strong>.R</strong></p>

<pre><code>library(readr)
library(infuser)

query &lt;- read_file(""query.sql"") %&gt;%
         infuse(a = 1, b = 2) 
</code></pre>
"
39974665,684229,2016-10-11T09:52:30Z,1445964,0,FALSE,"<p>You do it by setting</p>

<pre><code>options(show.error.locations = TRUE)
</code></pre>

<p>I just wonder why this setting is not a default in R? It should be, as it is in every other language.</p>
"
40009177,4477364,2016-10-12T22:08:22Z,2375587,2,FALSE,"<p>Since this question was last active Hadley has released his new <code>forcats</code> package for manipulating factors and I'm finding it outrageously useful. Examples from the OP's data frame:</p>

<pre><code>levels(df$letters)
# [1] ""a"" ""b"" ""c"" ""d""
</code></pre>

<p>To reverse levels:</p>

<pre><code>library(forcats)
fct_rev(df$letters) %&gt;% levels
# [1] ""d"" ""c"" ""b"" ""a""
</code></pre>

<p>To add more levels:</p>

<pre><code>fct_expand(df$letters, ""e"") %&gt;% levels
# [1] ""a"" ""b"" ""c"" ""d"" ""e""
</code></pre>

<p>And many more useful <code>fct_xxx()</code> functions.</p>
"
40164111,161921,2016-10-20T20:38:47Z,1743698,12,FALSE,"<p>Sorry but I don't understand why too many people even think a string was something that could be evaluated.  You must change your mindset, really.
  Forget all connections between strings on one side  and  expressions, calls, evaluation  on the other side.</p>

<p>The (possibly) only connection is via  <code>parse(text = ....)</code>     and  all good R programmers should know that  this is rarely an efficient or safe means to construct expressions (or calls).   Rather learn more about <code>substitute()</code> , <code>quote()</code>, and possibly the power of using  <code>do.call(substitute, ......)</code></p>

<blockquote>
  <p>R> fortunes::fortune(""answer is parse"")</p>
  
  <p>If the answer is parse() you should usually rethink the question.
        -- Thomas Lumley
        R-help (February 2005)</p>
  
  <p>R> </p>
</blockquote>
"
40265341,3277393,2016-10-26T14:45:01Z,2185958,1,FALSE,"<p>The <a href=""https://cran.r-project.org/web/packages/clust.bin.pair/"" rel=""nofollow"">clust.bin.pair</a> package for clustered binary matched-pair data was recently published to CRAN.</p>

<p>It contains implementations of Eliasziw and Donner (1991) and Obuchowski (1998), as well as two more recent tests in the same family Durkalski (2003) and Yang (2010).</p>
"
40292374,4752959,2016-10-27T19:06:29Z,2679193,-1,FALSE,"<p>And this option?</p>

<pre><code>list_name&lt;-list()
for(i in 1:100){
    paste(""orca"",i,sep="""")-&gt;list_name[[i]]
}
</code></pre>

<p>It works perfectly. In the example you put,  first line is missing, and then gives you the error message.</p>
"
40294218,1951065,2016-10-27T21:08:03Z,2661402,0,FALSE,"<p>As others have pointed out, this might be framed as a model selection question. It is a wrong approach to use the distribution that fits the data best without taking into account the complexity of the distribution. This is because the more complicated distribution will generally have better fit, but it will likely overfit the data.</p>

<p>You can use the Akaike Information Criteria (AIC) to take into account the complexity of the distribution. This is still unsatisfactory as you're only considering a limited number of distributions, but is still better than just using the log likelihood.</p>

<p>I use just a few distributions, but you can <a href=""https://cran.r-project.org/web/packages/fitdistrplus/vignettes/FAQ.html#how-do-i-know-the-root-name-of-a-distribution"" rel=""nofollow"">check the documentation</a> to find others that could be relevant</p>

<p>Using the <code>fitdistrplus</code> you can run: </p>

<pre><code>library(fitdistrplus)

distributions = c(""norm"", ""lnorm"", ""exp"",
          ""cauchy"", ""gamma"", ""logis"",
          ""weibull"")


# the x vector is defined as in the question

# Plot to see which distributions make sense. This should influence
# your choice of candidate distributions
descdist(x, discrete = FALSE, boot = 500)

distr_aic = list()
distr_fit = list()
for (distribution in distributions) {
    distr_fit[[distribution]] = fitdist(x, distribution)
    distr_aic[[distribution]] = distr_fit[[distribution]]$aic
}

&gt; distr_aic
$norm
[1] 5032.269

$lnorm
[1] 5421.815

$exp
[1] 6602.334

$cauchy
[1] 5382.643

$gamma
[1] 5184.17

$logis
[1] 5047.796

$weibull
[1] 5058.336
</code></pre>

<p>According to our plot and the AIC, it makes sense to use a normal. You can automatize this by just picking the distribution with the minimum AIC. You can check the estimated parameters with</p>

<pre><code>&gt; distr_fit[['norm']]
Fitting of the distribution ' norm ' by maximum likelihood 
Parameters:
     estimate Std. Error
mean 9.975849 0.09454476
sd   2.989768 0.06685321
</code></pre>
"
40427355,7115960,2016-11-04T16:18:18Z,1174799,-1,FALSE,"<p>I made an example for you, I hope it is useful</p>

<pre><code># 1st let make a graph with limits xlim =0:10 e ylim=0:10.
plot(0:10,0:10, type=""n"")

# let use the 'for' to put texts on graph:
for(i in 1:10)
    text(i,i, paste(""**"", i))

## let retard steps 1 sec

plot(0:10,0:10, type=""n"")
for(i in 1:9){
    text(i,i, paste(""step"", i))
    Sys.sleep(1) 
}

# please wait some seconds.......
# now faster
plot(0:10,0:10, type=""n"")
for(k in 1:9){
    text(k,k, paste(""step"", k))
    Sys.sleep(.1) ## retard steps 0,1 sec
}
</code></pre>
"
40680889,5490241,2016-11-18T15:37:17Z,2080774,0,FALSE,"<p>I think this question should be complemented with the <code>poly/polym</code> function, which goes futher: it generates not only interactions between the variables, but its power until the selected degree. And <a href=""https://en.wikipedia.org/wiki/Orthogonal_polynomials"" rel=""nofollow noreferrer"">orthogonal iteractions</a>, which may be very usefull.</p>

<p>The directly solution to the asked problem would be:</p>

<pre><code>&gt; polym(x$V1, x$V2, x$V3, x$V4, degree = 2, raw = T)
     1.0.0.0 2.0.0.0 0.1.0.0 1.1.0.0 0.2.0.0 0.0.1.0 1.0.1.0 0.1.1.0 0.0.2.0 0.0.0.1 1.0.0.1 0.1.0.1 0.0.1.1 0.0.0.2
[1,]       1       1       9       9      81      25      25     225     625      18      18     162     450     324
[2,]       2       4       5      10      25      20      40     100     400      10      20      50     200     100
[3,]       3       9       4      12      16      30      90     120     900      12      36      48     360     144
[4,]       4      16       4      16      16      34     136     136    1156      16      64      64     544     256
attr(,""degree"")
 [1] 1 2 1 2 2 1 2 2 2 1 2 2 2 2
</code></pre>

<p>The columns 4, 7, 8, 11, 12, 13 has the requested in the question. Other columns have other kinds of interactions. If you would like to get orthogonal interactions, just set <code>raw = FALSE</code>.</p>
"
40705476,1577113,2016-11-20T14:54:23Z,2050790,1,FALSE,"<p>Alhough this is pretty old question I must say it is touching exactly the knowledge I was missing during my first steps in R - i.e. how to express data in my hand as object in R or how to select from existing objects. It is not easy for a R novice to think ""in a R box"" from the very beginning.</p>

<p>So I myself started to use crutches below which helped me a lot to find out what object to use for what data, and basically to imagine real world usage.</p>

<p>Though I not giving exact answers to the question the short text below might help reader who just started with R and is asking simmilar questions.</p>

<ul>
<li>Atomic vector ... I called that ""sequence"" for myself, no direction, just sequence of same types. <code>[</code> subsets.</li>
<li>Vector ... sequence with one direction from 2D, <code>[</code> subsets.</li>
<li>Matrix ... bunch of vectors with same length forming rows or columns, <code>[</code> subsets by rows and columns, or by sequence.</li>
<li>Arrays ... layered matrices forming 3D</li>
<li>Dataframe ... a 2D table like in excel, where I can sort, add or remove rows or columns or make arit. operations with them, only after some time I truly recognized that dataframe is an clever implementation of <code>list</code> where I can subset using <code>[</code> by rows and columns, but even using <code>[[</code>.</li>
<li>List ... to help myself I thought about list as of <code>tree structure</code> where <code>[i]</code> selects and returns whole branches and <code>[[i]]</code> returns item from the branch. And because it it is <code>tree like structure</code>, you can even use an <code>index sequence</code> to address every single leaf on a very complex <code>list</code> using its <code>[[index_vector]]</code>. Lists can be simple or very complex and can mix together various types of objects into one.</li>
</ul>

<p>So for <code>lists</code> you can end up with more ways how to select a <code>leaf</code> depending on situation like in following example.</p>

<pre><code>l &lt;- list(""aaa"",5,list(1:3),LETTERS[1:4],matrix(1:9,3,3))
l[[c(5,4)]] # selects 4 from matrix using [[index_vector]] in list
l[[5]][4] # selects 4 from matrix using sequential index in matrix
l[[5]][1,2] # selects 4 from matrix using row and column in matrix
</code></pre>

<p>This way of thinking helped me a lot.</p>
"
40805919,1851270,2016-11-25T13:13:47Z,1815606,0,FALSE,"<p>99% of the cases you might simply use:</p>

<pre><code>sys.calls()[[1]] [[2]]
</code></pre>

<p>It will not work for crazy calls where the script is not the first argument, i.e., <code>source(some args, file=""myscript"")</code>. Use @hadley's in these fancy cases.</p>
"
40819349,956107,2016-11-26T14:26:28Z,2078468,0,FALSE,"<p>The data is from <a href=""https://data.gov.sg/dataset/street-and-places"" rel=""nofollow noreferrer"">data.gov.sg</a>. Loading the packages required to plot the maps</p>

<pre><code>library(rgdal)  #for shapefiles
library(ggmap)  #plotting maps
library(ggplot2) #general plots
</code></pre>

<p>Reading the shapefile using readOGR</p>

<pre><code> shpfile &lt;- readOGR(dsn = 'data/street-and-places/','StreetsandPlaces',verbose = FALSE)
head(coordinates(shpfile),5)

     coords.x1 coords.x2
[1,]  28640.40  29320.77
[2,]  29429.83  28548.69
[3,]  29224.01  28360.38
[4,]  29827.20  29664.26
[5,]  28451.00  29451.78
</code></pre>

<p>We observe that the coordinates are in a different projection system. So we have to convert this to web mercator projection system. We transform the shapefile using spTransform.</p>

<pre><code>crsobj &lt;- CRS(""+proj=longlat +datum=WGS84"")   #Web Mercator projection system
shpfile_t &lt;- spTransform(shpfile,crsobj)      #Applying projection transformation
df &lt;- as.data.frame(coordinates(shpfile_t))   #Converting to a data frame
head(df,5)

coords.x1&lt;dbl&gt;coords.x2&lt;dbl&gt;
1   103.8391    1.281441        
2   103.8462    1.274459        
3   103.8443    1.272756        
4   103.8497    1.284547        
5   103.8374    1.282626    
</code></pre>

<p>We notice the transformed projection system. Let us plot it on top of a base map.</p>

<pre><code>sgmap &lt;- get_map(location=""Singapore"", zoom=11,   
             maptype=""roadmap"", source=""google"") #Using Osm base map of Singapore
p &lt;- ggmap(sgmap) +
    geom_point(data = df,
    aes(x = coords.x1, y = coords.x2),
    color = 'orange',size = 1)
p
</code></pre>
"
40899800,1587132,2016-11-30T22:57:20Z,1975110,-1,FALSE,"<p>I wrote a solution that works like <code>try</code>, except that it also returns the call stack. </p>

<pre><code>tryStack &lt;- function(
expr,
silent=FALSE
)
{
tryenv &lt;- new.env()
out &lt;- try(withCallingHandlers(expr, error=function(e)
  {
  stack &lt;- sys.calls()
  stack &lt;- stack[-(2:7)]
  stack &lt;- head(stack, -2)
  stack &lt;- sapply(stack, deparse)
  if(!silent &amp;&amp; isTRUE(getOption(""show.error.messages""))) 
    cat(""This is the error stack: "", stack, sep=""\n"")
  assign(""stackmsg"", value=paste(stack,collapse=""\n""), envir=tryenv)
  }), silent=silent)
if(inherits(out, ""try-error"")) out[2] &lt;- tryenv$stackmsg
out
}

lower &lt;- function(a) a+10
upper &lt;- function(b) {plot(b, main=b) ; lower(b) }

d &lt;- tryStack(upper(4))
d &lt;- tryStack(upper(""4""))
cat(d[2])
</code></pre>

<p>More info in my answer here: 
<a href=""https://stackoverflow.com/a/40899766/1587132"">https://stackoverflow.com/a/40899766/1587132</a></p>
"
40962608,2966222,2016-12-04T19:20:29Z,2185252,1,FALSE,"<p>Here is another example showing the use of <code>gather</code> from <code>tidyr</code>. You can select the columns to <code>gather</code> either by removing them individually (as I do here), or by including the years you want explicitly.</p>

<p>Note that, to handle the commas (and X's added if <code>check.names = FALSE</code> is not set), I am also using <code>dplyr</code>'s mutate with <code>parse_number</code> from <code>readr</code> to convert the text values back to numbers. These are all part of the <code>tidyverse</code> and so can be loaded together with <code>library(tidyverse)</code></p>

<pre><code>wide %&gt;%
  gather(Year, Value, -Code, -Country) %&gt;%
  mutate(Year = parse_number(Year)
         , Value = parse_number(Value))
</code></pre>

<p>Returns:</p>

<pre><code>   Code     Country Year Value
1   AFG Afghanistan 1950 20249
2   ALB     Albania 1950  8097
3   AFG Afghanistan 1951 21352
4   ALB     Albania 1951  8986
5   AFG Afghanistan 1952 22532
6   ALB     Albania 1952 10058
7   AFG Afghanistan 1953 23557
8   ALB     Albania 1953 11123
9   AFG Afghanistan 1954 24555
10  ALB     Albania 1954 12246
</code></pre>
"
41080533,4440387,2016-12-10T21:32:04Z,1741820,2,FALSE,"<p>This may also add to understanding of the difference between those two operators:</p>

<pre><code>df &lt;- data.frame(
      a = rnorm(10),
      b &lt;- rnorm(10)
)
</code></pre>

<p>For the first element R has assigned values and proper name, while the name of the second element looks a bit strange.</p>

<pre><code>str(df)
# 'data.frame': 10 obs. of  2 variables:
#  $ a             : num  0.6393 1.125 -1.2514 0.0729 -1.3292 ...
#  $ b....rnorm.10.: num  0.2485 0.0391 -1.6532 -0.3366 1.1951 ...
</code></pre>

<p>R version 3.3.2 (2016-10-31); macOS Sierra 10.12.1</p>
"
41429801,3746543,2017-01-02T15:54:47Z,2568234,1,FALSE,"<p><strong>No extra package needed</strong></p>

<p>I'm a bit late to the party, but I found that you can get the desired result very easily with the standard plot function -- simply convert the factor to a numeric value:</p>

<pre><code>plot(as.numeric(trans.factor), casp6)
</code></pre>
"
41528517,1851813,2017-01-08T01:31:15Z,1474081,3,FALSE,"<p><a href=""https://cran.r-project.org/web/packages/githubinstall/vignettes/githubinstall.html"" rel=""nofollow noreferrer"">From cran</a>, you can install directly from a github repository address. So if you want the package at <code>https://github.com/twitter/AnomalyDetection</code>:</p>

<pre><code>library(devtools)
install_github(""twitter/AnomalyDetection"")
</code></pre>

<p>does the trick.</p>
"
41581457,6147938,2017-01-11T01:47:08Z,359438,1,FALSE,"<p>I like Gurobi. It's very expensive for a license, but it can be obtained through many universities. See here <a href=""http://www.gurobi.com/products/modeling-languages/r"" rel=""nofollow noreferrer"">http://www.gurobi.com/products/modeling-languages/r</a></p>
"
41604028,4470365,2017-01-12T02:11:31Z,2479689,1,FALSE,"<p>The underlying issue is that this data is not in <a href=""http://vita.had.co.nz/papers/tidy-data.pdf"" rel=""nofollow noreferrer"">tidy format</a>.  Crosstabbing multiple variables will be easier when the data is reshaped into ""long"" form.  We can do that with <code>gather</code> from the tidyr package.</p>

<p>After reshaping, many crosstab functions will work; I'll use <code>crosstab</code> from the janitor package (since - full disclosure - I maintain that package and built the function for this purpose).</p>

<pre><code># Create reproducible sample data
set.seed(1)
possible_values &lt;- c(""1 (Very Often)"", ""2 (Rarely)"", ""3 (Never)"")
some_values &lt;- sample(possible_values, 100, replace = TRUE)
dat &lt;- data.frame(Q1 = some_values[1:25], Q2 = some_values[26:50], 
                 Q3 = some_values[51:75], Q4 = some_values[76:100])

library(tidyr)
library(janitor)

dat %&gt;%
  gather(question, response) %&gt;% 
  crosstab(question, response)
#&gt;   question 1 (Very Often) 2 (Rarely) 3 (Never)
#&gt; 1       Q1              8          8         9
#&gt; 2       Q2              4         11        10
#&gt; 3       Q3              8         12         5
#&gt; 4       Q4              7          7        11
</code></pre>

<p>From there, you can format as percentages, etc. with <code>janitor::adorn_crosstab()</code>.</p>
"
41684797,1650182,2017-01-16T20:51:32Z,2479059,0,FALSE,"<p>Old question but still a problem...</p>

<p>I'm using R vsn 3.3.2 on OSX 10.12.2, plotting with plot() to a pdf file that I import into Affinity Designer vsn 1.5.4.  Axis labels of the form ""2-0"" show up in Affinity Designer with the dash overlapping the ""0"".  I don't know if the problem lies with Affinity Designer or the pdf file or what.  It would be nice to be able to try various Unicode dash characters, but R and pdf files both seem to not yet be fully equipped to deal with Unicode using the default fonts.</p>

<p>Solution: the ""cairo"" package in R:</p>

<pre><code>library(""cairo"")
d = 0:11
names(d) = paste(0:11, ""-"", 11:0, sep="""")
names(d) = gsub(""-"", ""\U2012"", names(d)) # U+2012 is ""figure dash""
d
barplot(d)
cairo_pdf(filename=""x.pdf"", width=11, height=8)
barplot(d)
dev.off()
</code></pre>

<p>The dashes show up in the R console, default R plotting device, and the pdf file viewed with both Preview and Affinity Designer.</p>
"
41831277,3801801,2017-01-24T14:54:22Z,2375587,2,FALSE,"<p>I wish to add another case where the levels could be strings carrying numbers alongwith some special characters : like below example</p>

<pre><code>df &lt;- data.frame(x = c(""15-25"", ""0-4"", ""5-10"", ""11-14"", ""100+""))
</code></pre>

<p>The default levels of <code>x</code> is :</p>

<pre><code>df$x
# [1] 15-25 0-4   5-10  11-14 100+ 
# Levels: 0-4 100+ 11-14 15-25 5-10
</code></pre>

<p>Here if we want to reorder the factor levels according to the numeric value, without explicitly writing out the levels, what we could do is </p>

<pre><code>library(gtools)
df$x &lt;- factor(df$x, levels = mixedsort(df$x))

df$x
# [1] 15-25 0-4   5-10  11-14 100+ 
# Levels: 0-4 5-10 11-14 15-25 100+
as.numeric(df$x)
# [1] 4 1 2 3 5
</code></pre>

<p>I hope this can be considered as useful information for future readers. </p>
"
41833090,7463860,2017-01-24T16:17:06Z,2048304,0,FALSE,"<p>right click the mouse on the output plot
Copy as metafile
then save plot into a word document (right click to edit picture to covert to the plot to Microsoft Office drawing Object)</p>
"
41851693,1331446,2017-01-25T12:28:56Z,2566766,4,FALSE,"<p>Aniko mentioned this in a comment, but it was never provided as an answer.</p>

<p>I found this independently and then noticed it was here in a comment, so credit to Aniko for getting it first. </p>

<p><code>addmargins</code> is the answer:</p>

<blockquote>
  <p>For a given table one can specify which of the classifying factors to
  expand by one or more levels to hold margins to be calculated. One may
  for example form sums and means over the first dimension and medians
  over the second. The resulting table will then have two extra levels
  for the first dimension and one extra level for the second. The
  default is to sum over all margins in the table. Other possibilities
  may give results that depend on the order in which the margins are
  computed. This is flagged in the printed output from the function.</p>
</blockquote>
"
41854273,936420,2017-01-25T14:36:32Z,2564258,2,FALSE,"<p>You can also create your plot using <a href=""https://cran.r-project.org/web/packages/ggvis/index.html"" rel=""nofollow noreferrer"">ggvis</a>:</p>

<pre><code>library(ggvis)

x  &lt;- seq(-2, 2, 0.05)
y1 &lt;- pnorm(x)
y2 &lt;- pnorm(x,1,1)
df &lt;- data.frame(x, y1, y2)

df %&gt;%
  ggvis(~x, ~y1, stroke := 'red') %&gt;%
  layer_paths() %&gt;%
  layer_paths(data = df, x = ~x, y = ~y2, stroke := 'blue')
</code></pre>

<p>This will create the following plot:</p>

<p><a href=""https://i.stack.imgur.com/TWfYW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TWfYW.png"" alt=""enter image description here""></a></p>
"
41881447,554531,2017-01-26T19:30:28Z,2478352,2,FALSE,"<p>For anyone working in the <a href=""https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/"" rel=""nofollow noreferrer"">tidyverse</a> (dplyr, etc.), the <code>rownames_to_column()</code> function from the <a href=""https://www.rdocumentation.org/packages/radiant.data/versions/0.6.0/topics/rownames_to_column"" rel=""nofollow noreferrer"">tibble</a> package can be used to easily convert row.names to a column, e.g.:</p>

<pre><code>library('tibble')
a = as.data.frame(matrix(1:9, nrow=3, ncol=3, 
                  dimnames=list(LETTERS[1:3], LETTERS[1:3])))

a %&gt;% rownames_to_column('my_id')

  my_id A B C
1     A 1 4 7
2     B 2 5 8
3     C 3 6 9
</code></pre>

<p>Combining this with the <code>row.names=FALSE</code> option in <code>write.table()</code> results in output with header names for all columns.</p>
"
41906916,3574723,2017-01-28T06:09:54Z,1782704,0,FALSE,"<p>another simple answer. This one takes care of 1st value being NA. Thats a dead end so my loop stats from index 2. </p>

<pre><code>my_vec &lt;- c(1, NA, NA, 2, 3, NA, 4)
fill.it &lt;- function(vector){
  new_vec &lt;- vector
  for (i in 2:length(new_vec)){
    if(is.na(new_vec[i])) {
      new_vec[i] &lt;- new_vec[i-1]
    } else {
      next
    }
  } 
  return(new_vec)
}
</code></pre>
"
41929343,5420677,2017-01-30T05:09:43Z,1249548,1,FALSE,"<p>Using <code>dplyr</code>and <code>tidyr</code>:</p>

<pre><code>x &lt;- rnorm(100)
eps &lt;- rnorm(100,0,.2)
df &lt;- as.data.frame(cbind(x, eps)) %&gt;% 
  mutate(p1 = 3*x+eps, p2 = 2*x+eps) %&gt;% 
  tidyr::gather(""plot"", ""value"", 3:4) %&gt;% 
  ggplot(aes(x = x , y = value))+ geom_point()+geom_smooth()+facet_wrap(~plot, ncol =2)

df
</code></pre>
"
41959471,6295598,2017-01-31T14:01:23Z,1616983,0,FALSE,"<p>I had a very similar problem.</p>

<p>What worked for me was to define a project directory (rstudio can do that for you), and then add a <code>.Renviron</code> file that modifies the PATH and LD_LIBRARY_PATH, to include the directory with the new gcc.
In your case, for example, the <code>.Renviron</code> will look something like:</p>

<p><code>LD_LIBRARY_PATH=/usr/local/bin/gcc/lib:/usr/local/bin/gcc/lib64:/usr/local/bin/gcc/libexec:</code><em>other paths</em></p>

<p><code>PATH=/usr/local/bin/gcc/bin:/usr/local/bin:</code><em>other paths</em></p>
"
42094305,7015675,2017-02-07T15:55:36Z,1716012,0,FALSE,"<p>Just for completeness: you can actually 'simulate' tic
and toc in R, so that you can write</p>

<pre><code>tic
## do something
toc
</code></pre>

<p>without parentheses. The trick is to abuse the <code>print</code>
function, as demonstrated in <a href=""http://enricoschumann.net/R/tictoc.htm"" rel=""nofollow noreferrer"" title=""Fun: tic and toc in R"">Fun: tic and toc in R</a>:</p>

<pre><code>tic &lt;- 1
class(tic) &lt;- ""tic""

toc &lt;- 1
class(toc) &lt;- ""toc""

print.tic &lt;- function(x, ...) {
    if (!exists(""proc.time""))
        stop(""cannot measure time"")
    gc(FALSE)
    assign("".temp.tictime"", proc.time(), envir = .GlobalEnv)
}

print.toc &lt;- function(x,...) {
    if (!exists("".temp.tictime"", envir = .GlobalEnv))
        stop(""did you tic?"")
    time &lt;- get("".temp.tictime"", envir = .GlobalEnv)
    rm("".temp.tictime"", envir = .GlobalEnv)
    print(res &lt;- structure(proc.time() - time,
                           class = ""proc_time""), ...)
    invisible(res)
}
</code></pre>

<p>So typing</p>

<pre><code>tic
Sys.sleep(2)
toc
</code></pre>

<p>should results in something like this:</p>

<pre><code>   user  system elapsed 
  0.000   0.000   2.002 
</code></pre>

<p>As I said, it's a trick; <code>system.time</code>, <code>Rprof</code> and
packages such as <a href=""https://CRAN.R-project.org/package=rbenchmark"" rel=""nofollow noreferrer""><code>rbenchmark</code></a> are the way to measure
computing time in R.</p>
"
42308996,5003606,2017-02-17T23:13:34Z,1975110,0,FALSE,"<p>This is a followup to @chrispy's answer above where he presented a <code>withJavaLogging</code> function.  I commented that his solution is inspirational, but for me, is marred by some output at the start of the stack trace that I do not want to see.</p>

<p>To illustrate, consider this code:</p>

<pre><code>f1 = function() {
        # line #2 of the function definition; add this line to confirm that the stack trace line number for this function is line #3 below
        catA(""f2 = "", f2(), ""\n"", sep = """")
    }

    f2 = function() {
        # line #2 of the function definition; add this line to confirm that the stack trace line number for this function is line #4 below
        # line #3 of the function definition; add this line to confirm that the stack trace line number for this function is line #4 below
        stop(""f2 always causes an error for testing purposes"")
    }
</code></pre>

<p>If I execute the line <code>withJavaLogging( f1() )</code> I get the output</p>

<pre><code>2017-02-17 17:58:29.556 FATAL f2 always causes an error for testing purposes
      at .handleSimpleError(function (obj) 
    {
        level = sapply(class(obj), switch, debug = ""DEBUG"", message = ""INFO"", warning = ""WARN"", caughtError = ""ERROR"", error = if (stopIsFatal) 
            ""FATAL""
        else ""ERROR"", """")
        level = c(level[level != """"], ""ERROR"")[1]
        simpleMessage = switch(level, DEBUG = , INFO = TRUE
      at #4: stop(""f2 always causes an error for testing purposes"")
      at f2()
      at catA.R#8: cat(...)
      at #3: catA(""f2 = "", f2(), ""\n"", sep = """")
      at f1()
      at withVisible(expr)
      at #43: withCallingHandlers(withVisible(expr), debug = logger, message = logger, warning = logger, caughtError = logger, error = logger)
      at withJavaLogging(f1())
    Error in f2() : f2 always causes an error for testing purposes
</code></pre>

<p>I do not want to see that <code>at .handleSimpleError(function (obj)</code> line followed by the source code of the logger function defined inside the <code>withJavaLogging</code> function.  I commented above that I could suppress that undesired output by changing <code>trace = trace[length(trace):1]</code> to <code>trace = trace[(length(trace) - 1):1]</code></p>

<p>For the convenience of anyone else reading this, here is a complete version of the function that I now use (renamed from withJavaLogging to logFully, and slightly reformatted to fit my readability preferences):</p>

<pre><code>logFully = function(expr, silentSuccess = FALSE, stopIsFatal = TRUE) {
    hasFailed = FALSE
    messages = list()
    warnings = list()

    logger = function(obj) {
        # Change behaviour based on type of message
        level = sapply(
            class(obj),
            switch,
            debug = ""DEBUG"",
            message = ""INFO"",
            warning = ""WARN"",
            caughtError = ""ERROR"",
            error = if (stopIsFatal) ""FATAL"" else ""ERROR"",
            """"
        )
        level = c(level[level != """"], ""ERROR"")[1]
        simpleMessage = switch(level, DEBUG = TRUE, INFO = TRUE, FALSE)
        quashable = switch(level, DEBUG = TRUE, INFO = TRUE, WARN = TRUE, FALSE)

        # Format message
        time = format(Sys.time(), ""%Y-%m-%d %H:%M:%OS3"")
        txt = conditionMessage(obj)
        if (!simpleMessage) txt = paste(txt, ""\n"", sep = """")
        msg = paste(time, level, txt, sep = "" "")
        calls = sys.calls()
        calls = calls[1:length(calls) - 1]
        trace = limitedLabels(c(calls, attr(obj, ""calls"")))
        if (!simpleMessage &amp;&amp; length(trace) &gt; 0) {
            trace = trace[(length(trace) - 1):1]
            msg = paste(msg, ""  "", paste(""at"", trace, collapse = ""\n  ""), ""\n"", sep = """")
        }

        # Output message
        if (silentSuccess &amp;&amp; !hasFailed &amp;&amp; quashable) {
            messages &lt;&lt;- append(messages, msg)
            if (level == ""WARN"") warnings &lt;&lt;- append(warnings, msg)
        } else {
            if (silentSuccess &amp;&amp; !hasFailed) {
                cat(paste(messages, collapse = """"))
                hasFailed &lt;&lt;- TRUE
            }
            cat(msg)
        }

        # Muffle any redundant output of the same message
        optionalRestart = function(r) { res = findRestart(r); if (!is.null(res)) invokeRestart(res) }
        optionalRestart(""muffleMessage"")
        optionalRestart(""muffleWarning"")
    }

    vexpr = withCallingHandlers( withVisible(expr), debug = logger, message = logger, warning = logger, caughtError = logger, error = logger )

    if (silentSuccess &amp;&amp; !hasFailed) {
        cat(paste(warnings, collapse = """"))
    }

    if (vexpr$visible) vexpr$value else invisible(vexpr$value)
}
</code></pre>

<p>If I execute the line <code>logFully( f1() )</code> I get the output I desire, which is simply</p>

<pre><code>2017-02-17 18:05:05.778 FATAL f2 always causes an error for testing purposes
  at #4: stop(""f2 always causes an error for testing purposes"")
  at f2()
  at catA.R#8: cat(...)
  at #3: catA(""f2 = "", f2(), ""\n"", sep = """")
  at f1()
  at withVisible(expr)
  at logFully.R#110: withCallingHandlers(withVisible(expr), debug = logger, message = logger, warning = logger, caughtError = logger, error = logger)
  at logFully(f1())
Error in f2() : f2 always causes an error for testing purposes
</code></pre>
"
42323708,3897179,2017-02-19T04:41:08Z,1309263,2,FALSE,"<p>Rolling median can be found by maintaining two partitions of numbers.</p>

<p>For maintaining partitions use Min Heap and Max Heap.</p>

<p><em>Max Heap will contain numbers smaller than equal to median.</em></p>

<p><em>Min Heap will contain numbers greater than equal to median.</em></p>

<p><strong>Balancing Constraint:</strong>
   if total number of elements are even then both heap should have equal elements.</p>

<p>if total number of elements are odd then Max Heap will have one more element than Min Heap.</p>

<p><strong>Median Element:</strong> If Both partitions has equal number of elements then median will be half of sum of max element from first partition and min element from second partition.</p>

<p>Otherwise median will be max element from first partition.</p>

<pre>
Algorithm-
1- Take two Heap(1 Min Heap and 1 Max Heap)
   Max Heap will contain first half number of elements
   Min Heap will contain second half number of elements

2- Compare new number from stream with top of Max Heap, 
   if it is smaller or equal add that number in max heap. 
   Otherwise add number in Min Heap.

3- if min Heap has more elements than Max Heap 
   then remove top element of Min Heap and add in Max Heap.
   if max Heap has more than one element than in Min Heap 
   then remove top element of Max Heap and add in Min Heap.

4- If Both heaps has equal number of elements then
   median will be half of sum of max element from Max Heap and min element from Min Heap.
   Otherwise median will be max element from the first partition.
</pre>

<pre><code>public class Solution {

    public static void main(String[] args) {
        Scanner in = new Scanner(System.in);
        RunningMedianHeaps s = new RunningMedianHeaps();
        int n = in.nextInt();
        for(int a_i=0; a_i &lt; n; a_i++){
            printMedian(s,in.nextInt());
        }
        in.close();       
    }

    public static void printMedian(RunningMedianHeaps s, int nextNum){
            s.addNumberInHeap(nextNum);
            System.out.printf(""%.1f\n"",s.getMedian());
    }
}

class RunningMedianHeaps{
    PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;Integer&gt;();
    PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;Integer&gt;(Comparator.reverseOrder());

    public double getMedian() {

        int size = minHeap.size() + maxHeap.size();     
        if(size % 2 == 0)
            return (maxHeap.peek()+minHeap.peek())/2.0;
        return maxHeap.peek()*1.0;
    }

    private void balanceHeaps() {
        if(maxHeap.size() &lt; minHeap.size())
        {
            maxHeap.add(minHeap.poll());
        }   
        else if(maxHeap.size() &gt; 1+minHeap.size())
        {
            minHeap.add(maxHeap.poll());
        }
    }

    public void addNumberInHeap(int num) {
        if(maxHeap.size()==0 || num &lt;= maxHeap.peek())
        {
            maxHeap.add(num);
        }
        else
        {
            minHeap.add(num);
        }
        balanceHeaps();
    }
}
</code></pre>
"
42365442,3927003,2017-02-21T10:58:20Z,2547402,0,FALSE,"<p>Below is the code which can be use to find the mode of a vector variable in R.</p>

<pre><code>a &lt;- table([vector])

names(a[a==max(a)])
</code></pre>
"
42461236,7622951,2017-02-25T20:27:49Z,2284446,0,FALSE,"<p>this is for benefit of others who are directed to this post upon their search.
I too faced exactly same scenario and found no resource which explained it clearly.
Here is my attempt to put the solution in a few simple steps:<br>
1) Create a new project directory<br>
2) Create a Package via R studio(same process as above)<br>
3) Keep both in same location(to avoid confusion).<br>
4) Install and load packages: devtools and and roxygen2.<br>
5) use function load_all().  </p>

<p>And you are done.</p>
"
42489082,3846421,2017-02-27T15:07:22Z,1402001,3,FALSE,"<p>check out the new <code>odbc</code> and <code>DBI</code> packages. <code>DBI::dbWriteTable</code> writes around 20,000 records per second... Much much faster than the Row Inserts from <code>RODBC::sqlSave()</code></p>
"
42511930,7636140,2017-02-28T14:53:37Z,1169539,3,FALSE,"<p>I now my answer comes a bit late, but I was looking for a similar functionality. It would seem the built-in function 'by' in R can also do the grouping easily:</p>

<p>?by contains the following example, which fits per group and extracts the coefficients with sapply:</p>

<pre><code>require(stats)
## now suppose we want to extract the coefficients by group 
tmp &lt;- with(warpbreaks,
            by(warpbreaks, tension,
               function(x) lm(breaks ~ wool, data = x)))
sapply(tmp, coef)
</code></pre>
"
42605492,7459751,2017-03-05T07:00:51Z,1568511,0,FALSE,"<p>In case you need to get order on ""y"" no matter if it's numbers or characters:</p>

<pre><code>x[order(ordered(x, levels = y))]
4 4 4 2 2 1 3 3 3
</code></pre>

<p>By steps:</p>

<pre><code>a &lt;- ordered(x, levels = y) # Create ordered factor from ""x"" upon order in ""y"".
[1] 2 2 3 4 1 4 4 3 3
Levels: 4 &lt; 2 &lt; 1 &lt; 3

b &lt;- order(a) # Define ""x"" order that match to order in ""y"".
[1] 4 6 7 1 2 5 3 8 9

x[b] # Reorder ""x"" according to order in ""y"".
[1] 4 4 4 2 2 1 3 3 3
</code></pre>
"
42609408,6713793,2017-03-05T14:13:04Z,2288485,2,FALSE,"<p>While your question is strictly on numeric, there are many conversions that are difficult to understand when beginning R. I'll aim to address methods to help. This question is similar to <a href=""https://stackoverflow.com/questions/42475874/how-can-i-convert-a-matrix-of-strings-into-a-tibble/42563612#42563612"">This Question</a>.  </p>

<p>Type conversion can be a pain in R because (1) factors can't be converted directly to numeric, they need to be converted to character class first, (2) dates are a special case that you typically need to deal with separately, and (3) looping across data frame columns can be tricky. Fortunately, the ""tidyverse"" has solved most of the issues. </p>

<p>This solution uses <code>mutate_each()</code> to apply a function to all columns in a data frame. In this case, we want to apply the <code>type.convert()</code> function, which converts strings to numeric where it can. Because R loves factors (not sure why) character columns that should stay character get changed to factor. To fix this, the <code>mutate_if()</code> function is used to detect columns that are factors and change to character. Last, I wanted to show how lubridate can be used to change a timestamp in character class to date-time because this is also often a sticking block for beginners.  </p>

<p>
<br/></p>

<pre class=""lang-r prettyprint-override""><code>library(tidyverse) 
library(lubridate)

# Recreate data that needs converted to numeric, date-time, etc
data_df
#&gt; # A tibble: 5 Ã— 9
#&gt;             TIMESTAMP SYMBOL    EX  PRICE  SIZE  COND   BID BIDSIZ   OFR
#&gt;                 &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;
#&gt; 1 2012-05-04 09:30:00    BAC     T 7.8900 38538     F  7.89    523  7.90
#&gt; 2 2012-05-04 09:30:01    BAC     Z 7.8850   288     @  7.88  61033  7.90
#&gt; 3 2012-05-04 09:30:03    BAC     X 7.8900  1000     @  7.88   1974  7.89
#&gt; 4 2012-05-04 09:30:07    BAC     T 7.8900 19052     F  7.88   1058  7.89
#&gt; 5 2012-05-04 09:30:08    BAC     Y 7.8900 85053     F  7.88 108101  7.90

# Converting columns to numeric using ""tidyverse""
data_df %&gt;%
    mutate_each(funs(type.convert)) %&gt;%
    mutate_if(is.factor, as.character) %&gt;%
    mutate(TIMESTAMP = as_datetime(TIMESTAMP, tz = Sys.timezone()))
#&gt; # A tibble: 5 Ã— 9
#&gt;             TIMESTAMP SYMBOL    EX PRICE  SIZE  COND   BID BIDSIZ   OFR
#&gt;                &lt;dttm&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt;
#&gt; 1 2012-05-04 09:30:00    BAC     T 7.890 38538     F  7.89    523  7.90
#&gt; 2 2012-05-04 09:30:01    BAC     Z 7.885   288     @  7.88  61033  7.90
#&gt; 3 2012-05-04 09:30:03    BAC     X 7.890  1000     @  7.88   1974  7.89
#&gt; 4 2012-05-04 09:30:07    BAC     T 7.890 19052     F  7.88   1058  7.89
#&gt; 5 2012-05-04 09:30:08    BAC     Y 7.890 85053     F  7.88 108101  7.90
</code></pre>
"
42707803,7449706,2017-03-10T00:00:05Z,2053397,1,FALSE,"<p>There are many options you can use for R for big number. You can also use as.numeric(). The problem with as.numeric() is that I found a bug in the function for version R 3.02. If you multiply numbers using as.numeric() data type and the numbers happen to produce a result that is around 16 digits in length you will get an error result. This bug of as.numeric() has been tested against many libraries.</p>

<p>There is another option. </p>

<p>I wrote two programs for R, one is called infiX and the other is infiXF for R. This library currently only support multiplication calculation. They both calculate numbers to the precise decimal. Been tested 100,000+ times. infiX will deal with the number in string format where infiXF will take it to the file system base. </p>

<p>When you store the number in memory, you are limited to 8 - 128 Gb depend on your memory. Sometimes even less if the compiler does not let you utilize all the available resources. When you calculate numbers on a text file base, you can calculate 1/5 of the hard drive size. The only problem is, the time it would need for a calculation. </p>

<p>For example, if I was calculating 1 terabyte of digits to another terabyte of digits. That is about 2 trillion digits. That is doable on a 8 terabytes hard-drive. Nevertheless, do I have the time to do the calculation? </p>

<p>InfiX for R can be found here. <a href=""http://kevinhng86.iblog.website/2017/02/21/working-with-number-infinity-multiplication-optimised-the-code-r/"" rel=""nofollow noreferrer"">http://kevinhng86.iblog.website/2017/02/21/working-with-number-infinity-multiplication-optimised-the-code-r/</a></p>
"
42729195,7134046,2017-03-10T23:05:14Z,1963492,10,TRUE,"<p>If you are not using a custom tick interval, you can control the grid and axes parameters directly from the <code>plot()</code> command:</p>

<pre><code>plot(cumsum(rnorm(100)), type='l', panel.first=grid())
</code></pre>

<p>The <code>plot.default()</code> documentation provides more information about these parameters.</p>
"
42820797,125921,2017-03-15T21:13:59Z,1815606,1,FALSE,"<p>Note that the getopt package provides the <code>get_Rscript_filename</code> function, which just uses the same solution presented here, but is already written for you in a standard R module, so you don't have to copy and paste the ""get script path"" function into every script you write.</p>
"
42911825,4411559,2017-03-20T18:57:46Z,1581232,1,FALSE,"<p>Here is a late answer, but you could also use <code>scales::comma_format</code> as follows:</p>

<pre><code>library(scales)
values &lt;- c(1000000.789, 8888.23)
comma_format(digits = 12)(values)
## [1] ""1,000,000.789"" ""8,888.230""
</code></pre>

<p>For just integer values, you can just use comma:</p>

<pre><code>int_vals &lt;- c(1234, 5678)
comma(int_vals)
## [1] ""1,234"" ""5,678""
</code></pre>
"
42949365,5780239,2017-03-22T10:46:16Z,1608130,2,FALSE,"<p>Actually the function <code>stopifnot</code> is very convenient to implement sanity checks in your code. It takes in several logical expressions and returns an error if any of them evaluates to false. </p>

<p>Example:
To check if column 'c' exists in the dataframe 'df':</p>

<pre><code>df &lt;- data.frame(a = numeric(), b = numeric())
stopifnot(!is.null(df$c))
</code></pre>

<p>This will throw the following error:</p>

<pre><code>Error: !is.null(df$c) is not TRUE
</code></pre>
"
43076054,6805670,2017-03-28T17:26:13Z,1169539,0,FALSE,"<p>The question seems to be about how to call regression functions with formulas which are modified inside a loop.</p>

<p>Here is how you can do it in (using diamonds dataset):</p>

<pre><code>attach(ggplot2::diamonds)
strCols = names(ggplot2::diamonds)

formula &lt;- list(); model &lt;- list()
for (i in 1:1) {
  formula[[i]] = paste0(strCols[7], "" ~ "", strCols[7+i])
  model[[i]] = glm(formula[[i]]) 

  #then you can plot the results or anything else ...
  png(filename = sprintf(""diamonds_price=glm(%s).png"", strCols[7+i]))
  par(mfrow = c(2, 2))      
  plot(model[[i]])
  dev.off()
  }
</code></pre>
"
43170920,4671109,2017-04-02T16:51:57Z,2169118,0,FALSE,"<p>I've tested this and it works</p>

<pre><code>availablePackages=available.packages()
availablePackages&lt;-as.vector(availablePackages[,1])
installedPackages=.packages(all.available = TRUE)
missedPackages&lt;-setdiff(availablePackages, installedPackages)
for (i in 1:length(missedPackages))
{
pkgName &lt;- missedPackages[i]
install.packages(pkgName)
}
print(""END"")
</code></pre>

<p>Regards</p>
"
43391265,3588876,2017-04-13T11:36:38Z,2181902,0,FALSE,"<p>There is a library called <code>ggimage</code> to do that. See an <a href=""https://cran.r-project.org/web/packages/ggimage/vignettes/ggimage.html"" rel=""nofollow noreferrer"">intro vignettes here</a></p>

<p>You just have to add a column to your <code>data.frame</code> with the address of the images, which can be stored on the web or locally on your computer and then you can use the <code>geom_image()</code>:</p>

<pre><code>library(""ggplot2"")
library(""ggimage"")

# create a df

set.seed(2017-02-21)
d &lt;- data.frame(x = rnorm(10),
                y = rnorm(10),
                image = sample(c(""https://www.r-project.org/logo/Rlogo.png"",
                                 ""https://jeroenooms.github.io/images/frink.png""),
                               size=10, replace = TRUE)
                )
# plot2
  ggplot(d, aes(x, y)) + geom_image(aes(image=image), size=.05)
</code></pre>

<p><a href=""https://i.stack.imgur.com/11D0q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/11D0q.png"" alt=""enter image description here""></a></p>

<p>ps. Note that <code>ggimage</code> depends on <a href=""http://www.bioconductor.org/packages/release/bioc/html/EBImage.html"" rel=""nofollow noreferrer"">EBImage</a>. So to install <code>gginamge</code> I had to do this:</p>

<pre><code># install EBImage
  source(""https://bioconductor.org/biocLite.R"")
  biocLite(""EBImage"")
# install ggimage
  install.packages(""ggimage"")
</code></pre>
"
43395503,5099519,2017-04-13T14:52:39Z,1745622,0,FALSE,"<pre><code>rows&lt;-3
cols&lt;-3    
x&lt;-rep(NA, rows*cols)
x1 &lt;- matrix(x,nrow=rows,ncol=cols)
</code></pre>
"
43400676,795276,2017-04-13T19:53:29Z,2643939,1,FALSE,"<pre><code>df[sapply(df, function(x) all(is.na(x)))] &lt;- NULL
</code></pre>
"
43489525,1686814,2017-04-19T07:44:17Z,2769510,0,FALSE,"<p>One more comment. The <code>all.equal</code> is a generic. For numeric values, it uses <code>all.equal.numeric</code>. An inspection of this function shows that it used <code>.Machine$double.eps^0.5</code>, where <code>.Machine$double.eps</code> is defined as</p>

<pre><code>double.eps: the smallest positive floating-point number ‘x’ such that
          ‘1 + x != 1’.  It equals ‘double.base ^ ulp.digits’ if either
          ‘double.base’ is 2 or ‘double.rounding’ is 0; otherwise, it
          is ‘(double.base ^ double.ulp.digits) / 2’.  Normally
          ‘2.220446e-16’.
</code></pre>

<p>(.Machine manual page).</p>

<p>In other words, that would be an acceptable choice for your tolerance:</p>

<pre><code>myeq &lt;- function(a, b, tol=.Machine$double.eps^0.5)
      abs(a - b) &lt;= tol
</code></pre>
"
43506485,6805670,2017-04-19T21:34:36Z,1249548,1,FALSE,"<p>The above solutions may not be efficient if you want to plot multiple ggplot plots using a loop (e.g. as asked here: <a href=""https://stackoverflow.com/questions/38223193/creating-multiple-plots-in-ggplot-with-different-y-axis-values-using-a-loop"">Creating multiple plots in ggplot with different Y-axis values using a loop</a>), which is a desired step in analyzing the unknown (or large) data-sets (e.g., when you  want to plot Counts of all variables in a data-set).</p>

<p>The code below shows how to do that using the mentioned above 'multiplot()', the source of which is here: <a href=""http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)"" rel=""nofollow noreferrer"">http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)</a>: </p>

<pre><code>plotAllCounts &lt;- function (dt){   
  plots &lt;- list();
  for(i in 1:ncol(dt)) {
    strX = names(dt)[i]
    print(sprintf(""%i: strX = %s"", i, strX))
    plots[[i]] &lt;- ggplot(dt) + xlab(strX) +
      geom_point(aes_string(strX),stat=""count"")
  }

  columnsToPlot &lt;- floor(sqrt(ncol(dt)))
  multiplot(plotlist = plots, cols = columnsToPlot)
}
</code></pre>

<p>Now run the function - to get Counts for all variables printed using ggplot on one page</p>

<pre><code>dt = ggplot2::diamonds
plotAllCounts(dt)
</code></pre>

<p>One things to note is that:<br>
 using  <code>aes(get(strX))</code>, which you would normally use in loops when working with <code>ggplot</code> , in the above code instead of <code>aes_string(strX)</code> will NOT draw the desired plots. Instead, it will plot the last plot many times. I have not figured out why - it may have to do the <code>aes</code> and <code>aes_string</code> are called in <code>ggplot</code>.</p>

<p>Otherwise, hope you'll find the function useful.</p>
"
43524677,1599018,2017-04-20T16:08:52Z,1395115,2,FALSE,"<p>Using textConnection / saveRDS / loadRDS is perhaps the most versatile and high level: </p>

<pre><code>zz&lt;-textConnection('tempConnection', 'wb')
saveRDS(myData, zz, ascii = T)
TEXT&lt;-paste(textConnectionValue(zz), collapse='\n')

#write TEXT into SQL
...
closeAllConnections()  #if the connection persists, new data will be appended

#reading back:
#1. pull from SQL into queryResult
...
#2. recover the object
recoveredData &lt;- readRDS(textConnection(queryResult$TEXT))
</code></pre>
"
43683687,2477097,2017-04-28T15:20:47Z,652136,0,FALSE,"<p>Just wanted to quickly add (because I didn't see it in any of the answers) that, for a named list, you can also do <code>l[""name""] &lt;- NULL</code>. For example:</p>

<pre><code>l &lt;- list(a = 1, b = 2, cc = 3)
l['b'] &lt;- NULL
</code></pre>
"
43709182,3588876,2017-04-30T17:18:01Z,1826519,0,FALSE,"<p>If you want to return the output of your function to the Global Environment, you can use <code>list2env</code>, like in this example:</p>

<pre><code>myfun &lt;- function(x) { a &lt;- 1:x
                       b &lt;- 5:x
                       df &lt;- data.frame(a=a, b=b)

                       newList &lt;- list(""my_obj1"" = a, ""my_obj2"" = b, ""myDF""=df)
                       list2env(newList ,.GlobalEnv)
                       }
    myfun(3)
</code></pre>

<p>This function will create three objects in your Global Environment:</p>

<pre><code>&gt; my_obj1
  [1] 1 2 3

&gt; my_obj2
  [1] 5 4 3

&gt; myDF
    a b
  1 1 5
  2 2 4
  3 3 3
</code></pre>
"
43760585,6029286,2017-05-03T12:51:58Z,77434,1,FALSE,"<p>The xts package provides a <code>last</code> function:</p>

<pre><code>library(xts)
a &lt;- 1:100
last(a)
[1] 100
</code></pre>
"
43762370,168689,2017-05-03T14:06:14Z,2003663,0,FALSE,"<p>It's possible to use <code>readChar()</code> instead of <code>readLines()</code>.  I had an ongoing issue with mixed commenting (<code>--</code> or <code>/* */</code>) and this has always worked well for me.</p>

<pre><code>sql &lt;- readChar(path.to.file, file.size(path.to.file))
query &lt;- sqlQuery(con, sql, stringsAsFactors = TRUE)
</code></pre>
"
43794497,6826824,2017-05-05T00:01:06Z,1608130,0,FALSE,"<p>You can check if the column exists and do whatever your want.<br>
Suppose a <code>data.frame</code> named <code>df1</code> and checking if column <code>col1</code> exists:</p>

<pre><code>if(! with(df1,exists(""col1""))) stop(""nonexistent column"")
</code></pre>

<p>or  </p>

<pre><code>if(with(df1,!exists(""col1""))) return(-1)
</code></pre>

<p>For instance</p>
"
43926210,3301344,2017-05-11T21:55:45Z,1523126,0,FALSE,"<p>A very convenient way is <code>readr::read_delim</code>-family. Taking the example from here:
  <a href=""https://stackoverflow.com/questions/43925783/importing-csv-with-multiple-separators-into-r"">Importing csv with multiple separators into R</a> you can do it as follows:</p>

<pre><code>txt &lt;- 'OBJECTID,District_N,ZONE_CODE,COUNT,AREA,SUM
1,Bagamoyo,1,""136,227"",""8,514,187,500.000000000000000"",""352,678.813105723350000""
2,Bariadi,2,""88,350"",""5,521,875,000.000000000000000"",""526,307.288878142830000""
3,Chunya,3,""483,059"",""30,191,187,500.000000000000000"",""352,444.699742995200000""'

require(readr)
read_csv(txt) # = read_delim(txt, delim = "","")
</code></pre>

<p>Which results in the expected result:</p>

<pre><code># A tibble: 3 × 6
  OBJECTID District_N ZONE_CODE  COUNT        AREA      SUM
     &lt;int&gt;      &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;
1        1   Bagamoyo         1 136227  8514187500 352678.8
2        2    Bariadi         2  88350  5521875000 526307.3
3        3     Chunya         3 483059 30191187500 352444.7
</code></pre>
"
43926609,7331548,2017-05-11T22:31:37Z,2614767,0,FALSE,"<p>You can get all three types of financial statements from <a href=""http://intrinio.com"" rel=""nofollow noreferrer"">Intrinio</a> in R for free. Additionally, you can get as reported statements and standardized statements. The problem with pulling XBRL filings from the SEC is that there is no standardized option, which means you have to manually map financial statement items if you want to do cross equity comparisons. Here is an example:</p>

<pre><code>#Install httr, which you need to request data via API
install.packages(""httr"")
require(""httr"")

#Install jsonlite which parses JSON
install.packages(""jsonlite"")
require(""jsonlite"")

#Create variables for your usename and password, get those at intrinio.com/login
username &lt;- ""Your_API_Username""
password &lt;- ""Your_API_Password""

#Making an api call for roic. This puts together the different parts of the API call

base &lt;- ""https://api.intrinio.com/""
endpoint &lt;- ""financials/""
type &lt;- ""standardized""
stock &lt;- ""YUM""
statement &lt;- ""income_statement""
fiscal_period &lt;- ""Q2""
fiscal_year &lt;- ""2015""

#Pasting them together to make the API call
call1 &lt;- paste(base,endpoint,type,""?"",""identifier"",""="", stock, ""&amp;"",""statement"",""="",statement,""&amp;"",""fiscal_period"",
               ""="", fiscal_period, ""&amp;"", ""fiscal_year"", ""="", fiscal_year, sep="""")

# call1 Looks like this ""https://api.intrinio.com/financials/standardized?identifier=YUM&amp;statement=income_statement&amp;fiscal_period=Q2&amp;fiscal_year=2015""

#Now we use the API call to request the data from Intrinio's database

YUM_Income &lt;- GET(call1, authenticate(username,password, type = ""basic""))

#That gives us the ROIC value, but it isn't in a good format so we parse it

test1 &lt;- unlist(content(YUM_Income, ""text""))

#Convert from JSON to flattened list

parsed_statement &lt;- fromJSON(test1)

#Then make your data frame:

df1 &lt;- data.frame(parsed_statement)
</code></pre>

<p><a href=""https://i.stack.imgur.com/GowuN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GowuN.png"" alt=""The resulting data frame""></a></p>

<p>I wrote this script to make it easy to change out the ticker, dates, and statement type so you can get the financial statement for any US company for any period.</p>
"
43988286,2074102,2017-05-15T20:29:06Z,2192316,0,FALSE,"<p>You could write your regex functions with C++, compile them into a DLL and call them from R.</p>

<pre><code>    #include &lt;regex&gt;

    extern ""C"" {
    __declspec(dllexport)
    void regex_match( const char **first, char **regexStr, int *_bool)
    {
        std::cmatch _cmatch;
        const char *last = *first + strlen(*first);
        std::regex rx(*regexStr);
        bool found = false;
        found = std::regex_match(*first,last,_cmatch, rx);
        *_bool = found;
    }

__declspec(dllexport)
void regex_search_results( const char **str, const char **regexStr, int *N, char **out )
{
    std::string s(*str);
    std::regex rgx(*regexStr);
    std::smatch m;

    int i=0;
    while(std::regex_search(s,m,rgx) &amp;&amp; i &lt; *N) {
        strcpy(out[i],m[0].str().c_str());
        i++;
        s = m.suffix().str();
    }
}
    };
</code></pre>

<p>call in R as </p>

<pre><code>dyn.load(""C:\\YourPath\\RegTest.dll"")
regex_match &lt;- function(str,regstr) {
.C(""regex_match"",x=as.character(str),y=as.character(regstr),z=as.logical(1))$z }

regex_match(""abc"",""a(b)c"")

regex_search_results &lt;- function(x,y,n) {
.C(""regex_search_results"",x=as.character(x),y=as.character(y),i=as.integer(n),z=character(n))$z }

regex_search_results(""aaa12aa34xxx"", ""[0-9]+"", 5)
</code></pre>
"
44020926,7611214,2017-05-17T09:32:41Z,1826519,0,FALSE,"<p>To obtain multiple outputs from a function and keep them in the desired format you can save the outputs to your hard disk (in the working directory) from within the function and then load them from outside the function:</p>

<pre><code>myfun &lt;- function(x) {
                      df1 &lt;- ...
                      df2 &lt;- ...
                      save(df1, file = ""myfile1"")
                      save(df2, file = ""myfile2"")
}
load(""myfile1"")
load(""myfile2"")
</code></pre>
"
44100949,8042986,2017-05-21T19:28:45Z,2581698,0,FALSE,"<p>This is another way to add a legend manually. This allows you to pick what color belongs to each legend name and can be used as a template. This is the explicit legend. </p>

<pre><code>x &lt;- 1:10
y &lt;- x^2
z &lt;- x^3
values = data.frame(x, y, z)
# Color has to be inside the aesthetic. 
ggplot(values, aes(x=x)) + 
geom_line(aes(y=y, 
              color=""x^2"")) + 
geom_line(aes(y=z, 
              color=""x^3"")) + 
scale_color_manual(name="""", 
                   values=c(""x^2""=""cornflowerblue"", ""x^3""=""lightgreen""))
</code></pre>

<p>This is a better way to define color variables. You tidy your data before the visualization. This is the implicit legend. </p>

<pre><code>library(tidyverse)

sp500 = rnorm(10, 2400, 50)
nasdaq = rnorm(10, 6250, 100)
date = seq(Sys.Date(), Sys.Date()+9, 1)

dataMatrix = tibble(sp500, nasdaq, date)

dataMatrix %&gt;% 
  # This creates a varaible for the indexes, which is used for coloring the lines. 
  gather(sp500, nasdaq, key=""index"", value=""price"") %&gt;% 
    ggplot(aes(x=date, 
               y=price, 
               color=index)) +  
    geom_line() + 
  # This is used for customizing the legend. 
    scale_color_manual(
         name=""Index"",                   
         values=c(""blue"", ""red""), 
         labels=c(""Nasdaq"", ""S&amp;P 500"")) + 
  # This is used for customizing the plot descriptions. 
    labs(title=""FINANCIAL MARKETS"", 
         subtitle=""USA INDEXES"", 
         caption=""MJR"", 
         x=""Date"", 
         y=""Price"") 
</code></pre>
"
44101619,6031200,2017-05-21T20:46:08Z,2531372,0,FALSE,"<p>A more recent version which seemed to work for me, and is a lot less verbose (you essentially keep normal underscores, but can set your own key for this smart behaviour!):</p>

<pre><code>(global-set-key (kbd ""C-;"")  (lambda () (interactive) (insert "" &lt;- "")))
(ess-toggle-underscore nil)
</code></pre>

<p>Insert your shortkey choice instead of <code>C-;</code>.</p>
"
44124118,8050800,2017-05-23T00:50:21Z,2315601,0,FALSE,"<p>This could help you at some point.</p>

<pre><code>a &lt;- c(45,50,10,96)
a[order(a)]
</code></pre>

<p>What you get is</p>

<pre><code>[1] 10 45 50 96
</code></pre>

<p>The code I wrote indicates you want ""a"" as a whole subset of ""a"" and you want it ordered from the lowest to highest value.</p>
"
44144170,1261176,2017-05-23T20:11:25Z,1395517,-1,FALSE,"<p>The r-connect package has been abandoned.</p>
"
44246595,5065796,2017-05-29T16:05:23Z,2161152,0,FALSE,"<p>What you're looking for is achieved with <a href=""https://www.gnu.org/software/emacs/"" rel=""nofollow noreferrer"">GNU Emacs</a> and <a href=""https://www.youtube.com/watch?v=lsYdK0C2RvQ"" rel=""nofollow noreferrer"">org-mode</a>*.  <code>org-mode</code> does <em>far</em> more than can be detailed in a single response, but the relevant points are:</p>

<ul>
<li>Support for literate programming with the ability to integrate multiple languages within the same document (including using one language's results as the input for another language).</li>
<li>Graphics integration.</li>
<li>Export to LaTeX, HTML, PDF, and a variety of other formats natively, automatically generating the markup (but you can do it manually, too).</li>
<li>Everything is <a href=""https://www.gnu.org/philosophy/free-sw.html"" rel=""nofollow noreferrer"">100% customizable</a>, allowing you to adapt the editor to your needs.</li>
</ul>

<p>I don't have Python installed on my system, but below is an example of two different languages being run within the same session.  The excerpt is modified from the wonderful <a href=""https://github.com/erikriverson/org-mode-R-tutorial/blob/master/org-mode-R-tutorial.org"" rel=""nofollow noreferrer"">org-mode R tutorial</a> by Erik Iverson which explains the set up and effective use of <code>org-mode</code> for literate programming tasks.  This <a href=""https://www.youtube.com/watch?v=1-dUkyn_fZA"" rel=""nofollow noreferrer"">SciPy 2013 presentation</a> demonstrates how <code>org-mode</code> can be integrated into a workflow (and happens to use Python).  </p>

<p><a href=""https://i.stack.imgur.com/D5NeL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D5NeL.png"" alt=""enter image description here""></a></p>

<p>Emacs may seem intimidating.  But for statistics/data science, it offers tremendous capabilities that either aren't offered anywhere else or are spread across various systems.  Emacs allows you to integrate them all into a single interface. I think Daniel Gopar says it best in his <a href=""https://www.youtube.com/watch?v=FxcYxdnnQqw&amp;list=PL3kg5TcOuFlrtDZoA4PmvRqvsRLo8bbI7"" rel=""nofollow noreferrer"">Emacs tutorial</a>,  </p>

<blockquote>
  <p>Are you guys that lazy? I mean, c'mon, just read the tutorial, man.</p>
</blockquote>

<p>An hour or so with the Emacs tutorial opens the door to some extremely powerful tools.</p>

<p>* Emacs comes with <code>org-mode</code>. No separate install is required.</p>
"
44286964,2597797,2017-05-31T14:04:40Z,2453326,0,FALSE,"<p>This will find the index of the N'th smallest or largest value in the input numeric vector x.  Set bottom=TRUE in the arguments if you want the N'th from the bottom, or bottom=FALSE if you want the N'th from the top.  N=1 and bottom=TRUE is equivalent to which.min, N=1 and bottom=FALSE is equivalent to which.max.</p>

<pre><code>FindIndicesBottomTopN &lt;- function(x=c(4,-2,5,-77,99),N=1,bottom=FALSE)
{

  k1 &lt;- rank(x)
  if(bottom==TRUE){
    Nindex &lt;- which(k1==N)
    Nindex &lt;- Nindex[1]
  }

  if(bottom==FALSE){
    Nindex &lt;- which(k1==(length(x)+1-N))
    Nindex &lt;- Nindex[1]
  }

  return(Nindex)
}
</code></pre>
"
44330995,8103679,2017-06-02T14:10:50Z,1755642,0,FALSE,"<p>Another opportunity:</p>

<pre><code>n = 100; mi = 0; sigma = 2
x = rnorm(n,mi,sigma)
e = rnorm(n,0,1)
b0 = 1; b1 = 2
y = b1*x + b0 + e
#plot observations
plot(x,y)
#model
lm_res= lm(y~x)
summary(lm_res)
arg= c(min(x),max(x))
out = coef(lm_res)[2]*arg+ coef(lm_res)[1]
#plot model line
lines(arg, out, col = 'red')
</code></pre>
"
44347652,1779128,2017-06-03T19:10:36Z,1699046,1,FALSE,"<p>You can use the <code>by_row</code> function from the package <code>purrrlyr</code> for this:</p>

<pre><code>myfn &lt;- function(row) {
  #row is a tibble with one row, and the same 
  #number of columns as the original df
  #If you'd rather it be a list, you can use as.list(row)
}

purrrlyr::by_row(df, myfn)
</code></pre>

<p>By default, the returned value from <code>myfn</code> is put into a new <a href=""http://r4ds.had.co.nz/many-models.html#list-columns-1"" rel=""nofollow noreferrer"">list column</a> in the df called <code>.out</code>.  </p>

<p>If this is the only output you desire, you could write <code>purrrlyr::by_row(df, myfn)$.out</code></p>
"
44364336,8110900,2017-06-05T08:00:32Z,2669598,0,FALSE,"<p>I found this works to run Rscript in C++ for Mac systems:</p>

<pre><code>string r_command = ""/usr/local/bin/RScript --vanilla /Users/&lt;enter file path here&gt;/your_script.R \"""" + argument_1 + ""\"" "" + ""\"""" + argument_2 + ""\"""";

system(r_command.c_str());
</code></pre>

<p>You can also pass variables into the arguments. Watch out on the escapes. Things can get tricky.</p>
"
44380035,7150776,2017-06-06T01:26:24Z,1358003,-1,FALSE,"<p>With only 4GB of RAM (running Windows 10, so make that about 2 or more realistically 1GB) I've had to be real careful with the allocation.</p>

<p>I use data.table almost exclusively. </p>

<p>The 'fread' function allows you to subset information by field names on import; only import the fields that are actually needed to begin with. If you're using base R read, null the spurious columns immediately after import.</p>

<p>As <em>42-</em> suggests, where ever possible I will then subset within the columns immediately after importing the information. </p>

<p>I frequently rm() objects from the environment as soon as they're no longer needed, e.g. on the next line after using them to subset something else, and call gc().</p>

<p>'fread' and 'fwrite' from data.table can be <em>very</em> fast by comparison with base R reads and writes.</p>

<p>As <em>kpierce8</em> suggests, I almost always fwrite everything out of the environment and fread it back in, even with thousand / hundreds of thousands of tiny files to get through. This not only keeps the environment 'clean' and keeps the memory allocation low but, possibly due to the severe lack of RAM available, R has a propensity for frequently crashing on my computer; really frequently. Having the information backed up on the drive itself as the code progresses through various stages means I don't have to start right from the beginning if it crashes.</p>

<p>As of 2017, I think the fastest SSDs are running around a few GB per second through the M2 port. I have a really basic 50GB Kingston V300 (550MB/s) SSD that I use as my primary disk (has Windows and R on it). I keep all the bulk information on a cheap 500GB WD platter. I move the data sets to the SSD when I start working on them. This, combined with 'fread'ing and 'fwrite'ing everything has been working out great. I've tried using 'ff' but prefer the former. 4K read/write speeds can create issues with this though; backing up a quarter of a million 1k files (250MBs worth) from the SSD to the platter can take hours. As far as I'm aware, there isn't any R package available yet that can automatically optimise the 'chunkification' process; e.g. look at how much RAM a user has, test the read/write speeds of the RAM / all the drives connected and then suggest an optimal 'chunkification' protocol. This could produce some significant workflow improvements / resource optimisations; e.g. split it to ... MB for the ram -> split it to ... MB for the SSD -> split it to ... MB on the platter -> split it to ... MB on the tape. It could sample data sets beforehand to give it a more realistic gauge stick to work from.</p>

<p>A lot of the problems I've worked on in R involve forming combination and permutation pairs, triples etc, which only makes having limited RAM more of a limitation as they will often <em>at least</em> exponentially expand at some point. This has made me focus a lot of attention on the <em>quality</em> as opposed to <em>quantity</em> of information going into them to begin with, rather than trying to clean it up afterwards, and on the sequence of operations in preparing the information to begin with (starting with the simplest operation and increasing the complexity); e.g. subset, then merge / join, then form combinations / permutations etc.</p>

<p>There do seem to be some benefits to using base R read and write in some instances. For instance, the error detection within 'fread' is so good it can be difficult trying to get really messy information into R to begin with to clean it up. Base R also seems to be a lot easier if you're using Linux. Base R seems to work fine in Linux, Windows 10 uses ~20GB of disc space whereas Ubuntu only needs a few GB, the RAM needed with Ubuntu is slightly lower. But I've noticed large quantities of warnings and errors when installing third party packages in (L)Ubuntu. I wouldn't recommend drifting too far away from (L)Ubuntu or other stock distributions with Linux as you can loose so much overall compatibility it renders the process almost pointless (I think 'unity' is due to be cancelled in Ubuntu as of 2017). I realise this won't go down well with some Linux users but some of the custom distributions are borderline pointless beyond novelty (I've spent years using Linux alone).</p>

<p>Hopefully some of that might help others out.</p>
"
44413215,865220,2017-06-07T12:52:46Z,1169248,7,FALSE,"<p>I will group the options based on output. Assume the following vector for all the examples.</p>

<pre><code>v &lt;- c('z', 'a','b','a','e')
</code></pre>

<p><strong><em>For checking presence:</em></strong></p>

<p><strong>%in%</strong></p>

<pre><code>&gt; 'a' %in% v
[1] TRUE
</code></pre>

<p><strong>any()</strong></p>

<pre><code>&gt; any('a'==v)
[1] TRUE
</code></pre>

<p><strong>is.element()</strong></p>

<pre><code>&gt; is.element('a', v)
[1] TRUE
</code></pre>

<p><strong><em>For finding first occurance:</em></strong></p>

<p><strong>match()</strong></p>

<pre><code>&gt; match('a', v)
[1] 2
</code></pre>

<p><strong><em>For finding all occurances as vector of indices:</em></strong></p>

<p><strong>which()</strong></p>

<pre><code>&gt; which('a' == v)
[1] 2 4
</code></pre>

<p><strong><em>For finding all occurances as logical vector</em></strong>:</p>

<p><strong>==</strong></p>

<pre><code>&gt; 'a' == v
[1] FALSE  TRUE FALSE  TRUE FALSE
</code></pre>

<p>Edit:
Removing <strong>grep()</strong> and <strong>grepl()</strong> from the list for reason mentioned in comments</p>
"
44413714,865220,2017-06-07T13:14:06Z,1923273,1,FALSE,"<pre><code>numbers &lt;- c(4,23,4,23,5,43,54,56,657,67,67,435 453,435,324,34,456,56,567,65,34,435)

&gt; length(grep(435, numbers))
[1] 3


&gt; length(which(435 == numbers))
[1] 3


&gt; require(plyr)
&gt; df = count(numbers)
&gt; df[df$x == 435, ] 
     x freq
11 435    3


&gt; sum(435 == numbers)
[1] 3


&gt; sum(grepl(435, numbers))
[1] 3


&gt; sum(435 == numbers)
[1] 3


&gt; tabulate(numbers)[435]
[1] 3


&gt; table(numbers)['435']
435 
  3 


&gt; length(subset(numbers, numbers=='435')) 
[1] 3
</code></pre>
"
44496277,6197649,2017-06-12T09:44:29Z,1195826,2,FALSE,"<p>For the sake of completeness, now there is also <code>fct_drop</code> in the <code>forcats</code> package <a href=""http://forcats.tidyverse.org/reference/fct_drop.html"" rel=""nofollow noreferrer"">http://forcats.tidyverse.org/reference/fct_drop.html</a>.</p>

<p>It differs from <code>droplevels</code> in the way it deals with <code>NA</code>:</p>

<pre><code>f &lt;- factor(c(""a"", ""b"", NA), exclude = NULL)

droplevels(f)
# [1] a    b    &lt;NA&gt;
# Levels: a b &lt;NA&gt;

forcats::fct_drop(f)
# [1] a    b    &lt;NA&gt;
# Levels: a b
</code></pre>
"
44570217,6388753,2017-06-15T14:32:22Z,2288485,0,FALSE,"<p>Considering there might exist char columns, this is based on @Abdou in <a href=""https://stackoverflow.com/questions/42735346/get-column-types-of-excel-sheet-automatically"">Get column types of excel sheet automatically</a> answer:</p>

<pre><code>makenumcols&lt;-function(df){
df&lt;-as.data.frame(df)
cond &lt;- apply(df, 2, function(x) {
  x &lt;- x[!is.na(x)]
  all(suppressWarnings(!is.na(as.numeric(x))))
})
numeric_cols &lt;- names(df)[cond]
df[,numeric_cols] &lt;- sapply(df[,numeric_cols], as.numeric)
return(df)
}
df&lt;-makenumcols(df)
</code></pre>
"
44570741,6264257,2017-06-15T14:56:20Z,2261079,0,FALSE,"<pre><code>myDummy[myDummy$country == ""Austria ""] &lt;- ""Austria""
</code></pre>

<p>After this, you'll need to force R not to recognize ""Austria "" as a level. Let's pretend you also have ""USA"" and ""Spain"" as levels:</p>

<pre><code>myDummy$country = factor(myDummy$country, levels=c(""Austria"", ""USA"", ""Spain""))
</code></pre>

<p>A little less intimidating than the highest voted response, but it should still work.</p>
"
44647216,5795592,2017-06-20T08:01:07Z,2160224,0,FALSE,"<p>Using data.table is pretty straight-forward as well. Just for completeness and also as an easy way to find the data.table solution.</p>

<pre><code>library(data.table)
year &lt;- rep(2001:2005, 2)
score &lt;- round(rnorm(10, 35, 3))

dt &lt;- data.table(score)


dt[, .(Percentile = ecdf(score)(score)), by = list(year)]
</code></pre>
"
44675556,7575156,2017-06-21T12:01:50Z,1813550,0,FALSE,"<p>DPLYR makes this really easy. </p>

<pre><code>x&lt;-santa%&gt;%
   count(Believe)
</code></pre>

<p>If you wanted to count by a group; for instance, how many males v females believe, just add a <code>group_by</code>:</p>

<pre><code>x&lt;-santa%&gt;%
   group_by(Gender)%&gt;%
   count(Believe)
</code></pre>
"
44768455,5124509,2017-06-26T20:49:56Z,2557863,0,FALSE,"<p>I have been doing a bit research on Kendall's tau. Directly using cor(x, y, method=""kendall"") will give you Kendall's tau-b, which is a little different from the original definition, i.e., Kendall's tau-a. Kendall's tau-b is more commonly used as it takes into account ties, hence, most available software packages (e.g. cor(), Kendall()) all calculate Kendall's tau-b. </p>

<p>The difference between Kendall's tau-a and tau-b is essentially the denominator. Specifically, for Kendall's tau-a, the denominator D=n*(n-1)/2, which is fixed, while for Kendall's tau-b, the denominator D=sqrt(No. pairs of Var1 excluding tied pairs)*sqrt(No. pairs of Var2 excluding tied pairs). The value of tua-b is usually larger than tau-a.</p>

<p>As a simple example, consider X=(1,2,3,4,4), Y=(2,3,4,4,4). Kendall's tau-b=0.88, while tau-a=0.7.</p>

<p>For Kendall's tau-c, I didn't see too much on it, so no comments.</p>
"
44774752,447537,2017-06-27T07:41:50Z,582653,0,FALSE,"<p>best practice is using lubridate package</p>

<p><a href=""https://www.rdocumentation.org/packages/lubridate/versions/1.5.6/topics/hm"" rel=""nofollow noreferrer"">https://www.rdocumentation.org/packages/lubridate/versions/1.5.6/topics/hm</a></p>

<pre><code>hm(c(""09:10"", ""09:02"", ""1:10""))
## [1] ""9H 10M 0S"" ""9H 2M 0S""  ""1H 10M 0S
</code></pre>

<p>Then use difftime for difference in the date time formats created above
<a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/difftime.html"" rel=""nofollow noreferrer"">https://stat.ethz.ch/R-manual/R-devel/library/base/html/difftime.html</a></p>

<pre><code>difftime(time1, time2, tz,
         units = c(""auto"", ""secs"", ""mins"", ""hours"",
                   ""days"", ""weeks""))
</code></pre>
"
44827824,5843243,2017-06-29T14:22:55Z,743812,0,FALSE,"<p>In order to complement the answer of  <a href=""https://stackoverflow.com/users/1600821/cantdutchthis"">cantdutchthis</a> and <a href=""https://stackoverflow.com/users/5490241/rodrigo-remedio"">Rodrigo Remedio</a>;</p>

<pre><code>moving_fun &lt;- function(x, w, FUN, ...) {
  # x: a double vector
  # w: the length of the window, i.e., the section of the vector selected to apply FUN
  # FUN: a function that takes a vector and return a summarize value, e.g., mean, sum, etc.
  # Given a double type vector apply a FUN over a moving window from left to the right, 
  #    when a window boundary is not a legal section, i.e. lower_bound and i (upper bound) 
  #    are not contained in the length of the vector, return a NA_real_
  if (w &lt; 1) {
    stop(""The length of the window 'w' must be greater than 0"")
  }
  output &lt;- x
  for (i in 1:length(x)) {
     # plus 1 because the index is inclusive with the upper_bound 'i'
    lower_bound &lt;- i - w + 1
    if (lower_bound &lt; 1) {
      output[i] &lt;- NA_real_
    } else {
      output[i] &lt;- FUN(x[lower_bound:i, ...])
    }
  }
  output
}

# example
v &lt;- seq(1:10)

# compute a MA(2)
moving_fun(v, 2, mean)

# compute moving sum of two periods
moving_fun(v, 2, sum)
</code></pre>
"
44979726,3871924,2017-07-07T21:13:33Z,1335830,-1,FALSE,"<p>Sometimes the user just needs a <code>switch</code> statement instead of an <code>ifelse</code>. In that case:</p>

<pre><code>condition &lt;- TRUE
switch(2-condition, c(1, 2), c(3, 4))
#### [1] 1 2
</code></pre>

<p>(which is another syntax option of Ken Williams's answer)</p>
"
45077254,4502495,2017-07-13T09:55:32Z,2175809,1,FALSE,"<p>The <code>shiny</code> package provides the convenient functions <code>validate()</code> and <code>need()</code> for checking that variables are both available and valid. <code>need()</code> evaluates an expression. If the expression is not valid, then an error message is returned. If the expression is valid, <code>NULL</code> is returned. One can use this to check if a variable is valid. See <code>?need</code> for more information.</p>

<p>I suggest defining a function like this:</p>

<pre><code>is.valid &lt;- function(x) {
  require(shiny)
  is.null(need(x, message = FALSE))  
}
</code></pre>

<p>This function <code>is.valid()</code> will return <code>FALSE</code> if <code>x</code> is <code>FALSE</code>, <code>NULL</code>, <code>NA</code>, <code>NaN</code>, an empty string <code>""""</code>, an empty atomic vector, a vector containing only missing values, a logical vector containing only <code>FALSE</code>, or an object of class <code>try-error</code>. In all other cases, it returns <code>TRUE</code>.</p>

<p>That means, <code>need()</code> (and <code>is.valid()</code>) covers a really broad range of failure cases. Instead of writing:</p>

<pre><code>if (!is.null(x) &amp;&amp; !is.na(x) &amp;&amp; !is.nan(x)) {
  ...
}
</code></pre>

<p>one can write simply:</p>

<pre><code>if (is.valid(x)) {
  ...
}
</code></pre>

<p>With the check for class <code>try-error</code>, it can even be used in conjunction with a <code>try()</code> block to silently catch errors: (see <a href=""https://csgillespie.github.io/efficientR/programming.html#communicating-with-the-user"" rel=""nofollow noreferrer"">https://csgillespie.github.io/efficientR/programming.html#communicating-with-the-user</a>)</p>

<pre><code>bad = try(1 + ""1"", silent = TRUE)
if (is.valid(bad)) {
  ...
}
</code></pre>
"
45130671,1901305,2017-07-16T16:07:39Z,1699046,0,FALSE,"<p>I think the best way to do this with basic R is:</p>

<pre><code>for( i in rownames(df) )
   print(df[i, ""column1""])
</code></pre>

<p>The advantage over the for( i in 1:nrow(df))-approach is that you do not get into trouble if df is empty and nrow(df)=0.</p>
"
45148514,5679791,2017-07-17T15:48:58Z,1826519,0,FALSE,"<p>I put together an R package <a href=""https://CRAN.R-project.org/package=zeallot"" rel=""nofollow noreferrer"">zeallot</a> to tackle this problem. zeallot includes a multiple assignment or unpacking assignment operator, <code>%&lt;-%</code>. The LHS of the operator is any number of variables to assign, built using calls to <code>c()</code>. The RHS of the operator is a vector, list, data frame, date object, or any custom object with an implemented <code>destructure</code> method (see <code>?zeallot::destructure</code>).</p>

<p>Here are a handful of examples based on the original post,</p>

<pre><code>library(zeallot)

functionReturningTwoValues &lt;- function() { 
  return(c(1, 2)) 
}

c(a, b) %&lt;-% functionReturningTwoValues()
a  # 1
b  # 2

functionReturningListOfValues &lt;- function() {
  return(list(1, 2, 3))
}

c(d, e, f) %&lt;-% functionReturningListOfValues()
d  # 1
e  # 2
f  # 3

functionReturningNestedList &lt;- function() {
  return(list(1, list(2, 3)))
}

c(f, c(g, h)) %&lt;-% functionReturningNestedList()
f  # 1
g  # 2
h  # 3

functionReturningTooManyValues &lt;- function() {
  return(as.list(1:20))
}

c(i, j, ...rest) %&lt;-% functionReturningTooManyValues()
i     # 1
j     # 2
rest  # list(3, 4, 5, ..)
</code></pre>

<p>Check out the package <a href=""https://cran.r-project.org/web/packages/zeallot/vignettes/unpacking-assignment.html"" rel=""nofollow noreferrer"">vignette</a> for more information and examples.</p>
"
45216553,2573061,2017-07-20T13:43:38Z,2547402,1,FALSE,"<p>A small modification to Ken Williams' answer, adding optional params <code>na.rm</code> and <code>return_multiple</code>.</p>

<p>Unlike the answers relying on <code>names()</code>, this answer maintains the data type of <code>x</code> in the returned value(s).</p>

<pre><code>stat_mode &lt;- function(x, return_multiple = TRUE, na.rm = FALSE) {
  if(na.rm){
    x &lt;- na.omit(x)
  }
  ux &lt;- unique(x)
  freq &lt;- tabulate(match(x, ux))
  mode_loc &lt;- if(return_multiple) which(freq==max(freq)) else which.max(freq)
  return(ux[mode_loc])
}
</code></pre>

<p>To show it works with the optional params and maintains data type:</p>

<pre><code>foo &lt;- c(2L, 2L, 3L, 4L, 4L, 5L, NA, NA)
bar &lt;- c('mouse','mouse','dog','cat','cat','bird',NA,NA)

str(stat_mode(foo)) # int [1:3] 2 4 NA
str(stat_mode(bar)) # chr [1:3] ""mouse"" ""cat"" NA
str(stat_mode(bar, na.rm=T)) # chr [1:2] ""mouse"" ""cat""
str(stat_mode(bar, return_mult=F, na.rm=T)) # chr ""mouse""
</code></pre>

<p>Thanks to @Frank for simplification.</p>
"
45228301,6562099,2017-07-21T03:02:36Z,2470248,1,FALSE,"<p>Based on <a href=""https://stackoverflow.com/a/2470277"">the best answer</a>:</p>

<pre><code>file &lt;- file(""test.txt"")
writeLines(yourObject, file)
close(file)
</code></pre>

<p>Note that the <code>yourObject</code> needs to be in a string format; use <code>as.character()</code> to convert if you need.</p>

<p>But this is too much typing for <strong>every</strong> save attempt. Let's create a snippet in RStudio.</p>

<p>In Global Options >> Code >> Snippet, type this:</p>

<pre><code>snippet wfile
    file &lt;- file(${1:filename})
    writeLines(${2:yourObject}, file)
    close(file)
</code></pre>

<p>Then, during coding, <strong>type <code>wfile</code> and press <kbd>Tab</kbd></strong>.</p>
"
45346773,8375227,2017-07-27T09:33:26Z,2288485,0,FALSE,"<p>To convert character to numeric you have to convert it into factor by applying </p>

<pre><code>BankFinal1 &lt;- transform(BankLoan,   LoanApproval=as.factor(LoanApproval))
BankFinal1 &lt;- transform(BankFinal1, LoanApp=as.factor(LoanApproval))
</code></pre>

<p>You have to make two columns with the same data, because one column cannot convert into numeric. If you do one conversion it gives the below error </p>

<pre><code>transform(BankData, LoanApp=as.numeric(LoanApproval))
</code></pre>

<blockquote>
<pre><code>Warning message:
  In eval(substitute(list(...)), `_data`, parent.frame()) :
  NAs introduced by coercion
</code></pre>
</blockquote>

<p>so, after doing two column of the same data apply</p>

<pre><code>BankFinal1 &lt; transform(BankFinal1, LoanApp      = as.numeric(LoanApp), 
                                   LoanApproval = as.numeric(LoanApproval))
</code></pre>

<p>it will transform the character to numeric successfully</p>
"
45353186,7296947,2017-07-27T14:05:07Z,1377130,0,FALSE,"<p>See <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html"" rel=""nofollow noreferrer"">sklearn.preprocessing.Imputer</a></p>

<pre><code>import numpy as np
from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
imp.fit([[1, 2], [np.nan, 3], [7, 6]])
X = [[np.nan, 2], [6, np.nan], [7, 6]]
print(imp.transform(X))  
</code></pre>

<p>Example from <a href=""http://scikit-learn.org/stable/modules/preprocessing.html#imputation"" rel=""nofollow noreferrer"">http://scikit-learn.org/</a></p>
"
45389384,7483538,2017-07-29T12:37:54Z,1815606,0,FALSE,"<p>Steamer25's approach works, but only if there is no whitespace in the path. On macOS at least the <code>cmdArgs[match]</code> returns something like <code>/base/some~+~dir~+~with~+~whitespace/</code> for <code>/base/some\ dir\ with\ whitespace/</code>.</p>

<p>I worked around this by replacing the ""~+~"" with a simple whitespace before returning it.</p>

<pre><code>thisFile &lt;- function() {
  cmdArgs &lt;- commandArgs(trailingOnly = FALSE)
  needle &lt;- ""--file=""
  match &lt;- grep(needle, cmdArgs)
  if (length(match) &gt; 0) {
    # Rscript
    path &lt;- cmdArgs[match]
    path &lt;- gsub(""\\~\\+\\~"", "" "", path)
    return(normalizePath(sub(needle, """", path)))
  } else {
    # 'source'd via R console
    return(normalizePath(sys.frames()[[1]]$ofile))
  }
}
</code></pre>

<p>Obviously you can still extend the else block like aprstar did.</p>
"
45428013,8367943,2017-08-01T02:29:00Z,1330989,3,FALSE,"<h1><strong>Use <code>+ coord_flip()</code>.</strong></h1>

<p>In ""R for Data Science,"" Wickham and Grolemund speak to this exact problem. In Chapter 3.8, <em>Position Adjustments</em>, they write:</p>

<blockquote>
  <p><code>coord_flip()</code> switches the x and y axes. This is useful (for example), if you want horizontal boxplots. It’s also useful for long labels: it’s hard to get them to fit without overlapping on the x-axis.</p>
</blockquote>

<p>Applying this to your plot, we just add <code>+ coord_flip()</code> to the ggplot:</p>

<pre><code>data(diamonds)
diamonds$cut &lt;- paste(""Super Dee-Duper"",as.character(diamonds$cut))

qplot(cut,carat,data = diamonds, geom = ""boxplot"") +
  coord_flip()
</code></pre>

<p><a href=""https://i.stack.imgur.com/bAzRR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bAzRR.png"" alt=""enter image description here""></a></p>

<p>And now the super long titles are horizontally spread out and very easy to read!</p>
"
45432684,7785027,2017-08-01T08:28:07Z,2061897,0,FALSE,"<p>Try below code using RJSONIO in console</p>

<pre><code>library(RJSONIO)
library(RCurl)


json_file = getURL(""https://raw.githubusercontent.com/isrini/SI_IS607/master/books.json"")

json_file2 = RJSONIO::fromJSON(json_file)

head(json_file2)
</code></pre>
"
45432949,7785027,2017-08-01T08:40:54Z,2617600,0,FALSE,"<p><strong>First install the RJSONIO and RCurl package:</strong></p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>install.packages(""RJSONIO"")
install.packages(""(RCurl"")</code></pre>
</div>
</div>
</p>

<p><strong>Try below code using RJSONIO in console</strong></p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>library(RJSONIO)
library(RCurl)
json_file = getURL(""https://raw.githubusercontent.com/isrini/SI_IS607/master/books.json"")
json_file2 = RJSONIO::fromJSON(json_file)
head(json_file2)</code></pre>
</div>
</div>
</p>
"
45500911,6694700,2017-08-04T07:41:31Z,1721536,0,FALSE,"<p>Changing factor levels really does change the order of dodged bars! Common pitfall: the colors still stay at a certain position, so taking a quick glance makes it look like the order has not changed. But if you look at the values you will see that the order really has changed.</p>

<p>Edit: My previous answer below only changes order of color scheme given to bars. This is still useful, as we may often want to reverse the color scheme at the same time as changing the order of the bars:</p>

<p>I was using scale_fill_manual because I wanted to manually fill the colors of my bars.</p>

<pre><code>ggplot(data, aes_string(x = ""countries"", y = ""population"", fill = ""agegroups"")) +
scale_fill_manual(values = CustomColorFunction(), limits = (levels(data$agegroups)))
</code></pre>

<p>Spent 5 hours tinkering with changing factor levels and arranging the dataframe hopes this helps someone!</p>
"
45536991,520172,2017-08-06T21:54:41Z,1169456,0,FALSE,"<p>Being terminological, <code>[[</code> operator <strong>extracts</strong> the element from a list whereas <code>[</code> operator takes <strong>subset</strong> of a list.</p>
"
45584204,4927850,2017-08-09T07:28:23Z,2315601,0,FALSE,"<p>In simple words, <code>order()</code> gives the locations of elements of increasing magnitude. </p>

<p>For example, <code>order(c(10,20,30))</code> will give <strong>1,2,3</strong> and 
<code>order(c(30,20,10))</code> will give <strong>3,2,1</strong>.</p>
"
45588084,3877783,2017-08-09T10:27:41Z,1493969,0,FALSE,"<p>Or you can do it using the insertRow function from the miscTools package. </p>

<pre><code>probes &lt;- rep(TRUE, 15)
ind &lt;- c(5,10)
for (i in ind){
    probes &lt;- as.vector(insertRow(as.matrix(probes), i, FALSE))
}
</code></pre>
"
45616280,1720085,2017-08-10T14:16:52Z,1686569,0,FALSE,"<p>Sometimes the column you want to filter may appear in a different position than column index 2 or have a variable name. </p>

<p>In this case, you can simply refer the <strong>column name</strong> you want to filter as:</p>

<pre><code>columnNameToFilter = ""cell_type""
expr[expr[[columnNameToFilter]] == ""hesc"", ]
</code></pre>
"
45737283,2125442,2017-08-17T14:02:26Z,2098368,0,FALSE,"<p>You can use <code>stri_paste</code> function with <code>collapse</code> parameter from <code>stringi</code> package like this:</p>

<pre><code>stri_paste(letters, collapse='')
## [1] ""abcdefghijklmnopqrstuvwxyz"" 
</code></pre>

<p>And some benchmarks:</p>

<pre><code>require(microbenchmark)
test &lt;- stri_rand_lipsum(100)
microbenchmark(stri_paste(test, collapse=''), paste(test,collapse=''), do.call(paste, c(as.list(test), sep="""")))
Unit: microseconds
                                      expr     min       lq     mean   median       uq     max neval
           stri_paste(test, collapse = """") 137.477 139.6040 155.8157 148.5810 163.5375 226.171   100
                paste(test, collapse = """") 404.139 406.4100 446.0270 432.3250 442.9825 723.793   100
do.call(paste, c(as.list(test), sep = """")) 216.937 226.0265 251.6779 237.3945 264.8935 405.989   100
</code></pre>
"
45738326,6621798,2017-08-17T14:46:57Z,2079784,0,FALSE,"<p>All this seems very complicated - there is a function specifically doing what you were asking for:</p>

<pre><code>lengths  #note the plural ""s""
</code></pre>

<p>Using Dirks sample data:</p>

<pre><code>fl &lt;- list(A=c(""un"", ""deux""), B=c(""one""), C=c(""eins"", ""zwei"", ""drei""))
lengths(fl)
</code></pre>

<p>will return a named integer vector:</p>

<pre><code>A B C 
2 1 3 
</code></pre>
"
45820798,3776870,2017-08-22T14:46:09Z,1395147,0,FALSE,"<p>Here is a small change to the excellent suggestion by Matt and a solution similar to Helgi but with ggplot. Only difference from above is that I have used the geom_smooth(method='lm) which plots regression lines directly. </p>

<pre><code>set.seed(1)
y = runif(100,1,10)
x = runif(100,1,10)
f = rep(c('level 1','level 2'),50)
thedata = data.frame(x,y,f)
library(ggplot2)
ggplot(thedata,aes(x=x,y=y,color=f))+geom_smooth(method='lm',se=F)
</code></pre>
"
45843160,1267815,2017-08-23T14:56:01Z,2603184,0,FALSE,"<p>On top of the other suggestions, you can also write C/C++ functions taking their arguments by reference and working <em>in-place</em>, and call them directly in R thanks to <code>Rcpp</code> (among others).
See in particular <a href=""https://stackoverflow.com/a/11300707/1267815"">this answer</a>.</p>
"
45844073,6470465,2017-08-23T15:38:52Z,1815606,1,FALSE,"<p>This works for me</p>

<pre><code>library(rstudioapi)    
rstudioapi::getActiveDocumentContext()$path
</code></pre>
"
45851537,1863950,2017-08-24T01:22:29Z,952275,0,FALSE,"<p>Solution with <code>strcapture</code> from the <code>utils</code>:</p>

<pre><code>x &lt;- c(""key1 :: 0.01"",
       ""key2 :: 0.02"")
strcapture(pattern = ""(.*) :: (0\\.[0-9]+)"",
           x = x,
           proto = list(key = character(), value = double()))
#&gt;    key value
#&gt; 1 key1  0.01
#&gt; 2 key2  0.02
</code></pre>
"
45889355,6388753,2017-08-25T21:00:37Z,652136,0,FALSE,"<p>Using lapply and grep:</p>

<pre><code>lst &lt;- list(a = 1:4, b = 4:8, c = 8:10)
# say you want to remove a and c
toremove&lt;-c(""a"",""c"")
lstnew&lt;-lst[-unlist(lapply(toremove, function(x) grep(x, names(lst)) ) ) ]
#or
pattern&lt;-""a|c""
lstnew&lt;-lst[-grep(pattern, names(lst))]
</code></pre>
"
45915307,1540340,2017-08-28T09:00:36Z,1474081,0,FALSE,"<p>I prefer installing a package from <a href=""https://cran.r-project.org/"" rel=""nofollow noreferrer"">R cran project</a>. I will search for the package name and if it is available I will execute the command from my R shell to install it directly from the R cran project. Your package is available in R directory. So this is what I will do</p>

<pre><code>install.packages(""RJSONIO"")
</code></pre>

<p><strong>Bonus</strong> - Loading a package into the current session of R</p>

<pre><code>library(RJSONIO)
</code></pre>
"
45930330,445565,2017-08-29T03:40:28Z,1249548,0,FALSE,"<p>The <code>cowplot</code> package gives you a nice way to do this, in a manner that suits publication.</p>

<p><code>
x &lt;- rnorm(100)
eps &lt;- rnorm(100,0,.2)
A = qplot(x,3*x+eps, geom = c(""point"", ""smooth""))+theme_gray()
B = qplot(x,2*x+eps, geom = c(""point"", ""smooth""))+theme_gray()
cowplot::plot_grid(A, B, labels = c(""A"", ""B""), align = ""v"")
</code></p>

<p><a href=""https://i.stack.imgur.com/Z9LGD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z9LGD.png"" alt=""enter image description here""></a></p>
"
46137458,7970813,2017-09-10T05:01:29Z,1445964,1,FALSE,"<p>Specifying the global R option for handling non-catastrophic errors worked for me, along with a customized workflow for retaining info about the error and examining this info after the failure. I am currently running R version 3.4.1. 
Below, I've included a description of the workflow that worked for me, as well as some code I used to set the global error handling option in R.</p>

<p>As I have it configured, the error handling also creates an RData file containing all objects in working memory at the time of the error. This dump can be read back into R using <code>load()</code> and then the various environments as they existed at the time of the error can be inspected interactively using <code>debugger(errorDump)</code>.</p>

<p>I will note that I was able to get line numbers in the <code>traceback()</code> output from any custom functions within the stack, but only if I used the <code>keep.source=TRUE</code> option when calling <code>source()</code> for any custom functions used in my script. Without this option, setting the global error handling option as below sent the full output of the <code>traceback()</code> to an error log named <code>error.log</code>, but line numbers were not available.</p>

<p>Here's the general steps I took in my workflow and how I was able to access the memory dump and error log after a non-interactive R failure.</p>

<ol>
<li><p>I put the following at the top of the main script I was calling from the command line. This sets the global error handling option for the R session. My main script was called <code>myMainScript.R</code>. The various lines in the code have comments after them describing what they do. Basically, with this option, when R encounters an error that triggers <code>stop()</code>, it will create an RData (*.rda) dump file of working memory across all active environments in the directory <code>~/myUsername/directoryForDump</code> and will also write an error log named <code>error.log</code> with some useful information to the same directory. You can modify this snippet to add other handling on error (e.g., add a timestamp to the dump file and error log filenames, etc.).</p>

<pre><code>options(error = quote({
  setwd('~/myUsername/directoryForDump'); # Set working directory where you want the dump to go, since dump.frames() doesn't seem to accept absolute file paths.
  dump.frames(""errorDump"", to.file=TRUE, include.GlobalEnv=TRUE); # First dump to file; this dump is not accessible by the R session.
  sink(file=""error.log""); # Specify sink file to redirect all output.
  dump.frames(); # Dump again to be able to retrieve error message and write to error log; this dump is accessible by the R session since not dumped to file.
  cat(attr(last.dump,""error.message"")); # Print error message to file, along with simplified stack trace.
  cat('\nTraceback:');
  cat('\n');
  traceback(2); # Print full traceback of function calls with all parameters. The 2 passed to traceback omits the outermost two function calls.
  sink();
  q()}))
</code></pre></li>
<li><p>Make sure that from the main script and any subsequent function calls, anytime a function is sourced, the option <code>keep.source=TRUE</code> is used. That is, to source a function, you would use <code>source('~/path/to/myFunction.R', keep.source=TRUE)</code>. This is required for the <code>traceback()</code> output to contain line numbers. It looks like you may also be able to set this option globally using <code>options( keep.source=TRUE )</code>, but I have not tested this to see if it works. If you don't need line numbers, you can omit this option. </p></li>
<li>From the terminal (outside R), call the main script in batch mode using <code>Rscript myMainScript.R</code>. This starts a new non-interactive R session and runs the script <code>myMainScript.R</code>. The code snippet given in step 1 that has been placed at the top of <code>myMainScript.R</code> sets the error handling option for the non-interactive R session.</li>
<li>Encounter an error somewhere within the execution of <code>myMainScript.R</code>. This may be in the main script itself, or nested several functions deep. When the error is encountered, handling will be performed as specified in step 1, and the R session will terminate.</li>
<li>An RData dump file named <code>errorDump.rda</code> and and error log named <code>error.log</code> are created in the directory specified by <code>'~/myUsername/directoryForDump'</code> in the global error handling option setting.</li>
<li><p>At your leisure, inspect <code>error.log</code> to review information about the error, including the error message itself and the full stack trace leading to the error. Here's an example of the log that's generated on error; note the numbers after the <code>#</code> character are the line numbers of the error at various points in the call stack:</p>

<pre><code>Error in callNonExistFunc() : could not find function ""callNonExistFunc""
Calls: test_multi_commodity_flow_cmd -&gt; getExtendedConfigDF -&gt; extendConfigDF

Traceback:
3: extendConfigDF(info_df, data_dir = user_dir, dlevel = dlevel) at test_multi_commodity_flow.R#304
2: getExtendedConfigDF(config_file_path, out_dir, dlevel) at test_multi_commodity_flow.R#352
1: test_multi_commodity_flow_cmd(config_file_path = config_file_path, 
spot_file_path = spot_file_path, forward_file_path = forward_file_path, 
data_dir = ""../"", user_dir = ""Output"", sim_type = ""spot"", 
sim_scheme = ""shape"", sim_gran = ""hourly"", sim_adjust = ""raw"", 
nsim = 5, start_date = ""2017-07-01"", end_date = ""2017-12-31"", 
compute_averages = opt$compute_averages, compute_shapes = opt$compute_shapes, 
overwrite = opt$overwrite, nmonths = opt$nmonths, forward_regime = opt$fregime, 
ltfv_ratio = opt$ltfv_ratio, method = opt$method, dlevel = 0)
</code></pre></li>
<li><p>At your leisure, you may load <code>errorDump.rda</code> into an interactive R session using <code>load('~/path/to/errorDump.rda')</code>. Once loaded, call <code>debugger(errorDump)</code> to browse all R objects in memory in any of the active environments. See the R help on <code>debugger()</code> for more info.</p></li>
</ol>

<p>This workflow is enormously helpful when running R in some type of production environment where you have non-interactive R sessions being initiated at the command line and you want information retained about unexpected errors. The ability to dump memory to a file you can use to inspect working memory at the time of the error, along with having the line numbers of the error in the call stack, facilitate speedy post-mortem debugging of what caused the error. </p>
"
46154770,2204410,2017-09-11T11:35:31Z,1299871,3,FALSE,"<p>For an inner join on all columns, you could also use <code>fintersect</code> from the <em>data.table</em>-package or <code>intersect</code> from the <em>dplyr</em>-package as an alternative to <code>merge</code> without specifying the <code>by</code>-columns. this will give the rows that are equal between two dataframes:</p>

<pre><code>&gt; merge(df1, df2)
  V1 V2
1  B  2
2  C  3
&gt; dplyr::intersect(df1, df2)
  V1 V2
1  B  2
2  C  3
&gt; data.table::fintersect(setDT(df1), setDT(df2))
   V1 V2
1:  B  2
2:  C  3
</code></pre>

<hr>

<p>Example data:</p>

<pre><code>df1 &lt;- data.frame(V1 = LETTERS[1:4], V2 = 1:4)
df2 &lt;- data.frame(V1 = LETTERS[2:3], V2 = 2:3)
</code></pre>
"
