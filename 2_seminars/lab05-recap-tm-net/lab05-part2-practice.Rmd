
# Очень самостоятельное задание

Сегодня мы работаем с новостными статьями из американских СМИ за осень 2016 года. В нашу выборку попало чуть больше чем 5 тысяч статей. Помимо самого текста у нас есть название, источник статьи (сайт), автор и дата публикации.

```{r}
library(tidyverse)
library(lubridate)
library(tidytext)
```

```{r}
news = read_csv("~/shared/minor2_2018/2-tm-net/lab05-recap-tm-net/news.csv")
```

Перед тем как переходит к анализу текстов статей, посмотрим на распределения по времени и источникам.

1. Нарисуйте график распределения статей по источникам? Равномерно ли оно? Что это за источники, слышали ли вы о них? Если нет, погуглите :)

```{r}
news %>% group_by(source) %>% summarise(count = n()) %>% ggplot()+
  geom_bar(aes(x = source, y = count), stat = "identity")
```

2. Нарисуйте график распределение статей по времени? Равномерно ли оно? В какие периоды происходил рост или падение количества статей? Есть ли у вас гипотезы, в связи с какими событиями это могло происходить?

```{r}
news %>% group_by(date) %>% summarise(count = n()) %>% ggplot()+
  geom_bar(aes(x = date, y = count), stat = "identity")
```

(*) Попробуйте составить словарик с терминами связанными с событиями, которые как вам кажется могут отражаться в динамике публикаций. Покажите на графике долю статей, с употреблением выбраных вами слов. Подтверждается ли ваша гипотеза?

```{r}
terms = c(..., ...)
terms = str_c(terms, collapse = "|")
news$... = str_detect(str_to_lower(news$...), ...)


```

3. Переходим к анализу текста. 
3.1 Токенизируйте статьи.
3.2 Удалите стоп-слова (tidytext::stop_words)
3.3 Посчитайте частотность слов. К какой теме могут относиться 5 самый часто употребляемых слов?
3.4 Нарисуйте график показывающий частоту употребления для топ-50 самых употребимых слов. 

```{r}
# stopwords
sw = tidytext::stop_words

```

4. Поразбираемся немного с выборами :)
4.1 В изначальном наборе данных (где тексты еще не токенизированны) создайте новую переменную, обозначающую упоминался ли в тексте Дональд Трамп.
4.2 Нарисуйте график, показывающий долю статей с упоминанием Трампа в динамике. 
Hint: помните, что делает параметр position = "fill"?

```{r}

```

4.3 Создайте еще одну новую переменную, обозначающую упоминалась ли в тексте Хиллари Клинтон.
4.4 Нарисуйте график, показывающий долю статей с упоминанием Клинтон в динамике. 
Hint: помните, что делает параметр position = "fill"?

```{r}

```

4.5 Какая возможная интерпретация может быть у наблюдаемой разницы?

5. Попробуем проанализировать эмоциональную окраску (оценку) статей. Для этого мы воспользуемся словарем, который называется AFINN. В нем словам соответствует числовая оценка их эмоциональности от -5 (крайне негативная) до 5 (крайне позитивная).

```{r}
sentdict = get_sentiments("afinn")
head(sentdict)
```

5.1 добавляем новую колонку с оценкой сентимента в базе с токенизированными текстами.

```{r}

```

5.2 Посчитатайте среднее по числовым оценкам сентимента для каждой статей
hint: При агрегации не потеряйте источник статьи, автора и идентификатор. Они нам понадобятся.

```{r}

```

5.3 Найдите три самых позитивных статьи. О чем они?

```{r}

```

5.4 Найдите три самых негативных статьи. О чем они?

```{r}

```

5.5 Нарисуйте график распределения сентимента статей по источникам. Можно ли его как-то проинтерпретировать? 

```{r}

```

5.6 К данным о среднем сентементе каждой статьи добавьте переменные о том, упоминались ли в тексте Трамп или Клинтон. Нарисуйте два графика, показыващие как отличаются по среднему сентименту тексты с упоминанием каждого из этих двух политиков в сравнении с текстами, где их не упоминали.

```{r}

```

6. Нарисуйте график с боксплотами, которые будут показывать, как распределен сентимент по авторам. Используйте функцию reorder, чтобы отсортировать боксплоты. 
Пример сортировки для графиков с аггрегацией: reorder(author, mean_score, FUN = median)

```{r}

```

6.1 Посмотрите примеры статей "самых негативных" и "самых позитивных" авторов. Нарисуйте облака слов для их статей. 

```{r}

```

7. Вспомним, как мы использовали стилометрию. Сейчас будем работать с базой, содержащей токенизированные тексты, из которых удалены стоп-слова. Попробуем разобраться как отличаются авторы по употребляемым ими словам.
7.1 Удалите из базы 400 самых часто употребляемых слов

```{r}

```

7.2 Удалите из базы топ слова употреблящиеся меньше 400 раз
Hint: проверьте что в качестве токенов нет чисел, если они есть, попробуйте удалить :)

```{r}

```

7.3 удалите из базы тексты от которых осталось менее 20 слов

```{r}

```

7.4 Постройте матрицу документов-термов. Но сделайте это так, чтобы строчкой был не документ а автор, а колонки - слова (получается матрица авторов-термов).
Hint: Проверьте что у вас нет NA в колонке с автором.

```{r}

```

7.6 Используйте метод главных компонет, чтобы визуализировать разницу между авторами в двумерном пространстве. Используйте визуализацию - функцию fviz_pca_biplot из пакета factoextra.

```{r}
library(factoextra)
news.pca = ...

fviz_pca_biplot(news.pca)
```

8. Постройте сеть употребления слов на основе биграмм. Какие темы выделяют сообщества?

```{r}
newsbi = news %>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2)
```
