---
title: Text mining
output: html_document
---

## Представление текста в данных

Задачи text mining обычно имеют дело с большим количеством фрагментов
текста, например: твиты, новости, отзывы и т.п. Собрав такие данные,
мы чаще всего получаем таблицу, одна из колонок которой содержит эти
тексты, по одному на строку. В остальных колонках содержатся
*метаданные* об этих текстах — автор, название, дата и т.п.

Наш сегодняшний пример: коллекция из 10 произведений двух авторов —
Фриды Вигдоровой и Любови Воронковой, которые писали прозу для
подростков в 1940-е — 1950-е годы. Произведения довольно длинные,
поэтому для удобства работы они разбиты на фрагменты длиной примерно в
400 слов.

```{r}
library(readr)
library(ggplot2)

vv <- read.csv("~/shared/minor2_2017/2-tm-net/lab01-tm/vv.csv", stringsAsFactors=FALSE)
```

Тексты находятся в колонке text. 

Мы хотим использовать текст как переменную, чтобы включить в наши
модели. Но непонятно, как к текстовой переменной применить простые
математические операции. Например, сравнение. Какой текст «больше» — в
первой строке или в тринадцатой? А какие тексты ближе друг к другу — 1
и 2 или 2 и 3? 

Вывод: нужно разложить текст на какие-то простые единицы, которые
понятно, как считать и сравнивать. 

Чаще всего удобно взять в качестве такой единицы *слово* и сделать
каждое слово в тексте отдельной переменной. 

Есть два способа представить разбитый на слова текст в виде таблицы: 

* длинный формат — для каждого слова в тексте создается отдельная
  строка в данных;
* широкий формат — для каждого слова создается отдельная колонка в
  данных, текст — это одна строка.
  
Мы начнем с длинного формата, знакомого вам, например, по
ggplot. Для преобразования текста в этот формат используем пакет
tidytext.

```{r}
library(dplyr)
library(tidytext)
vv.tidy <- vv %>%
    unnest_tokens(words, text)

#unnest_tokens(vv, words, text)
```

Токены — это единицы текста, в нашем случае — слова. В качестве токенов
могут выступать буквы, предложения и т.п.

## Частотность

Теперь у нас в таблице есть колонка со словами. Это категориальная
переменная, которая принимает столько значений, сколько есть разных
слов в текстах. 

```{r}
vv.tidy %>%
    dplyr::select(words) %>%
    n_distinct() # ого, их довольно много!
```

Мда, все значения не посмотришь. Но можно посмотреть самые частотные,
наверное, они самые важные и интересные?

```{r}
vv.tidy %>%
    count(words, sort = TRUE) %>%
    filter(row_number() < 15) %>%
    ggplot(aes(x = reorder(words, n), y = n)) +
    geom_col() +
    labs(x = "word") + 
    coord_flip() +
    theme_minimal()
```

Не слишком информативно. Так, а как вообще выглядит распределение?

## Частотность языковых знаков. Закон Ципфа

```{r}
vv.tidy %>%
    count(words, sort = TRUE) %>%
    filter(n>250) %>%
    ggplot(aes(rev(reorder(words, n)), n)) +
    geom_bar(stat="identity", show.legend=FALSE) +
    theme_minimal()
```

Распределение с длинным хвостом. Few giants, many dwarfs.

Более того, замечено, что для любого достаточно большого набора
текстов распределение частотностей слов лог-линейно:

```{r}
vv.tidy %>%
    count(words, sort = TRUE) %>%
    mutate(rank = row_number()) %>%
    ggplot(aes(rank, n)) +
    geom_line() +
    scale_x_log10() +
    scale_y_log10()
```

Эта закономерность называется Законом Ципфа.

## Стоп-слова

Самые частотные слова в языке — предлоги, союзы, местоимения — имеют
самые абстрактные значения. Если нас интересует содержание текста, они
для нас неинформативны. Назовем их «стоп-слова» и попробуем выкинуть
из текста. 

В пакете stopwords уже есть заготовленный список стоп-слов для
русского языка. 

```{r}
library(stopwords)
head(stopwords("ru"), 20)
```

Мы уже видели очень похожий список... 

```{r}
top100words <- vv.tidy %>%
    dplyr::count(words, sort=TRUE) %>%
    filter(row_number() < 100) %>%
    pull(words)

top100words$words
head(top100words, 20)
```

Выбросим из текстов стоп-слова и посмотрим, что осталось.

```{r}
rustopwords <- data.frame(words=stopwords("ru"), stringsAsFactors=FALSE)
vv.nonstop <- vv.tidy %>%
    anti_join(rustopwords)

#filter(!(words %in% stopwords("ru")))
```

На сколько процентов уменьшился объем текста?

## Облако слов

Теперь построим список самых частотных слов в форме облака слов.

```{r}
library(wordcloud)
vv.nonstop %>%
    count(words) %>%
    with(wordcloud(words, n, max.words = 100))
```

Различаются ли тексты двух писательниц?

Добавим в данные переменные «автор» и «книга»: 

```{r}
library(stringr)
vv <- vv %>%
    mutate(author = str_extract(title, '[^.]+')) %>%
    mutate(book = str_extract(title, '[^.]+[.][^.]+'))
vv.tidy <- vv %>%
    unnest_tokens(words, text)
```

**Ваша очередь**

1. Постройте частотные списки для текстов каждой из двух писательниц,
   выведите облака слов. Используйте список стоп-слов из пакета stopwords.
2. Используйте в качестве списка стоп-слов самые частотные слова из
   нашей базы (150+ слов).
3. (*) Постройте график закона Ципфа для каждой книги
   по-отдельности. Подсказка: пример есть в [книге о tidyverse](https://www.tidytextmining.com/tfidf.html#zipfs-law).
   
```{r}
vv.nonstop <- vv.tidy %>%
    anti_join(rustopwords)

vv2 = vv.nonstop %>%
    group_by(author) %>%
    count(words) 

vv2 %>% filter(author == "vigdorova") %>%
    with(wordcloud(words, n, max.words = 100))
```
```{r}
vv2 %>% filter(author == "voronkova") %>%
    with(wordcloud(words, n, max.words = 100))

```
   

## Нормализованная частотность

Рассчитаем частотности слов в наших книгах. Частотности будем измерять
в IPM (Instances Per Million — штук на миллион, стандартная мера в
корпусной лингвистике). 


```{r}
vv.freq <- vv.tidy %>%
    group_by(book) %>%
    mutate(booktotal=n()) %>%
    group_by(book, words) %>%
    mutate(count=n()) %>%
    mutate(freq = count * ( 10e+6 / booktotal ) )
```


## Широкий формат

Длинный формат представления текстов удобен для подсчета частотностей
и для интеграции с инструментами tidyverse. Но если мы хотим
использовать тексты, например, для машинного обучения, нам нужно,
чтобы каждому тексту соответствовал набор переменных, характеризующих
его содержание. 

Широкий формат представления текстов используется так широко, что
имеет собственное название — *матрица документов-термов*. Каждая строка
матрицы — текст, каждому возможному слову соответствует колонка, а в
качестве значения используется частотность слова в данном тексте.

В пакете tidyverse есть набор функций cast_* для преобразования длинного
формата в широкий. 

Оставим только стоп-слова:

```{r}
vv.stop <- vv.freq %>%
    inner_join(rustopwords)
```

Построим матрицу документов-термов для нескольких самых частотных слов
в наших книгах:

```{r}
books.dtm <- vv.stop %>%
 #  inner_join(rustopwords) %>%  # оставим только стоп-слова
    group_by(book) %>%
    dplyr::count(words, sort = TRUE) %>%
    cast_sparse(book, words, n)
```

## Стилометрия

Правда ли, что стоп-слова совсем не информативны и их всегда нужно
выкидывать? 

Сделаем анализ главных компонент для стоп-слов в наших книгах:

```{r}
books.pca <- prcomp(books.dtm)
```

Посмотрим, как распределение стоп-слов различается у разных авторов: 

```{r}
ggbiplot::ggbiplot(books.pca, labels = rownames(books.dtm))
```

Более интересная картинка: посмотрим на разницу в употреблении личных местоимений:

```{r}
pers.pronouns <- c("я", "меня", "мне",
                   "мы", "нас", "нам",
                   "ты", "тебя", "тебе",
                   "вы", "вас", "вам",
                   "он", "его", "него", "ему", "нем",
                   "она", "ее", "нее", "ей", "ней",
                   "они", "их", "них", "им", "ним", "ими", "ними")
pronouns.df <- data.frame(words = pers.pronouns, stringsAsFactors = FALSE)
pron.dtm <- vv.tidy %>%
    inner_join(pronouns.df) %>%
    group_by(book) %>%
    dplyr::count(words, sort=TRUE) %>%
    cast_sparse(book, words, n)
pron.pca <- prcomp(pron.dtm)
ggbiplot::ggbiplot(pron.pca, labels = rownames(pron.dtm))
```

