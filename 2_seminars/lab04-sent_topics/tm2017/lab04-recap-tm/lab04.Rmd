
---
title: Text mining. Lab 2
output: html_document
editor_options: 
  chunk_output_type: console
---

## Повторение - Text mining

Сегодняшнее занятие будет посвященно самостоятельной практике тех методов анализа текста, которые мы успели пройти.

Загрузим необходимые пакеты.

```{r}
library(tidyverse)
library(tidytext)
library(stopwords)
```

Датасет, который мы будем анализировать, это база из 1617 ангоязычных статей с сайта ООН, содержащих слово "peacekeeper" и написанных в промежуток между 2001 и 2016 годами. В данных содержится числовой идентификатор статьи (id), её название (title), текст статьи (text) и лемматизированный текст (text_lem).

```{r}
news = read.csv("~/shared/minor2_2017/2-tm-net/lab04-recap-tm/UN_news_peacekeepers.csv", stringsAsFactors=FALSE)
```

# Задание 1

Первое задание посвящено анализу частотности слов по текстам. 
Вам необходимо: 

1. Токенизировать тексты. Единицей анализа будет слово

2. Посчитать количество уникальных слов в текстах 

3. Удалить стопслова 

```{r}
# список стопслов на английском
stopwords("en")
```

4. Посчитать как часто встречается каждое слово

```{r}
words_count = ... %>% 
  dplyr::count(...) 
```

5. Удалить слишком редкие (n < 5) и, наоборот, слишком распространенные слова (1% самых употребляемых слов) 

6. Визуализируйте 15 самых употребляемых слов из оставшегося списка. Сделайте это с помощью столбчатой диаграммы (bar chart) или облака слов

# Задание 2. TF-IDF и Cosine similarity

Теперь мы переходим к анализу частотных распределений по текстам. Сейчас мы попробуем найти схожие по тематике тексты.

Продолжая работать с очищенной базой, которую вы создали в предыдущем задании надо сделать следующее:

1. Удалить новостные статьи в которых осталось меньше 5 слов

2. Посчитать метрику TF-IDF для получения взвешанной частотности слов (bind_tf_idf())
* TF — term frequency (частота слова в документе) 
* IDF — inverse documemnt frequency (обратная документная частота:
  логарифм отношения количества документов в коллекции к числу
  документов, в которых встречается данное слово)
  
3. Привести данные к широкому формату (term-document matrix) (spread())

4. Записать в новую переменную отфильтрованную базу, где будут только статьи с идентификаторами, записанными в переменную texts_id:

```{r}
texts_id = c("253", "252", "917", "901", "904", "910")
```

5. Посчитать косинусное расстояние для каждой пары текстов в этой базе (функция lsa::cosine). Найти тексты наиболее похожие друг на друга. Выведите их названия. О чем они?

# Задание 3. Sentiment analysis

Попробуем проанализировать эмоциональную окраску (оценку) статей про миротворцев. 
Для этого мы воспользуемся словарем, который называется AFINN. В нем словам соответствует числовая оценка их эмоциональности от -5 (крайне негативная) до 5 (крайне позитивная).

```{r}
sentdict = get_sentiments("afinn")

head(sentdict)
tail(sentdict)
```

1. Снова берем изначальную базу статей о миротворцах, токенизируем и добавляем новую колонку с оценкой сентимента.  

2. Посчитатайте среднее по числовым оценкам сентимента для каждой статей

3. Найдите три самых позитивных статьи. Выведите их названия

4. Найдите три самых негативных статьи. Выведите их названия

# Задание 4. Кластеризация

Теперь снова работаем с изначальной базой новостных статей о миротворцах
Сделаем подвыборку текстов, где будет встречаться упоминание Центральноафриканской республики. 

```{r}
africa = news %>% 
  filter(str_detect(text, "Central African Republic"))
```

1. Токенизируйте тексты 

2. Удалите стопслова, а также слова употребленные 1 раз и те, которые находятся в 1% самых употребляемых

3. Приведите данные к широкому формату (term-document matrix)

4. Далее, используя функции пакета factoextra, сделайте иерархическую кластеризацию статей. Нарисуйте дендрограмму и точечную диаграмму, визуализирующие получившуюся кластеризацию. 

```{r}
library(factoextra)
```

5. Посмотрите тексты, которые объединились в группы. О чем они? Соответствует ли кластеризация разделению текстов по темам?

# Задание 5. Dunning log-likelihood (G2)

А теперь, с помощью меры log-likelihood сравним статьи с упоминанием Центральноафриканской республики со всеми текстами вообще и найдем слова, более выраженные в первой подвыборке. 

```{r}
g2 = function(a, b) {
  c = sum(a)
  d = sum(b)
  E1 = c * ((a + b) / (c + d))
  E2 = d * ((a + b) / (c + d))
  return(2*((a*log(a/E1+1e-7)) + (b*log(b/E2+1e-7))))
}
```

1. Создайте переменную в которой будет хранится 50 слов, наиболее характерных для текстов из первой подвыборки

# Задание 6. N-граммы

Пока что единицей измерения текста у нас были отдельные слова. Теперь, вместо одного слова возьмем в качестве единицы анализа несколько стоящих подряд слов. 

```{r}
news = read.csv("~/shared/minor2_2017/2-tm-net/lab04-recap-tm/UN_news_peacekeepers.csv", stringsAsFactors=FALSE)
```

1. Токенизируйте лематизированные тексты. Единица анализа - би-граммы (2 слова подряд)

2. Посчитайте частоты употребления би-грамм

2. Удалите би-граммы, в которых хотя бы одно слово входит в список стопслов

3. Выведите самые частоупотребляемые би-граммы, в которые входит слово "africa"









